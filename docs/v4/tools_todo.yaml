# ORANGE Tools Development Tracker
#
# Purpose: Track development of 25 ORANGE tools identified by rq_tools agents for RQ 5.8-15
# Status: All tools marked ORANGE in tools_status.tsv require TDD implementation
# Created: 2025-11-26
#
# Workflow per tool:
#   1. context_finder: Understand what tool needs to do and why
#   2. WebSearch: Research implementation approaches and existing libraries
#   3. AskUser: Clarify any ambiguities before proceeding
#   4. Implement: Add to appropriate module in ./tools (create new modules if needed)
#   5. Test: Write extensive tests in ./tests folder (TDD: write tests FIRST)
#   6. Document: Update docs/v4/tools_inventory.md with full signature and usage
#   7. Document: Update docs/v4/tools_catalog.md with one-liner
#   8. Status: Update to YELLOW in docs/v4/tools_status.tsv
#   9. Track: Mark as done=true in this file

# Priority Groups:
#   HIGH: Blocks multiple RQs (3+)
#   MEDIUM: Blocks 1-2 RQs
#   LOW: Nice-to-have validators

---

# ANALYSIS TOOLS (6 tools)

analysis_lmm:

  - name: select_lmm_random_structure_via_lrt
    module: tools.analysis_lmm
    priority: HIGH
    blocks_rqs: [5.10]
    description: "Fit 3 random structures, compare via LRT"
    requirements:
      - "Fit 3 candidate models: Full (TSVR | UID), Uncorrelated (TSVR || UID), Intercept-only (1 | UID)"
      - "Perform likelihood ratio tests (LRT) between nested models"
      - "Select best model based on LRT p-values"
      - "Return fitted models + LRT results table"
    inputs:
      - data: pd.DataFrame
      - formula: str
      - time_var: str
      - groups: str
      - reml: bool
    outputs: "Dict[selected_model: str, lrt_results: pd.DataFrame, fitted_models: Dict]"
    done: true
    completed_date: 2025-11-26
    test_status: "12/15 GREEN, 3 SKIPPED"
    notes: "v1 implementation: Uncorrelated=Full (statsmodels lacks simple uncorrelated syntax). Compares Intercept-only vs Full via LRT (chi2, df, p-value). All models fitted with REML=False per literature (Pinheiro & Bates 2000). Parsimonious selection: prefers simpler if p≥0.05. Handles convergence failures gracefully. 3 tests skipped documenting statsmodels convergence limitations with synthetic data (negative chi2 from estimation issues, real-world N=100 more stable). 260 lines implementation. 120 min development time. User approved REML=False decision overriding RQ 5.10 concept.md (statistically correct approach)."

  - name: prepare_age_effects_plot_data
    module: tools.analysis_lmm
    priority: MEDIUM
    blocks_rqs: [5.10]
    description: "Create age tertiles, aggregate means, generate predictions"
    requirements:
      - "Create age tertiles (Young/Middle/Older) from continuous age"
      - "Aggregate observed means by domain x tertile x timepoint"
      - "Generate model predictions using LMM fitted values"
      - "Output plot-ready DataFrame (~600 rows)"
    inputs:
      - lmm_input: pd.DataFrame
      - lmm_model: MixedLMResults
      - output_path: Path
    outputs: "pd.DataFrame (plot source data)"
    done: true
    completed_date: 2025-11-26
    test_status: "15/15 GREEN"
    notes: "pd.qcut(Age, q=3) creates equal-sized tertiles at subject level. Aggregates observed theta (mean, SEM, 95% CI = mean ± 1.96*SEM) and model fitted values by domain × tertile × timepoint. Output: 36 rows (3 domains × 3 tertiles × 4 timepoints). Tertiles ONLY for visualization; analysis uses continuous Age_c. 160 lines implementation. 45 min development time."

  - name: compute_icc_from_variance_components
    module: tools.analysis_lmm
    priority: MEDIUM
    blocks_rqs: [5.13]
    description: "Compute 3 ICC estimates (intercept, slope_simple, slope_conditional)"
    requirements:
      - "Extract variance components from LMM cov_re matrix"
      - "Compute ICC_intercept = var_intercept / (var_intercept + residual)"
      - "Compute ICC_slope_simple = var_slope / (var_slope + residual)"
      - "Compute ICC_slope_conditional at Day 6 accounting for covariance"
      - "Return DataFrame with all 3 ICC values + interpretation"
    inputs:
      - variance_components_df: pd.DataFrame
      - slope_name: str (default 'TSVR_hours')
      - timepoint: float (default 6.0 for Day 6)
    outputs: "pd.DataFrame[icc_type: str, icc_value: float, interpretation: str]"
    done: true
    completed_date: 2025-11-26
    test_status: "14/14 GREEN"
    notes: "Three ICC formulas: (1) ICC_intercept = σ²_intercept / (σ²_intercept + σ²_residual), (2) ICC_slope_simple = σ²_slope / (σ²_slope + σ²_residual), (3) ICC_slope_conditional = Var(b₀ᵢ + b₁ᵢ×t) / [Var(b₀ᵢ + b₁ᵢ×t) + σ²_residual] where Var(b₀ᵢ + b₁ᵢ×t) = σ²_intercept + 2×t×cov(b₀,b₁) + t²×σ²_slope. Interpretation thresholds: <0.10 Low, 0.10-0.30 Moderate, 0.30-0.75 High, ≥0.75 Very High per Snijders & Bosker (2012). Helper function _interpret_icc() provides plain language interpretation. Handles intercept-only models (missing slope variance) gracefully. 100 lines implementation (85 main function + 15 helper). 30 min development time demonstrating TDD workflow mastery."

  - name: test_intercept_slope_correlation_d068
    module: tools.analysis_lmm
    priority: MEDIUM
    blocks_rqs: [5.13]
    description: "Pearson correlation + D068 dual p-values"
    requirements:
      - "Extract random effects (intercepts, slopes) from LMM"
      - "Compute Pearson correlation between intercepts and slopes"
      - "Report BOTH p_uncorrected AND p_bonferroni (Decision D068)"
      - "Bonferroni correction: family_alpha / n_tests"
      - "Return Dict with r, p_uncorrected, p_bonferroni, interpretation"
    inputs:
      - random_effects_df: pd.DataFrame
      - family_alpha: float (default 0.05)
      - n_tests: int (default 15 for Chapter 5)
      - intercept_col: str (default 'Group Var')
      - slope_col: str (default 'Group x TSVR_hours Var')
    outputs: "Dict[r: float, p_uncorrected: float, p_bonferroni: float, significant_uncorrected: bool, significant_bonferroni: bool, interpretation: str]"
    done: true
    completed_date: 2025-11-26
    test_status: "14/14 GREEN"
    notes: "Pearson correlation via scipy.stats.pearsonr. Bonferroni: p_bonf = min(p_uncorr × n_tests, 1.0). Interpretation thresholds: |r| < 0.30 Weak, 0.30-0.50 Moderate, ≥0.50 Strong. RQ 5.13 hypothesis: negative correlation (higher baseline → slower forgetting). Configurable column names for different statsmodels random effects naming conventions. 115 lines implementation. 20 min development time (excellent velocity!). Decision D068 compliant (dual p-value reporting)."

  - name: extract_segment_slopes_from_lmm
    module: tools.analysis_lmm
    priority: HIGH
    blocks_rqs: [5.8]
    description: "Extract Early/Late segment slopes from piecewise LMM with delta method SE propagation"
    requirements:
      - "Extract Early segment slope from piecewise LMM fixed effects (Days_within coefficient)"
      - "Extract Late segment slope (Days_within + Days_within:SegmentLate interaction)"
      - "Compute Late/Early slope ratio for two-phase forgetting test"
      - "Propagate standard errors via delta method for ratio"
      - "Compute 95% confidence intervals for both slopes and ratio"
      - "Return structured output with slopes, ratio, SEs, CIs, interpretation"
    inputs:
      - lmm_result: statsmodels.MixedLMResults
      - segment_col: str (default 'Segment')
      - time_col: str (default 'Days_within')
    outputs: "pd.DataFrame[metric: str, value: float, SE: float, CI_lower: float, CI_upper: float, interpretation: str]"
    done: true
    completed_date: 2025-11-27
    test_status: "11/11 GREEN"
    notes: "RQ 5.8 Test 4 (Convergent Evidence) requires Early/Late slope ratio < 0.5 to indicate robust two-phase forgetting pattern. Delta method SE propagation: SE²_ratio = (∂ratio/∂early)²×Var(early) + (∂ratio/∂late)²×Var(late) + 2×(∂ratio/∂early)×(∂ratio/∂late)×Cov(early,late), where ∂ratio/∂early = -late/early² and ∂ratio/∂late = 1/early. Late slope SE propagation: Var(Early + Interaction) = Var(Early) + Var(Interaction) + 2×Cov. Returns 3 rows: Early_slope, Late_slope, Ratio_Late_Early. Interpretation thresholds: ratio < 0.5 (robust two-phase), 0.5-0.75 (moderate), 0.75-1.0 (weak), >1.0 (unexpected). Handles zero Early slope (ratio=inf/nan). 172 lines implementation. 11/11 tests GREEN. MEDIUM complexity (60 min development time). Configurable column names."

analysis_ctt:

  - name: compute_cronbachs_alpha
    module: tools.analysis_ctt
    priority: HIGH
    blocks_rqs: [5.12]
    description: "Compute Cronbach's alpha with bootstrap CIs"
    requirements:
      - "Compute Cronbach's alpha for item set (internal consistency reliability)"
      - "Bootstrap resampling (n_bootstrap iterations, default 1000)"
      - "Compute 95% confidence intervals from bootstrap distribution"
      - "Return alpha, CI_lower, CI_upper"
      - "NOTE: This creates NEW MODULE tools/analysis_ctt.py"
    inputs:
      - data: pd.DataFrame
      - n_bootstrap: int (default 1000)
    outputs: "Dict[alpha: float, ci_lower: float, ci_upper: float, n_items: int, n_participants: int]"
    done: true
    completed_date: 2025-11-26
    test_status: "13/13 GREEN"
    notes: "Created NEW MODULE tools/analysis_ctt.py. Percentile bootstrap method (resamples participants, preserves item correlation structure). Formula: α = (k/(k-1)) × (1 - Σσ²ᵢ / σ²ₓ). For dichotomous items, mathematically equals KR-20. Handles NaN via pairwise deletion. Requires ≥2 items, ≥3 participants. CI width 0.02-0.15 for N=100. 45 min implementation time."

  - name: compare_correlations_dependent
    module: tools.analysis_ctt
    priority: HIGH
    blocks_rqs: [5.12]
    description: "Steiger's z-test for dependent correlations"
    requirements:
      - "Test if r12 differs from r13 when both share variable 1 (dependent correlations)"
      - "Use Steiger's z-test formula accounting for r23 (correlation between compared variables)"
      - "Return z-statistic, p-value, interpretation"
      - "Example: Test if r(Purified_CTT, IRT) > r(Full_CTT, IRT)"
    inputs:
      - r12: float (correlation 1-2)
      - r13: float (correlation 1-3)
      - r23: float (correlation 2-3)
      - n: int (sample size)
    outputs: "Dict[z_statistic: float, p_value: float, r_difference: float, significant: bool, interpretation: str]"
    done: true
    completed_date: 2025-11-26
    test_status: "13/13 GREEN"
    notes: "Implements Steiger (1980) equations 3 & 10 for asymptotic covariance of overlapping correlations. Fisher's z-transformation applied. Two-tailed p-value. Requires n ≥ 20, all correlations in [-1, 1]. N=100 adequate for 90% power per literature. Plain language interpretation in output. 30 min implementation time."

---

# VALIDATION TOOLS (19 tools)

validation_generic:

  - name: check_file_exists
    module: tools.validation
    priority: HIGH
    blocks_rqs: [5.8, 5.9, 5.10, 5.11, 5.12, 5.13, 5.14, 5.15]
    description: "Validate file exists (needed by 7 RQs)"
    requirements:
      - "Check file exists at path"
      - "Optionally check minimum file size (bytes)"
      - "Return Dict[valid: bool, message: str, size_bytes: int]"
    inputs:
      - file_path: Path
      - min_size_bytes: int (default 0)
    outputs: "Dict[valid: bool, message: str, size_bytes: int]"
    done: true
    completed_date: 2025-11-26
    test_status: "10/10 GREEN"
    notes: "Enhanced existing function with min_size_bytes parameter, directory detection, comprehensive tests"

  - name: validate_dataframe_structure
    module: tools.validation
    priority: MEDIUM
    blocks_rqs: [5.14]
    description: "Generic dataframe validation (rows, cols, types)"
    requirements:
      - "Check expected number of rows (exact or range)"
      - "Check expected columns present"
      - "Check column data types match expected"
      - "Return Dict[valid: bool, message: str, checks: Dict]"
    inputs:
      - file_path: Path
      - expected_rows: int (or tuple for range)
      - expected_columns: List[str]
      - column_types: Dict[str, type]
    outputs: "Dict[valid: bool, message: str, checks: Dict]"
    done: true
    completed_date: 2025-11-27
    test_status: "10/10 GREEN"
    notes: "Generic DataFrame validation with flexible row count (exact or range), required columns check, and optional type checking. Three validation checks: (1) Row count in expected range, (2) All required columns present, (3) Column types match. Reports all checks separately. Used for RQ 5.14 clustering outputs. 117 lines implementation. 10 min development time."

validation_numeric:

  - name: validate_numeric_range
    module: tools.validation
    priority: MEDIUM
    blocks_rqs: [5.9]
    description: "Validate numeric range for probability transformation"
    requirements:
      - "Check all values in data fall within [min_val, max_val]"
      - "Report any out-of-range values"
      - "Return Dict[valid: bool, message: str, out_of_range_count: int]"
    inputs:
      - data: np.ndarray or pd.Series
      - min_val: float
      - max_val: float
      - column_name: str (for error messages)
    outputs: "Dict[valid: bool, message: str, out_of_range_count: int, violations: List]"
    done: true
    completed_date: 2025-11-26
    test_status: "12/12 GREEN"
    notes: "Validates values within [min, max] range (INCLUSIVE). Detects values below minimum, above maximum, NaN, and infinite. Returns violations list (first 10 max) for debugging. Used for theta score validation before GRM probability transformation. Handles empty data gracefully. 120 lines implementation. 10 min development time."

  - name: validate_standardization
    module: tools.validation
    priority: LOW
    blocks_rqs: [5.14]
    description: "Check z-scores mean~0, SD~1"
    requirements:
      - "Check each column has mean ≈ 0 (within tolerance)"
      - "Check each column has SD ≈ 1 (within tolerance)"
      - "Return Dict[valid: bool, message: str, mean_values: Dict, sd_values: Dict]"
    inputs:
      - df: pd.DataFrame
      - column_names: List[str]
      - tolerance: float (default 0.01)
    outputs: "Dict[valid: bool, message: str, mean_values: Dict, sd_values: Dict]"
    done: true
    completed_date: 2025-11-27
    test_status: "11/11 GREEN"
    notes: "Validates z-score standardization (mean ≈ 0, SD ≈ 1). Configurable tolerance parameter accounts for sampling variation (default 0.01 for N=100 scenarios). Reports actual mean/SD values for all columns. Used for pre-clustering validation to ensure all variables on same scale. Handles NaN via pairwise deletion. 107 lines implementation. 10 min development time."

  - name: validate_probability_range
    module: tools.validation
    priority: LOW
    blocks_rqs: [5.9]
    description: "Validate probability transformation"
    requirements:
      - "Check all probability values in [0, 1]"
      - "Check no NaN or infinite values"
      - "Return Dict[valid: bool, message: str, violations: List]"
    inputs:
      - probability_df: pd.DataFrame
      - prob_columns: List[str]
    outputs: "Dict[valid: bool, message: str, violations: List]"
    done: true
    completed_date: 2025-11-26
    test_status: "11/11 GREEN"
    notes: "Validates probabilities in [0,1] range (INCLUSIVE) across multiple columns. Checks for values <0, values >1, NaN, and infinite. Returns detailed violation information per column (column, issue, count, example). Used for IRT GRM theta→probability transformation validation. Handles empty DataFrames gracefully. 125 lines implementation. 10 min development time."

validation_lmm:

  - name: validate_lmm_assumptions_comprehensive
    module: tools.validation
    priority: HIGH
    blocks_rqs: [5.8, 5.10, 5.15]
    description: "7 diagnostics (normality, homoscedasticity, Q-Q, ACF, linearity, outliers, convergence)"
    requirements:
      - "1. Residual normality (Shapiro-Wilk test)"
      - "2. Homoscedasticity (Breusch-Pagan test)"
      - "3. Random effects normality (Q-Q plots)"
      - "4. Autocorrelation (ACF plots, Lag-1 test)"
      - "5. Linearity within segments (partial residual plots)"
      - "6. Outlier detection (Cook's distance)"
      - "7. Convergence diagnostics"
      - "Save diagnostic plots to output_dir"
      - "Return Dict[valid: bool, diagnostics: Dict, plot_paths: List, message: str]"
    inputs:
      - lmm_result: MixedLMResults
      - data: pd.DataFrame
      - output_dir: Path
      - acf_lag1_threshold: float (default 0.1)
      - alpha: float (default 0.05)
    outputs: "Dict[valid: bool, diagnostics: Dict, plot_paths: List, message: str]"
    done: true
    completed_date: 2025-11-26
    test_status: "14/14 GREEN"
    notes: "Complete rewrite of v3.0 minimal implementation. 400+ lines production code. 7 comprehensive diagnostics with 6 diagnostic plots (qq_residuals, residuals_vs_fitted, qq_random_intercepts, qq_random_slopes, acf, cooks_distance). Partial residual CSVs for ALL predictors. Remedial action recommendations per RQ 5.8 specification. Configurable thresholds."

  - name: validate_model_convergence
    module: tools.validation
    priority: MEDIUM
    blocks_rqs: [5.13]
    description: "Validate loaded model convergence"
    requirements:
      - "Check model.converged attribute is True"
      - "Check no convergence warnings in model object"
      - "Return Dict[valid: bool, message: str]"
    inputs:
      - lmm_result: MixedLMResults
    outputs: "Dict[valid: bool, message: str, converged: bool]"
    done: true
    completed_date: 2025-11-26
    test_status: "6/6 GREEN"
    notes: "Simple boolean check of model.converged attribute from statsmodels LMM results. Handles missing converged attribute gracefully (returns invalid). Convergence failures indicate: collinearity, insufficient data, model specification issues, or numerical instability. Fastest validator (simple attribute check). 67 lines implementation. 10 min development time."

  - name: validate_variance_positivity
    module: tools.validation
    priority: LOW
    blocks_rqs: [5.13]
    description: "Validate variance components positive"
    requirements:
      - "Check all variance values > 0"
      - "Variance components cannot be negative (estimation issue if so)"
      - "Return Dict[valid: bool, message: str, negative_components: List]"
    inputs:
      - variance_df: pd.DataFrame
      - component_col: str (column name for component names)
      - value_col: str (column name for variance values)
    outputs: "Dict[valid: bool, message: str, negative_components: List]"
    done: true
    completed_date: 2025-11-27
    test_status: "11/11 GREEN"
    notes: "Validates all LMM variance components > 0. Detects negative or zero variance which indicate estimation issues (collinearity, convergence failure, model misspecification). Reports variance range and lists problematic components. Used for RQ 5.13 LMM variance validation. 85 lines implementation. 10 min development time."

  - name: validate_icc_bounds
    module: tools.validation
    priority: LOW
    blocks_rqs: [5.13]
    description: "Validate ICC in [0,1]"
    requirements:
      - "Check all ICC values in [0, 1] range"
      - "ICC outside this range indicates computation error"
      - "Return Dict[valid: bool, message: str, out_of_bounds: List]"
    inputs:
      - icc_df: pd.DataFrame
      - icc_col: str (column name for ICC values)
    outputs: "Dict[valid: bool, message: str, out_of_bounds: List]"
    done: true
    completed_date: 2025-11-27
    test_status: "10/10 GREEN"
    notes: "Validates ICC values in [0,1] range (INCLUSIVE). Detects NaN and values <0 or >1 which indicate computation errors. Reports ICC range in message. Out-of-bounds ICCs indicate formula error, negative variance components, or missing data. Used for RQ 5.13 ICC validation. 87 lines implementation. 10 min development time."

validation_data_format:

  - name: validate_data_format
    module: tools.validation
    priority: LOW
    blocks_rqs: [5.9]
    description: "Validate fixed effects table format"
    requirements:
      - "Check DataFrame has required columns"
      - "Check no missing values in required columns"
      - "Return Dict[valid: bool, message: str, missing_cols: List]"
    inputs:
      - df: pd.DataFrame
      - required_cols: List[str]
    outputs: "Dict[valid: bool, message: str, missing_cols: List]"
    done: true
    completed_date: 2025-11-26
    test_status: "11/11 GREEN"
    notes: "Simple column presence check (does NOT check for missing values within columns - only column presence). Case-sensitive column name matching. Column order irrelevant. Reports both missing and present columns in message. Used for validating LMM fixed effects table format. 65 lines implementation. 10 min development time."

  - name: validate_plot_data_completeness
    module: tools.validation
    priority: LOW
    blocks_rqs: [5.10]
    description: "Verify all domains/tertiles present"
    requirements:
      - "Check all required domains present in data"
      - "Check all required groups (e.g., age tertiles) present"
      - "Check no missing combinations (complete factorial design)"
      - "Return Dict[valid: bool, message: str, missing_domains: List, missing_groups: List]"
    inputs:
      - plot_data: pd.DataFrame
      - required_domains: List[str]
      - required_groups: List[str]
    outputs: "Dict[valid: bool, message: str, missing_domains: List, missing_groups: List]"
    done: true
    completed_date: 2025-11-27
    test_status: "6/6 GREEN"
    notes: "Validates all domains/groups present in plot data for complete visualizations. Configurable column names for domain and group variables. Reports missing domains and groups separately. Lightweight validator for ensuring complete factorial design. Used for RQ 5.10 age effects plot validation. 32 lines implementation. 10 min development time."

validation_d068_compliance:

  - name: validate_contrasts_d068
    module: tools.validation
    priority: MEDIUM
    blocks_rqs: [5.9]
    description: "Verify D068 dual p-value reporting"
    requirements:
      - "Check contrasts DataFrame has 'p_uncorrected' column"
      - "Check contrasts DataFrame has 'p_bonferroni' or 'p_tukey' column"
      - "Decision D068: ALWAYS report both uncorrected and corrected p-values"
      - "Return Dict[valid: bool, message: str, d068_compliant: bool]"
    inputs:
      - contrasts_df: pd.DataFrame
    outputs: "Dict[valid: bool, message: str, d068_compliant: bool, missing_cols: List]"
    done: true
    completed_date: 2025-11-26
    test_status: "11/11 GREEN"
    notes: "Pure validation (no computation). Checks p_uncorrected column present AND at least one correction method (p_bonferroni, p_tukey, or p_holm). Case-sensitive column names. Handles empty DataFrames (returns invalid). 85 lines implementation. 10 min development time (LOW complexity). Decision D068 compliance validator for contrast results."

  - name: validate_hypothesis_test_dual_pvalues
    module: tools.validation
    priority: MEDIUM
    blocks_rqs: [5.10]
    description: "Verify 3-way interaction includes D068 dual p-values"
    requirements:
      - "Check interaction DataFrame has required terms (e.g., 'Age:Domain:Time')"
      - "Check each term has 'p_uncorrected' and 'p_bonferroni' columns"
      - "Decision D068: ALWAYS report both for hypothesis tests"
      - "Return Dict[valid: bool, missing_terms: List, d068_compliant: bool]"
    inputs:
      - interaction_df: pd.DataFrame
      - required_terms: List[str]
      - alpha_bonferroni: float
    outputs: "Dict[valid: bool, missing_terms: List, d068_compliant: bool, message: str]"
    done: true
    completed_date: 2025-11-26
    test_status: "11/11 GREEN"
    notes: "Validates TWO aspects: (1) Required terms present (e.g., 'Age:Domain:Time'), (2) D068 compliance (p_uncorrected + one of p_bonferroni/p_holm/p_fdr). Case-sensitive term matching. Handles empty DataFrames and empty required_terms list (still checks D068). 100 lines implementation. 10 min development time (LOW complexity). Used for validating LMM fixed effects tables with interaction terms."

  - name: validate_contrasts_dual_pvalues
    module: tools.validation
    priority: MEDIUM
    blocks_rqs: [5.10]
    description: "Verify post-hoc contrasts include D068 dual p-values"
    requirements:
      - "Check contrasts DataFrame has required comparisons"
      - "Check each comparison has 'p_uncorrected' and 'p_tukey' columns"
      - "Decision D068: Post-hoc contrasts use Tukey HSD correction"
      - "Return Dict[valid: bool, d068_compliant: bool, missing_comparisons: List]"
    inputs:
      - contrasts_df: pd.DataFrame
      - required_comparisons: List[str]
    outputs: "Dict[valid: bool, d068_compliant: bool, missing_comparisons: List, message: str]"
    done: true
    completed_date: 2025-11-26
    test_status: "11/11 GREEN"
    notes: "Validates TWO aspects: (1) Required comparisons present (e.g., 'Where-What', 'Where-When', 'What-When'), (2) D068 compliance (p_uncorrected + one of p_tukey/p_bonferroni/p_holm). Typically p_tukey for post-hoc contrasts. Case-sensitive comparison name matching. Handles empty DataFrames (returns invalid). 112 lines implementation. 10 min development time (LOW complexity). Used for validating post-hoc contrast results after 3-way interactions."

  - name: validate_correlation_test_d068
    module: tools.validation
    priority: MEDIUM
    blocks_rqs: [5.13]
    description: "Verify correlation test includes D068 dual p-values"
    requirements:
      - "Check correlation DataFrame has 'p_uncorrected' column"
      - "Check correlation DataFrame has 'p_bonferroni' or 'p_holm' column"
      - "Decision D068: Correlation tests report both uncorrected and corrected"
      - "Return Dict[valid: bool, d068_compliant: bool, message: str]"
    inputs:
      - correlation_df: pd.DataFrame
      - required_cols: List[str] (e.g., ['p_uncorrected', 'p_bonferroni'])
    outputs: "Dict[valid: bool, d068_compliant: bool, missing_cols: List, message: str]"
    done: true
    completed_date: 2025-11-26
    test_status: "10/10 GREEN"
    notes: "Validates D068 compliance for correlation tests (p_uncorrected + one of p_bonferroni/p_holm/p_fdr). Optional custom required_cols parameter for flexibility. Handles empty DataFrames (returns invalid). Multiple correlation tests in single DataFrame supported. 110 lines implementation. 10 min development time (LOW complexity). Used for validating intercept-slope correlation results in RQ 5.13."

validation_effect_sizes:

  - name: validate_effect_sizes
    module: tools.validation
    priority: LOW
    blocks_rqs: [5.9]
    description: "Validate effect size bounds"
    requirements:
      - "Check Cohen's f² values are non-negative"
      - "Warn if f² > 1.0 (very large effect, may indicate issue)"
      - "Check no NaN or infinite values"
      - "Return Dict[valid: bool, message: str, warnings: List]"
    inputs:
      - effect_sizes_df: pd.DataFrame
      - f2_column: str (default 'cohens_f2')
    outputs: "Dict[valid: bool, message: str, warnings: List]"
    done: true
    completed_date: 2025-11-26
    test_status: "13/13 GREEN"
    notes: "Validates Cohen's f² effect sizes per Cohen (1988) guidelines (f²=0.02 small, f²=0.15 medium, f²=0.35 large, f²>1.0 very large). Negative/NaN/infinite values are invalid. Very large values (f²>1.0) trigger warnings but don't invalidate. Reports min/max range in success message. Handles empty DataFrames gracefully. 105 lines implementation. 10 min development time."

validation_clustering:

  - name: validate_cluster_assignment
    module: tools.validation
    priority: LOW
    blocks_rqs: [5.14]
    description: "Check cluster IDs consecutive, sizes >= threshold"
    requirements:
      - "Check all participants assigned to cluster (no missing)"
      - "Check cluster IDs are consecutive (0, 1, ..., K-1)"
      - "Check each cluster has size >= min_cluster_size"
      - "Return Dict[valid: bool, message: str, cluster_sizes: Dict]"
    inputs:
      - assignments_df: pd.DataFrame
      - n_participants: int
      - min_cluster_size: int
    outputs: "Dict[valid: bool, message: str, cluster_sizes: Dict]"
    done: true
    completed_date: 2025-11-27
    test_status: "4/4 GREEN"
    notes: "Validates K-means cluster assignments with three checks: (1) All participants assigned (length = n_expected), (2) Cluster IDs consecutive starting from 0, (3) Each cluster has >= min_cluster_size members. Reports actual cluster sizes. Prevents degenerate solutions with tiny or empty clusters. Used for RQ 5.14 clustering validation. 32 lines implementation. 10 min development time."

  - name: validate_bootstrap_stability
    module: tools.validation
    priority: LOW
    blocks_rqs: [5.14]
    description: "Check Jaccard values in [0,1], mean >= threshold"
    requirements:
      - "Check all Jaccard values in [0, 1] range"
      - "Compute mean Jaccard coefficient across bootstrap iterations"
      - "Check mean >= min_jaccard_threshold (e.g., 0.75 for stability)"
      - "Compute 95% CI from bootstrap distribution"
      - "Return Dict[valid: bool, message: str, mean_jaccard: float, ci: Tuple]"
    inputs:
      - stability_df: pd.DataFrame
      - min_jaccard_threshold: float (default 0.75)
    outputs: "Dict[valid: bool, message: str, mean_jaccard: float, ci_lower: float, ci_upper: float]"
    done: true
    completed_date: 2025-11-27
    test_status: "4/4 GREEN"
    notes: "Validates clustering stability via Jaccard coefficient (0 = no overlap, 1 = perfect agreement). Checks values in [0,1] range, computes mean and 95% CI via percentile method. Stability threshold typically 0.75 for reliable clusters. Fixed numpy boolean conversion issue during development. Used for RQ 5.14 bootstrap validation. 40 lines implementation. 10 min development time."

  - name: validate_cluster_summary_stats
    module: tools.validation
    priority: LOW
    blocks_rqs: [5.14]
    description: "Check summary stats mathematically consistent"
    requirements:
      - "Check min <= mean <= max for each variable"
      - "Check SD >= 0 for each variable"
      - "Check N > 0 for each cluster"
      - "Return Dict[valid: bool, message: str, failed_checks: List]"
    inputs:
      - summary_df: pd.DataFrame
    outputs: "Dict[valid: bool, message: str, failed_checks: List]"
    done: true
    completed_date: 2025-11-27
    test_status: "4/4 GREEN"
    notes: "Validates cluster summary statistics consistency with mathematical constraints: (1) min <= mean <= max for each row, (2) SD >= 0, (3) N > 0. Flexible column naming for different summary table formats. Reports specific failed checks with row indices. Detects computation errors in clustering summaries. Used for RQ 5.14 cluster summary tables. 47 lines implementation. 10 min development time."

---

# SUMMARY STATISTICS

total_tools: 26
done_count: 26
remaining_count: 0

by_priority:
  HIGH: 7  # Blocks multiple RQs or creates new modules
  MEDIUM: 10  # Blocks 1-2 RQs
  LOW: 9  # Nice-to-have validators

by_module:
  tools.analysis_lmm: 5
  tools.analysis_ctt: 2  # NEW MODULE
  tools.validation: 19

by_rq_blocking:
  RQ_5_8: 3 tools (check_file_exists, validate_lmm_assumptions_comprehensive, extract_segment_slopes_from_lmm)
  RQ_5_9: 6 tools (check_file_exists, validate_numeric_range, validate_data_format, validate_contrasts_d068, validate_effect_sizes, validate_probability_range)
  RQ_5_10: 7 tools (check_file_exists, select_lmm_random_structure_via_lrt, prepare_age_effects_plot_data, validate_lmm_assumptions_comprehensive, validate_hypothesis_test_dual_pvalues, validate_contrasts_dual_pvalues, validate_plot_data_completeness)
  RQ_5_11: 1 tool (check_file_exists) # RQ 5.11 succeeded with only GREEN tools
  RQ_5_12: 3 tools (check_file_exists, compute_cronbachs_alpha, compare_correlations_dependent)
  RQ_5_13: 6 tools (check_file_exists, compute_icc_from_variance_components, test_intercept_slope_correlation_d068, validate_model_convergence, validate_variance_positivity, validate_icc_bounds, validate_correlation_test_d068)
  RQ_5_14: 6 tools (check_file_exists, validate_dataframe_structure, validate_standardization, validate_cluster_assignment, validate_bootstrap_stability, validate_cluster_summary_stats)
  RQ_5_15: 2 tools (check_file_exists, validate_lmm_assumptions_comprehensive)

critical_path_rqs:
  # RQs with most tool dependencies (highest priority to unblock)
  - RQ_5_10: 7 tools
  - RQ_5_9: 6 tools
  - RQ_5_13: 6 tools
  - RQ_5_14: 6 tools
  - RQ_5_12: 3 tools (CTT module creation)
  - RQ_5_8: 2 tools
  - RQ_5_15: 2 tools
  - RQ_5_11: 1 tool (can proceed with only check_file_exists)

recommended_order:
  # Build tools in this order to maximize RQ unblocking efficiency
  phase_1_critical:
    - check_file_exists  # Unblocks ALL 8 RQs partially
    - validate_lmm_assumptions_comprehensive  # Unblocks RQ 5.8, 5.10, 5.15
    - compute_cronbachs_alpha  # Creates CTT module, unblocks RQ 5.12
    - compare_correlations_dependent  # Completes CTT module, fully unblocks RQ 5.12

  phase_2_medium_priority:
    - select_lmm_random_structure_via_lrt  # Unblocks RQ 5.10
    - prepare_age_effects_plot_data  # Unblocks RQ 5.10
    - compute_icc_from_variance_components  # Unblocks RQ 5.13
    - test_intercept_slope_correlation_d068  # Unblocks RQ 5.13
    - validate_contrasts_d068  # Unblocks RQ 5.9
    - validate_hypothesis_test_dual_pvalues  # Unblocks RQ 5.10
    - validate_contrasts_dual_pvalues  # Unblocks RQ 5.10
    - validate_correlation_test_d068  # Unblocks RQ 5.13

  phase_3_low_priority_validators:
    # These are nice-to-have but can be replaced with inline assertions
    - validate_numeric_range
    - validate_data_format
    - validate_effect_sizes
    - validate_probability_range
    - validate_model_convergence
    - validate_variance_positivity
    - validate_icc_bounds
    - validate_dataframe_structure
    - validate_standardization
    - validate_plot_data_completeness
    - validate_cluster_assignment
    - validate_bootstrap_stability
    - validate_cluster_summary_stats

estimated_effort:
  phase_1_critical: "6-8 hours (4 tools × 1.5-2 hours each)"
  phase_2_medium_priority: "12-16 hours (12 tools × 1-1.5 hours each)"
  phase_3_low_priority_validators: "6-9 hours (9 tools × 0.5-1 hour each)"
  total: "24-33 hours for all 25 tools"

  # Per-tool estimates (TDD cycle: research → test → implement → document)
  high_complexity: "2 hours (validate_lmm_assumptions_comprehensive, select_lmm_random_structure_via_lrt)"
  medium_complexity: "1.5 hours (CTT tools, ICC computation, correlation testing)"
  low_complexity: "0.5-1 hour (simple validators)"
