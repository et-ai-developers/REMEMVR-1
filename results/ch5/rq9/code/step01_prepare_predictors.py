#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step01
Step Name: prepare_predictors
RQ: results/ch5/rq9
Generated: 2025-11-28

PURPOSE:
Grand-mean center Age variable and create time transformations (linear + log) to prepare
predictors for LMM testing age effects on baseline memory and forgetting rate.

EXPECTED INPUTS:
  - data/step00_lmm_input_raw.csv
    Columns: ['composite_ID', 'UID', 'TEST', 'TSVR_hours', 'theta', 'se_all', 'age']
    Format: Merged theta + TSVR + Age from RQ 5.7 and dfData.csv
    Expected rows: ~400 (100 participants x 4 tests)

EXPECTED OUTPUTS:
  - data/step01_lmm_input_prepared.csv
    Columns: ['composite_ID', 'UID', 'TEST', 'TSVR_hours', 'theta', 'se_all', 'age', 'Age_c', 'Time', 'Time_log']
    Format: Prepared data with centered Age and time transformations
    Expected rows: ~400 (same as input)

VALIDATION CRITERIA:
  - validate_standardization: Age_c has mean ≈ 0 (within 0.01)
  - validate_numeric_range: Age_c in [-30, 30], Time_log in [0, 6], no NaN/inf

g_code REASONING:
- Approach: Grand-mean center Age to facilitate interpretation of interaction effects
  (Age_c = 0 represents average-aged participant). Create both linear (Time) and
  logarithmic (Time_log) transformations of TSVR to capture different functional forms
  of forgetting curve.

- Why this approach: Centering predictors in interaction models allows main effects to
  represent the effect at the average level of the centered variable (interpretability).
  Lin+Log time captures both immediate rapid forgetting (log component) and sustained
  linear decline (linear component).

- Data flow: Load merged data -> compute grand mean age -> create Age_c (age - mean) ->
  create Time (copy of TSVR_hours) -> create Time_log (log(TSVR_hours + 1)) -> validate
  centering and ranges -> save prepared data

- Expected performance: <5 seconds (simple pandas transformations, no modeling)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib pandas operations (no custom analysis functions)
- Validation tool: validate_standardization from tools.validation
- Validation tool: validate_numeric_range from tools.validation
- Parameters: Grand-mean centering (not z-score), log offset = 1 to avoid log(0)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tools
from tools.validation import validate_standardization, validate_numeric_range

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/rq9 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step01_prepare_predictors.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step05_lmm_model_comparison.csv
#   CORRECT: data/step03_theta_scores.csv
#   WRONG:   results/lmm_model_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv             (missing step prefix)
#   WRONG:   logs/step02_removed_items.csv     (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 01: prepare_predictors")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Merged theta + TSVR + Age data from Step 0
        # Purpose: Prepare predictors for LMM (Age x Time interaction)

        log("[LOAD] Loading merged data from Step 0...")
        input_path = RQ_DIR / "data" / "step00_lmm_input_raw.csv"
        df = pd.read_csv(input_path, encoding='utf-8')
        log(f"[LOADED] {input_path.name} ({len(df)} rows, {len(df.columns)} cols)")

        # Verify expected columns exist
        required_cols = ['composite_ID', 'UID', 'TEST', 'TSVR_hours', 'theta', 'se_all', 'age']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")
        log(f"[VALIDATED] All required columns present: {required_cols}")

        # =========================================================================
        # STEP 2: Create Age-Centered Predictor
        # =========================================================================
        # Tool: Grand-mean centering (age - mean(age))
        # What it does: Centers Age variable around 0 (average age)
        # Expected output: Age_c with mean ≈ 0, preserves SD of original age

        log("[TRANSFORM] Computing grand-mean centered Age...")
        mean_age = df['age'].mean()
        df['Age_c'] = df['age'] - mean_age
        log(f"[COMPUTED] Grand mean age = {mean_age:.2f} years")
        log(f"[COMPUTED] Age_c: mean = {df['Age_c'].mean():.6f}, SD = {df['Age_c'].std():.2f}")

        # Verify centering worked (mean should be ≈ 0)
        if abs(df['Age_c'].mean()) > 0.01:
            log(f"[WARNING] Age_c mean = {df['Age_c'].mean():.6f} (expected ≈ 0)")

        # =========================================================================
        # STEP 3: Create Time Transformations
        # =========================================================================
        # Tool: Linear and logarithmic transformations
        # What it does: Creates Time (linear) and Time_log (logarithmic) predictors
        # Expected output: Time = TSVR_hours, Time_log = log(TSVR_hours + 1)

        log("[TRANSFORM] Creating time transformations...")

        # Linear time (direct copy)
        df['Time'] = df['TSVR_hours']
        log(f"[COMPUTED] Time (linear): range = [{df['Time'].min():.1f}, {df['Time'].max():.1f}] hours")

        # Logarithmic time (log offset = 1 to avoid log(0))
        df['Time_log'] = np.log(df['TSVR_hours'] + 1)
        log(f"[COMPUTED] Time_log: range = [{df['Time_log'].min():.3f}, {df['Time_log'].max():.3f}]")

        # Check for invalid values (NaN or inf)
        if df['Time_log'].isna().any() or np.isinf(df['Time_log']).any():
            raise ValueError("Time_log contains NaN or inf values")
        log("[VALIDATED] Time_log contains no NaN or inf values")

        # =========================================================================
        # STEP 4: Save Prepared Data
        # =========================================================================
        # These outputs will be used by: Step 2 (fit_lmm)

        log("[SAVE] Saving prepared data...")
        output_path = RQ_DIR / "data" / "step01_lmm_input_prepared.csv"
        output_cols = ['composite_ID', 'UID', 'TEST', 'TSVR_hours', 'theta', 'se_all',
                      'age', 'Age_c', 'Time', 'Time_log']
        df[output_cols].to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(df)} rows, {len(output_cols)} cols)")

        # =========================================================================
        # STEP 5: Run Validation Tool #1 (validate_standardization)
        # =========================================================================
        # Tool: validate_standardization
        # Validates: Age_c has mean ≈ 0 (within tolerance 0.01)
        # Threshold: Centering deviation < 0.01

        log("[VALIDATION] Running validate_standardization...")
        validation_result_1 = validate_standardization(
            df=df,
            column_names=['Age_c'],
            tolerance=0.01  # Age_c mean must be within ±0.01 of 0
        )

        # Report validation results
        if isinstance(validation_result_1, dict):
            for key, value in validation_result_1.items():
                log(f"[VALIDATION] {key}: {value}")
        else:
            log(f"[VALIDATION] {validation_result_1}")

        # Check if validation passed (if dict has 'status' key)
        if isinstance(validation_result_1, dict) and 'status' in validation_result_1:
            if validation_result_1['status'] != 'pass':
                log(f"[WARNING] validate_standardization did not pass (non-fatal)")

        # =========================================================================
        # STEP 6: Run Validation Tool #2 (validate_numeric_range)
        # =========================================================================
        # Tool: validate_numeric_range
        # Validates: Age_c in [-30, 30], Time_log in [0, 6]
        # Threshold: Values must be within specified ranges

        log("[VALIDATION] Running validate_numeric_range for Age_c...")
        validation_result_2a = validate_numeric_range(
            data=df['Age_c'],
            min_val=-30.0,
            max_val=30.0,
            column_name='Age_c'  # Age_c should be in [-30, 30] (centered around 0)
        )

        if isinstance(validation_result_2a, dict):
            for key, value in validation_result_2a.items():
                log(f"[VALIDATION] {key}: {value}")
        else:
            log(f"[VALIDATION] {validation_result_2a}")

        # Check if validation passed
        if isinstance(validation_result_2a, dict) and 'status' in validation_result_2a:
            if validation_result_2a['status'] != 'pass':
                raise ValueError(f"validate_numeric_range failed for Age_c: {validation_result_2a.get('message', 'Unknown error')}")

        log("[VALIDATION] Running validate_numeric_range for Time_log...")
        validation_result_2b = validate_numeric_range(
            data=df['Time_log'],
            min_val=0.0,
            max_val=6.0,
            column_name='Time_log'  # Time_log should be in [0, 6] (log(169) ≈ 5.13)
        )

        if isinstance(validation_result_2b, dict):
            for key, value in validation_result_2b.items():
                log(f"[VALIDATION] {key}: {value}")
        else:
            log(f"[VALIDATION] {validation_result_2b}")

        # Check if validation passed
        if isinstance(validation_result_2b, dict) and 'status' in validation_result_2b:
            if validation_result_2b['status'] != 'pass':
                raise ValueError(f"validate_numeric_range failed for Time_log: {validation_result_2b.get('message', 'Unknown error')}")

        log("[SUCCESS] Step 01 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
