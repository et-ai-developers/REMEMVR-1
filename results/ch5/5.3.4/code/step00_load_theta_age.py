#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step00
Step Name: Load Theta Scores and Age Data
RQ: results/ch5/5.3.4 (Age x Paradigm Interactions)
Generated: 2025-12-02

PURPOSE:
Load paradigm-specific theta scores from RQ 5.3.1 (Free Recall, Cued Recall,
Recognition) and merge with participant Age variable from dfData.csv. This
creates the base dataset for analyzing age-related differences in forgetting
trajectories across retrieval paradigms.

EXPECTED INPUTS:
  - results/ch5/5.3.1/data/step03_theta_scores.csv
    Columns: ['composite_ID', 'domain_name', 'theta']
    Format: 1200 rows (100 participants × 4 tests × 3 domains)
    Domain values: 'free_recall', 'cued_recall', 'recognition'

  - data/cache/dfData.csv
    Columns: ['UID', 'age', ...] (many other columns)
    Format: 100 rows (one per participant)
    Age range: 20-70 years

EXPECTED OUTPUTS:
  - data/step00_theta_age_merged.csv
    Columns: ['composite_ID', 'UID', 'test', 'paradigm', 'theta', 'Age']
    Format: 1200 rows (100 participants × 4 tests × 3 paradigms)
    Paradigm values: 'IFR', 'ICR', 'IRE' (mapped from domain_name)

VALIDATION CRITERIA:
  - Exactly 1200 rows (no data loss from merge)
  - No NaN values in any column (all theta observations must have Age)
  - 100 unique UIDs (all participants present)
  - 400 observations per paradigm (balanced design)
  - theta in [-3, 3] (typical IRT ability range)
  - Age in [20, 70] (study inclusion criteria)
  - paradigm in {IFR, ICR, IRE}
  - test in {1, 2, 3, 4}

g_code REASONING:
- Approach: Load theta scores, parse composite_ID to extract UID/test, map
  domain names to paradigm abbreviations, merge with Age on UID
- Why this approach: Theta scores are already in long format (1200 rows) with
  composite_ID containing participant and test info. Age is participant-level
  (100 rows), so merge replicates Age for each observation.
- Data flow: RQ 5.3.1 theta (1200 rows) + parse composite_ID → merge with
  dfData.csv Age (100 rows) → 1200 rows with theta + Age
- Expected performance: <5 seconds (simple pandas operations)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib operations (pandas/numpy - NOT catalogued tool)
- Validation: Inline validation checks embedded in script
- Domain name mapping: free_recall → IFR, cued_recall → ICR, recognition → IRE
- composite_ID parsing: Format is 'UID_test' (e.g., 'A010_1')
- Age handling: Input column is lowercase 'age', output is uppercase 'Age'
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/ch5/5.3.4/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 5.3.4/
#   parents[2] = ch5/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.3.4 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step00_load_theta_age.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step00_theta_age_merged.csv
#   CORRECT: logs/step00_load_theta_age.log
#   WRONG:   results/theta_age_merged.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_age.csv             (missing step prefix)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 0: Load Theta Scores and Age Data")

        # =========================================================================
        # STEP 1: Load Theta Scores from RQ 5.3.1
        # =========================================================================
        # Expected: 1200 rows (100 participants × 4 tests × 3 domains)
        # Purpose: These are paradigm-specific IRT ability estimates from RQ 5.3.1

        log("[LOAD] Loading theta scores from RQ 5.3.1...")
        theta_path = PROJECT_ROOT / "results" / "ch5" / "5.3.1" / "data" / "step03_theta_scores.csv"

        if not theta_path.exists():
            raise FileNotFoundError(f"Theta scores file not found: {theta_path}")

        df_theta = pd.read_csv(theta_path, encoding='utf-8')
        log(f"[LOADED] Theta scores: {len(df_theta)} rows, {len(df_theta.columns)} columns")
        log(f"[INFO] Columns: {list(df_theta.columns)}")
        log(f"[INFO] Domain values: {sorted(df_theta['domain_name'].unique())}")

        # =========================================================================
        # STEP 2: Parse composite_ID to Extract UID and Test
        # =========================================================================
        # composite_ID format: 'UID_test' (e.g., 'A010_1')
        # Extract UID (participant ID) and test (session number)

        log("[PARSE] Extracting UID and test from composite_ID...")

        # Split composite_ID into components
        df_theta[['UID', 'test']] = df_theta['composite_ID'].str.split('_', expand=True)

        # Convert test to integer (T1=1, T2=2, T3=3, T4=4)
        df_theta['test'] = df_theta['test'].astype(int)

        log(f"[PARSED] UID range: {df_theta['UID'].nunique()} unique participants")
        log(f"[PARSED] Test range: {sorted(df_theta['test'].unique())}")

        # =========================================================================
        # STEP 3: Map Domain Names to Paradigm Abbreviations
        # =========================================================================
        # Map domain_name values to paradigm abbreviations for consistency with
        # other RQs and literature conventions

        log("[MAP] Mapping domain names to paradigm abbreviations...")

        domain_to_paradigm = {
            'free_recall': 'IFR',
            'cued_recall': 'ICR',
            'recognition': 'IRE'
        }

        df_theta['paradigm'] = df_theta['domain_name'].map(domain_to_paradigm)

        # Validate mapping (no NaN values = all domains mapped)
        if df_theta['paradigm'].isna().any():
            unmapped = df_theta[df_theta['paradigm'].isna()]['domain_name'].unique()
            raise ValueError(f"Unmapped domain names found: {unmapped}")

        log(f"[MAPPED] Paradigm distribution:")
        for paradigm, count in df_theta['paradigm'].value_counts().items():
            log(f"  {paradigm}: {count} observations")

        # =========================================================================
        # STEP 4: Load Age Variable from dfData.csv
        # =========================================================================
        # Expected: 100 rows (one per participant)
        # Purpose: Age is the continuous predictor for age-related forgetting effects

        log("[LOAD] Loading Age variable from dfData.csv...")
        age_path = PROJECT_ROOT / "data" / "cache" / "dfData.csv"

        if not age_path.exists():
            raise FileNotFoundError(f"dfData.csv not found: {age_path}")

        # Load only UID and age columns (dfData has many columns we don't need)
        # NOTE: dfData has 400 rows (100 participants × 4 tests) - need to deduplicate
        df_age_raw = pd.read_csv(age_path, usecols=['UID', 'age'], encoding='utf-8')

        # Deduplicate: Get unique Age per UID (Age is constant within participant)
        df_age = df_age_raw.drop_duplicates(subset=['UID']).reset_index(drop=True)

        # Rename 'age' to 'Age' for consistency (output should have uppercase)
        df_age = df_age.rename(columns={'age': 'Age'})

        log(f"[LOADED] Age data: {len(df_age)} unique participants (deduplicated from {len(df_age_raw)} rows)")
        log(f"[INFO] Age range: {df_age['Age'].min():.1f} - {df_age['Age'].max():.1f} years")
        log(f"[INFO] Age mean: {df_age['Age'].mean():.1f}, SD: {df_age['Age'].std():.1f}")

        # =========================================================================
        # STEP 5: Merge Theta Scores with Age on UID
        # =========================================================================
        # Left join: Keep all theta observations (1200 rows), add Age for each
        # Expected: All 1200 theta observations should match with Age (no NaN)

        log("[MERGE] Merging theta scores with Age on UID...")

        df_merged = df_theta.merge(df_age, on='UID', how='left')

        log(f"[MERGED] Output: {len(df_merged)} rows, {len(df_merged.columns)} columns")

        # =========================================================================
        # STEP 6: Validation - Row Count
        # =========================================================================
        # CRITICAL: Must have exactly 1200 rows (no data loss from merge)

        log("[VALIDATION] Checking row count...")

        expected_rows = 1200
        actual_rows = len(df_merged)

        if actual_rows != expected_rows:
            raise ValueError(
                f"Row count mismatch: expected {expected_rows}, got {actual_rows}"
            )

        log(f"[PASS] Row count: {actual_rows} rows (100 participants x 4 tests x 3 paradigms)")

        # =========================================================================
        # STEP 7: Validation - No Missing Values
        # =========================================================================
        # CRITICAL: All theta observations must have Age (merge success)

        log("[VALIDATION] Checking for missing values...")

        # Check each column for NaN values
        missing_summary = df_merged.isna().sum()
        total_missing = missing_summary.sum()

        if total_missing > 0:
            log("[FAIL] Missing values detected:")
            for col, count in missing_summary[missing_summary > 0].items():
                log(f"  {col}: {count} missing")
            raise ValueError(f"Missing values found: {total_missing} total NaN values")

        log("[PASS] No missing values (all 1200 theta observations matched with Age)")

        # =========================================================================
        # STEP 8: Validation - Unique Participants
        # =========================================================================
        # Expected: 100 unique UIDs (all participants present)

        log("[VALIDATION] Checking unique participants...")

        expected_uids = 100
        actual_uids = df_merged['UID'].nunique()

        if actual_uids != expected_uids:
            raise ValueError(
                f"UID count mismatch: expected {expected_uids}, got {actual_uids}"
            )

        log(f"[PASS] Unique participants: {actual_uids} UIDs")

        # =========================================================================
        # STEP 9: Validation - Balanced Paradigm Design
        # =========================================================================
        # Expected: 400 observations per paradigm (balanced design)

        log("[VALIDATION] Checking paradigm balance...")

        paradigm_counts = df_merged['paradigm'].value_counts()
        expected_per_paradigm = 400

        for paradigm, count in paradigm_counts.items():
            if count != expected_per_paradigm:
                raise ValueError(
                    f"Paradigm imbalance: {paradigm} has {count} observations, "
                    f"expected {expected_per_paradigm}"
                )

        log("[PASS] Paradigm balance:")
        for paradigm in ['IFR', 'ICR', 'IRE']:
            log(f"  {paradigm}: {paradigm_counts[paradigm]} observations")

        # =========================================================================
        # STEP 10: Validation - Value Ranges
        # =========================================================================
        # theta: Typical IRT ability range is [-3, 3]
        # Age: Study inclusion criteria is 20-70 years

        log("[VALIDATION] Checking value ranges...")

        # Check theta range
        theta_min, theta_max = df_merged['theta'].min(), df_merged['theta'].max()
        log(f"[INFO] Theta range: [{theta_min:.2f}, {theta_max:.2f}]")

        if theta_min < -3 or theta_max > 3:
            log(f"[WARNING] Theta values outside typical range [-3, 3]")
            # Not a critical error - some extreme abilities are possible
        else:
            log("[PASS] Theta values in typical IRT range [-3, 3]")

        # Check Age range
        age_min, age_max = df_merged['Age'].min(), df_merged['Age'].max()
        log(f"[INFO] Age range: [{age_min:.1f}, {age_max:.1f}]")

        if age_min < 20 or age_max > 70:
            raise ValueError(
                f"Age values outside study criteria [20, 70]: [{age_min}, {age_max}]"
            )

        log("[PASS] Age values within study inclusion criteria [20, 70]")

        # =========================================================================
        # STEP 11: Validation - Categorical Values
        # =========================================================================
        # paradigm: Must be in {IFR, ICR, IRE}
        # test: Must be in {1, 2, 3, 4}

        log("[VALIDATION] Checking categorical values...")

        # Check paradigm values
        expected_paradigms = {'IFR', 'ICR', 'IRE'}
        actual_paradigms = set(df_merged['paradigm'].unique())

        if actual_paradigms != expected_paradigms:
            raise ValueError(
                f"Paradigm values mismatch: expected {expected_paradigms}, "
                f"got {actual_paradigms}"
            )

        log(f"[PASS] Paradigm values: {sorted(actual_paradigms)}")

        # Check test values
        expected_tests = {1, 2, 3, 4}
        actual_tests = set(df_merged['test'].unique())

        if actual_tests != expected_tests:
            raise ValueError(
                f"Test values mismatch: expected {expected_tests}, got {actual_tests}"
            )

        log(f"[PASS] Test values: {sorted(actual_tests)}")

        # =========================================================================
        # STEP 12: Select and Order Output Columns
        # =========================================================================
        # Output columns: composite_ID, UID, test, paradigm, theta, Age
        # Drop intermediate column: domain_name (replaced by paradigm)

        log("[FORMAT] Selecting output columns...")

        output_columns = ['composite_ID', 'UID', 'test', 'paradigm', 'theta', 'Age']
        df_output = df_merged[output_columns].copy()

        log(f"[INFO] Output columns: {list(df_output.columns)}")

        # =========================================================================
        # STEP 13: Save Merged Data
        # =========================================================================
        # Output: data/step00_theta_age_merged.csv
        # Contains: 1200 rows with theta scores + Age for 3-way interaction analysis

        log("[SAVE] Saving merged data...")

        output_path = RQ_DIR / "data" / "step00_theta_age_merged.csv"
        df_output.to_csv(output_path, index=False, encoding='utf-8')

        log(f"[SAVED] {output_path}")
        log(f"[INFO] Saved {len(df_output)} rows, {len(df_output.columns)} columns")

        # =========================================================================
        # STEP 14: Final Summary
        # =========================================================================

        log("[SUMMARY] Data merge complete:")
        log(f"  Input theta: 1200 rows (RQ 5.3.1 paradigm-specific theta scores)")
        log(f"  Input Age: 100 rows (participant demographics)")
        log(f"  Output: 1200 rows (theta + Age for all observations)")
        log(f"  Participants: {df_output['UID'].nunique()} unique UIDs")
        log(f"  Paradigms: {', '.join(sorted(df_output['paradigm'].unique()))}")
        log(f"  Age range: {df_output['Age'].min():.1f} - {df_output['Age'].max():.1f} years")
        log(f"  Theta range: {df_output['theta'].min():.2f} - {df_output['theta'].max():.2f}")

        log("[SUCCESS] Step 0 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
