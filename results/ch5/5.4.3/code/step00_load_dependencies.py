#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: 00
Step Name: step00_load_dependencies
RQ: results/ch5/5.4.3
Generated: 2025-12-02

PURPOSE:
Load theta scores from RQ 5.4.1, TSVR mapping from RQ 5.4.1, and Age variable
from master data. Validate all dependencies exist and have correct structure
before proceeding with analysis steps.

EXPECTED INPUTS:
  - results/ch5/5.4.1/data/step03_theta_scores.csv
    Columns: ['composite_ID', 'theta_common', 'theta_congruent', 'theta_incongruent',
              'se_common', 'se_congruent', 'se_incongruent']
    Format: Wide-format theta scores from RQ 5.4.1 Step 3 (final calibration)
    Expected rows: ~400 (100 participants x 4 tests)

  - results/ch5/5.4.1/data/step00_tsvr_mapping.csv
    Columns: ['composite_ID', 'UID', 'test', 'TSVR_hours']
    Format: TSVR mapping from RQ 5.4.1 Step 0
    Expected rows: ~400 (100 participants x 4 tests)

  - data/cache/dfData.csv
    Columns: ['UID', 'age', ...] (note: lowercase 'age')
    Format: Master data with participant demographics
    Expected rows: ~400 (100 participants x 4 tests, need unique UIDs)

EXPECTED OUTPUTS:
  - data/step00_theta_wide.csv
    Columns: ['composite_ID', 'theta_common', 'theta_congruent', 'theta_incongruent',
              'se_common', 'se_congruent', 'se_incongruent']
    Format: Validated copy of theta scores
    Expected rows: ~400

  - data/step00_tsvr_mapping.csv
    Columns: ['composite_ID', 'UID', 'test', 'TSVR_hours']
    Format: Validated copy of TSVR mapping
    Expected rows: ~400

  - data/step00_age_data.csv
    Columns: ['UID', 'Age'] (note: capitalized 'Age' in output)
    Format: Extracted Age data for unique participants
    Expected rows: ~100 (unique participants only)

VALIDATION CRITERIA:
  - All 3 dependency files exist
  - Expected column counts and names match specifications
  - Expected row counts (400 for theta/TSVR, 100+ for unique UIDs)
  - No NaN in critical columns (theta, se, TSVR_hours, Age)
  - Value ranges: theta in [-4,4], se in [0.1,1.5], TSVR in [0,250], Age in [20,70]
  - All composite_IDs from theta have matching TSVR entries
  - All UIDs parsed from composite_IDs have matching Age entries

g_code REASONING:
- Approach: Load all dependency files, extract UIDs from composite_IDs, validate
  structure and value ranges, save validated copies to RQ data folder
- Why this approach: RQ 5.4.3 depends on outputs from RQ 5.4.1 (cross-RQ dependency).
  Must validate dependencies exist and have correct format before proceeding with
  LMM analysis steps.
- Data flow: External files -> validation checks -> local copies in data/ folder
- Expected performance: <1 second (simple CSV loads and validation)

IMPLEMENTATION NOTES:
- Analysis tool: Standard library (pandas, numpy)
- Validation tool: Manual validation checks (no tools.validation function for this)
- Parameters: Value ranges based on REMEMVR data expectations
- composite_ID format: "UID_testnum" (e.g., "A010_1", "A010_2")
- dfData.csv has lowercase "age" column, rename to "Age" for consistency
- dfData.csv has 400 rows (100 UIDs x 4 tests), extract unique UIDs
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/ (5.4.3)
#   parents[2] = chX/ (ch5)
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.4.3
LOG_FILE = RQ_DIR / "logs" / "step00_load_dependencies.log"

# Dependency file paths (relative to project root)
THETA_SCORES_PATH = PROJECT_ROOT / "results/ch5/5.4.1/data/step03_theta_scores.csv"
TSVR_MAPPING_PATH = PROJECT_ROOT / "results/ch5/5.4.1/data/step00_tsvr_mapping.csv"
MASTER_DATA_PATH = PROJECT_ROOT / "data/cache/dfData.csv"

# Output file paths
OUTPUT_THETA_PATH = RQ_DIR / "data" / "step00_theta_wide.csv"
OUTPUT_TSVR_PATH = RQ_DIR / "data" / "step00_tsvr_mapping.csv"
OUTPUT_AGE_PATH = RQ_DIR / "data" / "step00_age_data.csv"

# Validation parameters
EXPECTED_THETA_ROWS = 400
EXPECTED_TSVR_ROWS = 400
EXPECTED_MIN_UNIQUE_UIDS = 100  # At least 100 unique participants

VALUE_RANGES = {
    'theta_common': (-4.0, 4.0),
    'theta_congruent': (-4.0, 4.0),
    'theta_incongruent': (-4.0, 4.0),
    'se_common': (0.0, 2.0),  # Allow some flexibility (0.1-1.5 expected, but allow wider)
    'se_congruent': (0.0, 2.0),
    'se_incongruent': (0.0, 2.0),
    'TSVR_hours': (0.0, 250.0),  # Actual max ~148h, allow margin
    'Age': (20.0, 70.0)
}

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step00_theta_wide.csv
#   CORRECT: data/step00_age_data.csv
#   WRONG:   results/theta_wide.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_wide.csv     (missing step prefix)
#   WRONG:   logs/step00_age.csv     (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Validation Functions
# =============================================================================

def validate_file_exists(file_path: Path, file_label: str) -> None:
    """Validate that a file exists."""
    if not file_path.exists():
        raise FileNotFoundError(f"[ERROR] {file_label} not found at: {file_path}")
    if not file_path.is_file():
        raise ValueError(f"[ERROR] {file_label} is not a file: {file_path}")
    log(f"[PASS] {file_label} exists: {file_path}")


def validate_columns(df: pd.DataFrame, expected_cols: List[str],
                      data_label: str) -> None:
    """Validate that DataFrame has expected columns."""
    df_cols = df.columns.tolist()
    missing_cols = [c for c in expected_cols if c not in df_cols]
    extra_cols = [c for c in df_cols if c not in expected_cols]

    if missing_cols:
        raise ValueError(
            f"[ERROR] {data_label} missing columns: {missing_cols}\n"
            f"  Expected: {expected_cols}\n"
            f"  Found: {df_cols}"
        )

    log(f"[PASS] {data_label} has all expected columns: {expected_cols}")
    if extra_cols:
        log(f"[INFO] {data_label} has additional columns: {extra_cols}")


def validate_row_count(df: pd.DataFrame, expected_rows: int, data_label: str,
                        tolerance: int = 10) -> None:
    """Validate that DataFrame has approximately expected row count."""
    actual_rows = len(df)

    if abs(actual_rows - expected_rows) > tolerance:
        raise ValueError(
            f"[ERROR] {data_label} row count mismatch: "
            f"expected ~{expected_rows} (Â±{tolerance}), got {actual_rows}"
        )

    log(f"[PASS] {data_label} row count: {actual_rows} (expected ~{expected_rows})")


def validate_no_nan(df: pd.DataFrame, columns: List[str], data_label: str) -> None:
    """Validate that specified columns have no NaN values."""
    nan_counts = {}
    has_nan = False

    for col in columns:
        if col in df.columns:
            n_nan = df[col].isna().sum()
            if n_nan > 0:
                nan_counts[col] = n_nan
                has_nan = True

    if has_nan:
        raise ValueError(
            f"[ERROR] {data_label} has NaN values:\n" +
            "\n".join([f"  {col}: {count} NaN" for col, count in nan_counts.items()])
        )

    log(f"[PASS] {data_label} has no NaN in critical columns: {columns}")


def validate_value_ranges(df: pd.DataFrame, ranges: Dict[str, Tuple[float, float]],
                          data_label: str) -> None:
    """Validate that numeric columns fall within expected ranges."""
    violations = []

    for col, (min_val, max_val) in ranges.items():
        if col not in df.columns:
            continue

        below_min = (df[col] < min_val).sum()
        above_max = (df[col] > max_val).sum()

        if below_min > 0 or above_max > 0:
            actual_min = df[col].min()
            actual_max = df[col].max()
            violations.append(
                f"  {col}: expected [{min_val}, {max_val}], "
                f"got [{actual_min:.2f}, {actual_max:.2f}] "
                f"({below_min} below, {above_max} above)"
            )

    if violations:
        raise ValueError(
            f"[ERROR] {data_label} has values out of expected ranges:\n" +
            "\n".join(violations)
        )

    log(f"[PASS] {data_label} values within expected ranges")


def parse_uids_from_composite_id(composite_ids: pd.Series) -> pd.Series:
    """
    Parse UIDs from composite_ID column.

    composite_ID format: "UID_testnum" (e.g., "A010_1", "A010_2")
    Returns UIDs: "A010", "A010"
    """
    return composite_ids.str.split('_').str[0]


# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 00: Load and Validate Dependency Files")
        log("")

        # =====================================================================
        # Create output directories if they don't exist
        # =====================================================================
        (RQ_DIR / "data").mkdir(parents=True, exist_ok=True)
        (RQ_DIR / "logs").mkdir(parents=True, exist_ok=True)
        log("[INFO] Created output directories (if not exist)")
        log("")

        # =====================================================================
        # STEP 1: Validate Dependency Files Exist
        # =====================================================================
        log("[STEP 1] Validate Dependency Files Exist")
        log("-" * 70)

        validate_file_exists(THETA_SCORES_PATH, "Theta scores (RQ 5.4.1)")
        validate_file_exists(TSVR_MAPPING_PATH, "TSVR mapping (RQ 5.4.1)")
        validate_file_exists(MASTER_DATA_PATH, "Master data (dfData.csv)")
        log("")

        # =====================================================================
        # STEP 2: Load Theta Scores from RQ 5.4.1
        # =====================================================================
        log("[STEP 2] Load Theta Scores from RQ 5.4.1")
        log("-" * 70)

        df_theta = pd.read_csv(THETA_SCORES_PATH)
        log(f"[LOADED] Theta scores: {len(df_theta)} rows, {len(df_theta.columns)} columns")

        # Validate theta scores structure
        expected_theta_cols = [
            'composite_ID', 'theta_common', 'theta_congruent', 'theta_incongruent',
            'se_common', 'se_congruent', 'se_incongruent'
        ]
        validate_columns(df_theta, expected_theta_cols, "Theta scores")
        validate_row_count(df_theta, EXPECTED_THETA_ROWS, "Theta scores")

        # Validate no NaN in theta columns
        theta_critical_cols = [
            'composite_ID', 'theta_common', 'theta_congruent', 'theta_incongruent',
            'se_common', 'se_congruent', 'se_incongruent'
        ]
        validate_no_nan(df_theta, theta_critical_cols, "Theta scores")

        # Validate theta value ranges
        theta_ranges = {
            'theta_common': VALUE_RANGES['theta_common'],
            'theta_congruent': VALUE_RANGES['theta_congruent'],
            'theta_incongruent': VALUE_RANGES['theta_incongruent'],
            'se_common': VALUE_RANGES['se_common'],
            'se_congruent': VALUE_RANGES['se_congruent'],
            'se_incongruent': VALUE_RANGES['se_incongruent']
        }
        validate_value_ranges(df_theta, theta_ranges, "Theta scores")

        log(f"[INFO] Theta score sample:")
        log(f"  theta_common: [{df_theta['theta_common'].min():.2f}, {df_theta['theta_common'].max():.2f}]")
        log(f"  theta_congruent: [{df_theta['theta_congruent'].min():.2f}, {df_theta['theta_congruent'].max():.2f}]")
        log(f"  theta_incongruent: [{df_theta['theta_incongruent'].min():.2f}, {df_theta['theta_incongruent'].max():.2f}]")
        log("")

        # =====================================================================
        # STEP 3: Load TSVR Mapping from RQ 5.4.1
        # =====================================================================
        log("[STEP 3] Load TSVR Mapping from RQ 5.4.1")
        log("-" * 70)

        df_tsvr = pd.read_csv(TSVR_MAPPING_PATH)
        log(f"[LOADED] TSVR mapping: {len(df_tsvr)} rows, {len(df_tsvr.columns)} columns")

        # Validate TSVR structure
        expected_tsvr_cols = ['composite_ID', 'UID', 'test', 'TSVR_hours']
        validate_columns(df_tsvr, expected_tsvr_cols, "TSVR mapping")
        validate_row_count(df_tsvr, EXPECTED_TSVR_ROWS, "TSVR mapping")

        # Validate no NaN in TSVR columns
        tsvr_critical_cols = ['composite_ID', 'UID', 'test', 'TSVR_hours']
        validate_no_nan(df_tsvr, tsvr_critical_cols, "TSVR mapping")

        # Validate TSVR value ranges
        tsvr_ranges = {'TSVR_hours': VALUE_RANGES['TSVR_hours']}
        validate_value_ranges(df_tsvr, tsvr_ranges, "TSVR mapping")

        log(f"[INFO] TSVR_hours range: [{df_tsvr['TSVR_hours'].min():.2f}, {df_tsvr['TSVR_hours'].max():.2f}]")
        log("")

        # =====================================================================
        # STEP 4: Validate Theta <-> TSVR Composite ID Match
        # =====================================================================
        log("[STEP 4] Validate Theta <-> TSVR Composite ID Match")
        log("-" * 70)

        theta_composite_ids = set(df_theta['composite_ID'])
        tsvr_composite_ids = set(df_tsvr['composite_ID'])

        missing_in_tsvr = theta_composite_ids - tsvr_composite_ids
        missing_in_theta = tsvr_composite_ids - theta_composite_ids

        if missing_in_tsvr:
            raise ValueError(
                f"[ERROR] {len(missing_in_tsvr)} composite_IDs in theta but not in TSVR: "
                f"{list(missing_in_tsvr)[:10]}..."
            )

        if missing_in_theta:
            log(f"[WARNING] {len(missing_in_theta)} composite_IDs in TSVR but not in theta "
                f"(acceptable if theta is filtered): {list(missing_in_theta)[:10]}...")
        else:
            log("[PASS] All composite_IDs match between theta and TSVR")

        log("")

        # =====================================================================
        # STEP 5: Load Age Data from Master
        # =====================================================================
        log("[STEP 5] Load Age Data from Master")
        log("-" * 70)

        # Load master data (only needed columns)
        df_master = pd.read_csv(MASTER_DATA_PATH, usecols=['UID', 'age'])
        log(f"[LOADED] Master data: {len(df_master)} rows, {len(df_master.columns)} columns")

        # Rename 'age' to 'Age' for consistency
        df_master = df_master.rename(columns={'age': 'Age'})
        log("[INFO] Renamed 'age' column to 'Age' for consistency")

        # Extract unique UIDs (master has 400 rows = 100 UIDs x 4 tests)
        df_age = df_master[['UID', 'Age']].drop_duplicates(subset=['UID']).reset_index(drop=True)
        log(f"[INFO] Extracted {len(df_age)} unique UIDs from master data")

        # Validate Age data structure
        validate_columns(df_age, ['UID', 'Age'], "Age data")

        if len(df_age) < EXPECTED_MIN_UNIQUE_UIDS:
            raise ValueError(
                f"[ERROR] Age data has only {len(df_age)} unique UIDs, "
                f"expected at least {EXPECTED_MIN_UNIQUE_UIDS}"
            )
        log(f"[PASS] Age data has {len(df_age)} unique UIDs (>= {EXPECTED_MIN_UNIQUE_UIDS})")

        # Validate no NaN in Age
        validate_no_nan(df_age, ['UID', 'Age'], "Age data")

        # Validate Age value ranges
        age_ranges = {'Age': VALUE_RANGES['Age']}
        validate_value_ranges(df_age, age_ranges, "Age data")

        log(f"[INFO] Age range: [{df_age['Age'].min():.1f}, {df_age['Age'].max():.1f}]")
        log(f"[INFO] Age mean: {df_age['Age'].mean():.1f} (SD: {df_age['Age'].std():.1f})")
        log("")

        # =====================================================================
        # STEP 6: Validate All Theta UIDs Have Age Data
        # =====================================================================
        log("[STEP 6] Validate All Theta UIDs Have Age Data")
        log("-" * 70)

        # Parse UIDs from composite_IDs
        theta_uids = parse_uids_from_composite_id(df_theta['composite_ID'])
        unique_theta_uids = set(theta_uids.unique())
        log(f"[INFO] Parsed {len(unique_theta_uids)} unique UIDs from theta composite_IDs")

        # Check all theta UIDs have Age data
        age_uids = set(df_age['UID'])
        missing_age = unique_theta_uids - age_uids

        if missing_age:
            raise ValueError(
                f"[ERROR] {len(missing_age)} UIDs from theta scores missing Age data: "
                f"{list(missing_age)[:10]}..."
            )

        log("[PASS] All UIDs from theta scores have matching Age data")
        log("")

        # =====================================================================
        # STEP 7: Save Validated Copies
        # =====================================================================
        log("[STEP 7] Save Validated Copies")
        log("-" * 70)

        # Save theta scores
        df_theta.to_csv(OUTPUT_THETA_PATH, index=False, encoding='utf-8')
        log(f"[SAVED] Theta scores: {OUTPUT_THETA_PATH}")
        log(f"  {len(df_theta)} rows, {len(df_theta.columns)} columns")

        # Save TSVR mapping
        df_tsvr.to_csv(OUTPUT_TSVR_PATH, index=False, encoding='utf-8')
        log(f"[SAVED] TSVR mapping: {OUTPUT_TSVR_PATH}")
        log(f"  {len(df_tsvr)} rows, {len(df_tsvr.columns)} columns")

        # Save Age data
        df_age.to_csv(OUTPUT_AGE_PATH, index=False, encoding='utf-8')
        log(f"[SAVED] Age data: {OUTPUT_AGE_PATH}")
        log(f"  {len(df_age)} rows, {len(df_age.columns)} columns")
        log("")

        # =====================================================================
        # STEP 8: Final Validation Summary
        # =====================================================================
        log("[STEP 8] Final Validation Summary")
        log("-" * 70)
        log("[PASS] All dependency files loaded and validated successfully")
        log("")
        log("Summary:")
        log(f"  Theta scores: {len(df_theta)} rows (400 expected)")
        log(f"  TSVR mapping: {len(df_tsvr)} rows (400 expected)")
        log(f"  Age data: {len(df_age)} rows (100 unique UIDs)")
        log(f"  Composite IDs: {len(theta_composite_ids)} in theta, {len(tsvr_composite_ids)} in TSVR")
        log(f"  UIDs in theta: {len(unique_theta_uids)}")
        log(f"  UIDs with Age: {len(age_uids)}")
        log("")
        log("Value Ranges:")
        log(f"  theta_common: [{df_theta['theta_common'].min():.2f}, {df_theta['theta_common'].max():.2f}]")
        log(f"  theta_congruent: [{df_theta['theta_congruent'].min():.2f}, {df_theta['theta_congruent'].max():.2f}]")
        log(f"  theta_incongruent: [{df_theta['theta_incongruent'].min():.2f}, {df_theta['theta_incongruent'].max():.2f}]")
        log(f"  TSVR_hours: [{df_tsvr['TSVR_hours'].min():.2f}, {df_tsvr['TSVR_hours'].max():.2f}]")
        log(f"  Age: [{df_age['Age'].min():.1f}, {df_age['Age'].max():.1f}]")
        log("")

        log("[SUCCESS] Step 00 complete - all dependencies validated and saved")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
