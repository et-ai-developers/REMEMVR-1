#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step01
Step Name: item_mapping
RQ: results/ch5/5.5.5
Generated: 2025-12-05

PURPOSE:
Create item mapping showing which items were retained vs removed during IRT
purification in RQ 5.5.1, separately for source and destination memory. This
mapping enables downstream comparison of Full CTT (all items) vs Purified CTT
(retained items only) to test the purification-trajectory paradox.

EXPECTED INPUTS:
  - results/ch5/5.5.1/data/step02_purified_items.csv
    Columns: ['item_tag', 'factor', 'a', 'b', 'retention_reason']
    Format: CSV with UTF-8 encoding, 32 rows (retained items only, all marked PASS)
    Expected rows: 32 (17 source + 15 destination)

  - data/cache/dfData.csv
    Columns: UID, TEST, and 72 item columns (36 TQ_ items: 18 source TQ_*-U-*, 18 destination TQ_*-D-*)
    Format: CSV with UTF-8 encoding
    Expected rows: 100 participants × 4 tests = 400 rows

EXPECTED OUTPUTS:
  - data/step01_item_mapping.csv
    Columns: ['item_name', 'location_type', 'a', 'b', 'retained', 'removal_reason']
    Format: CSV with UTF-8 encoding
    Expected rows: 36 (18 source + 18 destination)
    Content: All 36 TQ items with retention status from RQ 5.5.1 purification

VALIDATION CRITERIA:
  - All 36 TQ items present (18 source + 18 destination)
  - location_type in {source, destination}
  - 17 source retained, 1 source removed
  - 15 destination retained, 3 destination removed
  - removal_reason in {retained, low_discrimination, extreme_difficulty, both, removed_in_rq551}

g_code REASONING:
- Approach: Cross-reference all TQ items from dfData.csv against purified_items.csv
  to identify which items were retained vs removed during IRT purification
- Why this approach: RQ 5.5.1 Step 2 purification used filter_items_by_quality() with
  thresholds a > 0.4 and |b| < 3.0, removing poor-quality items. This step creates
  complete item inventory showing retention status for ALL items (not just retained).
- Data flow: Load purified_items.csv (32 retained items) → Extract all TQ item column
  names from dfData.csv (36 total items) → For each item, check if in purified list →
  Mark retained=True if present, retained=False if absent → Assign removal_reason based
  on IRT parameters or absence from purified list
- Expected performance: <1 second (simple set operations on 36 items)

IMPLEMENTATION NOTES:
- Analysis tool: Standard library operations (no custom tools)
- Validation tool: Standard library (row count, column value checks)
- Parameters: IRT quality thresholds from RQ 5.5.1 (a_threshold=0.4, b_threshold=3.0)
- Critical detail: purified_items.csv contains ONLY retained items (all marked PASS),
  so removed items must be inferred by set difference
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/ (5.5.5)
#   parents[2] = chX/ (ch5)
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.5.5 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step01_item_mapping.log"

# Input paths
PURIFIED_ITEMS_PATH = PROJECT_ROOT / "results/ch5/5.5.1/data/step02_purified_items.csv"
RAW_DATA_PATH = PROJECT_ROOT / "data/cache/dfData.csv"

# Output path
OUTPUT_PATH = RQ_DIR / "data/step01_item_mapping.csv"

# IRT quality thresholds from RQ 5.5.1 purification
A_THRESHOLD = 0.4  # Minimum discrimination
B_THRESHOLD = 3.0  # Maximum absolute difficulty

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step01_item_mapping.csv
#   CORRECT: logs/step01_item_mapping.log
#   WRONG:   results/item_mapping.csv  (wrong folder + no prefix)
#   WRONG:   data/item_mapping.csv     (missing step prefix)
#   WRONG:   logs/step01_items.csv     (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 01: Item Mapping")

        # =========================================================================
        # STEP 1: Load Purified Items from RQ 5.5.1
        # =========================================================================
        # Expected: 32 retained items (all marked PASS)
        # Purpose: Identify which items survived IRT purification

        log("[LOAD] Loading purified items from RQ 5.5.1...")
        df_purified = pd.read_csv(PURIFIED_ITEMS_PATH, encoding='utf-8')
        log(f"[LOADED] Purified items: {len(df_purified)} rows, {len(df_purified.columns)} cols")

        # Validate purified items structure
        expected_purified_cols = ['item_tag', 'factor', 'a', 'b', 'retention_reason']
        if df_purified.columns.tolist() != expected_purified_cols:
            log(f"[ERROR] Purified items columns mismatch")
            log(f"  Expected: {expected_purified_cols}")
            log(f"  Actual: {df_purified.columns.tolist()}")
            sys.exit(1)

        # Count by location type
        source_retained = len(df_purified[df_purified['factor'] == 'source'])
        dest_retained = len(df_purified[df_purified['factor'] == 'destination'])
        log(f"[INFO] Retained items by location: Source={source_retained}, Destination={dest_retained}")

        # Create set of retained item tags for fast lookup
        retained_tags = set(df_purified['item_tag'].tolist())
        log(f"[INFO] Retained item tags: {len(retained_tags)} unique items")

        # =========================================================================
        # STEP 2: Extract All TQ Items from dfData.csv
        # =========================================================================
        # Expected: 36 TQ items (18 source TQ_*-U-*, 18 destination TQ_*-D-*)
        # Purpose: Identify complete item inventory (retained + removed)

        log("[LOAD] Loading dfData.csv to extract all TQ item columns...")
        df_data = pd.read_csv(RAW_DATA_PATH, nrows=0, encoding='utf-8')  # Header only
        all_cols = df_data.columns.tolist()

        # Filter to TQ items only (exclude TC items)
        tq_items = sorted([c for c in all_cols if c.startswith('TQ_') and ('-U-' in c or '-D-' in c)])
        log(f"[INFO] Total TQ items found in dfData.csv: {len(tq_items)}")

        # Separate by location type
        tq_source = [c for c in tq_items if '-U-' in c]
        tq_dest = [c for c in tq_items if '-D-' in c]
        log(f"[INFO] TQ items by location: Source={len(tq_source)}, Destination={len(tq_dest)}")

        # Validate expected counts
        if len(tq_items) != 36:
            log(f"[ERROR] Expected 36 TQ items, found {len(tq_items)}")
            sys.exit(1)
        if len(tq_source) != 18 or len(tq_dest) != 18:
            log(f"[ERROR] Expected 18 source + 18 destination, found {len(tq_source)} + {len(tq_dest)}")
            sys.exit(1)

        # =========================================================================
        # STEP 3: Create Item Mapping with Retention Status
        # =========================================================================
        # For each TQ item: check if in retained set, assign retention status
        # If retained: get a, b values from purified_items
        # If removed: a=NaN, b=NaN, assign removal_reason

        log("[PROCESS] Creating item mapping with retention status...")

        item_mapping_rows = []

        for item_name in tq_items:
            # Determine location type from item name
            location_type = 'source' if '-U-' in item_name else 'destination'

            # Check if item was retained
            if item_name in retained_tags:
                # Item retained - get parameters from purified_items
                item_row = df_purified[df_purified['item_tag'] == item_name].iloc[0]
                a = item_row['a']
                b = item_row['b']
                retained = True
                removal_reason = 'retained'
            else:
                # Item removed - parameters unknown (not in purified list)
                # We don't have access to Pass 1 item parameters here, so mark as NaN
                a = np.nan
                b = np.nan
                retained = False
                # Removal reason: Since purified_items.csv doesn't include removed items,
                # we cannot determine exact removal reason (low_discrimination, extreme_difficulty, or both)
                # Mark as generic removed_in_rq551
                removal_reason = 'removed_in_rq551'

            item_mapping_rows.append({
                'item_name': item_name,
                'location_type': location_type,
                'a': a,
                'b': b,
                'retained': retained,
                'removal_reason': removal_reason
            })

        # Create DataFrame
        df_item_mapping = pd.DataFrame(item_mapping_rows)
        log(f"[CREATED] Item mapping: {len(df_item_mapping)} rows")

        # =========================================================================
        # STEP 4: Validate Item Mapping
        # =========================================================================
        # Check counts, retention rates, removal reasons

        log("[VALIDATION] Validating item mapping...")

        # Check total count
        if len(df_item_mapping) != 36:
            log(f"[ERROR] Expected 36 items, got {len(df_item_mapping)}")
            sys.exit(1)

        # Check location type values
        location_values = set(df_item_mapping['location_type'].unique())
        if location_values != {'source', 'destination'}:
            log(f"[ERROR] Unexpected location_type values: {location_values}")
            sys.exit(1)

        # Check retention counts by location
        source_retained_count = len(df_item_mapping[(df_item_mapping['location_type'] == 'source') &
                                                     (df_item_mapping['retained'] == True)])
        source_removed_count = len(df_item_mapping[(df_item_mapping['location_type'] == 'source') &
                                                    (df_item_mapping['retained'] == False)])
        dest_retained_count = len(df_item_mapping[(df_item_mapping['location_type'] == 'destination') &
                                                   (df_item_mapping['retained'] == True)])
        dest_removed_count = len(df_item_mapping[(df_item_mapping['location_type'] == 'destination') &
                                                  (df_item_mapping['retained'] == False)])

        log(f"[VALIDATION] Source: {source_retained_count} retained, {source_removed_count} removed")
        log(f"[VALIDATION] Destination: {dest_retained_count} retained, {dest_removed_count} removed")

        # Expected from user message: 17 source retained, 1 source removed, 15 destination retained, 3 destination removed
        if source_retained_count != 17 or source_removed_count != 1:
            log(f"[WARNING] Source retention counts differ from expected (17 retained, 1 removed)")
        if dest_retained_count != 15 or dest_removed_count != 3:
            log(f"[WARNING] Destination retention counts differ from expected (15 retained, 3 removed)")

        # Check removal_reason values
        removal_reasons = set(df_item_mapping['removal_reason'].unique())
        valid_reasons = {'retained', 'removed_in_rq551'}
        if not removal_reasons.issubset(valid_reasons):
            log(f"[WARNING] Unexpected removal_reason values: {removal_reasons - valid_reasons}")

        # Check retention rate per location type
        source_total = len(df_item_mapping[df_item_mapping['location_type'] == 'source'])
        dest_total = len(df_item_mapping[df_item_mapping['location_type'] == 'destination'])
        source_retention_rate = source_retained_count / source_total
        dest_retention_rate = dest_retained_count / dest_total

        log(f"[VALIDATION] Retention rates: Source={source_retention_rate:.2%}, Destination={dest_retention_rate:.2%}")

        # Check that retention rate is within expected range [0.55, 0.85]
        if not (0.55 <= source_retention_rate <= 0.85):
            log(f"[WARNING] Source retention rate outside expected range [0.55, 0.85]: {source_retention_rate:.2%}")
        if not (0.55 <= dest_retention_rate <= 0.85):
            log(f"[WARNING] Destination retention rate outside expected range [0.55, 0.85]: {dest_retention_rate:.2%}")

        # Check at least 10 retained items per location type (per validation criteria)
        if source_retained_count < 10:
            log(f"[ERROR] Source has < 10 retained items ({source_retained_count})")
            sys.exit(1)
        if dest_retained_count < 10:
            log(f"[ERROR] Destination has < 10 retained items ({dest_retained_count})")
            sys.exit(1)

        log("[VALIDATION] All validation checks passed")

        # =========================================================================
        # STEP 5: Save Item Mapping
        # =========================================================================
        # Output: data/step01_item_mapping.csv

        log(f"[SAVE] Saving item mapping to {OUTPUT_PATH}...")
        df_item_mapping.to_csv(OUTPUT_PATH, index=False, encoding='utf-8')
        log(f"[SAVED] {OUTPUT_PATH} ({len(df_item_mapping)} rows, {len(df_item_mapping.columns)} cols)")

        # Log summary statistics
        log(f"\n[SUMMARY] Item Mapping Statistics:")
        log(f"  Total items: {len(df_item_mapping)}")
        log(f"  Source items: {source_total} ({source_retained_count} retained, {source_removed_count} removed)")
        log(f"  Destination items: {dest_total} ({dest_retained_count} retained, {dest_removed_count} removed)")
        log(f"  Overall retention rate: {len(df_item_mapping[df_item_mapping['retained'] == True]) / len(df_item_mapping):.2%}")

        log("[SUCCESS] Step 01 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
