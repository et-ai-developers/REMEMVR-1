#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step04
Step Name: Reliability Assessment (Cronbach's Alpha)
RQ: results/ch5/5.5.5
Generated: 2025-12-05

PURPOSE:
Assess internal consistency reliability (Cronbach's alpha) for Full vs Purified CTT scores
per location type (source vs destination), testing whether IRT item purification improves
reliability. Compares alpha values across 4 conditions with bootstrap 95% confidence intervals.

EXPECTED INPUTS:
  - data/step01_item_mapping.csv
    Columns: ['item_name', 'location_type', 'a', 'b', 'retained', 'removal_reason']
    Format: 36 rows (18 source + 18 destination items)
    Expected rows: 36
  - data/cache/dfData.csv
    Columns: ['UID', 'TEST', 'TQ_IFR-U-i1', ..., 'TQ_IRE-D-i6', ...]
    Format: Raw binary item responses (0/1) for all participants
    Expected rows: ~400 (100 UIDs x 4 tests)

EXPECTED OUTPUTS:
  - data/step04_reliability_assessment.csv
    Columns: ['location_type', 'version', 'n_items', 'alpha', 'CI_lower', 'CI_upper', 'alpha_improvement']
    Format: 4 rows (2 location types x 2 versions)
    Expected rows: 4

VALIDATION CRITERIA:
  - alpha in [0.60, 0.95] (acceptable to excellent internal consistency)
  - CI_lower < CI_upper (valid confidence intervals)
  - Bootstrap completed with 10,000 iterations
  - All n_items > 0

g_code REASONING:
- Approach: Compute Cronbach's alpha from item-level covariance matrix using tools.analysis_ctt.compute_cronbachs_alpha
- Why this approach: Alpha must be computed from ITEM-LEVEL data (not sum scores) to properly assess inter-item correlations. Bootstrap provides non-parametric confidence intervals robust to normality violations.
- Data flow: Load item mapping -> identify retained items per location -> extract raw item responses from dfData -> compute alpha with bootstrap CIs -> save results
- Expected performance: ~30 seconds (10,000 bootstrap iterations x 4 conditions)

IMPLEMENTATION NOTES:
- Analysis tool: compute_cronbachs_alpha from tools.analysis_ctt
- Validation tool: validate_numeric_range from tools.validation
- Parameters: n_bootstrap=10000 (recommended for stable CIs per tools_inventory.md)
- CRITICAL: Alpha computed from ITEM-LEVEL data (covariance matrix), NOT sum scores
- Bootstrap percentile method: 2.5th and 97.5th percentiles for 95% CI
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/ch5/5.5.5/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 5.5.5/
#   parents[2] = ch5/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_ctt import compute_cronbachs_alpha

# Import validation tool
from tools.validation import validate_numeric_range

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.5.5 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step04_reliability_assessment.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step04_reliability_assessment.csv
#   CORRECT: logs/step04_reliability_assessment.log
#   WRONG:   results/reliability_assessment.csv  (wrong folder + no prefix)
#   WRONG:   data/reliability_assessment.csv     (missing step prefix)
#   WRONG:   logs/step04_reliability.csv         (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 04: Reliability Assessment (Cronbach's Alpha)")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Item mapping from Step 1 with retention status
        # Purpose: Identify which items to include in Full vs Purified alpha computation

        log("[LOAD] Loading item mapping...")
        item_mapping = pd.read_csv(RQ_DIR / "data" / "step01_item_mapping.csv", encoding='utf-8')
        log(f"[LOADED] step01_item_mapping.csv ({len(item_mapping)} rows, {len(item_mapping.columns)} cols)")
        log(f"[INFO] Location types: {sorted(item_mapping['location_type'].unique())}")
        log(f"[INFO] Retention counts: {item_mapping['retained'].value_counts().to_dict()}")

        # Load raw binary item responses
        log("[LOAD] Loading raw binary item responses from dfData.csv...")
        dfData = pd.read_csv(PROJECT_ROOT / "data" / "cache" / "dfData.csv", encoding='utf-8')
        log(f"[LOADED] dfData.csv ({len(dfData)} rows, {len(dfData.columns)} cols)")

        # =========================================================================
        # STEP 2: Extract Item Lists per Condition
        # =========================================================================
        # Extract 4 item sets:
        #   1. Source_Full: all 18 TQ_*-U-* items
        #   2. Source_Purified: retained TQ_*-U-* items
        #   3. Destination_Full: all 18 TQ_*-D-* items
        #   4. Destination_Purified: retained TQ_*-D-* items

        log("[EXTRACT] Building item lists for 4 conditions...")

        # Source items (location_type = 'source')
        source_items_full = item_mapping[item_mapping['location_type'] == 'source']['item_name'].tolist()
        source_items_purified = item_mapping[
            (item_mapping['location_type'] == 'source') & (item_mapping['retained'] == True)
        ]['item_name'].tolist()

        # Destination items (location_type = 'destination')
        dest_items_full = item_mapping[item_mapping['location_type'] == 'destination']['item_name'].tolist()
        dest_items_purified = item_mapping[
            (item_mapping['location_type'] == 'destination') & (item_mapping['retained'] == True)
        ]['item_name'].tolist()

        log(f"[INFO] Source Full: {len(source_items_full)} items")
        log(f"[INFO] Source Purified: {len(source_items_purified)} items (retention {len(source_items_purified)/len(source_items_full):.1%})")
        log(f"[INFO] Destination Full: {len(dest_items_full)} items")
        log(f"[INFO] Destination Purified: {len(dest_items_purified)} items (retention {len(dest_items_purified)/len(dest_items_full):.1%})")

        # =========================================================================
        # STEP 3: Compute Cronbach's Alpha for Each Condition
        # =========================================================================
        # Tool: tools.analysis_ctt.compute_cronbachs_alpha
        # What it does: Computes Cronbach's alpha from item covariance matrix with bootstrap CIs
        # Expected output: Dict with alpha, ci_lower, ci_upper, n_items, n_participants

        log("[ANALYSIS] Computing Cronbach's alpha with bootstrap CIs (10,000 iterations)...")
        log("[INFO] This will take ~30 seconds for 4 conditions...")

        results = []
        n_bootstrap = 10000  # Recommended per tools_inventory.md

        # Condition 1: Source Full
        log("[COMPUTE] Condition 1/4: Source_Full...")
        df_source_full = dfData[source_items_full].copy()
        alpha_source_full = compute_cronbachs_alpha(df_source_full, n_bootstrap=n_bootstrap)
        results.append({
            'location_type': 'source',
            'version': 'full',
            'n_items': alpha_source_full['n_items'],
            'alpha': alpha_source_full['alpha'],
            'CI_lower': alpha_source_full['ci_lower'],
            'CI_upper': alpha_source_full['ci_upper'],
            'alpha_improvement': 0.0  # Computed later
        })
        log(f"[DONE] Source_Full: alpha={alpha_source_full['alpha']:.3f} [{alpha_source_full['ci_lower']:.3f}, {alpha_source_full['ci_upper']:.3f}]")

        # Condition 2: Source Purified
        log("[COMPUTE] Condition 2/4: Source_Purified...")
        df_source_purified = dfData[source_items_purified].copy()
        alpha_source_purified = compute_cronbachs_alpha(df_source_purified, n_bootstrap=n_bootstrap)
        results.append({
            'location_type': 'source',
            'version': 'purified',
            'n_items': alpha_source_purified['n_items'],
            'alpha': alpha_source_purified['alpha'],
            'CI_lower': alpha_source_purified['ci_lower'],
            'CI_upper': alpha_source_purified['ci_upper'],
            'alpha_improvement': alpha_source_purified['alpha'] - alpha_source_full['alpha']
        })
        log(f"[DONE] Source_Purified: alpha={alpha_source_purified['alpha']:.3f} [{alpha_source_purified['ci_lower']:.3f}, {alpha_source_purified['ci_upper']:.3f}]")
        log(f"[INFO] Source alpha improvement: {alpha_source_purified['alpha'] - alpha_source_full['alpha']:+.3f}")

        # Condition 3: Destination Full
        log("[COMPUTE] Condition 3/4: Destination_Full...")
        df_dest_full = dfData[dest_items_full].copy()
        alpha_dest_full = compute_cronbachs_alpha(df_dest_full, n_bootstrap=n_bootstrap)
        results.append({
            'location_type': 'destination',
            'version': 'full',
            'n_items': alpha_dest_full['n_items'],
            'alpha': alpha_dest_full['alpha'],
            'CI_lower': alpha_dest_full['ci_lower'],
            'CI_upper': alpha_dest_full['ci_upper'],
            'alpha_improvement': 0.0  # Computed later
        })
        log(f"[DONE] Destination_Full: alpha={alpha_dest_full['alpha']:.3f} [{alpha_dest_full['ci_lower']:.3f}, {alpha_dest_full['ci_upper']:.3f}]")

        # Condition 4: Destination Purified
        log("[COMPUTE] Condition 4/4: Destination_Purified...")
        df_dest_purified = dfData[dest_items_purified].copy()
        alpha_dest_purified = compute_cronbachs_alpha(df_dest_purified, n_bootstrap=n_bootstrap)
        results.append({
            'location_type': 'destination',
            'version': 'purified',
            'n_items': alpha_dest_purified['n_items'],
            'alpha': alpha_dest_purified['alpha'],
            'CI_lower': alpha_dest_purified['ci_lower'],
            'CI_upper': alpha_dest_purified['ci_upper'],
            'alpha_improvement': alpha_dest_purified['alpha'] - alpha_dest_full['alpha']
        })
        log(f"[DONE] Destination_Purified: alpha={alpha_dest_purified['alpha']:.3f} [{alpha_dest_purified['ci_lower']:.3f}, {alpha_dest_purified['ci_upper']:.3f}]")
        log(f"[INFO] Destination alpha improvement: {alpha_dest_purified['alpha'] - alpha_dest_full['alpha']:+.3f}")

        # =========================================================================
        # STEP 4: Save Reliability Assessment Results
        # =========================================================================
        # Output: data/step04_reliability_assessment.csv
        # Contains: 4 rows (2 location types x 2 versions) with alpha, CIs, improvement

        log("[SAVE] Saving reliability assessment results...")
        df_results = pd.DataFrame(results)
        output_path = RQ_DIR / "data" / "step04_reliability_assessment.csv"
        df_results.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] step04_reliability_assessment.csv ({len(df_results)} rows, {len(df_results.columns)} cols)")

        # =========================================================================
        # STEP 5: Run Validation Tool
        # =========================================================================
        # Tool: tools.validation.validate_numeric_range
        # Validates: alpha values in [0.0, 1.0] range (mathematically required)
        # Threshold: alpha in [0.60, 0.95] for acceptable to excellent reliability

        log("[VALIDATION] Running validate_numeric_range on alpha values...")
        validation_result = validate_numeric_range(
            data=df_results['alpha'].values,
            min_val=0.0,
            max_val=1.0,
            column_name='alpha'
        )

        # Report validation results
        if validation_result['valid']:
            log(f"[VALIDATION] Alpha range validation: PASS")
            log(f"[VALIDATION] All alpha values in [0.0, 1.0]")
        else:
            log(f"[VALIDATION] Alpha range validation: FAIL")
            log(f"[VALIDATION] {validation_result['message']}")
            log(f"[ERROR] Out-of-range count: {validation_result['out_of_range_count']}")

        # Additional validation: CI_upper > CI_lower
        log("[VALIDATION] Checking confidence interval validity...")
        ci_valid = all(df_results['CI_upper'] > df_results['CI_lower'])
        if ci_valid:
            log("[VALIDATION] Confidence intervals: PASS (all CI_upper > CI_lower)")
        else:
            log("[VALIDATION] Confidence intervals: FAIL (some CI_upper <= CI_lower)")
            invalid_cis = df_results[df_results['CI_upper'] <= df_results['CI_lower']]
            log(f"[ERROR] Invalid CIs:\n{invalid_cis}")

        # Reliability interpretation
        log("[INTERPRETATION] Reliability assessment summary:")
        for _, row in df_results.iterrows():
            alpha = row['alpha']
            if alpha >= 0.90:
                interpretation = "Excellent"
            elif alpha >= 0.80:
                interpretation = "Good"
            elif alpha >= 0.70:
                interpretation = "Acceptable"
            elif alpha >= 0.60:
                interpretation = "Questionable"
            else:
                interpretation = "Poor"

            log(f"  {row['location_type'].capitalize()} {row['version'].capitalize()}: "
                f"alpha={alpha:.3f} [{row['CI_lower']:.3f}, {row['CI_upper']:.3f}] "
                f"({interpretation}, {row['n_items']} items)")

        # Final validation check
        if not validation_result['valid'] or not ci_valid:
            log("[ERROR] Validation failed - see errors above")
            sys.exit(1)

        log("[SUCCESS] Step 04 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
