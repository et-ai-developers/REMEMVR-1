#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step06
Step Name: Z-Standardize All Measurements
RQ: results/ch5/5.5.5
Generated: 2025-12-05

PURPOSE:
Standardize all measurements (IRT theta, Full CTT, Purified CTT) to z-scores
(mean=0, SD=1) within location type to enable valid AIC comparison across
different measurement scales in Step 7.

EXPECTED INPUTS:
  - results/ch5/5.5.1/data/step03_theta_scores.csv
    Columns: ['composite_ID', 'theta_source', 'theta_destination', 'se_source', 'se_destination']
    Format: IRT theta scores from RQ 5.5.1 (Pass 2 purified calibration)
    Expected rows: 400 (100 participants × 4 tests)

  - results/ch5/5.5.5/data/step02_ctt_full_scores.csv
    Columns: ['UID', 'test', 'location_type', 'ctt_full_score']
    Format: Full CTT sum scores (all 18 items per location type)
    Expected rows: 800 (100 participants × 4 tests × 2 location types)

  - results/ch5/5.5.5/data/step03_ctt_purified_scores.csv
    Columns: ['UID', 'test', 'location_type', 'ctt_purified_score']
    Format: Purified CTT sum scores (retained items only, typically 12-16 per location)
    Expected rows: 800 (same structure as Full CTT)

EXPECTED OUTPUTS:
  - data/step06_standardized_scores.csv
    Columns: ['UID', 'test', 'location_type', 'irt_z', 'ctt_full_z', 'ctt_purified_z']
    Format: Z-standardized measurements (mean=0, SD=1 per location_type)
    Expected rows: 800 (100 participants × 4 tests × 2 location types)

VALIDATION CRITERIA:
  - 800 rows total
  - No NaN values in z-score columns
  - For each location_type (source, destination):
    - mean(irt_z) ≈ 0 (±0.05)
    - std(irt_z) ≈ 1 (±0.05)
    - mean(ctt_full_z) ≈ 0 (±0.05)
    - std(ctt_full_z) ≈ 1 (±0.05)
    - mean(ctt_purified_z) ≈ 0 (±0.05)
    - std(ctt_purified_z) ≈ 1 (±0.05)

g_code REASONING:
- Approach: Standardize all measurements to z-scores within location_type groups
- Why this approach:
  - IRT theta, Full CTT, and Purified CTT are on different scales
  - Direct AIC comparison requires measurements on comparable scales
  - Z-standardization (mean=0, SD=1) puts all measurements on same scale
  - Standardizing within location_type preserves location-specific distributions
- Data flow:
  1. Load theta scores (wide format: composite_ID, theta_source, theta_destination)
  2. Load CTT scores (long format: UID, test, location_type, ctt_score)
  3. Parse composite_ID into UID and test
  4. Reshape theta to long format (source → location_type='source', destination → location_type='destination')
  5. Merge all on (UID, test, location_type)
  6. Group by location_type, z-standardize each measurement column
  7. Validate standardization (mean ≈ 0, SD ≈ 1)
- Expected performance: ~1-2 seconds (simple pandas operations)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib operations (pandas groupby + transform)
- Validation tool: Custom validation logic (mean/SD checks per group)
- Parameters: Use sample std (ddof=1) for consistency with LMM estimation
- Z-score formula: z = (x - mean(x)) / std(x) where mean/std computed per location_type
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 5.5.5/ (RQ directory)
#   parents[2] = ch5/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.5.5 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step06_standardized_scores.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step06_standardized_scores.csv
#   CORRECT: logs/step06_standardized_scores.log
#   WRONG:   results/standardized_scores.csv  (wrong folder + no prefix)
#   WRONG:   data/standardized_scores.csv     (missing step prefix)
#   WRONG:   logs/step06_scores.csv           (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 6: Z-Standardize All Measurements")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected:
        #   - Theta scores: 400 rows (100 UID × 4 test), wide format
        #   - Full CTT: 800 rows (100 UID × 4 test × 2 location_type), long format
        #   - Purified CTT: 800 rows (same structure as Full CTT)
        # Purpose: Merge all measurements for standardization

        log("[LOAD] Loading IRT theta scores from RQ 5.5.1...")
        theta_path = Path("results/ch5/5.5.1/data/step03_theta_scores.csv")
        theta_df = pd.read_csv(theta_path, encoding='utf-8')
        log(f"[LOADED] {theta_path.name} ({len(theta_df)} rows, {len(theta_df.columns)} cols)")
        log(f"[INFO] Theta columns: {theta_df.columns.tolist()}")

        log("[LOAD] Loading Full CTT scores from Step 2...")
        full_ctt_path = RQ_DIR / "data" / "step02_ctt_full_scores.csv"
        full_ctt_df = pd.read_csv(full_ctt_path, encoding='utf-8')
        log(f"[LOADED] {full_ctt_path.name} ({len(full_ctt_df)} rows, {len(full_ctt_df.columns)} cols)")

        log("[LOAD] Loading Purified CTT scores from Step 3...")
        purif_ctt_path = RQ_DIR / "data" / "step03_ctt_purified_scores.csv"
        purif_ctt_df = pd.read_csv(purif_ctt_path, encoding='utf-8')
        log(f"[LOADED] {purif_ctt_path.name} ({len(purif_ctt_df)} rows, {len(purif_ctt_df.columns)} cols)")

        # =========================================================================
        # STEP 2: Parse composite_ID and Reshape Theta to Long Format
        # =========================================================================
        # Tool: pandas string split + melt
        # What it does: Convert theta from wide (source/destination columns) to long (location_type column)
        # Expected output: 800 rows (400 composite_IDs × 2 location_types)

        log("[TRANSFORM] Parsing composite_ID into UID and test...")
        # composite_ID format: "A010_1" -> UID="A010", test="T1" (converted to match CTT format)
        theta_df[['UID', 'test_num']] = theta_df['composite_ID'].str.split('_', expand=True)
        # Convert test number to T1/T2/T3/T4 format to match CTT scores
        theta_df['test'] = 'T' + theta_df['test_num']
        theta_df.drop(columns=['test_num'], inplace=True)
        log(f"[INFO] Parsed {len(theta_df)} composite_IDs")
        log(f"[INFO] Test values: {theta_df['test'].unique().tolist()}")

        log("[TRANSFORM] Reshaping theta to long format...")
        # Create two dataframes: one for source, one for destination
        theta_source = theta_df[['UID', 'test', 'theta_source']].copy()
        theta_source['location_type'] = 'source'
        theta_source.rename(columns={'theta_source': 'theta'}, inplace=True)

        theta_dest = theta_df[['UID', 'test', 'theta_destination']].copy()
        theta_dest['location_type'] = 'destination'
        theta_dest.rename(columns={'theta_destination': 'theta'}, inplace=True)

        # Concatenate
        theta_long = pd.concat([theta_source, theta_dest], ignore_index=True)
        log(f"[TRANSFORMED] Theta reshaped to long format ({len(theta_long)} rows)")
        log(f"[INFO] Theta location_type counts: {theta_long['location_type'].value_counts().to_dict()}")

        # =========================================================================
        # STEP 3: Merge All Measurements
        # =========================================================================
        # Purpose: Create single DataFrame with all measurements aligned by (UID, test, location_type)
        # Expected: 800 rows with theta, ctt_full_score, ctt_purified_score columns

        log("[MERGE] Merging theta with Full CTT...")
        merged_df = theta_long.merge(
            full_ctt_df,
            on=['UID', 'test', 'location_type'],
            how='inner',
            validate='one_to_one'
        )
        log(f"[MERGED] After Full CTT merge: {len(merged_df)} rows")

        log("[MERGE] Merging with Purified CTT...")
        merged_df = merged_df.merge(
            purif_ctt_df,
            on=['UID', 'test', 'location_type'],
            how='inner',
            validate='one_to_one'
        )
        log(f"[MERGED] After Purified CTT merge: {len(merged_df)} rows")

        # Validate merge
        if len(merged_df) != 800:
            log(f"[ERROR] Expected 800 rows after merge, got {len(merged_df)}")
            raise ValueError(f"Merge produced {len(merged_df)} rows, expected 800")

        log(f"[INFO] Merged columns: {merged_df.columns.tolist()}")
        log(f"[INFO] Missing values: {merged_df.isnull().sum().to_dict()}")

        # =========================================================================
        # STEP 4: Z-Standardize Within Location Type
        # =========================================================================
        # Tool: pandas groupby + transform
        # What it does: For each location_type, compute z = (x - mean) / std
        # Formula: z_i = (x_i - mean(x)) / std(x) where mean/std computed within location_type
        # ddof=1: Sample standard deviation (N-1 denominator) for consistency with LMM

        log("[STANDARDIZE] Z-standardizing measurements within location_type...")
        log("[INFO] Using sample std (ddof=1) for consistency with LMM estimation")

        # Standardize theta
        merged_df['irt_z'] = merged_df.groupby('location_type')['theta'].transform(
            lambda x: (x - x.mean()) / x.std(ddof=1)
        )
        log("[STANDARDIZED] IRT theta -> irt_z")

        # Standardize Full CTT
        merged_df['ctt_full_z'] = merged_df.groupby('location_type')['ctt_full_score'].transform(
            lambda x: (x - x.mean()) / x.std(ddof=1)
        )
        log("[STANDARDIZED] Full CTT -> ctt_full_z")

        # Standardize Purified CTT
        merged_df['ctt_purified_z'] = merged_df.groupby('location_type')['ctt_purified_score'].transform(
            lambda x: (x - x.mean()) / x.std(ddof=1)
        )
        log("[STANDARDIZED] Purified CTT -> ctt_purified_z")

        # =========================================================================
        # STEP 5: Validate Standardization
        # =========================================================================
        # Criteria: For each location_type and each z-score column:
        #   - mean ≈ 0 (±0.05 tolerance)
        #   - std ≈ 1 (±0.05 tolerance)
        # Validation ensures standardization was computed correctly

        log("[VALIDATION] Validating z-score standardization...")

        validation_results = []
        z_cols = ['irt_z', 'ctt_full_z', 'ctt_purified_z']
        location_types = ['source', 'destination']

        all_valid = True
        for loc in location_types:
            loc_data = merged_df[merged_df['location_type'] == loc]
            log(f"\n[VALIDATION] Location type: {loc}")

            for col in z_cols:
                mean_val = loc_data[col].mean()
                std_val = loc_data[col].std(ddof=1)

                mean_ok = abs(mean_val) <= 0.05
                std_ok = abs(std_val - 1.0) <= 0.05

                status = "[PASS]" if (mean_ok and std_ok) else "[FAIL]"
                log(f"  {status} {col}: mean={mean_val:.6f}, std={std_val:.6f}")

                validation_results.append({
                    'location_type': loc,
                    'column': col,
                    'mean': mean_val,
                    'std': std_val,
                    'mean_ok': mean_ok,
                    'std_ok': std_ok,
                    'valid': mean_ok and std_ok
                })

                if not (mean_ok and std_ok):
                    all_valid = False

        if not all_valid:
            log("\n[ERROR] Standardization validation failed")
            log("[ERROR] Some z-scores do not have mean ≈ 0 or std ≈ 1")
            validation_df = pd.DataFrame(validation_results)
            log(f"[ERROR] Validation details:\n{validation_df.to_string()}")
            raise ValueError("Z-standardization validation failed")

        log("\n[VALIDATION] All z-scores validated: mean ≈ 0, std ≈ 1")

        # Check for NaN values
        nan_counts = merged_df[z_cols].isnull().sum()
        if nan_counts.any():
            log(f"[ERROR] NaN values found in z-scores: {nan_counts.to_dict()}")
            raise ValueError("NaN values detected in standardized scores")
        log("[VALIDATION] No NaN values in z-score columns")

        # =========================================================================
        # STEP 6: Save Standardized Scores
        # =========================================================================
        # Output: 800 rows with UID, test, location_type, irt_z, ctt_full_z, ctt_purified_z
        # These z-scores will be used by Step 7 for parallel LMM fitting and AIC comparison

        output_cols = ['UID', 'test', 'location_type', 'irt_z', 'ctt_full_z', 'ctt_purified_z']
        output_df = merged_df[output_cols].copy()

        log(f"[SAVE] Saving standardized scores...")
        output_path = RQ_DIR / "data" / "step06_standardized_scores.csv"
        output_df.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(output_df)} rows, {len(output_df.columns)} cols)")

        # Final summary
        log("\n[SUMMARY] Step 6 Complete:")
        log(f"  Input files: 3 (theta, full_ctt, purified_ctt)")
        log(f"  Output rows: {len(output_df)}")
        log(f"  Location types: {output_df['location_type'].nunique()}")
        log(f"  Participants: {output_df['UID'].nunique()}")
        log(f"  Test sessions: {output_df['test'].nunique()}")
        log(f"  Z-score columns: {len(z_cols)}")
        log(f"  Validation: All z-scores have mean ≈ 0, std ≈ 1 per location_type")

        log("[SUCCESS] Step 6 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
