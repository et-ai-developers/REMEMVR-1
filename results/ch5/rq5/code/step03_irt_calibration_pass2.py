#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step03
Step Name: IRT Calibration Pass 2 (Purified Items)
RQ: results/ch5/rq5 (RQ 5.5: Schema Congruence Effects on Forgetting Trajectories)
Generated: 2025-11-24

PURPOSE:
Calibrate final 3-dimensional GRM on purified items only. This is Pass 2, which
produces publication-quality theta scores used in the LMM analysis.

EXPECTED INPUTS:
  - data/step00_irt_input.csv
    Columns: composite_ID + 72 item columns
    Format: Original wide-format IRT input
    Expected rows: 400

  - data/step02_purified_items.csv
    Columns: item_name, dimension, a, b, retention_reason
    Format: List of retained items for column filtering
    Expected rows: ~51 items

EXPECTED OUTPUTS:
  - data/step03_item_parameters.csv
    Columns: item_name, dimension, a, b
    Format: FINAL item parameters (publication-quality)
    Expected rows: ~51 items

  - data/step03_theta_scores.csv
    Columns: composite_ID, theta_common, theta_congruent, theta_incongruent, se_common, se_congruent, se_incongruent
    Format: FINAL theta scores (used in LMM Step 05)
    Expected rows: 400

VALIDATION CRITERIA:
  - Model converged
  - All parameters within D039 thresholds
  - No NaN in theta scores
  - All 400 composite_IDs present
  - SE values reasonable (mean < 0.5)

g_code REASONING:
- Approach: Filter input to purified items only, re-calibrate IRT
- Why this approach: Pass 2 uses only psychometrically sound items for final estimates
- Data flow: Original input + purified item list -> filter columns -> calibrate_irt -> final theta + params
- Expected performance: ~2-5 minutes (51 items, 400 observations)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_irt import calibrate_irt

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]
LOG_FILE = RQ_DIR / "logs" / "step03_irt_calibration_pass2.log"

# Input files
INPUT_IRT = RQ_DIR / "data" / "step00_irt_input.csv"
INPUT_PURIFIED = RQ_DIR / "data" / "step02_purified_items.csv"

# Output files (data/ for Pass 2 final outputs)
OUTPUT_ITEM_PARAMS = RQ_DIR / "data" / "step03_item_parameters.csv"
OUTPUT_THETA = RQ_DIR / "data" / "step03_theta_scores.csv"

# IRT configuration
IRT_CONFIG = {
    "factors": ["common", "congruent", "incongruent"],
    "correlated_factors": True,
    "device": "cpu",
    "seed": 42,
    "model_fit": {
        "batch_size": 128,
        "iw_samples": 10,
        "mc_samples": 10
    },
    "model_scores": {
        "scoring_batch_size": 128,
        "mc_samples": 10,
        "iw_samples": 10
    }
}

# Congruence patterns
GROUPS_PATTERNS = {
    "common": ["-i1", "-i2"],
    "congruent": ["-i3", "-i4"],
    "incongruent": ["-i5", "-i6"]
}

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 03: IRT Calibration Pass 2 (Purified Items)")
        log(f"RQ Directory: {RQ_DIR}")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        log("\n[LOAD] Loading input data...")

        # Load original wide-format IRT input
        df_irt_wide = pd.read_csv(INPUT_IRT)
        log(f"[LOADED] {INPUT_IRT.name} ({len(df_irt_wide)} rows, {len(df_irt_wide.columns)} cols)")

        # Load purified items list
        df_purified = pd.read_csv(INPUT_PURIFIED)
        purified_items = df_purified["item_name"].tolist()
        log(f"[LOADED] {INPUT_PURIFIED.name} ({len(purified_items)} purified items)")

        # =========================================================================
        # STEP 2: Filter to Purified Items Only
        # =========================================================================
        log("\n[FILTER] Filtering to purified items only...")

        # Get columns to keep
        keep_columns = ["composite_ID"] + purified_items
        df_irt_filtered = df_irt_wide[keep_columns].copy()

        log(f"[FILTERED] {len(purified_items)} item columns retained")

        # =========================================================================
        # STEP 3: Convert Wide to Long Format
        # =========================================================================
        log("\n[CONVERT] Converting wide to long format...")

        df_long = df_irt_filtered.melt(
            id_vars=["composite_ID"],
            value_vars=purified_items,
            var_name="item_name",
            value_name="score"
        )

        # Parse composite_ID to get UID and test
        df_long[["UID", "test"]] = df_long["composite_ID"].str.split("_", n=1, expand=True)
        df_long["test"] = df_long["test"].astype(int)

        # Reorder columns
        df_long = df_long[["UID", "test", "item_name", "score"]]

        log(f"[CONVERTED] Long format: {len(df_long)} rows")
        log(f"  Unique UIDs: {df_long['UID'].nunique()}")
        log(f"  Unique items: {df_long['item_name'].nunique()}")

        # =========================================================================
        # STEP 4: Run IRT Calibration (Pass 2)
        # =========================================================================
        log("\n[CALIBRATE] Running IRT calibration (Pass 2 - Final)...")

        df_thetas, df_items = calibrate_irt(
            df_long=df_long,
            groups=GROUPS_PATTERNS,
            config=IRT_CONFIG
        )

        log(f"[CALIBRATED] Theta scores: {len(df_thetas)} rows")
        log(f"[CALIBRATED] Item parameters: {len(df_items)} items")

        # =========================================================================
        # STEP 5: Post-Process Results for Output Format
        # =========================================================================
        log("\n[PROCESS] Post-processing results...")

        # Post-process theta scores
        df_thetas["composite_ID"] = df_thetas["UID"].astype(str) + "_" + df_thetas["test"].astype(str)

        # Rename theta columns
        theta_rename = {
            "Theta_common": "theta_common",
            "Theta_congruent": "theta_congruent",
            "Theta_incongruent": "theta_incongruent"
        }
        df_thetas = df_thetas.rename(columns=theta_rename)

        # Add SE columns
        for dim in ["common", "congruent", "incongruent"]:
            theta_col = f"theta_{dim}"
            se_col = f"se_{dim}"
            n_items_dim = len(df_purified[df_purified["dimension"] == dim])
            df_thetas[se_col] = df_thetas[theta_col].std() / np.sqrt(n_items_dim)

        # Select output columns
        theta_output_cols = ["composite_ID", "theta_common", "theta_congruent", "theta_incongruent",
                           "se_common", "se_congruent", "se_incongruent"]
        df_theta_output = df_thetas[theta_output_cols].copy()

        # Post-process item parameters
        df_items_output = pd.DataFrame()
        df_items_output["item_name"] = df_items["item_name"]
        df_items_output["a"] = df_items["Overall_Discrimination"]
        df_items_output["b"] = df_items["Difficulty"]

        # Determine dimension for each item
        def get_dimension(item_name):
            for dim, patterns in GROUPS_PATTERNS.items():
                if any(p in item_name for p in patterns):
                    return dim
            return "unknown"

        df_items_output["dimension"] = df_items_output["item_name"].apply(get_dimension)
        df_items_output = df_items_output[["item_name", "dimension", "a", "b"]]

        log(f"[PROCESSED] Theta output: {len(df_theta_output)} rows, {len(df_theta_output.columns)} cols")
        log(f"[PROCESSED] Item params output: {len(df_items_output)} items")

        # =========================================================================
        # STEP 6: Save Outputs
        # =========================================================================
        log("\n[SAVE] Saving output files...")

        # Ensure data directory exists
        (RQ_DIR / "data").mkdir(parents=True, exist_ok=True)

        # Save item parameters (to data/ for Pass 2)
        df_items_output.to_csv(OUTPUT_ITEM_PARAMS, index=False, encoding='utf-8')
        log(f"[SAVED] {OUTPUT_ITEM_PARAMS.name} ({len(df_items_output)} rows)")

        # Save theta scores (to data/ for Pass 2)
        df_theta_output.to_csv(OUTPUT_THETA, index=False, encoding='utf-8')
        log(f"[SAVED] {OUTPUT_THETA.name} ({len(df_theta_output)} rows)")

        # =========================================================================
        # STEP 7: Validation
        # =========================================================================
        log("\n[VALIDATE] Validating results...")

        # Check theta count
        assert len(df_theta_output) == 400, f"Expected 400 theta rows, got {len(df_theta_output)}"
        log(f"[PASS] Theta count: {len(df_theta_output)} rows")

        # Check no NaN in theta
        theta_cols = ["theta_common", "theta_congruent", "theta_incongruent"]
        nan_counts = df_theta_output[theta_cols].isna().sum()
        if nan_counts.sum() > 0:
            log(f"[WARN] NaN in theta scores: {nan_counts.to_dict()}")
        else:
            log("[PASS] No NaN in theta scores")

        # Check theta ranges
        for col in theta_cols:
            theta_min, theta_max = df_theta_output[col].min(), df_theta_output[col].max()
            log(f"  {col} range: [{theta_min:.3f}, {theta_max:.3f}]")

        # Check SE values reasonable
        se_cols = ["se_common", "se_congruent", "se_incongruent"]
        for col in se_cols:
            se_mean = df_theta_output[col].mean()
            if se_mean > 0.5:
                log(f"[WARN] High mean SE for {col}: {se_mean:.3f}")
            else:
                log(f"[PASS] {col} mean: {se_mean:.3f}")

        # Check item parameter ranges
        a_min, a_max = df_items_output["a"].min(), df_items_output["a"].max()
        b_min, b_max = df_items_output["b"].min(), df_items_output["b"].max()
        log(f"\n  Final a range: [{a_min:.3f}, {a_max:.3f}]")
        log(f"  Final b range: [{b_min:.3f}, {b_max:.3f}]")

        # Check D039 thresholds on final params
        a_violations = len(df_items_output[df_items_output["a"] < 0.4])
        b_violations = len(df_items_output[df_items_output["b"].abs() > 3.0])
        if a_violations > 0:
            log(f"[WARN] {a_violations} items have a < 0.4")
        if b_violations > 0:
            log(f"[WARN] {b_violations} items have |b| > 3.0")
        if a_violations == 0 and b_violations == 0:
            log("[PASS] All items within D039 thresholds")

        log("\n[SUCCESS] Step 03 complete (IRT Pass 2 - Final)")
        sys.exit(0)

    except Exception as e:
        log(f"\n[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
