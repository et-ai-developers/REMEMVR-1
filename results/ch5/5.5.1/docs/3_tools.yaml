# 3_tools.yaml - Tool Catalog for RQ 5.5.1
# Created by: rq_tools agent
# Date: 2025-12-04
# Architecture: Tool Catalog (each tool listed once, deduplication)
# RQ: 5.5.1 Source-Destination Spatial Memory Trajectories

# =============================================================================
# ANALYSIS TOOLS
# =============================================================================

analysis_tools:

  # ---------------------------------------------------------------------------
  # Data Extraction (Step 0)
  # ---------------------------------------------------------------------------

  extract_vr_items_wide:
    module: "tools.data"
    function: "extract_vr_items_wide"
    signature: "extract_vr_items_wide(df_raw: pd.DataFrame, tag_patterns: List[str], output_format: str = 'wide') -> Tuple[pd.DataFrame, pd.DataFrame]"
    validation_tool: "validate_data_format"

    input_files:
      - path: "data/cache/dfData.csv"
        required_columns: ["UID", "test", "TQ_VR_*"]
        expected_rows: "~14400 (100 participants x 4 tests x ~36 items in long format)"
        data_types:
          UID: "string (format: P### with leading zeros)"
          test: "string (T1, T2, T3, T4)"
          TQ_VR_columns: "float (raw TQ values, dichotomized to 0/1)"

    output_files:
      - path: "data/step00_irt_input.csv"
        columns: ["composite_ID", "36 item columns (TQ_VR_* format)"]
        description: "Wide-format IRT input (400 rows x 37 columns)"
      - path: "data/step00_q_matrix.csv"
        columns: ["item_tag", "source", "destination"]
        description: "Q-matrix defining 2-factor structure (36 items x 3 columns)"
      - path: "data/step00_tsvr_mapping.csv"
        columns: ["composite_ID", "UID", "test", "TSVR_hours"]
        description: "Time mapping for Decision D070 (400 rows x 4 columns)"

    parameters:
      tag_patterns: ["TQ_VR_IFR_*_U", "TQ_VR_IFR_*_D", "TQ_VR_ICR_*_U", "TQ_VR_ICR_*_D", "TQ_VR_IRE_*_U", "TQ_VR_IRE_*_D"]
      dichotomize_threshold: 1.0
      output_format: "wide"
      create_q_matrix: true
      extract_tsvr: true

    description: "Extract source (-U-) and destination (-D-) location items from interactive paradigms (IFR/ICR/IRE), dichotomize responses, create Q-matrix for 2-factor IRT, extract TSVR time variable"
    source_reference: "tools_inventory.md section 'Data Extraction Tools' (inferred - stdlib operations)"

  # ---------------------------------------------------------------------------
  # IRT Analysis (Steps 1, 3)
  # ---------------------------------------------------------------------------

  calibrate_irt:
    module: "tools.analysis_irt"
    function: "calibrate_irt"
    signature: "calibrate_irt(df_long: pd.DataFrame, groups: Dict[str, List[str]], config: dict) -> Tuple[pd.DataFrame, pd.DataFrame]"
    validation_tool: "validate_irt_convergence"

    input_files:
      - path: "data/step00_irt_input.csv"
        required_columns: ["composite_ID", "36 item columns"]
        expected_rows: "400"
        data_types:
          composite_ID: "string (format: {UID}_{test})"
          items: "int (values: 0, 1, NaN)"

    output_files:
      - path: "data/step01_pass1_item_params.csv"
        columns: ["item_tag", "factor", "a", "b"]
        description: "Pass 1 item parameters (36 items x 4 columns)"
      - path: "data/step01_pass1_theta.csv"
        columns: ["composite_ID", "theta_source", "theta_destination", "se_source", "se_destination"]
        description: "Pass 1 theta estimates (400 rows x 5 columns)"
      - path: "data/step01_pass1_diagnostics.txt"
        description: "Convergence diagnostics (text file)"

    parameters:
      model_type: "GRM"
      n_cats: 2
      correlated_factors: true
      prior: "p1_med"
      estimation_method: "variational_inference"
      max_iter: 200
      mc_samples: 100
      iw_samples: 100
      device: "cpu"
      minimal_settings_test: true
      minimal_max_iter: 50
      minimal_mc_samples: 10
      minimal_iw_samples: 10

    description: "Calibrate 2-dimensional GRM on all items (Pass 1) or purified items (Pass 2). Used in Step 1 (all 36 items) and Step 3 (25-32 purified items). Decision D039 2-pass purification."
    source_reference: "tools_inventory.md section 'Module: tools.analysis_irt' - calibrate_irt"

  # ---------------------------------------------------------------------------
  # Item Purification (Step 2)
  # ---------------------------------------------------------------------------

  filter_items_by_quality:
    module: "tools.analysis_irt"
    function: "filter_items_by_quality"
    signature: "filter_items_by_quality(df_items: pd.DataFrame, a_threshold: float = 0.4, b_threshold: float = 3.0) -> Tuple[pd.DataFrame, pd.DataFrame]"
    validation_tool: "validate_irt_parameters"

    input_files:
      - path: "data/step01_pass1_item_params.csv"
        required_columns: ["item_tag", "factor", "a", "b"]
        expected_rows: "36"

    output_files:
      - path: "data/step02_purified_items.csv"
        columns: ["item_tag", "factor", "a", "b", "retention_reason"]
        description: "Items retained after purification (25-32 items expected)"
      - path: "data/step02_purification_report.txt"
        description: "Text report with exclusion breakdown"

    parameters:
      a_threshold: 0.4
      b_threshold: 3.0
      min_items_per_factor: 10

    description: "Filter items by Decision D039 thresholds: retain if (|b| <= 3.0) AND (a >= 0.4). Require minimum 10 items per factor."
    source_reference: "tools_inventory.md section 'Module: tools.analysis_irt' - filter_items_by_quality"

  # ---------------------------------------------------------------------------
  # Data Transformation (Step 4)
  # ---------------------------------------------------------------------------

  merge_theta_tsvr:
    module: "pandas"
    function: "merge"
    signature: "pd.merge(left: pd.DataFrame, right: pd.DataFrame, on: str, how: str = 'inner') -> pd.DataFrame"
    validation_tool: "validate_data_format"

    input_files:
      - path: "data/step03_theta_scores.csv"
        required_columns: ["composite_ID", "theta_source", "theta_destination", "se_source", "se_destination"]
        expected_rows: "400"
      - path: "data/step00_tsvr_mapping.csv"
        required_columns: ["composite_ID", "UID", "test", "TSVR_hours"]
        expected_rows: "400"

    output_files:
      - path: "data/step04_lmm_input.csv"
        columns: ["UID", "test", "composite_ID", "TSVR_hours", "Days", "log_Days_plus1", "Days_squared", "LocationType", "LocationType_coded", "theta", "se"]
        description: "Long-format LMM input (800 rows: 400 composite_IDs x 2 location types)"

    parameters:
      merge_on: "composite_ID"
      merge_how: "inner"
      reshape_method: "melt"
      location_types: ["source", "destination"]
      treatment_coding: {"source": 0, "destination": 1}
      time_transformations: ["Days", "log_Days_plus1", "Days_squared"]

    description: "Merge theta scores with TSVR (Decision D070), reshape wide to long for LocationType factor, create time transformations, apply treatment coding"
    source_reference: "Standard pandas operations (no tools_inventory.md entry needed)"

  # ---------------------------------------------------------------------------
  # LMM Model Selection (Step 5)
  # ---------------------------------------------------------------------------

  compare_lmm_models_by_aic:
    module: "tools.analysis_lmm"
    function: "compare_lmm_models_by_aic"
    signature: "compare_lmm_models_by_aic(data: pd.DataFrame, n_factors: int, reference_group: str, groups: str, save_dir: Path) -> Dict"
    validation_tool: "validate_lmm_convergence"

    input_files:
      - path: "data/step04_lmm_input.csv"
        required_columns: ["UID", "Days", "log_Days_plus1", "Days_squared", "LocationType", "LocationType_coded", "theta"]
        expected_rows: "800"

    output_files:
      - path: "data/step05_model_comparison.csv"
        columns: ["model_name", "AIC", "delta_AIC", "weight"]
        description: "AIC comparison of 5 candidate models (5 rows)"
      - path: "data/step05_lmm_fitted_model.pkl"
        description: "Best-fitting LMM (pickle file, statsmodels MixedLMResults)"
      - path: "data/step05_lmm_summary.txt"
        description: "Best model summary (text file)"

    parameters:
      n_factors: 1
      reference_group: null
      groups: "UID"
      candidate_models: ["Linear", "Quadratic", "Logarithmic", "Linear+Logarithmic", "Quadratic+Logarithmic"]
      reml: false
      quality_threshold: 0.30

    description: "Fit 5 candidate LMMs with LocationType x Time interactions, compare by AIC, return best model. All models include random intercepts + random slopes for Days by UID."
    source_reference: "tools_inventory.md section 'Module: tools.analysis_lmm' - compare_lmm_models_by_aic"

  # ---------------------------------------------------------------------------
  # Post-Hoc Contrasts (Step 6)
  # ---------------------------------------------------------------------------

  compute_contrasts_pairwise:
    module: "tools.analysis_lmm"
    function: "compute_contrasts_pairwise"
    signature: "compute_contrasts_pairwise(lmm_result: MixedLMResults, comparisons: List[str], family_alpha: float = 0.05) -> pd.DataFrame"
    validation_tool: "validate_contrasts_d068"

    input_files:
      - path: "data/step05_lmm_fitted_model.pkl"
        description: "Fitted LMM result object"
      - path: "data/step05_model_comparison.csv"
        description: "Model comparison table (to identify best model)"

    output_files:
      - path: "data/step06_post_hoc_contrasts.csv"
        columns: ["test_name", "coefficient", "SE", "z", "p_uncorrected", "p_bonferroni", "CI_lower", "CI_upper"]
        description: "Hypothesis tests with dual p-values (2 rows: main effect + interaction)"
      - path: "data/step06_effect_sizes.csv"
        columns: ["timepoint", "source_mean", "destination_mean", "mean_difference", "cohens_d", "CI_lower", "CI_upper"]
        description: "Effect sizes at 4 timepoints (4 rows: Days 0, 1, 3, 6)"

    parameters:
      comparisons: ["LocationType_main_effect", "LocationType_x_Time_interaction"]
      family_alpha: 0.05
      bonferroni_n_tests: 2
      effect_size_timepoints: [0, 1, 3, 6]

    description: "Extract LocationType main effect and LocationType x Time interaction from best LMM with Decision D068 dual p-value reporting (uncorrected + Bonferroni). Compute effect sizes at Days 0, 1, 3, 6."
    source_reference: "tools_inventory.md section 'Module: tools.analysis_lmm' - compute_contrasts_pairwise"

  compute_effect_sizes_cohens:
    module: "tools.analysis_lmm"
    function: "compute_effect_sizes_cohens"
    signature: "compute_effect_sizes_cohens(lmm_result: MixedLMResults, include_interactions: bool = False) -> pd.DataFrame"
    validation_tool: "validate_effect_sizes"

    input_files:
      - path: "data/step05_lmm_fitted_model.pkl"
        description: "Fitted LMM for marginal means extraction"

    output_files:
      - path: "data/step06_effect_sizes.csv"
        columns: ["timepoint", "source_mean", "destination_mean", "mean_difference", "cohens_d", "CI_lower", "CI_upper"]
        description: "Effect sizes at Days 0, 1, 3, 6"

    parameters:
      timepoints: [0, 1, 3, 6]
      include_interactions: false

    description: "Compute Cohen's d effect sizes for source vs destination at specified timepoints using marginal means from best LMM"
    source_reference: "tools_inventory.md section 'Module: tools.analysis_lmm' - compute_effect_sizes_cohens"

  # ---------------------------------------------------------------------------
  # Plot Data Preparation (Step 7)
  # ---------------------------------------------------------------------------

  convert_theta_to_probability:
    module: "tools.plotting"
    function: "convert_theta_to_probability"
    signature: "convert_theta_to_probability(theta: np.ndarray, discrimination: float = 1.0, difficulty: float = 0.0) -> np.ndarray"
    validation_tool: "validate_probability_range"

    input_files:
      - path: "data/step05_lmm_fitted_model.pkl"
        description: "For extracting marginal means on theta scale"
      - path: "data/step06_effect_sizes.csv"
        description: "Contains marginal means at Days 0, 1, 3, 6"
      - path: "data/step03_item_parameters.csv"
        description: "For average discrimination and difficulty (IRT 2PL transformation)"

    output_files:
      - path: "data/step07_trajectory_theta_data.csv"
        columns: ["LocationType", "Days", "theta_mean", "CI_lower", "CI_upper"]
        description: "Theta-scale plot data (8 rows: 2 location types x 4 timepoints)"
      - path: "data/step07_trajectory_probability_data.csv"
        columns: ["LocationType", "Days", "prob_mean", "CI_lower", "CI_upper"]
        description: "Probability-scale plot data (8 rows, Decision D069 dual-scale)"

    parameters:
      timepoints: [0, 1, 3, 6]
      location_types: ["source", "destination"]
      transformation_method: "IRT_2PL"
      use_average_item_params: true

    description: "Create plot source CSVs on theta scale and probability scale (Decision D069 dual-scale requirement). Transform theta to probability via IRT 2PL formula using average item parameters from Pass 2."
    source_reference: "tools_inventory.md section 'Module: tools.plotting' - convert_theta_to_probability"


# =============================================================================
# VALIDATION TOOLS
# =============================================================================

validation_tools:

  # ---------------------------------------------------------------------------
  # Data Extraction Validation
  # ---------------------------------------------------------------------------

  validate_data_format:
    module: "tools.validation"
    function: "validate_data_format"
    signature: "validate_data_format(df: pd.DataFrame, required_cols: List[str]) -> Dict[str, Any]"

    input_files:
      - path: "data/step00_irt_input.csv"
        required_columns: ["composite_ID", "36 item columns"]
        source: "extract_vr_items_wide output"

    parameters:
      required_cols: ["composite_ID"]
      item_count_expected: 36
      composite_id_format: "{UID}_{test}"

    criteria:
      - "All required columns present (composite_ID + 36 items)"
      - "Exactly 400 rows (100 participants x 4 tests)"
      - "Item values in {0, 1, NaN}"
      - "No duplicate composite_IDs"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        message: "str"
        missing_cols: "List[str]"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step00_extract_vr_data.log"
      invoke: "g_debug (master invokes)"

    description: "Validate extraction output has correct structure (400 rows, 37 columns, correct data types)"
    source_reference: "tools_inventory.md section 'Module: tools.validation' - validate_data_format"

  # ---------------------------------------------------------------------------
  # IRT Calibration Validation
  # ---------------------------------------------------------------------------

  validate_irt_convergence:
    module: "tools.validation"
    function: "validate_irt_convergence"
    signature: "validate_irt_convergence(results: Dict[str, Any]) -> Dict[str, Any]"

    input_files:
      - path: "data/step01_pass1_item_params.csv"
        required_columns: ["item_tag", "factor", "a", "b"]
        source: "calibrate_irt output (Pass 1 or Pass 2)"
      - path: "data/step01_pass1_theta.csv"
        required_columns: ["composite_ID", "theta_source", "theta_destination", "se_source", "se_destination"]
        source: "calibrate_irt output"
      - path: "data/step01_pass1_diagnostics.txt"
        source: "calibrate_irt diagnostics"

    parameters:
      check_convergence_status: true
      check_parameter_bounds: true
      check_theta_bounds: true
      a_range: [0.0, 10.0]
      b_range: [-6.0, 6.0]
      theta_range: [-4.0, 4.0]
      se_range: [0.1, 1.5]

    criteria:
      - "Model converged (converged=True in diagnostics)"
      - "All discrimination (a) in [0.0, 10.0]"
      - "All difficulty (b) in [-6.0, 6.0]"
      - "All theta estimates in [-4, 4]"
      - "All standard errors in [0.1, 1.5]"
      - "No NaN values in item parameters or theta estimates"
      - "All items present (no dropped items during calibration)"
      - "All participants present (no excluded participants)"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        converged: "bool"
        checks: "list of check results"
        message: "str"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step01_irt_calibration_pass1.log"
      invoke: "g_debug (master invokes)"

    description: "Validate IRT model converged, parameters in acceptable ranges, no estimation failures"
    source_reference: "tools_inventory.md section 'Module: tools.validation' - validate_irt_convergence"

  # ---------------------------------------------------------------------------
  # Item Purification Validation
  # ---------------------------------------------------------------------------

  validate_irt_parameters:
    module: "tools.validation"
    function: "validate_irt_parameters"
    signature: "validate_irt_parameters(df_items: pd.DataFrame, a_min: float = 0.4, b_max: float = 3.0, a_col: str = 'Discrimination', b_col: str = 'Difficulty') -> Dict[str, Any]"

    input_files:
      - path: "data/step02_purified_items.csv"
        required_columns: ["item_tag", "factor", "a", "b", "retention_reason"]
        source: "filter_items_by_quality output"

    parameters:
      a_min: 0.4
      b_max: 3.0
      a_col: "a"
      b_col: "b"
      min_items_per_factor: 10

    criteria:
      - "All retained items have a >= 0.4 (Decision D039 threshold)"
      - "All retained items have |b| <= 3.0 (Decision D039 threshold)"
      - "Minimum 10 items per factor (source >= 10, destination >= 10)"
      - "No NaN values in purified_items.csv"
      - "No duplicates in purified_items.csv"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        n_items: "int"
        n_valid: "int"
        n_invalid: "int"
        invalid_items: "List[str]"
        message: "str"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step02_purify_items.log"
      invoke: "g_debug (master invokes)"

    description: "Validate all retained items meet Decision D039 thresholds, no edge cases slipped through, minimum items per factor met"
    source_reference: "tools_inventory.md section 'Module: tools.validation' - validate_irt_parameters"

  # ---------------------------------------------------------------------------
  # LMM Convergence Validation
  # ---------------------------------------------------------------------------

  validate_lmm_convergence:
    module: "tools.validation"
    function: "validate_lmm_convergence"
    signature: "validate_lmm_convergence(lmm_result: MixedLMResults) -> Dict[str, Any]"

    input_files:
      - path: "data/step05_lmm_fitted_model.pkl"
        source: "compare_lmm_models_by_aic output"
      - path: "data/step05_model_comparison.csv"
        source: "AIC comparison table"

    parameters:
      check_all_models_converged: true
      check_akaike_weights_sum: true
      min_best_model_weight: 0.30
      akaike_sum_tolerance: 0.01

    criteria:
      - "All 5 candidate models converged successfully"
      - "All 5 models have finite AIC values (no NaN or inf)"
      - "Akaike weights sum to 1.0 +/- 0.01"
      - "Best model weight > 0.30 (quality threshold for clear winner)"
      - "Exactly one model has delta_AIC = 0"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        converged: "bool"
        message: "str"
        warnings: "list"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step05_fit_lmm.log"
      invoke: "g_debug (master invokes)"

    description: "Validate all LMM models converged, AIC comparison valid, Akaike weights sum correctly, best model clearly identified"
    source_reference: "tools_inventory.md section 'Module: tools.validation' - validate_lmm_convergence"

  # ---------------------------------------------------------------------------
  # Decision D068 Dual P-Value Validation
  # ---------------------------------------------------------------------------

  validate_contrasts_d068:
    module: "tools.validation"
    function: "validate_contrasts_d068"
    signature: "validate_contrasts_d068(contrasts_df: pd.DataFrame) -> Dict[str, Any]"

    input_files:
      - path: "data/step06_post_hoc_contrasts.csv"
        required_columns: ["test_name", "coefficient", "SE", "z", "p_uncorrected", "p_bonferroni", "CI_lower", "CI_upper"]
        source: "compute_contrasts_pairwise output"

    parameters:
      required_p_cols: ["p_uncorrected", "p_bonferroni"]
      bonferroni_n_tests: 2

    criteria:
      - "p_uncorrected column present (Decision D068 requirement)"
      - "p_bonferroni column present (Decision D068 dual reporting)"
      - "p_bonferroni = p_uncorrected x 2 (Bonferroni correction formula, capped at 1.0)"
      - "All p-values in [0, 1] range"
      - "Exactly 2 hypothesis tests (main effect + interaction)"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        d068_compliant: "bool"
        missing_cols: "List[str]"
        message: "str"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step06_compute_post_hoc_contrasts.log"
      invoke: "g_debug (master invokes)"

    description: "Validate Decision D068 dual p-value reporting (uncorrected + Bonferroni) in post-hoc contrasts"
    source_reference: "tools_inventory.md section 'Module: tools.validation' - validate_contrasts_d068"

  # ---------------------------------------------------------------------------
  # Effect Size Validation
  # ---------------------------------------------------------------------------

  validate_effect_sizes:
    module: "tools.validation"
    function: "validate_effect_sizes"
    signature: "validate_effect_sizes(effect_sizes_df: pd.DataFrame, f2_column: str = 'cohens_f2') -> Dict[str, Any]"

    input_files:
      - path: "data/step06_effect_sizes.csv"
        required_columns: ["timepoint", "source_mean", "destination_mean", "mean_difference", "cohens_d", "CI_lower", "CI_upper"]
        source: "compute_effect_sizes_cohens output"

    parameters:
      cohens_d_col: "cohens_d"
      check_bounds: true
      check_direction: true

    criteria:
      - "Exactly 4 timepoints (Days 0, 1, 3, 6)"
      - "Cohen's d values are finite (no NaN or inf)"
      - "CI_lower < mean_difference < CI_upper for all rows"
      - "All marginal means (source_mean, destination_mean) in [-4, 4] theta range"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        message: "str"
        warnings: "List[str]"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step06_compute_post_hoc_contrasts.log"
      invoke: "g_debug (master invokes)"

    description: "Validate effect sizes are finite, CIs are correctly ordered, marginal means in theta range"
    source_reference: "tools_inventory.md section 'Module: tools.validation' - validate_effect_sizes"

  # ---------------------------------------------------------------------------
  # Probability Range Validation
  # ---------------------------------------------------------------------------

  validate_probability_range:
    module: "tools.validation"
    function: "validate_probability_range"
    signature: "validate_probability_range(probability_df: pd.DataFrame, prob_columns: List[str]) -> Dict[str, Any]"

    input_files:
      - path: "data/step07_trajectory_probability_data.csv"
        required_columns: ["LocationType", "Days", "prob_mean", "CI_lower", "CI_upper"]
        source: "convert_theta_to_probability output"

    parameters:
      prob_columns: ["prob_mean", "CI_lower", "CI_upper"]
      range_min: 0.0
      range_max: 1.0

    criteria:
      - "All probability values in [0, 1] range (inclusive)"
      - "No NaN values in probability columns"
      - "No infinite values in probability columns"
      - "Exactly 8 rows (2 location types x 4 timepoints)"
      - "CI_lower < prob_mean < CI_upper for all rows"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        message: "str"
        violations: "List[Dict]"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step07_prepare_trajectory_plot_data.log"
      invoke: "g_debug (master invokes)"

    description: "Validate IRT theta-to-probability transformation produced valid probabilities in [0,1] range (Decision D069 dual-scale requirement)"
    source_reference: "tools_inventory.md section 'Module: tools.validation' - validate_probability_range"

  # ---------------------------------------------------------------------------
  # Plot Data Completeness Validation
  # ---------------------------------------------------------------------------

  validate_plot_data_completeness:
    module: "tools.validation"
    function: "validate_plot_data_completeness"
    signature: "validate_plot_data_completeness(plot_data: pd.DataFrame, required_domains: List[str], required_groups: List[str], domain_col: str = 'domain', group_col: str = 'group') -> Dict[str, Any]"

    input_files:
      - path: "data/step07_trajectory_theta_data.csv"
        required_columns: ["LocationType", "Days", "theta_mean", "CI_lower", "CI_upper"]
        source: "plot data preparation (theta scale)"
      - path: "data/step07_trajectory_probability_data.csv"
        required_columns: ["LocationType", "Days", "prob_mean", "CI_lower", "CI_upper"]
        source: "plot data preparation (probability scale)"

    parameters:
      required_location_types: ["source", "destination"]
      required_timepoints: [0, 1, 3, 6]
      location_col: "LocationType"
      time_col: "Days"

    criteria:
      - "All location types present (source, destination)"
      - "All timepoints present (Days 0, 1, 3, 6)"
      - "Exactly 8 rows per file (2 location types x 4 timepoints)"
      - "No duplicate rows (LocationType x Days combinations unique)"
      - "No NaN values in any column"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        message: "str"
        missing_domains: "List[str]"
        missing_groups: "List[str]"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step07_prepare_trajectory_plot_data.log"
      invoke: "g_debug (master invokes)"

    description: "Validate plot data has all location types and timepoints (completeness check for Decision D069 dual-scale plots)"
    source_reference: "tools_inventory.md section 'Module: tools.validation' - validate_plot_data_completeness"


# =============================================================================
# SUMMARY
# =============================================================================

summary:
  analysis_tools_count: 8
  validation_tools_count: 8
  total_unique_tools: 16
  mandatory_decisions_embedded: ["D039", "D068", "D069", "D070"]
  notes:
    - "Each analysis tool documented ONCE (deduplication across steps)"
    - "calibrate_irt used in Step 1 AND Step 3 (Pass 1 and Pass 2)"
    - "Standard library functions (pandas.merge) included for completeness"
    - "All validation tools paired with analysis tools (architectural requirement)"
    - "Decision D039: 2-pass IRT purification (calibrate_irt -> filter_items_by_quality -> calibrate_irt)"
    - "Decision D068: Dual p-value reporting (validate_contrasts_d068 enforces uncorrected + Bonferroni)"
    - "Decision D069: Dual-scale trajectory plots (convert_theta_to_probability for probability scale)"
    - "Decision D070: TSVR_hours as time variable (merge_theta_tsvr includes TSVR, not nominal days)"


# =============================================================================
# VERSION HISTORY
# =============================================================================

# v1.0 (2025-12-04): Initial tool catalog created for RQ 5.5.1
#   - 8 analysis tools: extraction, IRT (x2 passes), purification, merge, LMM, contrasts (x2), plot prep
#   - 8 validation tools: data format, IRT convergence, IRT parameters, LMM convergence, D068, effect sizes, probability range, plot completeness
#   - 16 total unique tools (deduplication: calibrate_irt used in Step 1 and Step 3)
#   - All 4 mandatory decisions embedded (D039, D068, D069, D070)


# =============================================================================
# End of Tool Catalog
# =============================================================================
