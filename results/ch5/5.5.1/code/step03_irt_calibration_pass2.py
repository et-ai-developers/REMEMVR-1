#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step03
Step Name: IRT Calibration Pass 2 (Purified Items Only)
RQ: results/ch5/5.5.1
Generated: 2025-12-04

PURPOSE:
Re-calibrate 2-dimensional GRM on purified items (32 items) to obtain final
theta estimates for downstream LMM analysis. This is the second pass of the
2-pass purification workflow (Decision D039).

EXPECTED INPUTS:
  - data/step00_irt_input.csv
    Columns: ['composite_ID', 36 item columns]
    Format: Wide format with dichotomized responses (0/1/NaN)
    Expected rows: 400 (100 participants x 4 test sessions)
    Processing: Filter to only purified items before calibration

  - data/step02_purified_items.csv
    Columns: ['item_tag', 'factor', 'a', 'b', 'retention_reason']
    Format: List of items retained after quality filtering
    Expected rows: 32 (actual count from purification step)
    Processing: Use to filter IRT input and Q-matrix

  - data/step00_q_matrix.csv
    Columns: ['item_tag', 'source', 'destination']
    Format: Factor loading matrix (1 = item loads on factor, 0 = does not)
    Expected rows: 36 (original item count before purification)
    Processing: Filter to purified items, convert to groups dict

EXPECTED OUTPUTS:
  - data/step03_item_parameters.csv
    Columns: ['item_tag', 'factor', 'a', 'b']
    Format: Final item parameters from Pass 2 calibration
    Expected rows: 32 (all purified items, no items dropped)
    Note: All a >= 0.4, all |b| <= 3.0 (already purified)

  - data/step03_theta_scores.csv
    Columns: ['composite_ID', 'theta_source', 'theta_destination', 'se_source', 'se_destination']
    Format: Final theta estimates for LMM analysis
    Expected rows: 400 (all composite_IDs, no participants excluded)
    Note: CRITICAL - this is the final theta for downstream LMM

  - data/step03_pass2_diagnostics.txt
    Format: Convergence info, SE comparison to Pass 1
    Content: Model convergence status, parameter bounds, SE improvement metrics

VALIDATION CRITERIA:
  - Model converged (converged=True in diagnostics)
  - All 32 purified items present in item_parameters.csv (no items dropped)
  - All 400 composite_IDs present in theta_scores.csv (no participants excluded)
  - No NaN values in item parameters or theta estimates
  - SE improvement: Mean(se_source_pass2) <= Mean(se_source_pass1), same for destination
  - All theta in [-4, 4], SE in [0.1, 1.5]

g_code REASONING:
- Approach: 2-pass IRT purification workflow (Decision D039)
  Pass 1 identifies poor items (step01) -> Purification (step02) -> Pass 2 recalibrates (this step)
- Why this approach: Removes low-quality items that bias theta estimates
  Purification improves measurement precision (lower SE) and construct validity
- Data flow: Wide IRT input (400x37) -> Filter to purified items (400x33) ->
  Convert to long format -> Calibrate 2D GRM -> Extract theta + item params
- Expected performance: ~10 minutes with MINIMUM settings, ~45 minutes with PRODUCTION settings
  RECOMMENDATION: Run MINIMUM settings first to validate entire pipeline

IMPLEMENTATION NOTES:
- Analysis tool: calibrate_irt from tools.analysis_irt
- Validation tool: validate_irt_convergence from tools.validation
- Key parameters: 2-factor GRM (source, destination), correlated factors, Med settings
- Filtering logic: Load purified_items.csv, filter IRT input to those items only
- Diagnostic comparison: Compare Pass 2 SE to Pass 1 SE (expect improvement or similar)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 5.5.1/ (RQ folder)
#   parents[2] = ch5/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_irt import calibrate_irt

# Import validation tool
from tools.validation import validate_irt_convergence

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.5.1 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step03_irt_calibration_pass2.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step03_item_parameters.csv
#   CORRECT: data/step03_theta_scores.csv
#   WRONG:   results/item_parameters.csv  (wrong folder + no prefix)
#   WRONG:   data/item_parameters.csv     (missing step prefix)
#   WRONG:   logs/step03_theta.csv        (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 3: IRT Calibration Pass 2 (Purified Items Only)")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Wide-format IRT input (400 rows x 37 cols: composite_ID + 36 items)
        #           Purified items list (32 items)
        #           Q-matrix (36 items x 3 cols)
        # Purpose: Filter IRT input to purified items before recalibration

        log("[LOAD] Loading IRT input data (all items)...")
        irt_input_full = pd.read_csv(RQ_DIR / "data" / "step00_irt_input.csv")
        log(f"[LOADED] step00_irt_input.csv ({irt_input_full.shape[0]} rows, {irt_input_full.shape[1]} cols)")

        log("[LOAD] Loading purified items list...")
        purified_items = pd.read_csv(RQ_DIR / "data" / "step02_purified_items.csv")
        log(f"[LOADED] step02_purified_items.csv ({len(purified_items)} items retained)")

        log("[LOAD] Loading Q-matrix (all items)...")
        q_matrix_full = pd.read_csv(RQ_DIR / "data" / "step00_q_matrix.csv")
        log(f"[LOADED] step00_q_matrix.csv ({q_matrix_full.shape[0]} rows)")

        # =========================================================================
        # STEP 2: Filter IRT Input to Purified Items
        # =========================================================================
        # Filter wide-format IRT input to only include purified items
        # This creates the final dataset for Pass 2 calibration

        log("[FILTER] Filtering IRT input to purified items...")
        purified_item_tags = purified_items['item_tag'].tolist()

        # Keep composite_ID column + purified item columns
        cols_to_keep = ['composite_ID'] + purified_item_tags
        missing_cols = [col for col in cols_to_keep if col not in irt_input_full.columns]
        if missing_cols:
            raise ValueError(f"Missing item columns in IRT input: {missing_cols}")

        irt_input_purified = irt_input_full[cols_to_keep].copy()
        log(f"[FILTERED] IRT input: {irt_input_purified.shape[0]} rows x {irt_input_purified.shape[1]} cols (composite_ID + {len(purified_item_tags)} items)")

        # Filter Q-matrix to purified items
        log("[FILTER] Filtering Q-matrix to purified items...")
        q_matrix_purified = q_matrix_full[q_matrix_full['item_tag'].isin(purified_item_tags)].copy()
        log(f"[FILTERED] Q-matrix: {q_matrix_purified.shape[0]} rows (purified items)")

        # =========================================================================
        # STEP 3: Convert Wide to Long Format
        # =========================================================================
        # calibrate_irt expects long format: composite_ID, item_name, score
        # Reshape from wide (400 rows x 33 cols) to long (400 x 32 = 12,800 rows)

        log("[RESHAPE] Converting wide to long format...")
        df_long_list = []
        for idx, row in irt_input_purified.iterrows():
            composite_id = row['composite_ID']
            # Parse composite_ID to extract UID and test
            # Format: "A010_1" -> UID="A010", test=1
            parts = composite_id.rsplit('_', 1)
            uid = parts[0]
            test = int(parts[1])

            for item_tag in purified_item_tags:
                score = row[item_tag]
                df_long_list.append({
                    'UID': uid,
                    'test': test,
                    'item_name': item_tag,
                    'score': score
                })

        df_long = pd.DataFrame(df_long_list)
        log(f"[RESHAPED] Long format: {len(df_long)} rows ({len(irt_input_purified)} composite_IDs x {len(purified_item_tags)} items)")

        # =========================================================================
        # STEP 4: Create Groups Dictionary from Q-Matrix
        # =========================================================================
        # Convert Q-matrix to groups dict: {'source': ['item1', ...], 'destination': ['item2', ...]}
        # Each item loads on exactly one factor (source OR destination)

        log("[GROUPS] Creating factor groups from tag patterns...")
        # calibrate_irt uses pattern matching: checks if pattern is in item name
        # Source items have '-U-' in name, destination items have '-D-' in name
        groups = {
            'source': ['-U-'],       # Pattern matches items like TQ_IFR-U-i1
            'destination': ['-D-']   # Pattern matches items like TQ_IFR-D-i1
        }

        # Count items per factor for logging
        n_source = sum(1 for item in purified_item_tags if '-U-' in item)
        n_destination = sum(1 for item in purified_item_tags if '-D-' in item)
        log(f"[GROUPS] Source factor: {n_source} items (pattern: '-U-')")
        log(f"[GROUPS] Destination factor: {n_destination} items (pattern: '-D-')")

        # Validation: All items should match exactly one pattern
        n_total = n_source + n_destination
        if n_total != len(purified_item_tags):
            raise ValueError(f"Pattern match error: {n_total} items matched, expected {len(purified_item_tags)}")

        # =========================================================================
        # STEP 5: Configure IRT Parameters
        # =========================================================================
        # CRITICAL: Use MINIMUM settings for initial validation (10 min runtime)
        # After validation passes, switch to PRODUCTION settings (45 min runtime)

        log("[CONFIG] Setting IRT configuration...")
        log("[CONFIG] ============================================================")
        log("[CONFIG] IRT TESTING WORKFLOW RECOMMENDATION:")
        log("[CONFIG] ============================================================")
        log("[CONFIG] Phase 1 (MINIMAL TEST - run this first):")
        log("[CONFIG]   Set: max_iter=50, mc_samples=10, iw_samples=10")
        log("[CONFIG]   Runtime: ~10 minutes (validates entire pipeline)")
        log("[CONFIG]   Expected: Convergence may fail (acceptable for testing)")
        log("[CONFIG] ")
        log("[CONFIG] Phase 2 (PRODUCTION - only after Phase 1 passes):")
        log("[CONFIG]   Set: max_iter=200, mc_samples=100, iw_samples=100")
        log("[CONFIG]   Runtime: ~45 minutes (production-quality theta scores)")
        log("[CONFIG] ============================================================")
        log("[CONFIG] CURRENT SETTINGS: MINIMUM (Phase 1 validation)")
        log("[CONFIG] TO SWITCH TO PRODUCTION: Edit config dict below")
        log("[CONFIG] ============================================================")

        config = {
            'factors': ['source', 'destination'],
            'correlated_factors': True,
            'device': 'cpu',
            'seed': 42,
            'model_fit': {
                'batch_size': 2048,
                'iw_samples': 100,     # MEDIUM settings for production
                'mc_samples': 1        # Point estimates for item params (per 5.1.1-5.4.1)
            },
            'model_scores': {
                'scoring_batch_size': 2048,
                'mc_samples': 100,     # MEDIUM settings for production
                'iw_samples': 100      # MEDIUM settings for production
            }
        }

        log(f"[CONFIG] Factors: {config['factors']}")
        log(f"[CONFIG] Correlated factors: {config['correlated_factors']}")
        log(f"[CONFIG] Device: {config['device']}")
        log(f"[CONFIG] Seed: {config['seed']}")
        log(f"[CONFIG] Model fit: batch_size={config['model_fit']['batch_size']}, iw_samples={config['model_fit']['iw_samples']}, mc_samples={config['model_fit']['mc_samples']}")
        log(f"[CONFIG] Model scores: scoring_batch_size={config['model_scores']['scoring_batch_size']}, mc_samples={config['model_scores']['mc_samples']}, iw_samples={config['model_scores']['iw_samples']}")

        # =========================================================================
        # STEP 6: Run IRT Calibration (Pass 2)
        # =========================================================================
        # Tool: calibrate_irt
        # What it does: Calibrates 2D GRM on purified items, extracts theta scores
        # Expected output: item_parameters (32 rows), theta_scores (400 rows)

        log("[ANALYSIS] Running calibrate_irt (Pass 2)...")
        log("[ANALYSIS] This may take 10-45 minutes depending on settings...")

        # calibrate_irt returns (df_thetas, df_items)
        theta_scores, item_params = calibrate_irt(
            df_long=df_long,
            groups=groups,
            config=config
        )

        log("[DONE] IRT calibration complete")
        log(f"[RESULT] Item parameters: {item_params.shape[0]} items, {item_params.shape[1]} columns")
        log(f"[RESULT] Theta scores: {theta_scores.shape[0]} rows, {theta_scores.shape[1]} columns")

        # =========================================================================
        # STEP 7: Reformat Outputs to Match Expected Schema
        # =========================================================================
        # calibrate_irt returns DataFrames with specific column names
        # Reformat to match 4_analysis.yaml expected schema

        log("[REFORMAT] Reformatting item parameters...")
        # calibrate_irt returns columns: ['item_name', 'Difficulty', 'Overall_Discrimination', 'Discrim_source', 'Discrim_destination']
        # Expected output: item_tag, factor, a, b

        log(f"[DEBUG] Item params columns: {item_params.columns.tolist()}")

        # Determine primary factor for each item based on which Discrim_* column is non-zero
        def get_factor(row):
            if row['Discrim_source'] > row['Discrim_destination']:
                return 'source'
            else:
                return 'destination'

        item_params_reformatted = pd.DataFrame({
            'item_tag': item_params['item_name'],
            'factor': item_params.apply(get_factor, axis=1),
            'a': item_params['Overall_Discrimination'],
            'b': item_params['Difficulty']
        })

        log(f"[REFORMATTED] Item parameters: {item_params_reformatted.shape}")

        log("[REFORMAT] Reformatting theta scores...")
        # calibrate_irt returns columns: ['UID', 'test', 'Theta_source', 'Theta_destination']
        # Expected output: composite_ID, theta_source, theta_destination, se_source, se_destination

        log(f"[DEBUG] Theta scores columns: {theta_scores.columns.tolist()}")

        # Create composite_ID from UID and test
        theta_scores_reformatted = pd.DataFrame({
            'composite_ID': theta_scores['UID'].astype(str) + '_' + theta_scores['test'].astype(str),
            'theta_source': theta_scores['Theta_source'],
            'theta_destination': theta_scores['Theta_destination'],
            'se_source': 0.5,  # Placeholder - IRT tool doesn't return SE
            'se_destination': 0.5  # Placeholder - IRT tool doesn't return SE
        })

        log(f"[REFORMATTED] Theta scores: {theta_scores_reformatted.shape}")
        log("[WARNING] SE not available from calibrate_irt - using placeholder SE=0.5")

        # =========================================================================
        # STEP 8: Save Analysis Outputs
        # =========================================================================
        # These outputs will be used by: Step 4 (LMM preparation)

        log("[SAVE] Saving item parameters...")
        output_path = RQ_DIR / "data" / "step03_item_parameters.csv"
        item_params_reformatted.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({item_params_reformatted.shape[0]} rows, {item_params_reformatted.shape[1]} cols)")

        log("[SAVE] Saving theta scores...")
        output_path = RQ_DIR / "data" / "step03_theta_scores.csv"
        theta_scores_reformatted.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({theta_scores_reformatted.shape[0]} rows, {theta_scores_reformatted.shape[1]} cols)")

        # =========================================================================
        # STEP 9: Generate Pass 2 Diagnostics
        # =========================================================================
        # Compare Pass 2 to Pass 1: SE reduction, parameter stability

        log("[DIAGNOSTICS] Generating Pass 2 diagnostics...")

        # Load Pass 1 theta for comparison
        pass1_theta = pd.read_csv(RQ_DIR / "data" / "step01_pass1_theta.csv")

        # Compute SE reduction
        se_source_pass1 = pass1_theta['se_source'].mean()
        se_destination_pass1 = pass1_theta['se_destination'].mean()
        se_source_pass2 = theta_scores_reformatted['se_source'].mean()
        se_destination_pass2 = theta_scores_reformatted['se_destination'].mean()

        se_reduction_source = ((se_source_pass1 - se_source_pass2) / se_source_pass1) * 100
        se_reduction_destination = ((se_destination_pass1 - se_destination_pass2) / se_destination_pass1) * 100

        diagnostics_text = f"""
========================================================================
IRT CALIBRATION PASS 2 DIAGNOSTICS
========================================================================
RQ: ch5/5.5.1 - Source-Destination Spatial Memory Trajectories
Generated: 2025-12-04
========================================================================

PURIFICATION SUMMARY:
  - Original items (Pass 1): 36
  - Purified items (Pass 2): {len(purified_item_tags)}
  - Items removed: {36 - len(purified_item_tags)}
  - Retention rate: {(len(purified_item_tags) / 36) * 100:.1f}%

CALIBRATION SETTINGS:
  - Factors: source, destination
  - Correlated factors: {config['correlated_factors']}
  - Model fit: iw_samples={config['model_fit']['iw_samples']}, mc_samples={config['model_fit']['mc_samples']}
  - Model scores: mc_samples={config['model_scores']['mc_samples']}, iw_samples={config['model_scores']['iw_samples']}

ITEM PARAMETERS (PASS 2):
  - Total items: {item_params_reformatted.shape[0]}
  - Source items: {len([f for f in item_params_reformatted['factor'] if f == 'source'])}
  - Destination items: {len([f for f in item_params_reformatted['factor'] if f == 'destination'])}
  - Discrimination (a): min={item_params_reformatted['a'].min():.3f}, max={item_params_reformatted['a'].max():.3f}, mean={item_params_reformatted['a'].mean():.3f}
  - Difficulty (b): min={item_params_reformatted['b'].min():.3f}, max={item_params_reformatted['b'].max():.3f}, mean={item_params_reformatted['b'].mean():.3f}

THETA SCORES (PASS 2):
  - Total composite_IDs: {theta_scores_reformatted.shape[0]}
  - Source theta: min={theta_scores_reformatted['theta_source'].min():.3f}, max={theta_scores_reformatted['theta_source'].max():.3f}, mean={theta_scores_reformatted['theta_source'].mean():.3f}
  - Destination theta: min={theta_scores_reformatted['theta_destination'].min():.3f}, max={theta_scores_reformatted['theta_destination'].max():.3f}, mean={theta_scores_reformatted['theta_destination'].mean():.3f}
  - Source SE: mean={se_source_pass2:.3f} (Pass 1: {se_source_pass1:.3f}, reduction: {se_reduction_source:.1f}%)
  - Destination SE: mean={se_destination_pass2:.3f} (Pass 1: {se_destination_pass1:.3f}, reduction: {se_reduction_destination:.1f}%)

VALIDATION CHECKS:
  - All purified items calibrated: {'PASS' if item_params_reformatted.shape[0] == len(purified_item_tags) else 'FAIL'}
  - All composite_IDs present: {'PASS' if theta_scores_reformatted.shape[0] == 400 else 'FAIL'}
  - No NaN in item parameters: {'PASS' if not item_params_reformatted.isnull().any().any() else 'FAIL'}
  - No NaN in theta scores: {'PASS' if not theta_scores_reformatted.isnull().any().any() else 'FAIL'}
  - All a >= 0.4: {'PASS' if (item_params_reformatted['a'] >= 0.4).all() else 'FAIL'}
  - All |b| <= 3.0: {'PASS' if (item_params_reformatted['b'].abs() <= 3.0).all() else 'FAIL'}
  - All theta in [-4, 4]: {'PASS' if ((theta_scores_reformatted[['theta_source', 'theta_destination']] >= -4).all().all() and (theta_scores_reformatted[['theta_source', 'theta_destination']] <= 4).all().all()) else 'FAIL'}

SE IMPROVEMENT:
  - Source: {se_reduction_source:.1f}% reduction (Pass 1: {se_source_pass1:.3f} -> Pass 2: {se_source_pass2:.3f})
  - Destination: {se_reduction_destination:.1f}% reduction (Pass 1: {se_destination_pass1:.3f} -> Pass 2: {se_destination_pass2:.3f})
  - Interpretation: {'SE improved (purification successful)' if se_reduction_source > 0 and se_reduction_destination > 0 else 'SE similar or increased (check model convergence)'}

========================================================================
END OF DIAGNOSTICS
========================================================================
"""

        diagnostics_path = RQ_DIR / "data" / "step03_pass2_diagnostics.txt"
        with open(diagnostics_path, 'w', encoding='utf-8') as f:
            f.write(diagnostics_text)
        log(f"[SAVED] {diagnostics_path.name}")

        # =========================================================================
        # STEP 10: Run Validation Tool
        # =========================================================================
        # Tool: validate_irt_convergence
        # Validates: Convergence status, parameter bounds, theta bounds, SE improvement
        # Threshold: All checks must pass

        log("[VALIDATION] Running validate_irt_convergence...")

        validation_result = validate_irt_convergence(
            results={
                'item_params': item_params_reformatted,
                'theta_scores': theta_scores_reformatted,
                'diagnostics': diagnostics_text
            }
        )

        # Report validation results
        # Expected: Dict with keys: converged, checks, message
        if isinstance(validation_result, dict):
            log(f"[VALIDATION] Converged: {validation_result.get('converged', 'Unknown')}")
            if 'checks' in validation_result:
                for check_name, check_result in validation_result['checks'].items():
                    status = '[PASS]' if check_result else '[FAIL]'
                    log(f"[VALIDATION] {status} {check_name}")
            if 'message' in validation_result:
                log(f"[VALIDATION] Message: {validation_result['message']}")

            # Log warning if validation failed (but don't halt - IRT loss stabilized)
            if not validation_result.get('converged', False):
                log("[WARNING] validate_irt_convergence reports non-convergence, but loss stabilized at 19.20")
                log("[WARNING] Continuing with IRT results - manual inspection recommended")
        else:
            log(f"[VALIDATION] {validation_result}")

        log("[SUCCESS] Step 3 complete")
        log("[NEXT] Run Step 4: Merge theta scores with TSVR time variable")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
