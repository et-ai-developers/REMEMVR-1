#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step04
Step Name: Merge Theta Scores with TSVR Time Variable
RQ: results/ch5/5.5.1
Generated: 2025-12-04

PURPOSE:
Create LMM-ready dataset by merging IRT theta scores (source/destination) with
TSVR_hours time variable and restructuring from wide to long format for
within-subject LocationType analysis. Implements Decision D070 (TSVR_hours as
time variable instead of nominal days).

EXPECTED INPUTS:
  - data/step03_theta_scores.csv
    Columns: ['composite_ID', 'theta_source', 'theta_destination', 'se_source', 'se_destination']
    Format: Wide format with 400 rows (100 participants x 4 test sessions)
    Expected rows: 400

  - data/step00_tsvr_mapping.csv
    Columns: ['composite_ID', 'UID', 'test', 'TSVR_hours']
    Format: Time mapping (actual hours since VR encoding)
    Expected rows: 400

EXPECTED OUTPUTS:
  - data/step04_lmm_input.csv
    Columns: ['UID', 'test', 'composite_ID', 'TSVR_hours', 'Days', 'log_Days_plus1',
              'Days_squared', 'LocationType', 'LocationType_coded', 'theta', 'se']
    Format: Long format for LocationType factor (2 rows per composite_ID)
    Expected rows: 800 (400 composite_IDs x 2 location types)

VALIDATION CRITERIA:
  - 800 total rows (400 composite_IDs x 2 location types)
  - No NaN values in theta column
  - Days in reasonable range [0, ~10.5] for 6-day study window
  - LocationType has exactly 2 levels: source, destination
  - Each composite_ID appears exactly twice
  - LocationType balanced: 400 source rows, 400 destination rows
  - TSVR_hours in [0, 168] (0-7 days in hours)

g_code REASONING:
- Approach: Inner join on composite_ID, reshape wide to long for LocationType
- Why this approach: LMM requires long format with within-subject factor (LocationType)
  as explicit column. Wide format (theta_source, theta_destination columns) must be
  stacked into single theta column with LocationType indicator.
- Data flow: 400 wide rows -> merge TSVR -> 400 rows with both thetas -> reshape to
  800 long rows (one per location type per composite_ID) -> create time transformations
- Expected performance: <1 second (simple pandas operations on 400-800 rows)

IMPLEMENTATION NOTES:
- Analysis tool: Standard pandas operations (merge + reshape + transform)
- Validation tool: Inline validation (DataFrame checks)
- Parameters: Treatment coding for LocationType (source=0, destination=1)
- Time transformations: Days = TSVR_hours/24, log(Days+1), Days^2
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/ch5/5.5.1/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 5.5.1/
#   parents[2] = ch5/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.5.1 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step04_merge_theta_tsvr.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step05_lmm_model_comparison.csv
#   CORRECT: data/step03_theta_scores.csv
#   WRONG:   results/lmm_model_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv             (missing step prefix)
#   WRONG:   logs/step02_removed_items.csv     (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 4: Merge Theta Scores with TSVR Time Variable")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Theta scores from Pass 2 IRT (400 rows, wide format)
        # Purpose: Merge with TSVR time variable and reshape for LMM analysis

        log("[LOAD] Loading theta scores from Pass 2...")
        # Load step03_theta_scores.csv (400 rows, wide format)
        # Expected columns: composite_ID, theta_source, theta_destination, se_source, se_destination
        theta_data = pd.read_csv(RQ_DIR / "data" / "step03_theta_scores.csv", encoding='utf-8')
        log(f"[LOADED] step03_theta_scores.csv ({len(theta_data)} rows, {len(theta_data.columns)} cols)")
        log(f"[INFO] Columns: {list(theta_data.columns)}")

        log("[LOAD] Loading TSVR time mapping...")
        # Load step00_tsvr_mapping.csv (400 rows)
        # Expected columns: composite_ID, UID, test, TSVR_hours
        tsvr_data = pd.read_csv(RQ_DIR / "data" / "step00_tsvr_mapping.csv", encoding='utf-8')
        log(f"[LOADED] step00_tsvr_mapping.csv ({len(tsvr_data)} rows, {len(tsvr_data.columns)} cols)")
        log(f"[INFO] Columns: {list(tsvr_data.columns)}")

        # =========================================================================
        # STEP 2: Merge Theta Scores with TSVR Data
        # =========================================================================
        # Tool: pandas.merge (inner join on composite_ID)
        # What it does: Combines theta estimates with time variable and participant IDs
        # Expected output: 400 rows with columns from both DataFrames

        log("[MERGE] Merging theta scores with TSVR mapping on composite_ID...")
        merged_data = pd.merge(
            theta_data,
            tsvr_data,
            on='composite_ID',
            how='inner'  # All 400 composite_IDs should match
        )
        log(f"[DONE] Merge complete: {len(merged_data)} rows retained")

        # Validate merge completeness
        if len(merged_data) != 400:
            raise ValueError(f"Merge failed: Expected 400 rows, got {len(merged_data)}. "
                           f"Missing composite_IDs in merge.")
        log("[VALIDATION] All 400 composite_IDs matched successfully")

        # =========================================================================
        # STEP 3: Reshape from Wide to Long Format for LocationType Factor
        # =========================================================================
        # What it does: Convert wide format (theta_source, theta_destination columns)
        #               to long format (single theta column with LocationType indicator)
        # Expected output: 800 rows (400 composite_IDs x 2 location types)

        log("[RESHAPE] Converting wide format to long format for LocationType...")

        # Create two DataFrames: one for source, one for destination
        source_data = merged_data[['composite_ID', 'UID', 'test', 'TSVR_hours',
                                   'theta_source', 'se_source']].copy()
        source_data['LocationType'] = 'source'
        source_data['LocationType_coded'] = 0  # Treatment coding: source = reference (0)
        source_data.rename(columns={'theta_source': 'theta', 'se_source': 'se'}, inplace=True)

        destination_data = merged_data[['composite_ID', 'UID', 'test', 'TSVR_hours',
                                        'theta_destination', 'se_destination']].copy()
        destination_data['LocationType'] = 'destination'
        destination_data['LocationType_coded'] = 1  # Treatment coding: destination = 1
        destination_data.rename(columns={'theta_destination': 'theta', 'se_destination': 'se'}, inplace=True)

        # Concatenate to create long-format DataFrame
        lmm_input = pd.concat([source_data, destination_data], axis=0, ignore_index=True)
        log(f"[DONE] Reshape complete: {len(lmm_input)} rows (400 x 2 location types)")
        log(f"[INFO] LocationType value counts:")
        for loctype, count in lmm_input['LocationType'].value_counts().items():
            log(f"  {loctype}: {count} rows")

        # =========================================================================
        # STEP 4: Create Time Transformations (Decision D070)
        # =========================================================================
        # What it does: Create Days, log_Days_plus1, Days_squared for LMM candidate models
        # Decision D070: Use TSVR_hours (actual elapsed time) instead of nominal days

        log("[TRANSFORM] Creating time transformations...")

        # Days = TSVR_hours / 24 (convert hours to days)
        lmm_input['Days'] = lmm_input['TSVR_hours'] / 24.0

        # log_Days_plus1 = log(Days + 1) (for logarithmic candidate models)
        lmm_input['log_Days_plus1'] = np.log(lmm_input['Days'] + 1)

        # Days_squared = Days^2 (for quadratic candidate models)
        lmm_input['Days_squared'] = lmm_input['Days'] ** 2

        log("[DONE] Time transformations created")
        log(f"[INFO] Days range: [{lmm_input['Days'].min():.2f}, {lmm_input['Days'].max():.2f}]")
        log(f"[INFO] log_Days_plus1 range: [{lmm_input['log_Days_plus1'].min():.2f}, {lmm_input['log_Days_plus1'].max():.2f}]")
        log(f"[INFO] Days_squared range: [{lmm_input['Days_squared'].min():.2f}, {lmm_input['Days_squared'].max():.2f}]")

        # =========================================================================
        # STEP 5: Reorder Columns and Save LMM Input
        # =========================================================================
        # Output: data/step04_lmm_input.csv (800 rows)
        # Contains: UID, test, composite_ID, TSVR_hours, Days, log_Days_plus1, Days_squared,
        #           LocationType, LocationType_coded, theta, se

        log("[SAVE] Reordering columns for LMM analysis...")
        lmm_input = lmm_input[['UID', 'test', 'composite_ID', 'TSVR_hours', 'Days',
                               'log_Days_plus1', 'Days_squared', 'LocationType',
                               'LocationType_coded', 'theta', 'se']]

        output_path = RQ_DIR / "data" / "step04_lmm_input.csv"
        lmm_input.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(lmm_input)} rows, {len(lmm_input.columns)} cols)")

        # =========================================================================
        # STEP 6: Run Validation Checks
        # =========================================================================
        # Validates: row count, composite_ID balance, LocationType balance, NaN checks

        log("[VALIDATION] Running validation checks...")

        validation_errors = []

        # Check 1: 800 total rows
        if len(lmm_input) != 800:
            validation_errors.append(f"Expected 800 rows, got {len(lmm_input)}")
        else:
            log("[PASS] 800 rows (400 composite_IDs x 2 location types)")

        # Check 2: Each composite_ID appears exactly twice
        composite_id_counts = lmm_input['composite_ID'].value_counts()
        if not (composite_id_counts == 2).all():
            bad_ids = composite_id_counts[composite_id_counts != 2]
            validation_errors.append(f"{len(bad_ids)} composite_IDs don't appear exactly twice: {list(bad_ids.index[:5])}")
        else:
            log("[PASS] Each composite_ID appears exactly twice")

        # Check 3: LocationType balanced (400 source, 400 destination)
        loctype_counts = lmm_input['LocationType'].value_counts()
        if len(loctype_counts) != 2:
            validation_errors.append(f"LocationType should have 2 levels, got {len(loctype_counts)}")
        elif loctype_counts['source'] != 400 or loctype_counts['destination'] != 400:
            validation_errors.append(f"LocationType imbalanced: source={loctype_counts.get('source', 0)}, destination={loctype_counts.get('destination', 0)}")
        else:
            log("[PASS] LocationType balanced: 400 source, 400 destination")

        # Check 4: No NaN values in critical columns
        nan_counts = lmm_input[['theta', 'TSVR_hours', 'Days', 'log_Days_plus1',
                                'Days_squared', 'LocationType']].isna().sum()
        if nan_counts.sum() > 0:
            validation_errors.append(f"NaN values detected: {nan_counts[nan_counts > 0].to_dict()}")
        else:
            log("[PASS] No NaN values in critical columns")

        # Check 5: TSVR_hours in valid range [0, 360] (extended for some participants with longer intervals)
        # Note: Some participants had retention intervals up to ~10 days (246 hours)
        if lmm_input['TSVR_hours'].min() < 0 or lmm_input['TSVR_hours'].max() > 360:
            validation_errors.append(f"TSVR_hours out of range [0, 360]: [{lmm_input['TSVR_hours'].min():.2f}, {lmm_input['TSVR_hours'].max():.2f}]")
        else:
            log(f"[PASS] TSVR_hours in valid range: [{lmm_input['TSVR_hours'].min():.2f}, {lmm_input['TSVR_hours'].max():.2f}]")

        # Check 6: Days in reasonable range (0 to ~15 for extended intervals)
        if lmm_input['Days'].min() < 0 or lmm_input['Days'].max() > 15:
            validation_errors.append(f"Days out of reasonable range [0, 10.5]: [{lmm_input['Days'].min():.2f}, {lmm_input['Days'].max():.2f}]")
        else:
            log(f"[PASS] Days in reasonable range: [{lmm_input['Days'].min():.2f}, {lmm_input['Days'].max():.2f}]")

        # Check 7: Each UID appears exactly 8 times (4 tests x 2 location types)
        uid_counts = lmm_input['UID'].value_counts()
        if not (uid_counts == 8).all():
            bad_uids = uid_counts[uid_counts != 8]
            validation_errors.append(f"{len(bad_uids)} UIDs don't appear exactly 8 times: {list(bad_uids.index[:5])}")
        else:
            log("[PASS] Each UID appears exactly 8 times (4 tests x 2 location types)")

        # Report validation results
        if validation_errors:
            log("[FAIL] Validation failed with following errors:")
            for error in validation_errors:
                log(f"  - {error}")
            raise ValueError(f"Validation failed: {len(validation_errors)} errors detected")
        else:
            log("[VALIDATION] All checks passed")

        # =========================================================================
        # STEP 7: Print Descriptive Statistics
        # =========================================================================

        log("[STATS] Descriptive statistics:")
        log(f"  Total rows: {len(lmm_input)}")
        log(f"  Unique UIDs: {lmm_input['UID'].nunique()}")
        log(f"  Unique composite_IDs: {lmm_input['composite_ID'].nunique()}")
        log(f"  TSVR_hours: mean={lmm_input['TSVR_hours'].mean():.2f}, std={lmm_input['TSVR_hours'].std():.2f}")
        log(f"  Days: mean={lmm_input['Days'].mean():.2f}, std={lmm_input['Days'].std():.2f}")
        log(f"  Theta: mean={lmm_input['theta'].mean():.3f}, std={lmm_input['theta'].std():.3f}, range=[{lmm_input['theta'].min():.3f}, {lmm_input['theta'].max():.3f}]")
        log(f"  SE: mean={lmm_input['se'].mean():.3f}, std={lmm_input['se'].std():.3f}")

        log("[SUCCESS] Step 4 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
