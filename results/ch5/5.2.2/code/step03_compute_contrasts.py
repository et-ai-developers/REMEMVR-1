#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step03
Step Name: Compute Planned Contrasts with Bonferroni Correction (Decision D068)
RQ: results/ch5/5.2.2
Generated: 2025-11-23

PURPOSE:
Compute 6 planned contrasts with dual p-value reporting (D068). These contrasts
test specific hypotheses about domain differences in forgetting slopes within
segments (Early/Late) and differences in slope changes between segments.

EXPECTED INPUTS:
- data/step01_piecewise_lmm_model.pkl
  Format: Pickle of fitted MixedLMResults object
  Description: Fitted piecewise LMM model with 3-way interaction
- results/step02_segment_domain_slopes.csv
  Columns: segment, domain, slope, se, CI_lower, CI_upper
  Description: Pre-computed slopes for each segment x domain combination

EXPECTED OUTPUTS:
- results/step03_planned_contrasts.csv
  Columns: comparison, beta, se, z, p_uncorrected, alpha_corrected, p_corrected, sig_uncorrected, sig_corrected
  Description: 6 planned contrasts with dual p-values (D068)
- results/step03_effect_sizes.csv
  Columns: effect, f_squared, interpretation
  Description: Cohen's f-squared effect sizes for model terms

VALIDATION CRITERIA:
- Contrast count: 6 rows (6 planned contrasts)
- Dual p-values present: Both p_uncorrected and p_corrected columns
- Bonferroni alpha correct: alpha_corrected = 0.0083 (0.05/6)
- p-values valid: p_uncorrected in [0, 1]; p_corrected >= p_uncorrected
- Effect sizes computed: df_effect_sizes has rows with f_squared and interpretation

g_code REASONING:
- Approach: Use pre-computed slopes from step02 to compute contrasts via delta method.
  The 6 contrasts compare:
  C1: Where vs What slope in Early segment (spatial consolidation advantage)
  C2: When vs What slope in Early segment (temporal consolidation)
  C3: Where vs What slope in Late segment (decay comparison)
  C4: When vs What slope in Late segment (decay comparison)
  C5: Where slope change (Late-Early) vs What slope change (consolidation benefit)
  C6: When slope change (Late-Early) vs What slope change (consolidation benefit)

- Why this approach: Piecewise model with 3-way interaction requires computing linear
  combinations of fixed effects. Pre-computed slopes simplify contrast calculations.
  Dual p-value reporting (D068) shows both raw effects and Bonferroni-corrected results.

- Data flow: Load slopes -> Compute contrast estimates -> Delta method SE -> z-tests -> Dual p-values

- Expected performance: ~seconds (statistical computation only)

IMPLEMENTATION NOTES:
- Analysis tool: compute_contrasts_pairwise from tools.analysis_lmm (for basic contrasts)
- Effect sizes: compute_effect_sizes_cohens from tools.analysis_lmm
- Custom contrasts computed via delta method using pre-computed slopes
- Parameters: family_alpha=0.05, bonferroni_alpha=0.0083 (0.05/6)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback
from scipy import stats
from statsmodels.regression.mixed_linear_model import MixedLMResults

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (5 levels deep from project root)
# parents[4] = REMEMVR/ (code -> rqY -> chX -> results -> REMEMVR)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tools
from tools.analysis_lmm import compute_contrasts_pairwise, compute_effect_sizes_cohens

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.2.2
LOG_FILE = RQ_DIR / "logs" / "step03_compute_contrasts.log"

# Ensure log directory exists
LOG_FILE.parent.mkdir(parents=True, exist_ok=True)

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html, .csv for summary tables)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Contrast Computation Functions
# =============================================================================

def compute_slope_contrast(
    df_slopes: pd.DataFrame,
    domain1: str,
    domain2: str,
    segment: str
) -> Dict[str, float]:
    """
    Compute contrast between two domain slopes within a segment.

    Args:
        df_slopes: DataFrame with segment, domain, slope, se columns
        domain1: First domain (e.g., 'where')
        domain2: Second domain (reference, e.g., 'what')
        segment: Segment ('Early' or 'Late')

    Returns:
        Dict with beta, se, z, p_value
    """
    row1 = df_slopes[(df_slopes['segment'] == segment) & (df_slopes['domain'] == domain1)].iloc[0]
    row2 = df_slopes[(df_slopes['segment'] == segment) & (df_slopes['domain'] == domain2)].iloc[0]

    # Difference: domain1 - domain2
    beta = row1['slope'] - row2['slope']

    # Delta method SE (assuming independence): sqrt(se1^2 + se2^2)
    se = np.sqrt(row1['se']**2 + row2['se']**2)

    # z-statistic
    z = beta / se

    # Two-tailed p-value
    p_value = 2 * (1 - stats.norm.cdf(abs(z)))

    return {
        'beta': beta,
        'se': se,
        'z': z,
        'p_value': p_value
    }


def compute_slope_change_contrast(
    df_slopes: pd.DataFrame,
    domain1: str,
    domain2: str
) -> Dict[str, float]:
    """
    Compute contrast comparing slope changes (Late-Early) between two domains.

    Tests: (domain1_Late - domain1_Early) - (domain2_Late - domain2_Early)
    = domain1 consolidation benefit - domain2 consolidation benefit

    Args:
        df_slopes: DataFrame with segment, domain, slope, se columns
        domain1: First domain (e.g., 'where')
        domain2: Second domain (reference, e.g., 'what')

    Returns:
        Dict with beta, se, z, p_value
    """
    # Get slopes for both domains in both segments
    d1_early = df_slopes[(df_slopes['segment'] == 'Early') & (df_slopes['domain'] == domain1)].iloc[0]
    d1_late = df_slopes[(df_slopes['segment'] == 'Late') & (df_slopes['domain'] == domain1)].iloc[0]
    d2_early = df_slopes[(df_slopes['segment'] == 'Early') & (df_slopes['domain'] == domain2)].iloc[0]
    d2_late = df_slopes[(df_slopes['segment'] == 'Late') & (df_slopes['domain'] == domain2)].iloc[0]

    # Slope changes
    d1_change = d1_late['slope'] - d1_early['slope']  # domain1 Late - Early
    d2_change = d2_late['slope'] - d2_early['slope']  # domain2 Late - Early

    # Contrast: domain1 change - domain2 change
    beta = d1_change - d2_change

    # Delta method SE (assuming independence): sqrt(se1_early^2 + se1_late^2 + se2_early^2 + se2_late^2)
    se = np.sqrt(d1_early['se']**2 + d1_late['se']**2 + d2_early['se']**2 + d2_late['se']**2)

    # z-statistic
    z = beta / se

    # Two-tailed p-value
    p_value = 2 * (1 - stats.norm.cdf(abs(z)))

    return {
        'beta': beta,
        'se': se,
        'z': z,
        'p_value': p_value
    }


def compute_all_planned_contrasts(
    df_slopes: pd.DataFrame,
    family_alpha: float = 0.05
) -> pd.DataFrame:
    """
    Compute all 6 planned contrasts with dual p-value reporting (D068).

    Contrasts:
    C1: Where vs What slope in Early segment (spatial consolidation advantage)
    C2: When vs What slope in Early segment (temporal consolidation)
    C3: Where vs What slope in Late segment (decay comparison)
    C4: When vs What slope in Late segment (decay comparison)
    C5: Where slope change (Late-Early) vs What slope change (consolidation benefit)
    C6: When slope change (Late-Early) vs What slope change (consolidation benefit)

    Args:
        df_slopes: DataFrame with segment, domain, slope, se columns
        family_alpha: Family-wise alpha level (default: 0.05)

    Returns:
        DataFrame with contrast results and dual p-values
    """
    k = 6  # Number of contrasts
    alpha_corrected = family_alpha / k  # Bonferroni correction

    contrasts = []

    # C1: Where vs What in Early
    c1 = compute_slope_contrast(df_slopes, 'where', 'what', 'Early')
    c1['comparison'] = 'where-what_early'
    c1['description'] = 'Spatial consolidation advantage (Early)'
    contrasts.append(c1)

    # C2: When vs What in Early
    c2 = compute_slope_contrast(df_slopes, 'when', 'what', 'Early')
    c2['comparison'] = 'when-what_early'
    c2['description'] = 'Temporal consolidation (Early)'
    contrasts.append(c2)

    # C3: Where vs What in Late
    c3 = compute_slope_contrast(df_slopes, 'where', 'what', 'Late')
    c3['comparison'] = 'where-what_late'
    c3['description'] = 'Spatial decay comparison (Late)'
    contrasts.append(c3)

    # C4: When vs What in Late
    c4 = compute_slope_contrast(df_slopes, 'when', 'what', 'Late')
    c4['comparison'] = 'when-what_late'
    c4['description'] = 'Temporal decay comparison (Late)'
    contrasts.append(c4)

    # C5: Where slope change vs What slope change
    c5 = compute_slope_change_contrast(df_slopes, 'where', 'what')
    c5['comparison'] = 'where_slope_change-what_slope_change'
    c5['description'] = 'Spatial consolidation benefit'
    contrasts.append(c5)

    # C6: When slope change vs What slope change
    c6 = compute_slope_change_contrast(df_slopes, 'when', 'what')
    c6['comparison'] = 'when_slope_change-what_slope_change'
    c6['description'] = 'Temporal consolidation benefit'
    contrasts.append(c6)

    # Build DataFrame
    df_contrasts = pd.DataFrame(contrasts)

    # Rename p_value to p_uncorrected for clarity
    df_contrasts = df_contrasts.rename(columns={'p_value': 'p_uncorrected'})

    # Add Bonferroni correction
    df_contrasts['alpha_corrected'] = alpha_corrected
    df_contrasts['p_corrected'] = np.minimum(df_contrasts['p_uncorrected'] * k, 1.0)  # Cap at 1.0

    # Significance flags
    df_contrasts['sig_uncorrected'] = df_contrasts['p_uncorrected'] < 0.05
    df_contrasts['sig_corrected'] = df_contrasts['p_uncorrected'] < alpha_corrected

    # Reorder columns for clarity
    column_order = [
        'comparison', 'description', 'beta', 'se', 'z',
        'p_uncorrected', 'alpha_corrected', 'p_corrected',
        'sig_uncorrected', 'sig_corrected'
    ]
    df_contrasts = df_contrasts[column_order]

    return df_contrasts


# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 03: Compute Planned Contrasts (Decision D068)")
        log("=" * 60)

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Fitted LMM model from step01 and pre-computed slopes from step02
        # Purpose: Use these to compute the 6 planned contrasts

        log("\n[LOAD] Loading input data...")

        # Load fitted LMM model (for effect sizes)
        model_path = RQ_DIR / "data" / "step01_piecewise_lmm_model.pkl"
        if not model_path.exists():
            raise FileNotFoundError(f"Model file not found: {model_path}")

        lmm_model = MixedLMResults.load(str(model_path))
        log(f"[LOADED] data/step01_piecewise_lmm_model.pkl")

        # Load pre-computed slopes from step02
        slopes_path = RQ_DIR / "results" / "step02_segment_domain_slopes.csv"
        if not slopes_path.exists():
            raise FileNotFoundError(f"Slopes file not found: {slopes_path}")

        df_slopes = pd.read_csv(slopes_path)
        log(f"[LOADED] results/step02_segment_domain_slopes.csv ({len(df_slopes)} rows)")

        # Validate slopes data
        expected_cols = ['segment', 'domain', 'slope', 'se', 'CI_lower', 'CI_upper']
        missing_cols = [c for c in expected_cols if c not in df_slopes.columns]
        if missing_cols:
            raise ValueError(f"Missing columns in slopes file: {missing_cols}")
        log(f"[VALIDATION] Slopes file has all required columns")

        # Verify all segment-domain combinations present
        expected_combinations = 6  # 2 segments x 3 domains
        if len(df_slopes) != expected_combinations:
            raise ValueError(f"Expected {expected_combinations} slope rows, got {len(df_slopes)}")
        log(f"[VALIDATION] All {expected_combinations} segment-domain combinations present")

        # =========================================================================
        # STEP 2: Compute 6 Planned Contrasts
        # =========================================================================
        # Tool: Custom function using delta method
        # What it does: Computes 6 planned contrasts comparing domain slopes
        # Expected output: DataFrame with contrast estimates and dual p-values

        log("\n[ANALYSIS] Computing 6 planned contrasts...")
        log("=" * 60)
        log("POST-HOC PAIRWISE CONTRASTS (Decision D068)")
        log("=" * 60)

        family_alpha = 0.05
        k = 6
        alpha_corrected = family_alpha / k

        log(f"Family-wise alpha: {family_alpha}")
        log(f"Number of comparisons: {k}")
        log(f"Bonferroni-corrected alpha: {alpha_corrected:.4f}")

        df_contrasts = compute_all_planned_contrasts(df_slopes, family_alpha=family_alpha)

        log(f"\n[DONE] Computed {len(df_contrasts)} contrasts")

        # Report results
        log("\nResults:")
        log(f"  Significant (uncorrected alpha=0.05): {df_contrasts['sig_uncorrected'].sum()}/{k}")
        log(f"  Significant (corrected alpha={alpha_corrected:.4f}): {df_contrasts['sig_corrected'].sum()}/{k}")

        log("\n[DETAILS] Contrast estimates:")
        for _, row in df_contrasts.iterrows():
            sig_marker = "*" if row['sig_uncorrected'] else ""
            sig_marker_corr = "**" if row['sig_corrected'] else ""
            log(f"  {row['comparison']}: beta={row['beta']:.4f}, z={row['z']:.2f}, p={row['p_uncorrected']:.4f} {sig_marker}{sig_marker_corr}")

        # =========================================================================
        # STEP 3: Compute Effect Sizes (Cohen's f-squared)
        # =========================================================================
        # Tool: compute_effect_sizes_cohens from tools.analysis_lmm
        # What it does: Computes Cohen's f-squared for model fixed effects
        # Expected output: DataFrame with effect sizes and interpretations

        log("\n[ANALYSIS] Computing effect sizes (Cohen's f-squared)...")

        df_effect_sizes = compute_effect_sizes_cohens(
            lmm_result=lmm_model,
            include_interactions=True  # Include 3-way interaction terms
        )

        log(f"[DONE] Computed effect sizes for {len(df_effect_sizes)} terms")

        # =========================================================================
        # STEP 4: Save Analysis Outputs
        # =========================================================================
        # These outputs will be used by: step04 (consolidation benefit) and final results

        log("\n[SAVE] Saving outputs...")

        # Save contrasts
        contrasts_path = RQ_DIR / "results" / "step03_planned_contrasts.csv"
        df_contrasts.to_csv(contrasts_path, index=False, encoding='utf-8')
        log(f"[SAVED] results/step03_planned_contrasts.csv ({len(df_contrasts)} rows)")

        # Save effect sizes
        effects_path = RQ_DIR / "results" / "step03_effect_sizes.csv"
        df_effect_sizes.to_csv(effects_path, index=False, encoding='utf-8')
        log(f"[SAVED] results/step03_effect_sizes.csv ({len(df_effect_sizes)} rows)")

        # =========================================================================
        # STEP 5: Validation
        # =========================================================================
        # Validates: Output structure and statistical correctness
        # Threshold: 6 contrasts, dual p-values, valid ranges

        log("\n[VALIDATION] Validating outputs...")

        validation_errors = []

        # Check 1: Contrast count
        if len(df_contrasts) != 6:
            validation_errors.append(f"Expected 6 contrasts, got {len(df_contrasts)}")
        else:
            log("[PASS] Contrast count correct: 6 rows")

        # Check 2: Dual p-values present
        required_cols = ['p_uncorrected', 'p_corrected', 'alpha_corrected']
        missing_p_cols = [c for c in required_cols if c not in df_contrasts.columns]
        if missing_p_cols:
            validation_errors.append(f"Missing p-value columns: {missing_p_cols}")
        else:
            log("[PASS] Dual p-values present (D068 compliant)")

        # Check 3: Bonferroni alpha correct
        expected_alpha = 0.05 / 6
        if not np.isclose(df_contrasts['alpha_corrected'].iloc[0], expected_alpha, rtol=0.001):
            validation_errors.append(f"Expected alpha_corrected={expected_alpha:.4f}, got {df_contrasts['alpha_corrected'].iloc[0]:.4f}")
        else:
            log(f"[PASS] Bonferroni alpha correct: {expected_alpha:.4f}")

        # Check 4: p-values valid
        if (df_contrasts['p_uncorrected'] < 0).any() or (df_contrasts['p_uncorrected'] > 1).any():
            validation_errors.append("p_uncorrected values out of range [0, 1]")
        elif (df_contrasts['p_corrected'] < df_contrasts['p_uncorrected']).any():
            validation_errors.append("p_corrected should be >= p_uncorrected")
        else:
            log("[PASS] p-values valid and correctly ordered")

        # Check 5: Effect sizes computed
        if len(df_effect_sizes) == 0:
            validation_errors.append("No effect sizes computed")
        elif 'f_squared' not in df_effect_sizes.columns or 'interpretation' not in df_effect_sizes.columns:
            validation_errors.append("Effect sizes missing required columns (f_squared, interpretation)")
        else:
            log(f"[PASS] Effect sizes computed: {len(df_effect_sizes)} terms")

        # Report validation result
        if validation_errors:
            log("\n[FAIL] Validation failed:")
            for error in validation_errors:
                log(f"  - {error}")
            raise ValueError(f"Validation failed: {validation_errors}")
        else:
            log("\n[PASS] All validation criteria passed")

        # =========================================================================
        # STEP 6: Summary Statistics
        # =========================================================================

        log("\n" + "=" * 60)
        log("SUMMARY")
        log("=" * 60)

        log("\nContrasts (sorted by p-value):")
        df_sorted = df_contrasts.sort_values('p_uncorrected')
        for _, row in df_sorted.iterrows():
            sig_str = "[SIG]" if row['sig_corrected'] else "[sig]" if row['sig_uncorrected'] else "[ns]"
            log(f"  {sig_str} {row['comparison']}: beta={row['beta']:.4f}, p={row['p_uncorrected']:.4f}")

        log("\nEffect size distribution:")
        if len(df_effect_sizes) > 0:
            interp_counts = df_effect_sizes['interpretation'].value_counts()
            for interp, count in interp_counts.items():
                log(f"  {interp}: {count} terms")

        log("\n" + "=" * 60)
        log("[SUCCESS] Step 03 complete")
        log("=" * 60)
        sys.exit(0)

    except Exception as e:
        log(f"\n[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
