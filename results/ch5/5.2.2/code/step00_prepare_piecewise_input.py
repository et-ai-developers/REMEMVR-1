#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step00
Step Name: Prepare Piecewise LMM Input from RQ 5.1
RQ: results/ch5/5.2.2
Generated: 2025-11-23

PURPOSE:
Prepare piecewise LMM input by adding Segment and Days_within variables to RQ 5.1
theta data. This creates the foundation for the 3-way interaction analysis
(Days_within * Segment * domain) that tests whether forgetting slopes differ
between Early and Late consolidation phases across memory domains.

EXPECTED INPUTS:
  - results/ch5/5.2.1/data/step04_lmm_input.csv
    Columns: [composite_ID, UID, test, TSVR_hours, domain, theta]
    Format: CSV with RQ 5.2.1 domain-specific LMM input data
    Expected rows: ~1200 (100 participants x 4 tests x 3 domains)

EXPECTED OUTPUTS:
  - data/step00_piecewise_lmm_input.csv
    Columns: [composite_ID, UID, test, TSVR_hours, domain, theta, Segment, Days_within]
    Format: CSV with piecewise LMM input data
    Expected rows: ~800 (100 participants x 4 tests x 2 domains - When excluded)

VALIDATION CRITERIA:
  - RQ 5.1 dependency exists (CRITICAL)
  - When domain excluded (floor effect per RQ 5.2.1) (CRITICAL)
  - Row count ~800 after filtering (CRITICAL)
  - Segment assignment correct: test in {0,1} -> 'Early', test in {3,6} -> 'Late' (CRITICAL)
  - No NaN in new columns (CRITICAL)
  - Days_within range valid: Early [0, 2], Late [0, 6] (CRITICAL)
  - Only What/Where domains present: {what, where} (CRITICAL)

g_code REASONING:
- Approach: Transform RQ 5.1 continuous TSVR data into piecewise segments for
  testing consolidation vs decay phase differences
- Why this approach: Piecewise LMM allows testing whether early consolidation
  (Day 0-1) has different forgetting rates than late decay (Day 3-6)
- Data flow: RQ 5.1 theta scores -> Add Segment (Early/Late) -> Add Days_within
  (time relative to segment start)
- Expected performance: ~seconds (simple DataFrame operations)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib (pandas operations only)
- Validation tool: inline criteria checks
- Parameters:
  - Early segment: test in {0, 1} (Day 0 and Day 1)
  - Late segment: test in {3, 6} (Day 3 and Day 6)
  - Days_within Early: TSVR_hours / 24 (hours to days)
  - Days_within Late: (TSVR_hours - min_Late_TSVR_hours) / 24 (reset at segment start)
- Decision D070: Using TSVR (actual hours) as time variable
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (5 levels deep from project root)
# parents[4] = REMEMVR/ (code -> rqY -> chX -> results -> REMEMVR)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.2.2
LOG_FILE = RQ_DIR / "logs" / "step00_prepare_piecewise_input.log"

# Input from RQ 5.2.1 (domain-specific theta scores)
RQ521_INPUT = PROJECT_ROOT / "results" / "ch5" / "5.2.1" / "data" / "step04_lmm_input.csv"

# Output for this step
OUTPUT_FILE = RQ_DIR / "data" / "step00_piecewise_lmm_input.csv"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step00_piecewise_lmm_input.csv
#   WRONG:   data/piecewise_lmm_input.csv (missing step prefix)

# =============================================================================
# Segment Configuration
# =============================================================================

SEGMENT_MAPPING = {
    "Early": [1, 2],  # Test 1 (Day 0) and Test 2 (Day 1) - consolidation window
    "Late": [3, 4]    # Test 3 (Day 3) and Test 4 (Day 6) - decay phase
}

# Note: Test 2 (Day 1) is in Early segment ONLY (no overlap between segments)
# Test numbering: 1=Day0, 2=Day1, 3=Day3, 4=Day6 (sequential, not nominal days)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console (ASCII-only for Windows cp1252)."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 00: Prepare Piecewise LMM Input from RQ 5.2.1")
        log(f"[CONFIG] RQ Directory: {RQ_DIR}")
        log(f"[CONFIG] Input: {RQ521_INPUT}")
        log(f"[CONFIG] Output: {OUTPUT_FILE}")

        # =========================================================================
        # STEP 1: Validate RQ 5.2.1 Dependency
        # =========================================================================
        # Check that RQ 5.2.1 output exists before proceeding
        # This is a CRITICAL validation - cannot proceed without input

        log("[VALIDATE] Checking RQ 5.2.1 dependency...")

        if not RQ521_INPUT.exists():
            raise ValueError(
                f"CRITICAL: RQ 5.2.1 dependency missing. "
                f"Expected file: {RQ521_INPUT}. "
                f"Run RQ 5.2.1 first to generate this file."
            )

        log(f"[PASS] RQ 5.2.1 dependency exists: {RQ521_INPUT}")

        # =========================================================================
        # STEP 2: Load Input Data
        # =========================================================================
        # Expected: RQ 5.2.1 LMM input with domain-specific theta scores and TSVR
        # Purpose: Base data for piecewise segment transformation

        log("[LOAD] Loading RQ 5.2.1 LMM input data...")
        df_rq51 = pd.read_csv(RQ521_INPUT, encoding='utf-8')
        log(f"[LOADED] {RQ521_INPUT.name} ({len(df_rq51)} rows, {len(df_rq51.columns)} cols)")
        log(f"[INFO] Columns: {list(df_rq51.columns)}")

        # Validate required columns
        required_cols = ["composite_ID", "UID", "test", "TSVR_hours", "domain", "theta"]
        missing_cols = [c for c in required_cols if c not in df_rq51.columns]
        if missing_cols:
            raise ValueError(f"CRITICAL: Missing required columns: {missing_cols}")

        log(f"[PASS] All required columns present: {required_cols}")

        # =========================================================================
        # STEP 2.5: FILTER OUT WHEN DOMAIN (Floor Effect per RQ 5.2.1)
        # =========================================================================
        # When domain excluded due to floor effect discovered in RQ 5.2.1:
        # - Performance at 6-9% probability throughout study (near 0% floor)
        # - 20/26 When items (77%) excluded for low discrimination (a < 0.4)
        # - Only 6 items retained, limiting reliability
        # - Per RQ 5.2.1: "Exclude When domain from subsequent RQs until resolved"

        log("[FILTER] Excluding When domain (floor effect per RQ 5.2.1)...")
        original_rows = len(df_rq51)
        original_domains = df_rq51["domain"].unique()
        log(f"[INFO] Original domains: {list(original_domains)}")
        log(f"[INFO] Original row count: {original_rows}")

        # Filter to What and Where only
        df_rq51 = df_rq51[df_rq51["domain"].isin(["what", "where"])].copy()

        filtered_rows = len(df_rq51)
        filtered_domains = df_rq51["domain"].unique()
        log(f"[INFO] After filter - domains: {list(filtered_domains)}")
        log(f"[INFO] After filter - row count: {filtered_rows}")
        log(f"[INFO] Rows removed (When domain): {original_rows - filtered_rows}")
        log(f"[PASS] When domain excluded successfully")

        # =========================================================================
        # STEP 3: Create Segment Variable
        # =========================================================================
        # Early segment: test in {0, 1} (Day 0-1, consolidation phase)
        # Late segment: test in {3, 6} (Day 3-6, decay phase)
        # This divides the forgetting curve into two distinct phases

        log("[TRANSFORM] Creating Segment variable...")

        def assign_segment(test_value):
            """Assign segment based on test number."""
            if test_value in SEGMENT_MAPPING["Early"]:
                return "Early"
            elif test_value in SEGMENT_MAPPING["Late"]:
                return "Late"
            else:
                # Should not happen with valid data
                return np.nan

        df_piecewise = df_rq51.copy()
        df_piecewise["Segment"] = df_piecewise["test"].apply(assign_segment)

        # Validate segment assignment
        segment_counts = df_piecewise.groupby(["test", "Segment"]).size().reset_index(name="count")
        log("[INFO] Segment assignment by test:")
        for _, row in segment_counts.iterrows():
            log(f"       test={row['test']} -> Segment={row['Segment']} (n={row['count']})")

        # Check for NaN segments (indicates unexpected test values)
        nan_segments = df_piecewise["Segment"].isna().sum()
        if nan_segments > 0:
            raise ValueError(
                f"CRITICAL: {nan_segments} rows have NaN Segment values. "
                f"Unexpected test values found: {df_piecewise[df_piecewise['Segment'].isna()]['test'].unique()}"
            )

        log(f"[PASS] Segment assignment complete: no NaN values")

        # =========================================================================
        # STEP 4: Create Days_within Variable
        # =========================================================================
        # Days_within = time elapsed within each segment
        # For Early: Days_within = TSVR_hours / 24 (starts at Day 0)
        # For Late: Days_within = (TSVR_hours - min_Late_TSVR) / 24 (resets at Day 3)
        # This allows segment-specific slope estimation

        log("[TRANSFORM] Creating Days_within variable...")

        # Find minimum TSVR for Late segment (Day 3 baseline)
        min_late_tsvr = df_piecewise[df_piecewise["Segment"] == "Late"]["TSVR_hours"].min()
        log(f"[INFO] Min TSVR for Late segment: {min_late_tsvr:.2f} hours")

        def compute_days_within(row):
            """Compute days elapsed within segment."""
            if row["Segment"] == "Early":
                # Early: days from start (TSVR / 24)
                return row["TSVR_hours"] / 24.0
            else:
                # Late: days from segment start (reset at Day 3)
                return (row["TSVR_hours"] - min_late_tsvr) / 24.0

        df_piecewise["Days_within"] = df_piecewise.apply(compute_days_within, axis=1)

        # Log Days_within statistics by segment
        log("[INFO] Days_within statistics by segment:")
        for segment in ["Early", "Late"]:
            segment_data = df_piecewise[df_piecewise["Segment"] == segment]["Days_within"]
            log(f"       {segment}: min={segment_data.min():.2f}, max={segment_data.max():.2f}, mean={segment_data.mean():.2f}")

        # =========================================================================
        # STEP 5: Validate Output Data
        # =========================================================================
        # Run all validation criteria specified in 4_analysis.yaml

        log("[VALIDATE] Running validation checks...")

        # Check 1: Row count preserved (after When filtering)
        if len(df_piecewise) != len(df_rq51):
            raise ValueError(
                f"CRITICAL: Row count changed during transformation. "
                f"Input (after When filter): {len(df_rq51)}, Output: {len(df_piecewise)}"
            )
        log(f"[PASS] Row count preserved (after When filter): {len(df_piecewise)}")

        # Check 2: Segment assignment correct
        # test in {1,2} should be 100% Early
        # test in {3,4} should be 100% Late
        for test_val, expected_seg in [(1, "Early"), (2, "Early"), (3, "Late"), (4, "Late")]:
            test_data = df_piecewise[df_piecewise["test"] == test_val]
            if len(test_data) > 0:
                actual_seg = test_data["Segment"].unique()
                if len(actual_seg) != 1 or actual_seg[0] != expected_seg:
                    raise ValueError(
                        f"CRITICAL: test={test_val} should be {expected_seg}, "
                        f"but found: {actual_seg}"
                    )
        log("[PASS] Segment assignment correct: test 1,2 -> Early; test 3,4 -> Late")

        # Check 3: No NaN in new columns
        nan_segment = df_piecewise["Segment"].isna().sum()
        nan_days = df_piecewise["Days_within"].isna().sum()
        if nan_segment > 0 or nan_days > 0:
            raise ValueError(
                f"CRITICAL: NaN values found. "
                f"Segment NaN: {nan_segment}, Days_within NaN: {nan_days}"
            )
        log("[PASS] No NaN values in Segment or Days_within")

        # Check 4: Days_within range valid
        # Early: [0, ~2] days (Day 0 to Day 1)
        # Late: [0, ~6] days (Day 3 to Day 6, reset at Day 3)
        early_max = df_piecewise[df_piecewise["Segment"] == "Early"]["Days_within"].max()
        late_max = df_piecewise[df_piecewise["Segment"] == "Late"]["Days_within"].max()

        if early_max > 3:  # Allow some margin for actual TSVR variation
            raise ValueError(f"CRITICAL: Early Days_within max={early_max:.2f} exceeds expected ~2 days")
        if late_max > 10:  # Allow generous margin for real TSVR variation (some participants tested late)
            raise ValueError(f"CRITICAL: Late Days_within max={late_max:.2f} exceeds expected range")

        log(f"[PASS] Days_within range valid: Early max={early_max:.2f}, Late max={late_max:.2f}")

        # Check 5: Only What/Where domains present (When excluded)
        domains_present = set(df_piecewise["domain"].unique())
        expected_domains = {"what", "where"}  # When excluded due to floor effect
        if domains_present != expected_domains:
            raise ValueError(
                f"CRITICAL: Domain mismatch. "
                f"Expected: {expected_domains}, Found: {domains_present}"
            )
        log(f"[PASS] Expected domains present (When excluded): {expected_domains}")

        # =========================================================================
        # STEP 6: Save Output
        # =========================================================================
        # Save piecewise LMM input for Step 1 (model fitting)

        log("[SAVE] Saving piecewise LMM input...")

        # Ensure output directory exists
        OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)

        # Save with explicit column order
        output_columns = ["composite_ID", "UID", "test", "TSVR_hours", "domain", "theta", "Segment", "Days_within"]
        df_piecewise[output_columns].to_csv(OUTPUT_FILE, index=False, encoding='utf-8')

        log(f"[SAVED] {OUTPUT_FILE} ({len(df_piecewise)} rows, {len(output_columns)} cols)")
        log(f"[INFO] Output columns: {output_columns}")

        # =========================================================================
        # STEP 7: Final Summary
        # =========================================================================

        log("")
        log("=" * 70)
        log("[SUCCESS] Step 00 complete: Piecewise LMM Input Prepared")
        log("=" * 70)
        log(f"  Input:  {RQ521_INPUT}")
        log(f"  Output: {OUTPUT_FILE}")
        log(f"  Rows:   {len(df_piecewise)}")
        log("")
        log("  Segment distribution:")
        for segment in ["Early", "Late"]:
            n = len(df_piecewise[df_piecewise["Segment"] == segment])
            log(f"    - {segment}: {n} rows ({n/len(df_piecewise)*100:.1f}%)")
        log("")
        log("  Next step: step01_fit_piecewise_lmm.py")
        log("=" * 70)

        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
