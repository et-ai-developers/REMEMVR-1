#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step01
Step Name: Fit Piecewise LMM with 3-Way Interaction
RQ: results/ch5/5.2.2
Generated: 2025-11-23

PURPOSE:
Fit piecewise Linear Mixed Model testing the 3-way interaction:
Days_within * Segment * domain

This tests whether forgetting slopes differ by:
1. Time segment (Early: consolidation phase vs Late: decay phase)
2. Memory domain (What/Where/When)
3. Their interaction (differential consolidation benefit by domain)

EXPECTED INPUTS:
  - data/step00_piecewise_lmm_input.csv
    Columns: [composite_ID, UID, test, TSVR_hours, domain, theta, Segment, Days_within]
    Format: Long format with one row per person x domain x test
    Expected rows: ~1200 (100 participants x 3 domains x 4 tests)

EXPECTED OUTPUTS:
  - data/step01_piecewise_lmm_model.pkl
    Format: Pickled MixedLMResults object
    Contains: Fitted LMM with random intercepts and slopes by UID
  - results/step01_piecewise_lmm_summary.txt
    Format: Text file with full model summary
    Contains: Fixed effects, random effects, fit statistics

VALIDATION CRITERIA:
  - Model converged successfully
  - No singular fit (random effects variance > 0)
  - All fixed effect terms present (7 terms: intercept + 6 interactions)

g_code REASONING:
- Approach: Use statsmodels MixedLM with Treatment coding for interpretable contrasts
- Why this approach: Treatment('Early') and Treatment('what') make Early-What the reference,
  so main effect of Days_within = slope in Early-What, and interactions show slope differences
- Data flow: CSV input -> LMM fit -> pickle model + text summary
- Expected performance: ~30-60 seconds (1200 observations, 100 random effects groups)

IMPLEMENTATION NOTES:
- Analysis tool: fit_lmm_trajectory from tools.analysis_lmm
- Validation tool: validate_lmm_convergence from tools.validation
- Formula: theta ~ Days_within * C(Segment, Treatment('Early')) * C(domain, Treatment('what'))
- Random effects: ~Days_within (random intercepts + slopes per UID)
- REML: False (ML for model comparison capability)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, Any
import traceback
import pickle

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (5 levels deep from project root)
# parents[4] = REMEMVR/ (code -> rqY -> chX -> results -> REMEMVR)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_lmm import fit_lmm_trajectory

# Import validation tool
from tools.validation import validate_lmm_convergence

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.2.2 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step01_fit_piecewise_lmm.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step01_piecewise_lmm_model.pkl
#   CORRECT: results/step01_piecewise_lmm_summary.txt
#   WRONG:   results/lmm_model.pkl (wrong folder + no prefix)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        # Ensure log directory exists
        LOG_FILE.parent.mkdir(parents=True, exist_ok=True)

        log("[START] Step 01: Fit Piecewise LMM with 3-Way Interaction")
        log("="*70)

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Piecewise LMM input from step00 with Segment and Days_within
        # Purpose: Data for fitting 3-way interaction LMM

        log("[LOAD] Loading input data from step00...")

        input_path = RQ_DIR / "data" / "step00_piecewise_lmm_input.csv"

        if not input_path.exists():
            raise FileNotFoundError(f"Input file not found: {input_path}")

        df_lmm = pd.read_csv(input_path)
        log(f"[LOADED] step00_piecewise_lmm_input.csv ({len(df_lmm)} rows, {len(df_lmm.columns)} cols)")

        # Verify required columns
        required_cols = ['UID', 'domain', 'theta', 'Segment', 'Days_within']
        missing_cols = set(required_cols) - set(df_lmm.columns)
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")
        log(f"[PASS] All required columns present: {required_cols}")

        # Log data summary
        log(f"[INFO] Unique UIDs: {df_lmm['UID'].nunique()}")
        log(f"[INFO] Segments: {df_lmm['Segment'].unique().tolist()}")
        log(f"[INFO] Domains: {df_lmm['domain'].unique().tolist()}")
        log(f"[INFO] Days_within range: [{df_lmm['Days_within'].min():.2f}, {df_lmm['Days_within'].max():.2f}]")

        # =========================================================================
        # STEP 2: Run Analysis Tool
        # =========================================================================
        # Tool: fit_lmm_trajectory
        # What it does: Fits Linear Mixed Model with specified formula and random effects
        # Expected output: MixedLMResults object with fitted model

        log("")
        log("[ANALYSIS] Running fit_lmm_trajectory...")
        log("-"*70)

        # Define model parameters
        # Formula: 3-way interaction testing slope differences by Segment and Domain
        # Reference levels: Early segment, What domain (via Treatment coding)
        # This means:
        #   - Days_within main effect = slope in Early-What
        #   - Segment[T.Late] interaction = slope change from Early to Late in What
        #   - domain[T.where/when] interaction = slope difference from What in Early
        #   - 3-way = whether slope differences vary by segment

        formula = "theta ~ Days_within * C(Segment, Treatment('Early')) * C(domain, Treatment('what'))"
        groups = "UID"
        re_formula = "~Days_within"  # Random intercepts + slopes per participant
        reml = False  # Use ML for model comparison capability

        log(f"[PARAM] Formula: {formula}")
        log(f"[PARAM] Groups: {groups}")
        log(f"[PARAM] RE Formula: {re_formula}")
        log(f"[PARAM] REML: {reml}")

        # Fit the model
        lmm_model = fit_lmm_trajectory(
            data=df_lmm,
            formula=formula,
            groups=groups,
            re_formula=re_formula,
            reml=reml
        )

        log("[DONE] Model fitting complete")

        # =========================================================================
        # STEP 3: Save Analysis Outputs
        # =========================================================================
        # These outputs will be used by:
        #   - Step 02: Extract slopes from model
        #   - Step 03: Compute contrasts using model
        #   - Step 05: Generate predictions for plots

        log("")
        log("[SAVE] Saving model outputs...")
        log("-"*70)

        # Save pickle model
        model_path = RQ_DIR / "data" / "step01_piecewise_lmm_model.pkl"
        model_path.parent.mkdir(parents=True, exist_ok=True)

        with open(model_path, 'wb') as f:
            pickle.dump(lmm_model, f)
        log(f"[SAVED] data/step01_piecewise_lmm_model.pkl")

        # Save model summary
        summary_path = RQ_DIR / "results" / "step01_piecewise_lmm_summary.txt"
        summary_path.parent.mkdir(parents=True, exist_ok=True)

        summary_text = lmm_model.summary().as_text()
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("="*70 + "\n")
            f.write("PIECEWISE LMM MODEL SUMMARY\n")
            f.write("RQ 5.2: Consolidation Effects by Memory Domain\n")
            f.write("="*70 + "\n\n")
            f.write(f"Formula: {formula}\n")
            f.write(f"Groups: {groups}\n")
            f.write(f"Random Effects: {re_formula}\n")
            f.write(f"REML: {reml}\n\n")
            f.write("-"*70 + "\n")
            f.write("MODEL OUTPUT\n")
            f.write("-"*70 + "\n\n")
            f.write(summary_text)
        log(f"[SAVED] results/step01_piecewise_lmm_summary.txt")

        # Log key model statistics
        log("")
        log("[INFO] Model Statistics:")
        log(f"  - Number of observations: {lmm_model.nobs}")
        log(f"  - Number of groups (UIDs): {len(lmm_model.model.group_labels)}")
        log(f"  - Log-likelihood: {lmm_model.llf:.4f}")
        log(f"  - AIC: {lmm_model.aic:.4f}")
        log(f"  - BIC: {lmm_model.bic:.4f}")

        # =========================================================================
        # STEP 4: Run Validation Tool
        # =========================================================================
        # Tool: validate_lmm_convergence
        # Validates: Model convergence and structure
        # Criteria: Converged, no singular fit, all terms present

        log("")
        log("[VALIDATION] Running validate_lmm_convergence...")
        log("-"*70)

        validation_result = validate_lmm_convergence(lmm_model)

        # Report validation results
        if isinstance(validation_result, dict):
            for key, value in validation_result.items():
                log(f"[VALIDATION] {key}: {value}")
        else:
            log(f"[VALIDATION] {validation_result}")

        # Additional validation: Check expected fixed effects
        log("")
        log("[CHECK] Verifying fixed effect structure...")

        fe_names = lmm_model.fe_params.index.tolist()
        log(f"[INFO] Fixed effects ({len(fe_names)} terms):")
        for i, name in enumerate(fe_names):
            coef = lmm_model.fe_params[name]
            pval = lmm_model.pvalues[name]
            log(f"  {i+1}. {name}: {coef:.4f} (p={pval:.4f})")

        # Check we have the expected structure (intercept + 6 interaction terms)
        expected_min_terms = 7  # Intercept + main effects + interactions
        if len(fe_names) >= expected_min_terms:
            log(f"[PASS] Model has {len(fe_names)} fixed effects (expected >= {expected_min_terms})")
        else:
            log(f"[WARN] Model has only {len(fe_names)} fixed effects (expected >= {expected_min_terms})")

        # Check random effects variance
        log("")
        log("[CHECK] Random effects structure...")
        re_cov = lmm_model.cov_re
        log(f"[INFO] Random effects covariance matrix shape: {re_cov.shape}")

        # Check for singular fit (variance ~0)
        diag_var = np.diag(re_cov)
        log(f"[INFO] Random effect variances: {diag_var}")

        if np.all(diag_var > 1e-6):
            log("[PASS] No singular fit detected (all RE variances > 0)")
        else:
            log("[WARN] Potential singular fit - some RE variances near zero")

        log("")
        log("="*70)
        log("[SUCCESS] Step 01 complete: Piecewise LMM fitted successfully")
        log("="*70)
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
