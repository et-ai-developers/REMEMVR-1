#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step04
Step Name: Compute Consolidation Benefit Indices
RQ: results/ch5/5.2.2
Generated: 2025-11-23

PURPOSE:
Compute consolidation benefit index per domain (Early slope - Late slope).
Positive benefit = less forgetting in Early (consolidation protected memory).
Larger positive values indicate greater consolidation benefit.

EXPECTED INPUTS:
  - results/step02_segment_domain_slopes.csv
    Columns: [segment, domain, slope, se, CI_lower, CI_upper]
    Format: CSV with 6 rows (2 segments x 3 domains)
    Expected rows: 6

EXPECTED OUTPUTS:
  - results/step04_consolidation_benefit.csv
    Columns: [domain, early_slope, late_slope, consolidation_benefit, benefit_se, benefit_CI_lower, benefit_CI_upper, rank]
    Format: CSV with 3 rows (1 per domain)
    Expected rows: 3

VALIDATION CRITERIA:
  - All domains computed: df_benefit has 3 rows (what, where, when)
  - Ranks unique: rank values are {1, 2, 3}
  - Benefit values valid: consolidation_benefit and benefit_se are numeric
  - CI structure valid: benefit_CI_lower < consolidation_benefit < benefit_CI_upper

g_code REASONING:
- Approach: Compute Early slope minus Late slope per domain to quantify consolidation benefit
- Why this approach: Positive difference indicates consolidation protected memory (less forgetting in Early)
- Data flow: Load slopes -> Pivot by domain -> Compute benefit -> Rank by magnitude -> Save
- Expected performance: ~seconds (simple pandas operations)

IMPLEMENTATION NOTES:
- Analysis tool: Standard library (pandas) - no catalogued tool needed
- Validation tool: Inline validation (criteria checked in script)
- Parameters: benefit_formula = early_slope - late_slope; se_formula = sqrt(se_early^2 + se_late^2)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (5 levels deep from project root)
# parents[4] = REMEMVR/ (code -> rqY -> chX -> results -> REMEMVR)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.2.2 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step04_compute_consolidation_benefit.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Note: step04 output goes to results/ folder as it's a final report table
# (consolidation benefit indices are a key thesis result, not intermediate data)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg: str) -> None:
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 04: Compute Consolidation Benefit Indices")
        log("")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Segment-domain slopes from Step 2
        # Purpose: Compute consolidation benefit as difference between Early and Late slopes

        log("[LOAD] Loading segment-domain slopes from Step 2...")
        input_path = RQ_DIR / "results" / "step02_segment_domain_slopes.csv"

        if not input_path.exists():
            raise FileNotFoundError(f"Input file not found: {input_path}")

        df_slopes = pd.read_csv(input_path)
        log(f"[LOADED] step02_segment_domain_slopes.csv ({len(df_slopes)} rows, {len(df_slopes.columns)} cols)")

        # Validate input columns
        expected_cols = ["segment", "domain", "slope", "se", "CI_lower", "CI_upper"]
        actual_cols = list(df_slopes.columns)
        if actual_cols != expected_cols:
            raise ValueError(f"Column mismatch. Expected: {expected_cols}, Got: {actual_cols}")

        log(f"[VALIDATED] Input columns match expected: {expected_cols}")
        log("")

        # =========================================================================
        # STEP 2: Pivot Data by Domain and Segment
        # =========================================================================
        # Pivot to get Early and Late slopes side by side per domain
        # This enables vectorized benefit computation

        log("[PIVOT] Organizing slopes by domain and segment...")

        # Create pivot table: rows = domain, columns = segment
        df_early = df_slopes[df_slopes["segment"] == "Early"].set_index("domain")
        df_late = df_slopes[df_slopes["segment"] == "Late"].set_index("domain")

        # Verify both segments exist for all domains
        domains = ["what", "where", "when"]
        for domain in domains:
            if domain not in df_early.index:
                raise ValueError(f"Missing Early segment for domain: {domain}")
            if domain not in df_late.index:
                raise ValueError(f"Missing Late segment for domain: {domain}")

        log(f"[VERIFIED] All 3 domains present in both segments")
        log("")

        # =========================================================================
        # STEP 3: Compute Consolidation Benefit Per Domain
        # =========================================================================
        # Formula: consolidation_benefit = early_slope - late_slope
        # Interpretation:
        #   - Positive benefit = less forgetting in Early (consolidation protected memory)
        #   - Negative benefit = more forgetting in Early (no consolidation benefit)
        #   - Larger positive values = greater consolidation benefit
        #
        # SE computation via delta method: SE = sqrt(SE_early^2 + SE_late^2)
        # (assumes independence between segments, conservative estimate)

        log("[COMPUTE] Computing consolidation benefit indices...")

        benefit_records = []
        for domain in domains:
            early_slope = df_early.loc[domain, "slope"]
            early_se = df_early.loc[domain, "se"]
            late_slope = df_late.loc[domain, "slope"]
            late_se = df_late.loc[domain, "se"]

            # Consolidation benefit formula
            consolidation_benefit = early_slope - late_slope

            # SE via delta method (propagation of uncertainty)
            benefit_se = np.sqrt(early_se**2 + late_se**2)

            # 95% CI (z = 1.96 for normal approximation)
            z_critical = 1.96
            benefit_CI_lower = consolidation_benefit - z_critical * benefit_se
            benefit_CI_upper = consolidation_benefit + z_critical * benefit_se

            benefit_records.append({
                "domain": domain,
                "early_slope": early_slope,
                "late_slope": late_slope,
                "consolidation_benefit": consolidation_benefit,
                "benefit_se": benefit_se,
                "benefit_CI_lower": benefit_CI_lower,
                "benefit_CI_upper": benefit_CI_upper
            })

            log(f"  {domain}: Early={early_slope:.4f}, Late={late_slope:.4f}, Benefit={consolidation_benefit:.4f} (SE={benefit_se:.4f})")

        df_benefit = pd.DataFrame(benefit_records)
        log("")

        # =========================================================================
        # STEP 4: Rank Domains by Consolidation Benefit
        # =========================================================================
        # Rank by consolidation benefit magnitude (1 = most protected by consolidation)
        # Note: More negative benefit (more negative Early slope) means MORE forgetting in Early
        # We want to rank by PROTECTION, so larger (less negative) benefit = better = rank 1

        log("[RANK] Ranking domains by consolidation benefit...")

        # Sort by consolidation benefit descending (larger = more protected = rank 1)
        # Note: All benefits are negative (forgetting slopes are negative)
        # Less negative benefit = less forgetting difference = better consolidation
        # Actually: early_slope - late_slope, where slopes are negative
        # More negative early_slope means MORE forgetting in early
        # So positive benefit would mean early forgetting > late forgetting (unusual)
        # Negative benefit means late forgetting > early forgetting (expected for consolidation)
        # WAIT: That's backwards for interpretation!
        #
        # Let me re-interpret:
        # - Slopes are already NEGATIVE (forgetting)
        # - Early slope = -0.50 means losing 0.50 theta per day in Early
        # - Late slope = -0.03 means losing 0.03 theta per day in Late
        # - Benefit = Early - Late = -0.50 - (-0.03) = -0.47
        #
        # So NEGATIVE benefit means more forgetting in Early than Late
        # This suggests consolidation DIDN'T protect memory (unusual result)
        #
        # For ranking:
        # - Rank 1 = BEST consolidation = most protected = LEAST negative benefit (closest to 0)
        # - Rank 3 = WORST consolidation = least protected = MOST negative benefit

        df_benefit = df_benefit.sort_values("consolidation_benefit", ascending=False)
        df_benefit["rank"] = range(1, len(df_benefit) + 1)

        # Log ranking interpretation
        log("  Ranking interpretation:")
        log("    Rank 1 = Best consolidation benefit (least negative = least excess early forgetting)")
        log("    Rank 3 = Worst consolidation benefit (most negative = most excess early forgetting)")
        log("")

        for _, row in df_benefit.iterrows():
            log(f"  Rank {int(row['rank'])}: {row['domain']} (benefit={row['consolidation_benefit']:.4f})")
        log("")

        # =========================================================================
        # STEP 5: Save Consolidation Benefit Results
        # =========================================================================
        # Output goes to results/ folder as this is a final report table
        # (consolidation benefit indices are a key thesis result)

        output_path = RQ_DIR / "results" / "step04_consolidation_benefit.csv"

        log(f"[SAVE] Saving consolidation benefit indices to {output_path}...")

        # Ensure column order matches specification
        df_benefit = df_benefit[["domain", "early_slope", "late_slope", "consolidation_benefit",
                                  "benefit_se", "benefit_CI_lower", "benefit_CI_upper", "rank"]]

        df_benefit.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] step04_consolidation_benefit.csv ({len(df_benefit)} rows, {len(df_benefit.columns)} cols)")
        log("")

        # =========================================================================
        # STEP 6: Inline Validation
        # =========================================================================
        # Validate all criteria from 4_analysis.yaml

        log("[VALIDATION] Running inline validation checks...")
        validation_passed = True

        # Criterion 1: All domains computed (3 rows)
        if len(df_benefit) != 3:
            log(f"[FAIL] Row count: Expected 3, got {len(df_benefit)}")
            validation_passed = False
        else:
            log("[PASS] Row count: 3 rows (what, where, when)")

        # Criterion 2: Ranks unique {1, 2, 3}
        rank_set = set(df_benefit["rank"].astype(int).tolist())
        expected_ranks = {1, 2, 3}
        if rank_set != expected_ranks:
            log(f"[FAIL] Ranks: Expected {expected_ranks}, got {rank_set}")
            validation_passed = False
        else:
            log("[PASS] Ranks: {1, 2, 3} (all unique)")

        # Criterion 3: Benefit values are numeric (not NaN)
        if df_benefit["consolidation_benefit"].isna().any():
            log("[FAIL] Benefit values: Contains NaN")
            validation_passed = False
        elif df_benefit["benefit_se"].isna().any():
            log("[FAIL] Benefit SE: Contains NaN")
            validation_passed = False
        else:
            log("[PASS] Benefit values: All numeric (no NaN)")

        # Criterion 4: CI structure valid (CI_lower < benefit < CI_upper)
        ci_valid = True
        for _, row in df_benefit.iterrows():
            if not (row["benefit_CI_lower"] < row["consolidation_benefit"] < row["benefit_CI_upper"]):
                ci_valid = False
                log(f"[FAIL] CI structure for {row['domain']}: {row['benefit_CI_lower']:.4f} < {row['consolidation_benefit']:.4f} < {row['benefit_CI_upper']:.4f}")
                break
        if ci_valid:
            log("[PASS] CI structure: benefit_CI_lower < consolidation_benefit < benefit_CI_upper (all rows)")
        else:
            validation_passed = False

        log("")

        if not validation_passed:
            raise ValueError("Validation failed - see log for details")

        log("[SUCCESS] Step 04 complete - Consolidation benefit indices computed")
        log("")

        # Final summary
        log("=" * 60)
        log("CONSOLIDATION BENEFIT SUMMARY")
        log("=" * 60)
        log("")
        log("Interpretation:")
        log("  - Early slope: forgetting rate per day during consolidation phase (test 0-1)")
        log("  - Late slope: forgetting rate per day during decay phase (test 3-6)")
        log("  - Consolidation benefit: Early - Late (negative = more Early forgetting)")
        log("")
        log("Results:")
        for _, row in df_benefit.iterrows():
            sig = "significant" if not (row["benefit_CI_lower"] <= 0 <= row["benefit_CI_upper"]) else "non-significant"
            log(f"  {row['domain']}: benefit={row['consolidation_benefit']:.4f} [{row['benefit_CI_lower']:.4f}, {row['benefit_CI_upper']:.4f}] ({sig})")
        log("")
        log("=" * 60)

        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
