#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step01
Step Name: create_time_transformations
RQ: results/ch5/rq8
Generated: 2025-11-28

PURPOSE:
Create time variables for quadratic model (Time, Time_squared, Time_log) and
piecewise model (Segment, Days_within). This step prepares the temporal
predictors needed for RQ 5.8's two-phase forgetting analysis (Test 1: quadratic
term significance, Test 2: piecewise vs continuous comparison).

EXPECTED INPUTS:
  - data/step00_theta_tsvr.csv
    Columns: ['UID', 'test', 'TSVR_hours', 'theta']
    Format: Merged theta + TSVR mapping, domain-collapsed from RQ 5.7
    Expected rows: ~400 (100 participants x 4 tests)

EXPECTED OUTPUTS:
  - data/step01_time_transformed.csv
    Columns: ['UID', 'test', 'TSVR_hours', 'theta', 'Time', 'Time_squared',
              'Time_log', 'Segment', 'Days_within']
    Format: Original data + 5 new time transformation columns
    Expected rows: ~400 (same as input)

VALIDATION CRITERIA:
  - All 9 expected columns present
  - No NaN in any column (transformations are deterministic)
  - Segment ~50% Early, ~50% Late (2 tests per segment)
  - Days_within starts at 0 for both segments

g_code REASONING:
- Approach: Use assign_piecewise_segments() to create Segment and Days_within,
  then manually compute quadratic and log transformations
- Why this approach: assign_piecewise_segments() handles the piecewise logic
  (Early/Late segment assignment with 48h inflection point), while quadratic
  and log transformations are simple column operations
- Data flow: Load theta_tsvr -> assign_piecewise_segments() -> add quadratic
  terms -> add log term -> validate columns -> save
- Expected performance: ~seconds (simple DataFrame operations, no model fitting)

IMPLEMENTATION NOTES:
- Analysis tool: assign_piecewise_segments from tools.analysis_lmm
- Validation tool: validate_data_columns from tools.validation
- Parameters: tsvr_col='TSVR_hours', early_cutoff_hours=48.0 (RQ 5.8 uses 48h
  inflection point, not default 24h)
- Quadratic transformations: Time = copy of TSVR_hours, Time_squared = TSVR_hours^2
- Log transformation: Time_log = log(TSVR_hours + 1) to handle TSVR=0
- Piecewise: Segment='Early' for 0-48h, 'Late' for 48-240h; Days_within resets
  to 0 at segment boundaries
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_lmm import assign_piecewise_segments

# Import validation tool
from tools.validation import validate_data_columns

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/rq8 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step01_create_time_transformations.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step01_time_transformed.csv
#   CORRECT: logs/step01_create_time_transformations.log
#   WRONG:   results/time_transformed.csv  (wrong folder + no prefix)
#   WRONG:   data/time_transformed.csv     (missing step prefix)
#   WRONG:   logs/step01_data.csv          (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 01: Create Time Transformations")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Theta scores merged with TSVR mapping, domain-collapsed
        # Purpose: Base data for creating temporal predictors

        log("[LOAD] Loading input data...")
        input_path = RQ_DIR / "data" / "step00_theta_tsvr.csv"

        if not input_path.exists():
            raise FileNotFoundError(f"Input file not found: {input_path}")

        theta_tsvr_data = pd.read_csv(input_path, encoding='utf-8')
        log(f"[LOADED] step00_theta_tsvr.csv ({len(theta_tsvr_data)} rows, {len(theta_tsvr_data.columns)} cols)")
        log(f"[INFO] Columns: {list(theta_tsvr_data.columns)}")

        # =========================================================================
        # STEP 2: Run Analysis Tool (Piecewise Segment Assignment)
        # =========================================================================
        # Tool: assign_piecewise_segments
        # What it does: Assigns Early/Late segment labels and computes Days_within
        #               (time since segment start) for piecewise LMM
        # Expected output: Original DataFrame + 2 new columns (Segment, Days_within)

        log("[ANALYSIS] Running assign_piecewise_segments...")
        log("[INFO] Parameters: tsvr_col='TSVR_hours', early_cutoff_hours=48.0")

        # Call assign_piecewise_segments to create Segment and Days_within columns
        time_transformed_data = assign_piecewise_segments(
            df=theta_tsvr_data,
            tsvr_col='TSVR_hours',
            early_cutoff_hours=48.0  # RQ 5.8 uses 48h inflection (one night's sleep)
        )
        log("[DONE] Piecewise segment assignment complete")
        log(f"[INFO] Segments created: {time_transformed_data['Segment'].unique().tolist()}")

        # =========================================================================
        # STEP 3: Create Quadratic and Log Transformations
        # =========================================================================
        # Quadratic model needs: Time, Time_squared
        # Log model needs: Time_log
        # These are simple column operations (no function call needed)

        log("[TRANSFORM] Creating quadratic and log transformations...")

        # Copy TSVR_hours to Time column (for consistency with quadratic formula)
        time_transformed_data['Time'] = time_transformed_data['TSVR_hours'].copy()

        # Compute Time_squared
        time_transformed_data['Time_squared'] = time_transformed_data['TSVR_hours'] ** 2

        # Compute Time_log (add 1 to handle TSVR=0)
        time_transformed_data['Time_log'] = np.log(time_transformed_data['TSVR_hours'] + 1)

        log("[DONE] Transformations complete")
        log(f"[INFO] Added columns: Time, Time_squared, Time_log")

        # =========================================================================
        # STEP 4: Save Analysis Outputs
        # =========================================================================
        # This output will be used by: Step 2 (quadratic model) and Step 3 (piecewise model)

        output_path = RQ_DIR / "data" / "step01_time_transformed.csv"
        log(f"[SAVE] Saving {output_path.name}...")

        # Save with all transformations
        time_transformed_data.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] step01_time_transformed.csv ({len(time_transformed_data)} rows, {len(time_transformed_data.columns)} cols)")
        log(f"[INFO] Final columns: {list(time_transformed_data.columns)}")

        # =========================================================================
        # STEP 5: Run Validation Tool
        # =========================================================================
        # Tool: validate_data_columns
        # Validates: All 9 expected columns present, no missing columns
        # Threshold: Exact column set required

        log("[VALIDATION] Running validate_data_columns...")

        required_columns = [
            'UID', 'test', 'TSVR_hours', 'theta',  # Original columns
            'Time', 'Time_squared', 'Time_log',     # Quadratic/log transformations
            'Segment', 'Days_within'                # Piecewise transformations
        ]

        validation_result = validate_data_columns(
            df=time_transformed_data,
            required_columns=required_columns
        )

        # Report validation results
        # Expected: All 9 columns present, valid=True
        log(f"[VALIDATION] valid: {validation_result['valid']}")
        log(f"[VALIDATION] n_required: {validation_result['n_required']}")
        log(f"[VALIDATION] n_missing: {validation_result['n_missing']}")

        if validation_result['missing_columns']:
            log(f"[VALIDATION] missing_columns: {validation_result['missing_columns']}")

        if not validation_result['valid']:
            raise ValueError(f"Validation failed: {validation_result}")

        # Additional validation checks (beyond validate_data_columns)
        log("[VALIDATION] Additional checks...")

        # Check for NaN values (transformations should be deterministic)
        nan_counts = time_transformed_data[required_columns].isna().sum()
        if nan_counts.sum() > 0:
            log(f"[WARNING] NaN values found: {nan_counts[nan_counts > 0].to_dict()}")
        else:
            log("[VALIDATION] No NaN values (deterministic transformations confirmed)")

        # Check segment distribution
        segment_counts = time_transformed_data['Segment'].value_counts()
        log(f"[VALIDATION] Segment distribution: {segment_counts.to_dict()}")
        early_pct = segment_counts.get('Early', 0) / len(time_transformed_data) * 100
        late_pct = segment_counts.get('Late', 0) / len(time_transformed_data) * 100
        log(f"[VALIDATION] Early: {early_pct:.1f}%, Late: {late_pct:.1f}%")

        # Check Days_within starts at 0 for both segments
        early_min_days = time_transformed_data[time_transformed_data['Segment'] == 'Early']['Days_within'].min()
        late_min_days = time_transformed_data[time_transformed_data['Segment'] == 'Late']['Days_within'].min()
        log(f"[VALIDATION] Days_within min (Early): {early_min_days:.2f}")
        log(f"[VALIDATION] Days_within min (Late): {late_min_days:.2f}")

        if early_min_days != 0.0 or late_min_days != 0.0:
            log("[WARNING] Days_within does not start at 0 for both segments")
        else:
            log("[VALIDATION] Days_within starts at 0 for both segments (correct)")

        log("[SUCCESS] Step 01 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
