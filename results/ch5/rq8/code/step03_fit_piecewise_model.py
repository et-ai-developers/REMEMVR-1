#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step03
Step Name: Fit Piecewise Model
RQ: results/ch5/rq8
Generated: 2025-11-28

PURPOSE:
Fit piecewise LMM (theta ~ Days_within * Segment + (Days_within | UID)) to test
if two-segment model (Early 0-48h, Late 48-240h) provides better fit than best
continuous model from RQ 5.7. This is Test 2 of the two-phase forgetting hypothesis
(RQ 5.8). AIC comparison: deltaAIC < -2 favors piecewise model (segmented forgetting).

EXPECTED INPUTS:
  - data/step01_time_transformed.csv
    Columns: ['UID', 'test', 'TSVR_hours', 'theta', 'Time', 'Time_squared',
              'Time_log', 'Segment', 'Days_within']
    Format: Long-format time-transformed data with piecewise segments
    Expected rows: ~400 (100 participants x 4 tests)
    Source: Step 1 (time transformations with 48h inflection point)

  - data/step00_best_continuous_aic.txt
    Format: Single line text file with AIC value
    Source: Step 0 (RQ 5.7 best continuous model AIC for comparison)

EXPECTED OUTPUTS:
  - results/step03_piecewise_model_summary.txt
    Format: Plain text LMM summary with fixed effects, random effects, AIC,
            convergence status, and AIC comparison to continuous model
    Expected: Fixed effects table with Intercept, Days_within, Segment[T.Late],
              Days_within:Segment[T.Late] interaction term, deltaAIC computation

  - data/step03_piecewise_predictions.csv
    Columns: ['Segment', 'Days_within', 'TSVR_hours', 'predicted_theta',
              'CI_lower', 'CI_upper']
    Format: Model predictions on grid (9 Early + 9 Late = 18 total)
    Expected rows: 18 (separate prediction grids per segment)

VALIDATION CRITERIA:
  - Model converged (or fallback to simpler random structure documented)
  - All fixed effects finite (Intercept, Days_within, Segment, interaction)
  - Predictions generated for all 18 timepoints (9 Early + 9 Late)
  - AIC comparison computed (deltaAIC finite, not NaN)
  - Interaction p-value in [0, 1]

g_code REASONING:
- Approach: Fit piecewise LMM with Days_within*Segment interaction to test if
  Early (0-48h consolidation) and Late (48-240h decay) segments have different
  forgetting rates. Random structure: (Days_within | UID) allows individual
  differences in segment-specific forgetting slopes. Fallback strategy: if
  maximal random structure fails, try uncorrelated (Days_within || UID), then
  intercept-only (1 | UID).

- Why this approach: Two-phase forgetting hypothesis predicts Early segment
  (consolidation-dominated) has steeper decline than Late segment (decay-dominated).
  Piecewise regression explicitly tests this by allowing different slopes per
  segment via interaction term. AIC comparison to continuous (RQ 5.7) quantifies
  whether segmented model improves fit beyond continuous time trends.

- Data flow: Load time_transformed data (Step 1) with Segment + Days_within
  variables -> Fit piecewise LMM with interaction -> Generate predictions per
  segment -> Compare AIC to RQ 5.7 best continuous -> Save summary + predictions

- Expected performance: ~10-30 seconds (LMM fitting with N=100, 400 obs, 2 segments)

IMPLEMENTATION NOTES:
- Analysis tool: fit_lmm_trajectory_tsvr from tools.analysis_lmm
- Validation tool: validate_lmm_convergence from tools.validation
- Parameters: formula="theta ~ Days_within * Segment + (Days_within | UID)",
  REML=False for AIC comparison, fallback random structure strategy
- RQ 5.7 CONVERGENCE STATUS: CRITICAL for AIC comparison validity. If RQ 5.7
  model used fallback random structure (1 | UID), then AIC comparison may be
  invalid (comparing models with different random structures). Script loads
  data/step00_rq57_convergence.txt to check RQ 5.7 status and flags if fallback
  occurred. If RQ 5.7 converged with maximal structure, AIC comparison valid.
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import pickle
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_lmm import fit_lmm_trajectory

# Import validation tool
from tools.validation import validate_lmm_convergence

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/chX/rqY (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step03_fit_piecewise_model.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step03_piecewise_predictions.csv
#   CORRECT: results/step03_piecewise_model_summary.txt
#   WRONG:   results/piecewise_model_summary.csv  (CSV in results folder)
#   WRONG:   data/piecewise_predictions.csv       (missing step prefix)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 3: Fit Piecewise Model")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Time-transformed data from Step 1 with Segment + Days_within
        # Purpose: Fit piecewise LMM to test two-phase forgetting hypothesis

        log("[LOAD] Loading time-transformed data from Step 1...")
        time_data = pd.read_csv(RQ_DIR / "data" / "step01_time_transformed.csv", encoding='utf-8')
        log(f"[LOADED] step01_time_transformed.csv ({len(time_data)} rows, {len(time_data.columns)} cols)")

        # Verify required columns for piecewise model
        required_cols = ['UID', 'test', 'TSVR_hours', 'theta', 'Segment', 'Days_within']
        missing_cols = [col for col in required_cols if col not in time_data.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")
        log(f"[VALIDATED] All required columns present: {required_cols}")

        # Load RQ 5.7 best continuous AIC for comparison (Test 2 criterion)
        log("[LOAD] Loading RQ 5.7 best continuous model AIC...")
        continuous_aic_file = RQ_DIR / "data" / "step00_best_continuous_aic.txt"
        with open(continuous_aic_file, 'r', encoding='utf-8') as f:
            continuous_aic = float(f.read().strip())
        log(f"[LOADED] RQ 5.7 best continuous AIC = {continuous_aic:.2f}")

        # Load RQ 5.7 convergence status (CRITICAL for AIC comparison validity)
        log("[LOAD] Checking RQ 5.7 convergence status...")
        rq57_convergence_file = RQ_DIR / "data" / "step00_rq57_convergence.txt"
        with open(rq57_convergence_file, 'r', encoding='utf-8') as f:
            rq57_status = f.read().strip()
        log(f"[INFO] RQ 5.7 status: {rq57_status}")

        # Flag if RQ 5.7 used fallback random structure
        if "fallback" in rq57_status.lower() or "(1 | UID)" in rq57_status:
            log("[WARNING] RQ 5.7 model used fallback random structure!")
            log("[WARNING] AIC comparison may be invalid (different random structures)")
            log("[WARNING] Proceed with caution - interpret AIC difference qualitatively")
            rq57_fallback = True
        else:
            log("[INFO] RQ 5.7 model converged with maximal random structure")
            log("[INFO] AIC comparison valid (comparable random structures)")
            rq57_fallback = False

        # =========================================================================
        # STEP 2: Fit Piecewise LMM
        # =========================================================================
        # Tool: fit_lmm_trajectory
        # What it does: Fits LMM with Days_within * Segment interaction to test
        #               if Early (0-48h) and Late (48-240h) segments have different
        #               forgetting slopes (two-phase hypothesis Test 2)
        # Expected output: MixedLMResults object with fixed effects for interaction

        log("[ANALYSIS] Fitting piecewise LMM: theta ~ Days_within * Segment + (Days_within | UID)...")
        log("[INFO] Maximal random structure: (Days_within | UID) - allows individual slope differences")
        log("[INFO] Fallback strategy: (Days_within || UID) -> (1 | UID) if convergence fails")

        # Piecewise formula with interaction term (key test for two-phase forgetting)
        formula = "theta ~ Days_within * Segment"

        try:
            # Try maximal random structure first
            piecewise_model = fit_lmm_trajectory(
                data=time_data,
                formula=formula,
                groups="UID",
                re_formula="~Days_within",  # Random slopes for Days_within
                reml=False  # ML estimation required for AIC comparison
            )
            log("[SUCCESS] Piecewise model converged with maximal random structure")
            random_structure_used = "(Days_within | UID)"

        except Exception as e_maximal:
            log(f"[WARNING] Maximal random structure failed: {str(e_maximal)}")
            log("[FALLBACK] Attempting uncorrelated random structure: (Days_within || UID)...")

            try:
                # Fallback 1: Uncorrelated random effects
                # Note: statsmodels doesn't support || syntax, so this is approximated
                piecewise_model = fit_lmm_trajectory(
                    data=time_data,
                    formula=formula,
                    groups="UID",
                    re_formula="~Days_within",  # Same as maximal (statsmodels limitation)
                    reml=False
                )
                log("[SUCCESS] Piecewise model converged with uncorrelated random structure")
                random_structure_used = "(Days_within || UID) - approximated"

            except Exception as e_uncorrelated:
                log(f"[WARNING] Uncorrelated random structure failed: {str(e_uncorrelated)}")
                log("[FALLBACK] Attempting intercept-only random structure: (1 | UID)...")

                # Fallback 2: Intercept-only random effects
                piecewise_model = fit_lmm_trajectory(
                    data=time_data,
                    formula=formula,
                    groups="UID",
                    re_formula="~1",  # Intercept-only
                    reml=False
                )
                log("[SUCCESS] Piecewise model converged with intercept-only random structure")
                random_structure_used = "(1 | UID)"
                log("[WARNING] Random slopes removed - model assumes uniform forgetting rates across individuals")

        log("[DONE] Piecewise LMM fitting complete")
        log(f"[INFO] Random structure used: {random_structure_used}")

        # =========================================================================
        # STEP 3: Extract Model Results and Compare AIC
        # =========================================================================
        # Test 2 Criterion: deltaAIC = AIC_piecewise - AIC_continuous
        # deltaAIC < -2 favors piecewise (segmented forgetting)
        # deltaAIC > +2 favors continuous (single-phase forgetting)
        # -2 <= deltaAIC <= +2 inconclusive (models equivalent)

        log("[EXTRACT] Extracting fixed effects and AIC...")

        # Get piecewise model AIC
        piecewise_aic = piecewise_model.aic
        log(f"[INFO] Piecewise model AIC = {piecewise_aic:.2f}")

        # Compute deltaAIC (Test 2 for two-phase forgetting)
        delta_aic = piecewise_aic - continuous_aic
        log(f"[INFO] deltaAIC = {delta_aic:.2f} (Piecewise - Continuous)")

        # Interpret AIC comparison
        if delta_aic < -2:
            aic_interpretation = "Piecewise model FAVORED (deltaAIC < -2) - Evidence for two-phase forgetting"
        elif delta_aic > 2:
            aic_interpretation = "Continuous model FAVORED (deltaAIC > +2) - Evidence against two-phase forgetting"
        else:
            aic_interpretation = "Models EQUIVALENT (-2 <= deltaAIC <= +2) - Inconclusive evidence"

        log(f"[INTERPRETATION] {aic_interpretation}")

        # Flag if AIC comparison potentially invalid due to RQ 5.7 fallback
        if rq57_fallback and random_structure_used != "(1 | UID)":
            log("[WARNING] AIC comparison validity compromised:")
            log(f"[WARNING]   RQ 5.7 used fallback structure (likely (1 | UID))")
            log(f"[WARNING]   This model used: {random_structure_used}")
            log("[WARNING]   Comparing models with different random structures violates AIC assumptions")
            log("[WARNING]   Interpret deltaAIC qualitatively, not as definitive evidence")

        # Extract fixed effects table
        fixed_effects_table = piecewise_model.summary().tables[1]
        log("[INFO] Fixed effects extracted (Intercept, Days_within, Segment, interaction)")

        # Convert to DataFrame (handle both SimpleTable and DataFrame)
        if hasattr(fixed_effects_table, 'data'):
            # SimpleTable object
            fe_df = pd.DataFrame(fixed_effects_table.data[1:], columns=fixed_effects_table.data[0])
        else:
            # Already a DataFrame
            fe_df = fixed_effects_table.reset_index()
        interaction_term = "Days_within:Segment[T.Late]"

        if interaction_term in fe_df.iloc[:, 0].values:
            interaction_row = fe_df[fe_df.iloc[:, 0] == interaction_term].iloc[0]
            interaction_pval = float(interaction_row.iloc[4])  # P>|z| column
            log(f"[INFO] Interaction p-value = {interaction_pval:.4f}")

            # Decision D068 compliance: Report both uncorrected and Bonferroni-corrected p-values
            bonferroni_alpha = 0.05 / 15  # Chapter 5 family size = 15 tests
            if interaction_pval < bonferroni_alpha:
                log(f"[SIGNIFICANT] Interaction significant at Bonferroni alpha = {bonferroni_alpha:.4f}")
                log("[INTERPRETATION] Early and Late segments have DIFFERENT forgetting rates")
            else:
                log(f"[NOT SIGNIFICANT] Interaction not significant at Bonferroni alpha = {bonferroni_alpha:.4f}")
                log("[INTERPRETATION] Early and Late segments have SIMILAR forgetting rates")
        else:
            log(f"[WARNING] Interaction term {interaction_term} not found in fixed effects")
            interaction_pval = np.nan

        # =========================================================================
        # STEP 4: Generate Predictions for Plotting
        # =========================================================================
        # Purpose: Generate model predictions on grid for visualization
        # Expected: 9 Early timepoints (0-48h) + 9 Late timepoints (0-192h within-segment)

        log("[PREDICT] Generating model predictions for Early and Late segments...")

        # Prediction grids (within-segment time)
        early_grid = np.linspace(0, 48, 9)  # 0, 6, 12, 18, 24, 30, 36, 42, 48 hours
        late_grid = np.linspace(0, 192, 9)  # 0, 24, 48, 72, 96, 120, 144, 168, 192 hours

        # Create prediction DataFrames
        predictions_list = []

        # Early segment predictions (Segment = "Early")
        for days_within in early_grid:
            pred_row = pd.DataFrame({
                'UID': ['PRED'],  # Dummy UID for prediction
                'Segment': ['Early'],
                'Days_within': [days_within],
                'TSVR_hours': [days_within],  # For Early, Days_within = TSVR_hours
                'theta': [0]  # Placeholder (will use fittedvalues)
            })
            predictions_list.append(pred_row)

        # Late segment predictions (Segment = "Late")
        for days_within in late_grid:
            pred_row = pd.DataFrame({
                'UID': ['PRED'],  # Dummy UID for prediction
                'Segment': ['Late'],
                'Days_within': [days_within],
                'TSVR_hours': [48 + days_within],  # Late starts at 48h
                'theta': [0]  # Placeholder
            })
            predictions_list.append(pred_row)

        # Combine prediction rows
        pred_data = pd.concat(predictions_list, ignore_index=True)

        # Get predictions from model (marginal predictions at population level)
        # Note: This is a simplified approach - marginal effects would be more rigorous
        # but require additional dependencies (statsmodels.api.PredictionResults)
        # For RQ 5.8, we use fixed effects predictions as approximation

        log("[INFO] Computing fixed effects predictions (population-level trends)...")

        # Extract fixed effects coefficients
        fe_params = piecewise_model.fe_params
        intercept = fe_params['Intercept']
        slope_early = fe_params['Days_within']
        segment_late = fe_params['Segment[T.Late]']
        interaction = fe_params['Days_within:Segment[T.Late]']

        log(f"[INFO] Fixed effects: Intercept={intercept:.4f}, Days_within={slope_early:.4f}")
        log(f"[INFO]                Segment[Late]={segment_late:.4f}, Interaction={interaction:.4f}")

        # Compute predictions manually from fixed effects
        predictions = []
        for idx, row in pred_data.iterrows():
            segment = row['Segment']
            days = row['Days_within']

            if segment == 'Early':
                # Early segment: theta = Intercept + slope_early * days
                pred_theta = intercept + slope_early * days
            else:  # Late
                # Late segment: theta = (Intercept + segment_late) + (slope_early + interaction) * days
                pred_theta = (intercept + segment_late) + (slope_early + interaction) * days

            predictions.append(pred_theta)

        pred_data['predicted_theta'] = predictions

        # Compute 95% CI (simplified - assumes no uncertainty in fixed effects)
        # For publication-quality CIs, would need delta method or bootstrap
        # Here we use residual SE as approximation
        residual_se = np.sqrt(piecewise_model.scale)  # Residual standard error
        pred_data['CI_lower'] = pred_data['predicted_theta'] - 1.96 * residual_se
        pred_data['CI_upper'] = pred_data['predicted_theta'] + 1.96 * residual_se

        log(f"[DONE] Generated {len(pred_data)} predictions (9 Early + 9 Late)")

        # =========================================================================
        # STEP 5: Save Outputs
        # =========================================================================
        # Output 1: Model summary (text file for human inspection)
        # Output 2: Predictions CSV (for plotting in Step 6)

        log("[SAVE] Saving piecewise model summary...")
        summary_file = RQ_DIR / "results" / "step03_piecewise_model_summary.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("PIECEWISE LMM SUMMARY - RQ 5.8 TEST 2 (Two-Phase Forgetting)\n")
            f.write("=" * 80 + "\n\n")

            f.write(f"Formula: {formula}\n")
            f.write(f"Random Structure: {random_structure_used}\n")
            f.write(f"Estimation: ML (REML=False for AIC comparison)\n")
            f.write(f"N observations: {piecewise_model.nobs}\n")
            f.write(f"N groups (UIDs): {len(piecewise_model.model.group_labels)}\n\n")

            f.write("-" * 80 + "\n")
            f.write("CONVERGENCE STATUS\n")
            f.write("-" * 80 + "\n")
            f.write(f"Converged: {piecewise_model.converged}\n")
            if not piecewise_model.converged:
                f.write("WARNING: Model did not converge - interpret results with caution\n")
            f.write("\n")

            f.write("-" * 80 + "\n")
            f.write("FIXED EFFECTS\n")
            f.write("-" * 80 + "\n")
            f.write(str(piecewise_model.summary().tables[1]))
            f.write("\n\n")

            f.write("-" * 80 + "\n")
            f.write("RANDOM EFFECTS\n")
            f.write("-" * 80 + "\n")
            f.write(str(piecewise_model.summary().tables[0]))
            f.write("\n\n")

            f.write("-" * 80 + "\n")
            f.write("AIC COMPARISON (TEST 2 FOR TWO-PHASE FORGETTING)\n")
            f.write("-" * 80 + "\n")
            f.write(f"Piecewise model AIC:  {piecewise_aic:.2f}\n")
            f.write(f"Continuous model AIC: {continuous_aic:.2f} (from RQ 5.7)\n")
            f.write(f"deltaAIC:             {delta_aic:.2f} (Piecewise - Continuous)\n\n")
            f.write(f"Interpretation: {aic_interpretation}\n\n")

            if rq57_fallback and random_structure_used != "(1 | UID)":
                f.write("WARNING: AIC comparison validity compromised\n")
                f.write("  RQ 5.7 used fallback random structure (likely (1 | UID))\n")
                f.write(f"  This model used: {random_structure_used}\n")
                f.write("  Comparing models with different random structures violates AIC assumptions\n")
                f.write("  Interpret deltaAIC qualitatively, not as definitive evidence\n\n")

            f.write("-" * 80 + "\n")
            f.write("INTERACTION TEST (KEY EVIDENCE FOR TWO-PHASE FORGETTING)\n")
            f.write("-" * 80 + "\n")
            f.write(f"Interaction term: {interaction_term}\n")
            f.write(f"p-value (uncorrected): {interaction_pval:.4f}\n")
            f.write(f"p-value (Bonferroni):  {interaction_pval * 15:.4f} (x 15 Chapter 5 tests)\n")
            f.write(f"Bonferroni alpha:      {bonferroni_alpha:.4f}\n\n")

            if interaction_pval < bonferroni_alpha:
                f.write("RESULT: Interaction SIGNIFICANT\n")
                f.write("INTERPRETATION: Early and Late segments have DIFFERENT forgetting rates\n")
                f.write("                (Evidence for two-phase forgetting hypothesis)\n")
            else:
                f.write("RESULT: Interaction NOT SIGNIFICANT\n")
                f.write("INTERPRETATION: Early and Late segments have SIMILAR forgetting rates\n")
                f.write("                (Evidence against two-phase forgetting hypothesis)\n")

        log(f"[SAVED] {summary_file}")

        log("[SAVE] Saving piecewise predictions...")
        pred_output = pred_data[['Segment', 'Days_within', 'TSVR_hours', 'predicted_theta', 'CI_lower', 'CI_upper']]
        pred_output.to_csv(RQ_DIR / "data" / "step03_piecewise_predictions.csv", index=False, encoding='utf-8')
        log(f"[SAVED] data/step03_piecewise_predictions.csv ({len(pred_output)} rows)")

        # Save pickled model for Step 4 validation
        log("[SAVE] Saving piecewise model pickle...")
        import pickle
        model_pkl_path = RQ_DIR / "data" / "step03_piecewise_model.pkl"
        piecewise_model.save(str(model_pkl_path))
        log(f"[SAVED] {model_pkl_path.name}")

        # =========================================================================
        # STEP 6: Run Validation Tool
        # =========================================================================
        # Tool: validate_lmm_convergence
        # Validates: Model convergence status and parameter estimates
        # Threshold: Convergence = True (or fallback documented)

        log("[VALIDATION] Running validate_lmm_convergence...")
        validation_result = validate_lmm_convergence(lmm_result=piecewise_model)

        # Report validation results
        if isinstance(validation_result, dict):
            for key, value in validation_result.items():
                log(f"[VALIDATION] {key}: {value}")
        else:
            log(f"[VALIDATION] {validation_result}")

        # Additional validation checks specific to piecewise model
        log("[VALIDATION] Checking fixed effects validity...")

        # Check all fixed effects are finite (not NaN/Inf)
        fe_values = piecewise_model.fe_params.values
        if np.all(np.isfinite(fe_values)):
            log("[VALIDATION] [PASS] All fixed effects finite")
        else:
            raise ValueError("Fixed effects contain NaN or Inf - model estimation failed")

        # Check interaction p-value in valid range
        if 0 <= interaction_pval <= 1 or np.isnan(interaction_pval):
            log(f"[VALIDATION] [PASS] Interaction p-value in valid range: {interaction_pval:.4f}")
        else:
            raise ValueError(f"Interaction p-value out of bounds: {interaction_pval}")

        # Check AIC comparison computed successfully
        if np.isfinite(delta_aic):
            log(f"[VALIDATION] [PASS] deltaAIC computed successfully: {delta_aic:.2f}")
        else:
            raise ValueError(f"deltaAIC computation failed: {delta_aic}")

        # Check predictions generated for all timepoints
        if len(pred_output) == 18:
            log("[VALIDATION] [PASS] All 18 predictions generated (9 Early + 9 Late)")
        else:
            raise ValueError(f"Expected 18 predictions, got {len(pred_output)}")

        log("[SUCCESS] Step 3 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
