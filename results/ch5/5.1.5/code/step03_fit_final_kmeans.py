#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: 03
Step Name: Fit Final K-means Model with Optimal K
RQ: results/ch5/5.1.5
Generated: 2025-12-02

PURPOSE:
Fit final K-means clustering model with optimal K selected in Step 2 via BIC.
Extracts cluster assignments and centers. Enforces 10% minimum cluster size
threshold to prevent degenerate solutions with undersized clusters. If any
cluster falls below threshold, reduces K by 1 and refits until all clusters
meet size requirement.

EXPECTED INPUTS:
  - data/step01_standardized_features.csv
    Columns: ['UID', 'Intercept_z', 'Slope_z']
    Format: Z-scored clustering features (mean=0, SD=1)
    Expected rows: ~100 (all participants)

  - data/step02_optimal_k.txt
    Columns: Single integer value
    Format: Plain text file with optimal K from BIC model selection
    Expected value: K=2 (from Step 2 BIC minimum)

EXPECTED OUTPUTS:
  - data/step03_cluster_assignments.csv
    Columns: ['UID', 'cluster']
    Format: Cluster assignments for 100 participants (cluster IDs: 0, 1, ..., K-1)
    Expected rows: ~100

  - data/step03_cluster_centers.csv
    Columns: ['cluster', 'Intercept_z_center', 'Slope_z_center']
    Format: Cluster centers (mean Intercept_z, mean Slope_z per cluster)
    Expected rows: ~K (number of clusters)

  - data/step03_remedial_action.txt (conditional)
    Format: Report of K reduction if remedial action taken (K_initial -> K_final)
    Condition: Only created if any cluster < 10% threshold

VALIDATION CRITERIA:
  - All 100 participants assigned (no missing cluster labels)
  - Cluster IDs consecutive starting from 0 (0, 1, ..., K_final-1)
  - Each cluster has >= 10 participants (10% size threshold)
  - No NaN in cluster assignments

g_code REASONING:
- Approach: Fit K-means with optimal K from Step 2, extract assignments and
  centers, check cluster sizes, enforce 10% threshold via iterative K reduction
- Why this approach: Prevents degenerate clustering solutions with singleton
  or undersized clusters that lack statistical power for downstream analyses
- Data flow: Standardized features (100x2) + optimal K -> K-means fit -> cluster
  assignments (100x1) + cluster centers (Kx2), with remedial size enforcement
- Expected performance: ~1 second per K-means fit (100 points, 2 dimensions)

IMPLEMENTATION NOTES:
- Analysis tool: sklearn.cluster.KMeans (stdlib, no custom tools)
- Validation tool: tools.validation.validate_cluster_assignment
- Parameters: random_state=42 (reproducibility), n_init=50 (robust initialization),
  min_cluster_size=10 (10% threshold for N=100)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback
from sklearn.cluster import KMeans

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_cluster_assignment

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.1.5 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step03_fit_final_kmeans.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step03_cluster_assignments.csv
#   CORRECT: data/step03_cluster_centers.csv
#   WRONG:   results/cluster_assignments.csv  (wrong folder + no prefix)
#   WRONG:   data/cluster_assignments.csv     (missing step prefix)
#   WRONG:   logs/step03_assignments.csv      (CSV in logs folder)

# =============================================================================
# K-means Parameters
# =============================================================================

RANDOM_STATE = 42
N_INIT = 50
MIN_CLUSTER_SIZE = 10  # 10% of N=100

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 03: Fit Final K-means Model with Optimal K")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Standardized features (100x2: Intercept_z, Slope_z) from Step 1
        # Purpose: Clustering features with equal weighting (z-scored)

        log("[LOAD] Loading standardized features...")
        standardized_features = pd.read_csv(RQ_DIR / "data/step01_standardized_features.csv")
        log(f"[LOADED] standardized_features ({len(standardized_features)} rows, {len(standardized_features.columns)} cols)")

        # Extract feature matrix (100 x 2)
        X = standardized_features[['Intercept_z', 'Slope_z']].values
        log(f"[INFO] Feature matrix shape: {X.shape}")

        # =========================================================================
        # STEP 2: Read Optimal K from Step 2
        # =========================================================================
        # Expected: Single integer K_optimal from BIC model selection
        # Purpose: Use BIC-selected K as starting point for final clustering

        log("[LOAD] Reading optimal K from Step 2...")
        with open(RQ_DIR / "data/step02_optimal_k.txt", 'r') as f:
            K_initial = int(f.read().strip())
        log(f"[INFO] Optimal K from Step 2 (BIC minimum): K_initial = {K_initial}")

        # =========================================================================
        # STEP 3: Fit K-means with Remedial Size Enforcement
        # =========================================================================
        # Tool: sklearn.cluster.KMeans (stdlib)
        # What it does: Fit K-means with K_initial, check cluster sizes, reduce K
        #   if any cluster < 10% threshold, refit until all clusters meet threshold
        # Expected output: Cluster assignments and centers with valid cluster sizes

        K_current = K_initial
        remedial_action_taken = False
        remedial_log = []

        log(f"[ANALYSIS] Starting K-means fitting with K={K_current}...")
        log(f"[INFO] Min cluster size threshold: {MIN_CLUSTER_SIZE} (10% of N=100)")

        while True:
            # Fit K-means with current K
            log(f"[FIT] Fitting K-means with K={K_current} (random_state={RANDOM_STATE}, n_init={N_INIT})...")
            kmeans = KMeans(n_clusters=K_current, random_state=RANDOM_STATE, n_init=N_INIT)
            cluster_labels = kmeans.fit_predict(X)
            cluster_centers = kmeans.cluster_centers_

            # Check cluster sizes
            unique_labels, cluster_counts = np.unique(cluster_labels, return_counts=True)
            log(f"[INFO] Cluster sizes (K={K_current}): {dict(zip(unique_labels, cluster_counts))}")

            # Check if any cluster is undersized
            min_cluster_size = cluster_counts.min()
            if min_cluster_size < MIN_CLUSTER_SIZE:
                log(f"[WARNING] Undersized cluster detected: min size = {min_cluster_size} < {MIN_CLUSTER_SIZE}")
                if K_current == 1:
                    log(f"[ERROR] Cannot reduce K below 1. Accepting K=1 despite size issue.")
                    remedial_action_taken = True
                    remedial_log.append(f"K={K_current}: Min cluster size {min_cluster_size} < {MIN_CLUSTER_SIZE}, but K=1 is minimum possible.")
                    break
                else:
                    # Reduce K by 1 and refit
                    remedial_action_taken = True
                    remedial_log.append(f"K={K_current}: Min cluster size {min_cluster_size} < {MIN_CLUSTER_SIZE}. Reducing K to {K_current-1}.")
                    log(f"[REMEDIAL] Reducing K from {K_current} to {K_current-1} and refitting...")
                    K_current -= 1
            else:
                log(f"[PASS] All clusters meet size threshold (min size = {min_cluster_size} >= {MIN_CLUSTER_SIZE})")
                break

        K_final = K_current
        log(f"[DONE] Final K-means fit complete with K_final = {K_final}")

        # =========================================================================
        # STEP 4: Extract Cluster Assignments and Centers
        # =========================================================================
        # Output 1: Cluster assignments (100 rows: UID, cluster)
        # Output 2: Cluster centers (K rows: cluster, Intercept_z_center, Slope_z_center)

        log("[EXTRACT] Extracting cluster assignments...")
        cluster_assignments = pd.DataFrame({
            'UID': standardized_features['UID'],
            'cluster': cluster_labels
        })
        log(f"[INFO] Cluster assignments shape: {cluster_assignments.shape}")

        log("[EXTRACT] Extracting cluster centers...")
        cluster_centers_df = pd.DataFrame({
            'cluster': np.arange(K_final),
            'Intercept_z_center': cluster_centers[:, 0],
            'Slope_z_center': cluster_centers[:, 1]
        })
        log(f"[INFO] Cluster centers shape: {cluster_centers_df.shape}")

        # =========================================================================
        # STEP 5: Save Outputs
        # =========================================================================
        # These outputs will be used by: Step 4 (bootstrap stability), Step 6 (characterization), Step 7 (plots)

        log(f"[SAVE] Saving cluster assignments to data/step03_cluster_assignments.csv...")
        cluster_assignments.to_csv(RQ_DIR / "data/step03_cluster_assignments.csv", index=False, encoding='utf-8')
        log(f"[SAVED] step03_cluster_assignments.csv ({len(cluster_assignments)} rows, {len(cluster_assignments.columns)} cols)")

        log(f"[SAVE] Saving cluster centers to data/step03_cluster_centers.csv...")
        cluster_centers_df.to_csv(RQ_DIR / "data/step03_cluster_centers.csv", index=False, encoding='utf-8')
        log(f"[SAVED] step03_cluster_centers.csv ({len(cluster_centers_df)} rows, {len(cluster_centers_df.columns)} cols)")

        # Save remedial action report if K was reduced
        if remedial_action_taken:
            log(f"[SAVE] Saving remedial action report to data/step03_remedial_action.txt...")
            remedial_report = f"Remedial Action Report - K-means Cluster Size Enforcement\n"
            remedial_report += f"=" * 60 + "\n\n"
            remedial_report += f"K_initial (from Step 2 BIC): {K_initial}\n"
            remedial_report += f"K_final (after size enforcement): {K_final}\n\n"
            remedial_report += f"Remedial actions taken:\n"
            for entry in remedial_log:
                remedial_report += f"  - {entry}\n"
            remedial_report += f"\nFinal cluster sizes:\n"
            for label, count in zip(unique_labels, cluster_counts):
                remedial_report += f"  - Cluster {label}: {count} participants\n"

            with open(RQ_DIR / "data/step03_remedial_action.txt", 'w', encoding='utf-8') as f:
                f.write(remedial_report)
            log(f"[SAVED] step03_remedial_action.txt (K reduced from {K_initial} to {K_final})")

        # =========================================================================
        # STEP 6: Run Validation Tool
        # =========================================================================
        # Tool: tools.validation.validate_cluster_assignment
        # Validates: All participants assigned, cluster IDs consecutive, sizes >= threshold
        # Threshold: min_cluster_size=10 (10% of N=100)

        log("[VALIDATION] Running validate_cluster_assignment...")
        validation_result = validate_cluster_assignment(
            assignments_df=cluster_assignments,
            n_participants=100,
            min_cluster_size=MIN_CLUSTER_SIZE,
            cluster_col='cluster'
        )

        # Report validation results
        # Expected: valid=True, all 100 participants assigned, K_final clusters with sizes >= 10
        if validation_result['valid']:
            log(f"[VALIDATION] PASS - {validation_result['message']}")
            log(f"[VALIDATION] Cluster sizes: {validation_result['cluster_sizes']}")
        else:
            log(f"[VALIDATION] FAIL - {validation_result['message']}")
            raise ValueError(f"Cluster assignment validation failed: {validation_result['message']}")

        log(f"[SUCCESS] Step 03 complete - K_final={K_final}, all clusters meet size threshold")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
