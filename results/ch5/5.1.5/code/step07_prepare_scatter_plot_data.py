#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: 07
Step Name: prepare_scatter_plot_data
RQ: results/ch5/5.1.5
Generated: 2025-12-02

PURPOSE:
Prepare scatter plot source data for rq_plots agent visualization. Creates
participant scatter data (Intercept_z, Slope_z, cluster), cluster centers
data, and plot metadata YAML containing silhouette score, K_final, and axis
labels. Follows Option B plot architecture where analysis scripts prepare
plot-ready CSVs consumed by rq_plots.

EXPECTED INPUTS:
  - data/step01_standardized_features.csv
    Columns: ['UID', 'Intercept_z', 'Slope_z']
    Format: Z-scored clustering features (mean=0, SD=1)
    Expected rows: ~100

  - data/step03_cluster_assignments.csv
    Columns: ['UID', 'cluster']
    Format: Cluster assignments (0, 1, ..., K-1)
    Expected rows: ~100

  - data/step03_cluster_centers.csv
    Columns: ['cluster', 'Intercept_z_center', 'Slope_z_center']
    Format: Cluster centroids in z-score space
    Expected rows: ~K_final (2-3 expected)

  - data/step05_silhouette_score.txt
    Format: Float value with interpretation line
    Content: Silhouette coefficient (range [-1, 1])

EXPECTED OUTPUTS:
  - data/step07_scatter_plot_data.csv
    Columns: ['Intercept_z', 'Slope_z', 'cluster']
    Format: Participant points for scatter plot
    Expected rows: ~100

  - data/step07_scatter_plot_centers.csv
    Columns: ['Intercept_z_center', 'Slope_z_center', 'cluster']
    Format: Cluster centers for scatter plot overlay
    Expected rows: ~K_final

  - data/step07_scatter_plot_metadata.yaml
    Format: YAML with silhouette_score, K_final, axis_labels, reference_lines
    Purpose: Plot configuration for rq_plots agent

VALIDATION CRITERIA:
  - 100 participant rows in scatter_plot_data.csv
  - K_final cluster center rows in scatter_plot_centers.csv
  - Cluster IDs match between data and centers
  - No NaN values in plot data

g_code REASONING:
- Approach: Merge standardized features with cluster assignments, extract
  cluster centers, read silhouette score, package into plot-ready CSVs + YAML
- Why this approach: Option B plot architecture (rq_plots reads from data/,
  analysis scripts prepare plot-ready data). Separates analysis logic from
  plotting logic, enables reproducible visualizations.
- Data flow: Features + assignments -> merged participant data -> scatter CSV,
  centers CSV separately extracted, metadata bundled in YAML
- Expected performance: ~seconds (simple pandas merge + file I/O)

IMPLEMENTATION NOTES:
- Analysis tool: pandas.merge (stdlib)
- Validation tool: validate_plot_data_completeness from tools.validation
- Parameters: Inner merge on UID, extract cluster IDs for metadata
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import yaml
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_plot_data_completeness

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.1.5 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step07_prepare_scatter_plot_data.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step07_scatter_plot_data.csv
#   CORRECT: data/step07_scatter_plot_centers.csv
#   WRONG:   results/scatter_plot_data.csv  (wrong folder + no prefix)
#   WRONG:   data/scatter_data.csv           (missing step prefix)
#   WRONG:   logs/step07_plot_data.csv       (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 7: Prepare Scatter Plot Data")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Standardized features (100 x 3: UID, Intercept_z, Slope_z)
        #           Cluster assignments (100 x 2: UID, cluster)
        #           Cluster centers (K x 3: cluster, Intercept_z_center, Slope_z_center)
        #           Silhouette score (single float value)
        # Purpose: Merge features with assignments, extract centers, package metadata

        log("[LOAD] Loading standardized features...")
        standardized_features = pd.read_csv(RQ_DIR / "data/step01_standardized_features.csv")
        log(f"[LOADED] standardized_features ({len(standardized_features)} rows, {len(standardized_features.columns)} cols)")

        log("[LOAD] Loading cluster assignments...")
        cluster_assignments = pd.read_csv(RQ_DIR / "data/step03_cluster_assignments.csv")
        log(f"[LOADED] cluster_assignments ({len(cluster_assignments)} rows, {len(cluster_assignments.columns)} cols)")

        log("[LOAD] Loading cluster centers...")
        cluster_centers = pd.read_csv(RQ_DIR / "data/step03_cluster_centers.csv")
        log(f"[LOADED] cluster_centers ({len(cluster_centers)} rows, {len(cluster_centers.columns)} cols)")

        log("[LOAD] Loading silhouette score...")
        with open(RQ_DIR / "data/step05_silhouette_score.txt", 'r', encoding='utf-8') as f:
            lines = f.readlines()
            # First line should contain silhouette score (e.g., "0.45" or "Silhouette: 0.45")
            silhouette_line = lines[0].strip()
            # Extract float value (handle "Silhouette: 0.45" or "0.45" formats)
            if ':' in silhouette_line:
                silhouette_score = float(silhouette_line.split(':')[-1].strip())
            else:
                silhouette_score = float(silhouette_line)
        log(f"[LOADED] silhouette_score = {silhouette_score:.3f}")

        # =========================================================================
        # STEP 2: Merge Features with Assignments
        # =========================================================================
        # Tool: pandas.merge (inner join on UID)
        # What it does: Combines standardized features with cluster assignments
        # Expected output: 100 rows with Intercept_z, Slope_z, cluster

        log("[MERGE] Merging standardized features with cluster assignments...")
        # Merge on UID (inner join ensures only matched participants)
        merged_data = standardized_features.merge(cluster_assignments, on='UID', how='inner')
        log(f"[MERGE] Merged data: {len(merged_data)} rows (expected 100)")

        # Validate merge produced expected row count
        if len(merged_data) != 100:
            raise ValueError(f"Merge produced {len(merged_data)} rows, expected 100 (all participants)")

        # =========================================================================
        # STEP 3: Create Scatter Plot Data CSV
        # =========================================================================
        # Output: Participant points for scatter plot (drop UID, keep features + cluster)
        # Columns: Intercept_z, Slope_z, cluster
        # These are the points plotted in 2D space colored by cluster

        log("[CREATE] Creating scatter plot data CSV...")
        scatter_plot_data = merged_data[['Intercept_z', 'Slope_z', 'cluster']].copy()
        scatter_plot_data.to_csv(RQ_DIR / "data/step07_scatter_plot_data.csv", index=False, encoding='utf-8')
        log(f"[SAVED] data/step07_scatter_plot_data.csv ({len(scatter_plot_data)} rows, {len(scatter_plot_data.columns)} cols)")

        # =========================================================================
        # STEP 4: Create Cluster Centers CSV
        # =========================================================================
        # Output: Cluster centroids for overlay markers
        # Columns: Intercept_z_center, Slope_z_center, cluster
        # These are the large markers overlaid on participant points

        log("[CREATE] Creating cluster centers CSV...")
        scatter_plot_centers = cluster_centers[['Intercept_z_center', 'Slope_z_center', 'cluster']].copy()
        scatter_plot_centers.to_csv(RQ_DIR / "data/step07_scatter_plot_centers.csv", index=False, encoding='utf-8')
        log(f"[SAVED] data/step07_scatter_plot_centers.csv ({len(scatter_plot_centers)} rows, {len(scatter_plot_centers.columns)} cols)")

        # =========================================================================
        # STEP 5: Create Plot Metadata YAML
        # =========================================================================
        # Output: Plot configuration for rq_plots agent
        # Contains: silhouette_score, K_final, axis_labels, reference_lines
        # Purpose: Enables rq_plots to annotate plot with quality metrics

        log("[CREATE] Creating plot metadata YAML...")
        K_final = len(cluster_centers)  # Number of clusters

        plot_metadata = {
            'silhouette_score': float(silhouette_score),
            'K_final': int(K_final),
            'axis_labels': {
                'x': 'Random Intercept (z-scored)',
                'y': 'Random Slope (z-scored)'
            },
            'reference_lines': {
                'x_zero': True,  # Draw vertical line at x=0 (mean intercept)
                'y_zero': True   # Draw horizontal line at y=0 (mean slope)
            },
            'interpretation': {
                'silhouette': (
                    'Strong (>= 0.50)' if silhouette_score >= 0.50 else
                    'Reasonable (0.25-0.49)' if silhouette_score >= 0.25 else
                    'Weak (< 0.25)'
                )
            }
        }

        with open(RQ_DIR / "data/step07_scatter_plot_metadata.yaml", 'w', encoding='utf-8') as f:
            yaml.dump(plot_metadata, f, default_flow_style=False, sort_keys=False)
        log(f"[SAVED] data/step07_scatter_plot_metadata.yaml (silhouette={silhouette_score:.3f}, K={K_final})")

        # =========================================================================
        # STEP 6: Run Validation Tool
        # =========================================================================
        # Tool: validate_plot_data_completeness
        # Validates: All participants present, cluster IDs match, no NaN
        # Note: For this RQ, we don't have domains/groups - use None for those parameters

        log("[VALIDATION] Running validate_plot_data_completeness...")

        # NOTE: validate_plot_data_completeness expects domain/group columns
        # For clustering scatter plot, we treat 'cluster' as domain_col
        # Required domains are the cluster IDs (0, 1 for K=2)
        expected_cluster_ids = list(range(K_final))

        validation_result = validate_plot_data_completeness(
            plot_data=scatter_plot_data,
            required_domains=expected_cluster_ids,  # Cluster IDs 0, 1
            required_groups=[],                      # No group variable - empty list instead of None
            domain_col='cluster',                    # Use cluster as domain identifier
            group_col='cluster'                      # Use cluster as placeholder (required)
        )

        # Report validation results
        # Expected: valid=True, all participants present, cluster IDs match
        log(f"[VALIDATION] valid: {validation_result['valid']}")
        log(f"[VALIDATION] message: {validation_result['message']}")

        if not validation_result['valid']:
            raise ValueError(f"Plot data validation failed: {validation_result['message']}")

        # Additional manual checks specific to clustering scatter plot
        log("[VALIDATION] Additional clustering-specific checks...")

        # Check 100 participant rows
        if len(scatter_plot_data) != 100:
            raise ValueError(f"Expected 100 participant rows, got {len(scatter_plot_data)}")
        log(f"[VALIDATION] Participant rows: {len(scatter_plot_data)} (expected 100) [PASS]")

        # Check K_final center rows
        if len(scatter_plot_centers) != K_final:
            raise ValueError(f"Expected {K_final} center rows, got {len(scatter_plot_centers)}")
        log(f"[VALIDATION] Center rows: {len(scatter_plot_centers)} (expected {K_final}) [PASS]")

        # Check cluster IDs match between data and centers
        data_clusters = set(scatter_plot_data['cluster'].unique())
        center_clusters = set(scatter_plot_centers['cluster'].unique())
        if data_clusters != center_clusters:
            raise ValueError(f"Cluster ID mismatch: data={data_clusters}, centers={center_clusters}")
        log(f"[VALIDATION] Cluster IDs match: {sorted(data_clusters)} [PASS]")

        # Check no NaN values
        if scatter_plot_data.isna().any().any():
            raise ValueError("NaN values found in scatter_plot_data")
        if scatter_plot_centers.isna().any().any():
            raise ValueError("NaN values found in scatter_plot_centers")
        log("[VALIDATION] No NaN values [PASS]")

        log("[SUCCESS] Step 7 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
