#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: 02
Step Name: test_k_clusters
RQ: results/ch5/5.1.5
Generated: 2025-12-02
UPDATED: 2025-12-09 (EXTENDED TO K=1-10)

PURPOSE:
Test K-means clustering for K=1 to K=10 clusters (EXTENDED from K=1-6), compute BIC
for each model, and select optimal K via BIC minimization. Uses scipy.cluster.vq.kmeans2
for clustering implementation. BIC formula: N * log(inertia/N) + K * log(N).

CRITICAL UPDATE (2025-12-09): Extended K range from K=1-6 to K=1-10 for more thorough
exploration of cluster space. This follows GOLD standard protocol for cluster model selection.

EXPECTED INPUTS:
  - data/step01_standardized_features.csv
    Columns: ['UID', 'Intercept_z', 'Slope_z']
    Format: Z-scored random effects (mean=0, SD=1)
    Expected rows: ~100 participants

EXPECTED OUTPUTS:
  - data/step02_cluster_selection.csv
    Columns: ['K', 'inertia', 'BIC']
    Format: K-means results for K=1 to K=10
    Expected rows: 10 (one per K value)

  - data/step02_optimal_k.txt
    Format: Single integer (optimal K selected by BIC minimum)

VALIDATION CRITERIA:
  - Inertia values >= 0 (positive)
  - Inertia monotonically decreasing with K
  - BIC values finite (not NaN, not inf)
  - All 10 K values tested (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

g_code REASONING:
- Approach: Exhaustive K-means testing over K=1 to K=10 with BIC model selection
- Why this approach: BIC balances model fit (inertia) against complexity (K) to
  prevent overfitting while penalizing excessive cluster counts. Extended range
  (K=1-10) provides more thorough exploration per GOLD standard.
- Data flow: Standardized features → K-means loop → Inertia + BIC computation
  → Optimal K selection → Save results
- Expected performance: ~15-45 seconds for 10 K-means fits with n_init=50

IMPLEMENTATION NOTES:
- Analysis tool: scipy.cluster.vq.kmeans2 (stdlib K-means implementation)
- Validation tool: tools.validation.validate_numeric_range
- Parameters: K range [1-10], random_state=42, n_init=50, BIC formula per spec
- BIC interpretation: Lower BIC = better model (optimal K at minimum BIC)
- Inertia: Within-cluster sum of squared distances (WCSS)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/ (5.1.5)
#   parents[2] = chX/ (ch5)
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tools
from scipy.cluster.vq import kmeans2

# Import validation tool
from tools.validation import validate_numeric_range

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.1.5 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step02_test_k_clusters.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step02_cluster_selection.csv
#   CORRECT: data/step02_optimal_k.txt
#   WRONG:   results/cluster_selection.csv  (wrong folder + no prefix)
#   WRONG:   data/cluster_selection.csv     (missing step prefix)
#   WRONG:   logs/step02_clusters.csv       (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# K-means BIC Computation
# =============================================================================

def compute_bic_for_kmeans(X, K, n_init=50, random_state=42):
    """
    Fit K-means with K clusters and compute BIC.

    BIC formula: N * log(inertia/N) + K * log(N)
    where N = number of samples, inertia = within-cluster sum of squared distances

    Parameters:
    -----------
    X : np.ndarray
        Feature matrix (N x D)
    K : int
        Number of clusters
    n_init : int
        Number of initializations (best result kept)
    random_state : int
        Random seed for reproducibility

    Returns:
    --------
    inertia : float
        Within-cluster sum of squared distances
    bic : float
        Bayesian Information Criterion value
    """
    N = X.shape[0]

    # Handle K=1 case (all points in one cluster)
    if K == 1:
        # Inertia = sum of squared distances from global mean
        centroid = X.mean(axis=0, keepdims=True)
        distances = np.linalg.norm(X - centroid, axis=1)
        inertia = np.sum(distances ** 2)
        bic = N * np.log(inertia / N) + K * np.log(N)
        return inertia, bic

    # For K > 1, use scipy.cluster.vq.kmeans2
    # Run multiple initializations and keep best result (lowest inertia)
    best_inertia = np.inf
    best_labels = None
    best_centroids = None

    for seed_offset in range(n_init):
        try:
            # kmeans2 uses different random seed each iteration
            np.random.seed(random_state + seed_offset)
            centroids, labels = kmeans2(X, K, minit='points', iter=300)

            # Compute inertia for this initialization
            inertia = 0.0
            for k in range(K):
                cluster_points = X[labels == k]
                if len(cluster_points) > 0:
                    centroid = centroids[k]
                    distances = np.linalg.norm(cluster_points - centroid, axis=1)
                    inertia += np.sum(distances ** 2)

            # Keep best result
            if inertia < best_inertia:
                best_inertia = inertia
                best_labels = labels
                best_centroids = centroids

        except Exception as e:
            # Handle convergence failures gracefully (continue with other inits)
            log(f"[WARNING] K-means K={K} seed={random_state + seed_offset} failed: {e}")
            continue

    if best_inertia == np.inf:
        raise ValueError(f"All {n_init} initializations failed for K={K}")

    # Compute BIC using best inertia
    inertia = best_inertia
    bic = N * np.log(inertia / N) + K * np.log(N)

    return inertia, bic

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 02: Test K Clusters")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Standardized features (Intercept_z, Slope_z) from Step 1
        # Purpose: Feature matrix for K-means clustering (100 x 2)

        log("[LOAD] Loading standardized features from Step 1...")
        input_path = RQ_DIR / "data" / "step01_standardized_features.csv"

        if not input_path.exists():
            raise FileNotFoundError(f"Input file missing: {input_path}")

        df_features = pd.read_csv(input_path)
        log(f"[LOADED] {input_path.name} ({len(df_features)} rows, {len(df_features.columns)} cols)")

        # Validate required columns present
        required_cols = ['Intercept_z', 'Slope_z']
        if not all(col in df_features.columns for col in required_cols):
            raise ValueError(f"Missing required columns. Expected: {required_cols}, Found: {df_features.columns.tolist()}")

        # Extract feature matrix (N x 2)
        X = df_features[required_cols].values
        N = X.shape[0]
        log(f"[INFO] Feature matrix shape: {X.shape} (N={N} participants, D=2 features)")

        # Check for NaN values (not tolerated in clustering)
        if np.isnan(X).any():
            raise ValueError("NaN values found in feature matrix - cannot perform K-means")

        # =========================================================================
        # STEP 2: Run K-means for K=1 to K=10 and Compute BIC
        # =========================================================================
        # Tool: scipy.cluster.vq.kmeans2
        # What it does: Fit K-means clustering for each K value, compute inertia (WCSS)
        # Expected output: Table with K, inertia, BIC for K=1 to K=10

        log("[ANALYSIS] Testing K-means for K=1 to K=10 (EXTENDED range)...")

        k_range = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
        random_state = 42
        n_init = 50

        results = []

        for K in k_range:
            log(f"[KMEANS] Fitting K={K} clusters (n_init={n_init})...")

            # Fit K-means and compute BIC
            inertia, bic = compute_bic_for_kmeans(
                X=X,
                K=K,
                n_init=n_init,
                random_state=random_state
            )

            log(f"[RESULT] K={K}: inertia={inertia:.4f}, BIC={bic:.4f}")

            # Record results
            results.append({
                'K': K,
                'inertia': inertia,
                'BIC': bic
            })

        # Create results DataFrame
        df_results = pd.DataFrame(results)
        log(f"[DONE] K-means testing complete for {len(k_range)} K values")

        # =========================================================================
        # STEP 3: Select Optimal K via BIC Minimum (with Elbow Fallback)
        # =========================================================================
        # BIC interpretation: Lower BIC = better model
        # Optimal K: argmin(BIC) - K with lowest BIC value
        # FALLBACK: If BIC minimum is at boundary (K=10), use elbow method instead
        # Elbow method: Find K where second derivative of inertia is maximized

        log("[SELECTION] Selecting optimal K via BIC minimum...")

        bic_optimal_idx = df_results['BIC'].idxmin()
        bic_optimal_k = int(df_results.loc[bic_optimal_idx, 'K'])
        bic_optimal_value = df_results.loc[bic_optimal_idx, 'BIC']

        log(f"[BIC] BIC minimum at K={bic_optimal_k} (BIC = {bic_optimal_value:.4f})")

        # Check if BIC is at boundary (K=max tested)
        max_k_tested = max(k_range)
        if bic_optimal_k == max_k_tested:
            log(f"[WARNING] BIC minimum at boundary K={max_k_tested} - using elbow method instead")

            # Elbow method: Second derivative of inertia curve
            # Maximum second derivative indicates sharpest "bend" in the curve
            inertia_values = df_results['inertia'].values
            first_deriv = np.diff(inertia_values)  # Rate of decrease
            second_deriv = np.diff(first_deriv)    # Acceleration (positive = elbow)

            # Elbow at K where second derivative is maximum
            # second_deriv[i] corresponds to K=i+2 (since diff reduces length by 1 twice)
            elbow_idx = np.argmax(second_deriv)
            elbow_k = elbow_idx + 2  # Convert index to K value

            log(f"[ELBOW] Second derivative analysis:")
            for i, d in enumerate(second_deriv):
                k_value = i + 2
                marker = " <-- ELBOW" if k_value == elbow_k else ""
                log(f"  K={k_value}: second_deriv={d:.4f}{marker}")

            optimal_k = elbow_k
            optimal_method = "elbow"
            log(f"[OPTIMAL] K_optimal = {optimal_k} (via elbow method due to BIC boundary)")
        else:
            optimal_k = bic_optimal_k
            optimal_method = "BIC"
            log(f"[OPTIMAL] K_optimal = {optimal_k} (via BIC minimum)")

        optimal_bic = df_results.loc[df_results['K'] == optimal_k, 'BIC'].values[0]

        # Log full BIC comparison table
        log("[BIC COMPARISON]")
        for _, row in df_results.iterrows():
            marker = " <-- OPTIMAL" if row['K'] == optimal_k else ""
            log(f"  K={int(row['K'])}: BIC={row['BIC']:.4f}{marker}")

        # =========================================================================
        # STEP 4: Save Analysis Outputs
        # =========================================================================
        # These outputs will be used by: Step 3 (fit final K-means with optimal K)

        # Save cluster selection results (K, inertia, BIC for all 6 K values)
        output_path_results = RQ_DIR / "data" / "step02_cluster_selection.csv"
        log(f"[SAVE] Saving cluster selection results to {output_path_results.name}...")
        df_results.to_csv(output_path_results, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path_results.name} ({len(df_results)} rows, {len(df_results.columns)} cols)")

        # Save optimal K to text file (single integer)
        output_path_k = RQ_DIR / "data" / "step02_optimal_k.txt"
        log(f"[SAVE] Saving optimal K to {output_path_k.name}...")
        with open(output_path_k, 'w', encoding='utf-8') as f:
            f.write(f"{optimal_k}\n")
        log(f"[SAVED] {output_path_k.name} (K_optimal = {optimal_k})")

        # =========================================================================
        # STEP 5: Run Validation Tool
        # =========================================================================
        # Tool: validate_numeric_range
        # Validates: Inertia values >= 0 (positive)
        # Threshold: min_val=0.0, max_val=inf

        log("[VALIDATION] Running validate_numeric_range on inertia values...")

        validation_result = validate_numeric_range(
            data=df_results['inertia'].values,
            min_val=0.0,
            max_val=np.inf,
            column_name='inertia'
        )

        if not validation_result['valid']:
            raise ValueError(f"Validation failed: {validation_result['message']}")

        log(f"[VALIDATION] Inertia range validation: {validation_result['message']}")

        # Additional validation checks (from recipe criteria)
        log("[VALIDATION] Checking inertia monotonicity...")
        inertia_values = df_results['inertia'].values
        is_monotonic = all(inertia_values[i] >= inertia_values[i+1] for i in range(len(inertia_values)-1))

        if not is_monotonic:
            log("[WARNING] Inertia not strictly monotonically decreasing")
            log(f"[DEBUG] Inertia values: {inertia_values}")
        else:
            log("[VALIDATION] Inertia monotonically decreasing: PASS")

        log("[VALIDATION] Checking BIC finite values...")
        bic_values = df_results['BIC'].values
        if np.isnan(bic_values).any() or np.isinf(bic_values).any():
            raise ValueError("BIC values contain NaN or inf - validation failed")
        else:
            log("[VALIDATION] BIC finite values: PASS")

        log("[VALIDATION] Checking all 10 K values tested...")
        if len(df_results) != 10:
            raise ValueError(f"Expected 10 K values tested, found {len(df_results)}")
        if not all(df_results['K'].values == np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])):
            raise ValueError(f"K values not as expected. Found: {df_results['K'].values}")
        log("[VALIDATION] All 10 K values tested: PASS")

        log("[SUCCESS] Step 02 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
