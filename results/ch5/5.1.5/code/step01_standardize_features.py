#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: 01
Step Name: Standardize Clustering Features
RQ: results/ch5/5.1.5
Generated: 2025-12-02

PURPOSE:
Standardize Total_Intercept and Total_Slope to z-scores (mean=0, SD=1) for equal
weighting in K-means distance calculations. Raw random effects have different scales
(intercepts in theta units, slopes in theta/hour), requiring standardization to
prevent slope from dominating Euclidean distance in clustering.

EXPECTED INPUTS:
  - data/step00_random_effects_from_rq514.csv
    Columns: ['UID', 'Total_Intercept', 'Total_Slope']
    Format: CSV with individual random effects from RQ 5.1.4 best-fitting LMM
    Expected rows: ~100 participants

EXPECTED OUTPUTS:
  - data/step01_standardized_features.csv
    Columns: ['UID', 'Intercept_z', 'Slope_z']
    Format: CSV with z-scored features (mean~0, SD~1)
    Expected rows: ~100 participants

VALIDATION CRITERIA:
  - Mean of Intercept_z ~ 0 (|mean| < 0.01)
  - Mean of Slope_z ~ 0 (|mean| < 0.01)
  - SD of Intercept_z ~ 1 (0.95 < SD < 1.05)
  - SD of Slope_z ~ 1 (0.95 < SD < 1.05)
  - No NaN values introduced by standardization

g_code REASONING:
- Approach: scipy.stats.zscore for column-wise standardization
- Why this approach: Ensures equal weighting in K-means Euclidean distance (prevents
  larger-scale variable from dominating clustering)
- Data flow: Load raw random effects -> apply zscore to each column independently ->
  verify standardization quality -> save z-scored features
- Expected performance: <1 second (lightweight computation on 100 x 2 matrix)

IMPLEMENTATION NOTES:
- Analysis tool: scipy.stats.zscore (standard library function)
- Validation tool: tools.validation.validate_standardization
- Parameters: axis=0 (column-wise), ddof=0 (population SD), nan_policy='propagate'
- Key behavior: zscore computes (X - mean(X)) / std(X) for each column independently
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback
from scipy.stats import zscore

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/ch5/5.1.5/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 5.1.5/ (RQ directory)
#   parents[2] = ch5/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_standardization

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.1.5 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step01_standardize_features.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step01_standardized_features.csv
#   CORRECT: logs/step01_standardize_features.log
#   WRONG:   data/standardized_features.csv (missing step prefix)
#   WRONG:   results/step01_standardized_features.csv (CSV in results folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 01: Standardize Clustering Features")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Random effects from RQ 5.1.4 (100 participants x 3 columns)
        # Purpose: Load raw-scale individual random effects (intercepts and slopes)
        #          from best-fitting LMM trajectory model

        log("[LOAD] Loading random effects from Step 0...")
        input_path = RQ_DIR / "data" / "step00_random_effects_from_rq514.csv"

        random_effects = pd.read_csv(input_path, encoding='utf-8')
        log(f"[LOADED] {input_path.name} ({len(random_effects)} rows, {len(random_effects.columns)} cols)")
        log(f"[INFO] Columns: {random_effects.columns.tolist()}")

        # Check for NaN values (pre-standardization)
        if random_effects[['Total_Intercept', 'Total_Slope']].isnull().any().any():
            n_missing_intercept = random_effects['Total_Intercept'].isnull().sum()
            n_missing_slope = random_effects['Total_Slope'].isnull().sum()
            raise ValueError(f"NaN values detected before standardization: Total_Intercept={n_missing_intercept}, Total_Slope={n_missing_slope}")

        log(f"[INFO] Pre-standardization stats:")
        log(f"  Total_Intercept: mean={random_effects['Total_Intercept'].mean():.4f}, SD={random_effects['Total_Intercept'].std():.4f}")
        log(f"  Total_Slope: mean={random_effects['Total_Slope'].mean():.4f}, SD={random_effects['Total_Slope'].std():.4f}")

        # =========================================================================
        # STEP 2: Run Analysis Tool (scipy.stats.zscore)
        # =========================================================================
        # Tool: scipy.stats.zscore
        # What it does: Compute z-scores (standardized values) for each column independently
        # Expected output: Two arrays of z-scores with mean~0, SD~1
        # Formula: z = (X - mean(X)) / std(X)

        log("[ANALYSIS] Computing z-scores for Total_Intercept and Total_Slope...")

        # Extract raw features as numpy arrays
        intercept_raw = random_effects['Total_Intercept'].values
        slope_raw = random_effects['Total_Slope'].values

        # Compute z-scores using scipy.stats.zscore
        # Parameters:
        #   axis=0: Standardize along column axis (each column independently)
        #   ddof=0: Use population standard deviation (N denominator, not N-1)
        #   nan_policy='propagate': Propagate NaN values (will fail if NaN present)
        intercept_z = zscore(intercept_raw, axis=0, ddof=0, nan_policy='propagate')
        slope_z = zscore(slope_raw, axis=0, ddof=0, nan_policy='propagate')

        log("[DONE] Z-score computation complete")

        # Verify standardization quality
        mean_intercept_z = np.mean(intercept_z)
        sd_intercept_z = np.std(intercept_z, ddof=0)
        mean_slope_z = np.mean(slope_z)
        sd_slope_z = np.std(slope_z, ddof=0)

        log(f"[INFO] Post-standardization stats:")
        log(f"  Intercept_z: mean={mean_intercept_z:.6f}, SD={sd_intercept_z:.6f}")
        log(f"  Slope_z: mean={mean_slope_z:.6f}, SD={sd_slope_z:.6f}")

        # Quick sanity check (validation tool will do comprehensive check)
        if abs(mean_intercept_z) > 0.01 or abs(mean_slope_z) > 0.01:
            log(f"[WARNING] Mean not close to 0 (Intercept_z: {mean_intercept_z:.6f}, Slope_z: {mean_slope_z:.6f})")
        if not (0.95 < sd_intercept_z < 1.05) or not (0.95 < sd_slope_z < 1.05):
            log(f"[WARNING] SD not close to 1 (Intercept_z: {sd_intercept_z:.6f}, Slope_z: {sd_slope_z:.6f})")

        # =========================================================================
        # STEP 3: Save Analysis Outputs
        # =========================================================================
        # These outputs will be used by: Step 2 (K-means clustering)
        # Purpose: Provide equal-weighted features for Euclidean distance calculation

        log("[SAVE] Creating standardized features DataFrame...")

        # Create DataFrame with UID and z-scored features
        standardized_features = pd.DataFrame({
            'UID': random_effects['UID'],
            'Intercept_z': intercept_z,
            'Slope_z': slope_z
        })

        output_path = RQ_DIR / "data" / "step01_standardized_features.csv"
        standardized_features.to_csv(output_path, index=False, encoding='utf-8')

        log(f"[SAVED] {output_path.name} ({len(standardized_features)} rows, {len(standardized_features.columns)} cols)")
        log(f"[INFO] Columns: {standardized_features.columns.tolist()}")

        # =========================================================================
        # STEP 4: Run Validation Tool
        # =========================================================================
        # Tool: tools.validation.validate_standardization
        # Validates: Mean ~ 0 (|mean| < tolerance), SD ~ 1 (|SD - 1| < tolerance)
        # Threshold: tolerance=0.01 (strict validation for standardization quality)

        log("[VALIDATION] Running validate_standardization...")

        validation_result = validate_standardization(
            df=standardized_features,
            column_names=['Intercept_z', 'Slope_z'],
            tolerance=0.01  # Strict tolerance: mean within Â±0.01, SD within 0.99-1.01
        )

        # Report validation results
        # Expected: valid=True, mean_values close to 0, sd_values close to 1
        if validation_result['valid']:
            log("[VALIDATION] Standardization quality PASS")
            log(f"[VALIDATION] Mean values: {validation_result['mean_values']}")
            log(f"[VALIDATION] SD values: {validation_result['sd_values']}")
        else:
            log(f"[VALIDATION] Standardization quality FAIL")
            log(f"[VALIDATION] {validation_result['message']}")
            raise ValueError(f"Standardization validation failed: {validation_result['message']}")

        log("[SUCCESS] Step 01 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
