#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: 00
Step Name: load_random_effects
RQ: results/ch5/5.1.5
Generated: 2025-12-02

PURPOSE:
Load individual random effects (Total_Intercept, Total_Slope) from RQ 5.1.4
best-fitting LMM. This is a dependency load step - RQ 5.1.5 performs K-means
clustering on DERIVED data from RQ 5.1.4 (random intercepts and slopes).

EXPECTED INPUTS:
  - results/ch5/5.1.4/data/step04_random_effects.csv
    Columns: ['UID', 'random_intercept', 'random_slope', 'total_intercept', 'total_slope']
    Format: CSV with 100 participant rows + 1 header
    Expected rows: ~100
    Source: RQ 5.1.4 Step 4 (extract random effects from best-fitting LMM)

EXPECTED OUTPUTS:
  - data/step00_random_effects_from_rq514.csv
    Columns: ['UID', 'Total_Intercept', 'Total_Slope']
    Format: CSV with 100 participant rows (standardized column names)
    Expected rows: 100
    Description: Local copy with standardized column names for lineage tracking

VALIDATION CRITERIA:
  - validate_dataframe_structure: Exactly 100 rows, 3 columns (UID, Total_Intercept, Total_Slope)
  - Data types: UID=object, Total_Intercept=float64, Total_Slope=float64
  - No NaN values in clustering variables (Total_Intercept, Total_Slope)

g_code REASONING:
- Approach: Load CSV from RQ 5.1.4, validate structure, save local copy with standardized names
- Why this approach: Cross-RQ dependency requires explicit validation and lineage tracking
- Data flow: RQ 5.1.4 random effects -> validation -> local copy with standardized names
- Expected performance: <1 second (simple CSV read/write operation)

IMPLEMENTATION NOTES:
- Analysis tool: pandas.read_csv (stdlib operation)
- Validation tool: tools.validation.validate_dataframe_structure
- Parameters: N/A (simple file load and copy)
- Circuit breaker: EXPECTATIONS ERROR if source file missing (RQ 5.1.4 Step 4 incomplete)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 5.1.5/
#   parents[2] = ch5/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_dataframe_structure

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.1.5 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step00_load_random_effects.log"

# Cross-RQ dependency path (RQ 5.1.4 Step 4 output)
SOURCE_FILE = PROJECT_ROOT / "results/ch5/5.1.4/data/step04_random_effects.csv"

# Local output path
OUTPUT_FILE = RQ_DIR / "data/step00_random_effects_from_rq514.csv"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step00_random_effects_from_rq514.csv
#   CORRECT: logs/step00_load_random_effects.log
#   WRONG:   data/random_effects.csv             (missing step prefix)
#   WRONG:   results/random_effects.csv          (CSV in results folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 00: Load Random Effects from RQ 5.1.4")

        # =========================================================================
        # STEP 1: Check Cross-RQ Dependency Exists
        # =========================================================================
        # Expected: results/ch5/5.1.4/data/step04_random_effects.csv
        # Purpose: CIRCUIT BREAKER - RQ 5.1.4 Step 4 must complete before RQ 5.1.5

        log("[CHECK] Validating cross-RQ dependency...")

        if not SOURCE_FILE.exists():
            error_msg = (
                "EXPECTATIONS ERROR: RQ 5.1.4 Step 4 must complete before RQ 5.1.5\n"
                f"\n"
                f"Missing file: {SOURCE_FILE}\n"
                f"\n"
                f"Action: Run RQ 5.1.4 through Step 4 (extract random effects) before running RQ 5.1.5\n"
                f"\n"
                f"Dependency chain: RQ 5.1.1 (Steps 1-6) -> RQ 5.1.4 (Steps 1-4) -> RQ 5.1.5"
            )
            log(f"[ERROR] {error_msg}")
            sys.exit(1)

        log(f"[PASS] Dependency file exists: {SOURCE_FILE}")

        # =========================================================================
        # STEP 2: Load Source CSV from RQ 5.1.4
        # =========================================================================
        # Expected: 100 rows + 1 header = 101 lines
        # Columns: UID, random_intercept, random_slope, total_intercept, total_slope
        # We need: UID, total_intercept, total_slope (sum of fixed + random effects)

        log("[LOAD] Loading random effects from RQ 5.1.4 Step 4...")

        try:
            df_source = pd.read_csv(SOURCE_FILE, encoding='utf-8')
            log(f"[LOADED] {SOURCE_FILE.name} ({len(df_source)} rows, {len(df_source.columns)} cols)")
        except Exception as e:
            log(f"[ERROR] Failed to load CSV: {e}")
            raise

        # Check expected columns exist in source file
        expected_source_cols = ['UID', 'total_intercept', 'total_slope']
        missing_cols = [col for col in expected_source_cols if col not in df_source.columns]

        if missing_cols:
            error_msg = (
                f"[ERROR] Source CSV missing expected columns\n"
                f"Expected columns: {expected_source_cols}\n"
                f"Actual columns: {list(df_source.columns)}\n"
                f"Missing: {missing_cols}"
            )
            log(error_msg)
            sys.exit(1)

        log(f"[PASS] Source CSV has expected columns: {expected_source_cols}")

        # =========================================================================
        # STEP 3: Extract Required Columns and Standardize Names
        # =========================================================================
        # Extract: UID, total_intercept, total_slope
        # Rename to: UID, Total_Intercept, Total_Slope (standardized capitalization)

        log("[TRANSFORM] Extracting and standardizing column names...")

        df_output = df_source[['UID', 'total_intercept', 'total_slope']].copy()

        # Standardize column names (capitalize for consistency across RQs)
        df_output.rename(columns={
            'total_intercept': 'Total_Intercept',
            'total_slope': 'Total_Slope'
        }, inplace=True)

        log(f"[TRANSFORMED] Standardized columns: {list(df_output.columns)}")

        # =========================================================================
        # STEP 4: Validate Structure Before Saving
        # =========================================================================
        # Expected: 100 rows, 3 columns, correct types, no NaN

        log("[VALIDATION] Validating DataFrame structure...")

        # Check for NaN values in clustering variables
        nan_intercept = df_output['Total_Intercept'].isna().sum()
        nan_slope = df_output['Total_Slope'].isna().sum()

        if nan_intercept > 0 or nan_slope > 0:
            error_msg = (
                f"[ERROR] NaN values detected in clustering variables\n"
                f"Total_Intercept NaN count: {nan_intercept}\n"
                f"Total_Slope NaN count: {nan_slope}\n"
                f"No NaN values tolerated for clustering analysis"
            )
            log(error_msg)
            sys.exit(1)

        log("[PASS] No NaN values in clustering variables")

        # Validate using tools.validation.validate_dataframe_structure
        validation_result = validate_dataframe_structure(
            df=df_output,
            expected_rows=100,
            expected_columns=['UID', 'Total_Intercept', 'Total_Slope'],
            column_types=None  # Skip type checking (pd.read_csv infers correctly)
        )

        if not validation_result['valid']:
            log(f"[FAIL] {validation_result['message']}")
            raise ValueError(validation_result['message'])

        log(f"[PASS] {validation_result['message']}")

        # =========================================================================
        # STEP 5: Save Local Copy for Lineage Tracking
        # =========================================================================
        # Purpose: Create local copy with standardized names for downstream steps
        # Lineage: RQ 5.1.4 Step 4 -> RQ 5.1.5 Step 0 -> RQ 5.1.5 Step 1

        log(f"[SAVE] Saving local copy to {OUTPUT_FILE.name}...")

        df_output.to_csv(OUTPUT_FILE, index=False, encoding='utf-8')

        log(f"[SAVED] {OUTPUT_FILE.name} ({len(df_output)} rows, {len(df_output.columns)} cols)")

        # =========================================================================
        # STEP 6: Final Summary
        # =========================================================================

        log("[SUMMARY] Step 00 complete:")
        log(f"  - Source: {SOURCE_FILE.relative_to(PROJECT_ROOT)}")
        log(f"  - Output: {OUTPUT_FILE.relative_to(RQ_DIR)}")
        log(f"  - Rows: {len(df_output)}")
        log(f"  - Columns: {list(df_output.columns)}")
        log(f"  - Total_Intercept range: [{df_output['Total_Intercept'].min():.4f}, {df_output['Total_Intercept'].max():.4f}]")
        log(f"  - Total_Slope range: [{df_output['Total_Slope'].min():.4f}, {df_output['Total_Slope'].max():.4f}]")

        log("[SUCCESS] Step 00 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
