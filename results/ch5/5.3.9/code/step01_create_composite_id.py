#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step01
Step Name: Create Composite ID
RQ: results/ch5/5.3.9
Generated: 2025-12-04

PURPOSE:
Create composite_ID identifier (UID_Test format) for cross-classified LMM modeling.
Verify data structure integrity: each Item belongs to exactly one paradigm, no
duplicate UID × Test × Item observations.

EXPECTED INPUTS:
  - data/step00_response_level_data.csv
    Columns: UID, Test, Item, Response, paradigm, Difficulty
    Format: Long format (UID × Test × Item observations)
    Expected rows: ~20,000 (100 participants × 4 tests × ~50 items)

EXPECTED OUTPUTS:
  - data/step01_analysis_ready.csv
    Columns: composite_ID, UID, Test, Item, Response, paradigm, Difficulty
    Format: Long format with composite_ID added
    Expected rows: ~20,000 (unchanged from input)

VALIDATION CRITERIA:
  - composite_ID format: {UID}_{Test} (e.g., P001_T1)
  - composite_ID: 400 unique values (100 participants × 4 tests)
  - No duplicate UID × Test × Item combinations
  - Each Item belongs to exactly ONE paradigm

g_code REASONING:
- Approach: Concatenate UID + "_" + Test to create composite_ID. Verify data
  integrity by checking for duplicates and paradigm assignment consistency.
- Why this approach: composite_ID enables TSVR time merge (Step 02) and serves
  as grouping identifier for cross-classified random effects structure.
- Data flow: Load Step 0 output -> Create composite_ID -> Verify integrity -> Save
- Expected performance: ~seconds (string concatenation + duplicate checks)

IMPLEMENTATION NOTES:
- Analysis tool: Stdlib pandas operations (string concatenation, groupby)
- Validation tool: tools.validation.validate_data_format
- Parameters: composite_id_format = "{UID}_{Test}"
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tools
from tools.validation import validate_data_format

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.3.9
LOG_FILE = RQ_DIR / "logs" / "step01_create_composite_id.log"

# Input files
INPUT_RESPONSE_DATA = RQ_DIR / "data" / "step00_response_level_data.csv"

# Output files
OUTPUT_ANALYSIS_READY = RQ_DIR / "data" / "step01_analysis_ready.csv"

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 01: Create Composite ID")

        # =========================================================================
        # STEP 1: Load Response-Level Data
        # =========================================================================
        # Expected: ~20,000 rows (100 participants × 4 tests × ~50 items)
        # Purpose: Add composite_ID identifier for TSVR merge and LMM grouping

        log("[LOAD] Loading response-level data from Step 00...")
        df = pd.read_csv(INPUT_RESPONSE_DATA)
        log(f"[LOADED] {INPUT_RESPONSE_DATA.name} ({len(df)} rows, {len(df.columns)} cols)")
        log(f"[INFO] Columns: {df.columns.tolist()}")

        # =========================================================================
        # STEP 2: Create Composite ID
        # =========================================================================
        # Format: {UID}_{Test} (e.g., P001_T1, A010_1)
        # Purpose: Unique identifier for each participant × test session

        log("[PROCESS] Creating composite_ID (format: UID_Test)...")

        # Convert Test to string to handle numeric test values (1, 2, 3, 4)
        df['Test'] = df['Test'].astype(str)

        # Create composite_ID
        df['composite_ID'] = df['UID'] + '_' + df['Test']

        log(f"[CREATED] composite_ID column added")
        log(f"[INFO] Example composite_IDs: {df['composite_ID'].head(3).tolist()}")

        # Verify expected count
        n_unique_composite = df['composite_ID'].nunique()
        expected_composite = 400  # 100 participants × 4 tests
        log(f"[INFO] Unique composite_IDs: {n_unique_composite} (expected: {expected_composite})")

        if n_unique_composite != expected_composite:
            log(f"[WARNING] Unique composite_IDs ({n_unique_composite}) differs from expected ({expected_composite})")

        # =========================================================================
        # STEP 3: Verify Paradigm Assignment Consistency
        # =========================================================================
        # Check: Each Item should belong to exactly ONE paradigm across all observations
        # Purpose: Ensure paradigm is item-level property, not participant/test-dependent

        log("[VERIFY] Checking paradigm assignment consistency...")

        item_paradigm_check = df.groupby('Item')['paradigm'].nunique()
        items_multiple_paradigms = item_paradigm_check[item_paradigm_check > 1]

        if len(items_multiple_paradigms) > 0:
            log(f"[ERROR] {len(items_multiple_paradigms)} items belong to multiple paradigms:")
            for item, n_paradigms in items_multiple_paradigms.items():
                paradigms = df[df['Item'] == item]['paradigm'].unique()
                log(f"  {item}: {n_paradigms} paradigms ({', '.join(paradigms)})")
            raise ValueError(f"Data integrity violation: Items with multiple paradigm assignments")
        else:
            log(f"[VERIFIED] All items belong to exactly ONE paradigm")

        # =========================================================================
        # STEP 4: Check for Duplicate UID × Test × Item Observations
        # =========================================================================
        # Check: Each combination of (UID, Test, Item) should appear at most once
        # Purpose: Ensure data structure matches cross-classified design expectation

        log("[VERIFY] Checking for duplicate UID × Test × Item observations...")

        # Identify duplicates
        duplicate_check = df.groupby(['UID', 'Test', 'Item']).size()
        duplicates = duplicate_check[duplicate_check > 1]

        if len(duplicates) > 0:
            log(f"[ERROR] {len(duplicates)} duplicate UID × Test × Item combinations found:")
            for (uid, test, item), count in duplicates.head(10).items():
                log(f"  {uid} × {test} × {item}: {count} observations")
            raise ValueError(f"Data integrity violation: Duplicate observations detected")
        else:
            log(f"[VERIFIED] No duplicate UID × Test × Item observations")

        # =========================================================================
        # STEP 5: Sort and Save Analysis-Ready Data
        # =========================================================================
        # Output: Long format with composite_ID added

        log("[SAVE] Saving analysis-ready data...")

        # Reorder columns: composite_ID first
        output_cols = ['composite_ID', 'UID', 'Test', 'Item', 'Response', 'paradigm', 'Difficulty']
        df_output = df[output_cols].copy()

        # Sort for reproducibility
        df_output = df_output.sort_values(['UID', 'Test', 'Item']).reset_index(drop=True)

        df_output.to_csv(OUTPUT_ANALYSIS_READY, index=False, encoding='utf-8')
        log(f"[SAVED] {OUTPUT_ANALYSIS_READY.name} ({len(df_output)} rows, {len(df_output.columns)} cols)")

        # =========================================================================
        # STEP 6: Run Validation - Data Format
        # =========================================================================
        # Validate: All required columns present including composite_ID

        log("[VALIDATION] Running validate_data_format...")
        required_cols = ['composite_ID', 'UID', 'Test', 'Item', 'Response', 'paradigm', 'Difficulty']
        validation_result = validate_data_format(df_output, required_cols)

        if not validation_result['valid']:
            raise ValueError(f"Data format validation failed: {validation_result['message']}")

        log(f"[VALIDATION] Data format: {validation_result['message']}")

        # =========================================================================
        # STEP 7: Verify Composite ID Format
        # =========================================================================
        # Check: composite_ID follows expected format (contains "_")

        log("[VALIDATION] Verifying composite_ID format...")

        # Check format: should contain exactly one "_"
        format_check = df_output['composite_ID'].str.contains('_', na=False)
        invalid_format = (~format_check).sum()

        if invalid_format > 0:
            log(f"[ERROR] {invalid_format} composite_IDs with invalid format (missing '_')")
            raise ValueError(f"composite_ID format validation failed")
        else:
            log(f"[VALIDATED] All composite_IDs follow format: UID_Test")

        # Report summary
        log("[SUMMARY] Step 01 data structure verification complete:")
        log(f"  Total rows: {len(df_output)}")
        log(f"  Unique composite_IDs: {df_output['composite_ID'].nunique()}")
        log(f"  Unique participants: {df_output['UID'].nunique()}")
        log(f"  Unique tests: {df_output['Test'].nunique()}")
        log(f"  Unique items: {df_output['Item'].nunique()}")
        log(f"  Paradigms: {df_output['paradigm'].nunique()} ({', '.join(sorted(df_output['paradigm'].unique()))})")
        log(f"  Data integrity: [PASS] No duplicates, paradigm assignment consistent")

        log("[SUCCESS] Step 01 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
