#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step05
Step Name: Fit LMM Trajectory Models
RQ: ch5/5.2.1
Generated: 2025-11-22

PURPOSE:
Fit 5 candidate LMM trajectory models with Domain x Time interaction, select best by AIC.
This step implements trajectory modeling to test memory decay patterns across domains
(what, where, when) using actual TSVR hours as the time variable (Decision D070).

EXPECTED INPUTS:
- data/step04_lmm_input.csv
  Columns: [composite_ID, UID, test, TSVR_hours, domain, theta]
  Format: CSV with UTF-8 encoding (long format, 1 row per composite_ID x domain)
  Expected rows: ~1200 (400 composite_IDs x 3 domains)

EXPECTED OUTPUTS:
- results/step05_lmm_model_comparison.csv
  Columns: [model_name, AIC, delta_AIC, akaike_weight, converged]
  Format: CSV (AIC comparison of 5 candidate models)
  Expected rows: 5 (one per candidate model)

- results/step05_lmm_model_summary.txt
  Format: Text file (full summary of best model)

- data/step05_lmm_fitted_model.pkl
  Format: Python pickle (fitted statsmodels MixedLMResults object)

VALIDATION CRITERIA:
- Best model converged successfully
- No singular fit (random effects variance > 0)
- All fixed effects have finite estimates
- At least 3 of 5 candidate models converged

g_code REASONING:
- Approach: Use compare_lmm_models_by_aic from tools.analysis_lmm to fit 5 candidate
  trajectory models (Linear, Quadratic, Log, Lin+Log, Quad+Log) and compare via AIC.
- Why this approach: AIC comparison allows model-agnostic selection of best functional
  form for memory decay trajectory. Multiple candidates test different decay patterns.
- Data flow: Load step04_lmm_input.csv -> rename columns for tool compatibility ->
  fit candidate models -> select best by AIC -> save comparison table + model + summary
- Expected performance: ~30-60 seconds (5 models x ~100 subjects x 4 tests x 3 domains)

IMPLEMENTATION NOTES:
- Analysis tool: compare_lmm_models_by_aic from tools.analysis_lmm
- Validation tool: validate_lmm_convergence from tools.validation
- Parameters: n_factors=3, reference_group='what', groups='UID'
- Column mapping: TSVR_hours -> Days (converted from hours to days), domain -> Factor,
  theta -> Ability (to match tool's internal formula expectations)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, Any
import traceback
import pickle

# Add project root to path for imports
# parents[4] = REMEMVR/ (code -> 5.2.1 -> ch5 -> results -> REMEMVR)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_lmm import compare_lmm_models_by_aic

# Import validation tool
from tools.validation import validate_lmm_convergence

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]
LOG_FILE = RQ_DIR / "logs" / "step05_fit_lmm.log"

# =============================================================================
# Logging Function
# =============================================================================

def log(msg: str) -> None:
    """Write to both log file and console."""
    LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 05: Fit LMM Trajectory Models")
        log("=" * 60)

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Long-format LMM input from step04 with TSVR time variable
        # Purpose: Model memory decay trajectories across domains using actual hours

        log("\n[LOAD] Loading LMM input data from step04...")
        input_path = RQ_DIR / "data" / "step04_lmm_input.csv"
        df_lmm = pd.read_csv(input_path, encoding='utf-8')
        log(f"[LOADED] step04_lmm_input.csv ({len(df_lmm)} rows, {len(df_lmm.columns)} cols)")
        log(f"  Columns: {df_lmm.columns.tolist()}")
        log(f"  Unique UIDs: {df_lmm['UID'].nunique()}")
        log(f"  Domains: {sorted(df_lmm['domain'].unique().tolist())}")
        log(f"  TSVR_hours range: {df_lmm['TSVR_hours'].min():.1f} - {df_lmm['TSVR_hours'].max():.1f}")

        # =========================================================================
        # STEP 2: Transform Data for Tool Compatibility
        # =========================================================================
        # The compare_lmm_models_by_aic tool expects specific column names:
        # - 'Ability' as outcome variable
        # - 'Factor' as domain variable
        # - 'Days', 'Days_sq', 'log_Days' as time variables
        # We need to rename/create these columns from our input data

        log("\n[TRANSFORM] Preparing data for LMM tool...")

        # Rename columns to match tool expectations
        df_lmm_transformed = df_lmm.copy()

        # Convert TSVR_hours to Days (Decision D070: use actual hours / 24)
        df_lmm_transformed['Days'] = df_lmm_transformed['TSVR_hours'] / 24.0
        df_lmm_transformed['Days_sq'] = df_lmm_transformed['Days'] ** 2
        df_lmm_transformed['log_Days'] = np.log(df_lmm_transformed['Days'] + 1)

        # Rename domain -> Factor (capitalize for treatment coding)
        df_lmm_transformed['Factor'] = df_lmm_transformed['domain'].str.capitalize()

        # Rename theta -> Ability
        df_lmm_transformed['Ability'] = df_lmm_transformed['theta']

        log(f"  Created time variables: Days, Days_sq, log_Days")
        log(f"  Days range: {df_lmm_transformed['Days'].min():.2f} - {df_lmm_transformed['Days'].max():.2f}")
        log(f"  Factor levels: {sorted(df_lmm_transformed['Factor'].unique().tolist())}")

        # =========================================================================
        # STEP 3: Run Analysis Tool (compare_lmm_models_by_aic)
        # =========================================================================
        # Tool: compare_lmm_models_by_aic
        # What it does: Fits 5 candidate LMM models and compares via AIC
        # Expected output: Dict with 'models', 'aic_comparison', 'best_model_name', 'best_model'

        log("\n[ANALYSIS] Running compare_lmm_models_by_aic...")
        log(f"  n_factors: 3 (what, where, when)")
        log(f"  reference_group: 'What' (treatment coding)")
        log(f"  groups: 'UID' (random effects grouping)")

        # Create save directory for model files
        save_dir = RQ_DIR / "results"
        save_dir.mkdir(parents=True, exist_ok=True)

        comparison_results = compare_lmm_models_by_aic(
            data=df_lmm_transformed,       # Long-format LMM data
            n_factors=3,                   # Three domains (what, where, when)
            reference_group='What',        # Treatment coding: What as reference
            groups='UID',                  # Grouping variable for random effects
            save_dir=save_dir              # Directory to save fitted models
        )

        log("[DONE] Model comparison complete")

        # Extract results
        fitted_models = comparison_results['models']
        aic_df = comparison_results['aic_comparison']
        best_model_name = comparison_results['best_model_name']
        best_result = comparison_results['best_model']

        log(f"\n[RESULTS] Best model: {best_model_name}")
        log(f"  AIC: {best_result.aic:.2f}")
        log(f"  BIC: {best_result.bic:.2f}")
        log(f"  Log-Likelihood: {best_result.llf:.2f}")

        # =========================================================================
        # STEP 4: Save Analysis Outputs
        # =========================================================================
        # These outputs will be used by: step06 (post-hoc contrasts) and step07 (plots)

        # Save AIC comparison table
        aic_output_path = RQ_DIR / "results" / "step05_lmm_model_comparison.csv"

        # Add convergence column to AIC comparison
        aic_df['converged'] = aic_df['model_name'].apply(
            lambda x: fitted_models.get(x) is not None and
                      getattr(fitted_models.get(x), 'converged', False) if fitted_models.get(x) is not None else False
        )

        aic_df.to_csv(aic_output_path, index=False, encoding='utf-8')
        log(f"\n[SAVE] Saved AIC comparison: {aic_output_path}")
        log(f"  {len(aic_df)} models compared")
        log(f"  AIC Comparison Table:")
        for _, row in aic_df.iterrows():
            log(f"    {row['model_name']}: AIC={row['AIC']:.2f}, delta={row['delta_AIC']:.2f}, weight={row['AIC_weight']:.3f}")

        # Save model summary text file
        summary_output_path = RQ_DIR / "results" / "step05_lmm_model_summary.txt"
        with open(summary_output_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write(f"BEST MODEL: {best_model_name}\n")
            f.write("=" * 60 + "\n\n")
            f.write(best_result.summary().as_text())
            f.write("\n\n" + "=" * 60 + "\n")
            f.write("AIC COMPARISON\n")
            f.write("=" * 60 + "\n\n")
            f.write(aic_df.to_string(index=False))
        log(f"[SAVE] Saved model summary: {summary_output_path}")

        # Save fitted model as pickle for step06 post-hoc contrasts
        model_output_path = RQ_DIR / "data" / "step05_lmm_fitted_model.pkl"
        with open(model_output_path, 'wb') as f:
            pickle.dump(best_result, f)
        log(f"[SAVE] Saved fitted model: {model_output_path}")

        # =========================================================================
        # STEP 5: Run Validation Tool
        # =========================================================================
        # Tool: validate_lmm_convergence
        # Validates: Model convergence and fit quality
        # Criteria: converged=True, no singular fit, finite estimates

        log("\n[VALIDATION] Running validate_lmm_convergence...")
        validation_result = validate_lmm_convergence(lmm_result=best_result)

        # Report validation results
        for key, value in validation_result.items():
            log(f"  {key}: {value}")

        # Additional validation checks
        log("\n[VALIDATION] Additional checks:")

        # Check number of converged models
        n_converged = sum(1 for m in fitted_models.values() if m is not None and getattr(m, 'converged', False))
        log(f"  Converged models: {n_converged}/5")
        if n_converged < 3:
            raise ValueError(f"Only {n_converged}/5 models converged. Need at least 3.")

        # Check best model convergence
        if not validation_result.get('converged', False):
            raise ValueError(f"Best model ({best_model_name}) did not converge")

        # Check for singular fit (random effects variance > 0)
        re_cov = best_result.cov_re
        if isinstance(re_cov, pd.DataFrame):
            re_var = re_cov.values[0, 0] if re_cov.shape[0] > 0 else 0
        else:
            re_var = re_cov[0, 0] if re_cov.shape[0] > 0 else 0
        log(f"  Random effects variance: {re_var:.4f}")
        if re_var <= 0:
            log("[WARNING] Singular fit detected (random effects variance = 0)")

        # Check fixed effects are finite
        fe_params = best_result.params
        if fe_params.isna().any():
            raise ValueError("Some fixed effects have NaN estimates")
        if np.isinf(fe_params).any():
            raise ValueError("Some fixed effects have infinite estimates")
        log(f"  Fixed effects: All {len(fe_params)} estimates are finite")

        log("\n" + "=" * 60)
        log("[SUCCESS] Step 05 complete")
        log("=" * 60)
        log(f"\nOutputs:")
        log(f"  - {aic_output_path}")
        log(f"  - {summary_output_path}")
        log(f"  - {model_output_path}")
        log(f"\nBest Model: {best_model_name}")
        log(f"  Formula: Ability ~ Days * C(Factor, Treatment('What'))")
        log(f"  AIC: {best_result.aic:.2f}")

        sys.exit(0)

    except Exception as e:
        log(f"\n[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
