#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step05
Step Name: Test Intercept-Slope Correlation and Visualize Distribution
RQ: results/ch5/rq13
Generated: 2025-11-30

PURPOSE:
Test hypothesis that baseline memory ability (random intercepts) and forgetting
rate (random slopes) are correlated, using Decision D068 dual p-value reporting
(uncorrected + Bonferroni). Additionally, visualize the distribution of random
slopes with histogram and Q-Q plot to assess normality assumption.

Hypothesis: Negative correlation expected (higher baseline -> slower forgetting)

EXPECTED INPUTS:
  - data/step04_random_effects.csv
    Columns: UID, random_intercept, random_slope, total_intercept, total_slope
    Format: CSV with UTF-8 encoding
    Expected rows: 100 (one per participant)
    Purpose: Individual random effects from Step 4

EXPECTED OUTPUTS:
  - data/step05_intercept_slope_correlation.csv
    Columns: statistic, value
    Format: CSV with UTF-8 encoding
    Expected rows: 5 (correlation, p_uncorrected, p_bonferroni, df, alpha_corrected)
    Purpose: Correlation test results with dual p-values per Decision D068

  - results/step05_correlation_interpretation.txt
    Format: Plain text interpretation
    Purpose: Human-readable interpretation of correlation magnitude, direction,
             significance (uncorrected vs Bonferroni-corrected)

  - plots/step05_random_slopes_histogram.png
    Format: PNG image (800x600 @ 300 DPI)
    Purpose: Histogram of random slopes with normal overlay and mean reference line

  - plots/step05_random_slopes_qqplot.png
    Format: PNG image (800x600 @ 300 DPI)
    Purpose: Q-Q plot assessing normality of random slopes distribution

  - logs/step05_correlation_test.log
    Format: Text log
    Purpose: Correlation test results, Bonferroni correction, plotting confirmation

VALIDATION CRITERIA:
  - BOTH p_uncorrected and p_bonferroni present (Decision D068 requirement)
  - Correlation r in [-1, 1] range
  - P-values in [0, 1] range
  - Bonferroni correction correct: p_bonf = min(p_uncorr Ã— 15, 1.0)
  - Degrees of freedom correct: df = 98 (N-2 = 100-2)
  - Alpha threshold correct: alpha_corrected = 0.0033 (0.05/15)

g_code REASONING:
- Approach: Use catalogued tool test_intercept_slope_correlation_d068 which
  implements Pearson correlation with Decision D068 dual p-value reporting.
  Chapter 5 family size = 15 tests for Bonferroni correction. Generate
  visualization using matplotlib/seaborn for histogram and scipy.stats.probplot
  for Q-Q plot.

- Why this approach: Decision D068 mandates dual p-value reporting to balance
  Type I/II error concerns. Bonferroni correction is appropriate for family-wise
  error control across Chapter 5 analyses (n_tests=15). Normality visualization
  validates LMM random effects distributional assumptions.

- Data flow: Step 4 random effects -> Pearson correlation test -> dual p-values
  -> interpretation + visualization -> CSV/TXT/PNG outputs

- Expected performance: <5 seconds (simple correlation + plotting, N=100)

IMPLEMENTATION NOTES:
- Analysis tool: test_intercept_slope_correlation_d068 from tools.analysis_lmm
- Validation tool: validate_correlation_test_d068 from tools.validation
- Parameters: family_alpha=0.05, n_tests=15 (Chapter 5 family size),
  intercept_col='random_intercept', slope_col='random_slope'
- Decision D068: Dual p-value reporting (uncorrected + Bonferroni)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, Any
import traceback
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_lmm import test_intercept_slope_correlation_d068

# Import validation tool
from tools.validation import validate_correlation_test_d068

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/rq13
LOG_FILE = RQ_DIR / "logs" / "step05_correlation_test.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html, .txt)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step05_intercept_slope_correlation.csv
#   CORRECT: plots/step05_random_slopes_histogram.png
#   WRONG:   results/correlation.csv  (CSV in results/, should be data/)
#   WRONG:   data/correlation.csv     (missing step prefix)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 5: Test Intercept-Slope Correlation and Visualize Distribution")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Random effects from Step 4 (UID, random_intercept, random_slope)
        # Purpose: Test correlation between baseline ability and forgetting rate

        log("[LOAD] Loading random effects from Step 4...")
        random_effects_path = RQ_DIR / "data" / "step04_random_effects.csv"

        if not random_effects_path.exists():
            raise FileNotFoundError(f"Input file missing: {random_effects_path}")

        random_effects = pd.read_csv(random_effects_path, encoding='utf-8')
        log(f"[LOADED] {random_effects_path.name} ({len(random_effects)} rows, {len(random_effects.columns)} cols)")

        # Validate expected columns
        required_cols = ['UID', 'random_intercept', 'random_slope']
        missing_cols = [col for col in required_cols if col not in random_effects.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")

        log(f"[INFO] Columns present: {random_effects.columns.tolist()}")
        log(f"[INFO] Random intercepts: mean={random_effects['random_intercept'].mean():.4f}, SD={random_effects['random_intercept'].std():.4f}")
        log(f"[INFO] Random slopes: mean={random_effects['random_slope'].mean():.4f}, SD={random_effects['random_slope'].std():.4f}")

        # =========================================================================
        # STEP 2: Run Analysis Tool (Correlation Test with Decision D068)
        # =========================================================================
        # Tool: test_intercept_slope_correlation_d068
        # What it does: Pearson correlation with dual p-value reporting (uncorrected + Bonferroni)
        # Expected output: Dict with r, p_uncorrected, p_bonferroni, df, alpha_corrected

        log("[ANALYSIS] Running test_intercept_slope_correlation_d068...")
        log("[ANALYSIS] Parameters: family_alpha=0.05, n_tests=15 (Chapter 5 family size)")

        correlation_result = test_intercept_slope_correlation_d068(
            random_effects_df=random_effects,
            family_alpha=0.05,  # Significance threshold
            n_tests=15,  # Chapter 5 family size for Bonferroni correction
            intercept_col='random_intercept',  # Column name for random intercepts
            slope_col='random_slope'  # Column name for random slopes
        )

        log("[DONE] Correlation test complete")
        log(f"[RESULT] Correlation r = {correlation_result['r']:.4f}")
        log(f"[RESULT] p_uncorrected = {correlation_result['p_uncorrected']:.6f}")
        log(f"[RESULT] p_bonferroni = {correlation_result['p_bonferroni']:.6f}")
        log(f"[RESULT] Significant (uncorrected): {correlation_result['significant_uncorrected']}")
        log(f"[RESULT] Significant (Bonferroni): {correlation_result['significant_bonferroni']}")
        log(f"[RESULT] Interpretation: {correlation_result['interpretation']}")

        # =========================================================================
        # STEP 3: Save Analysis Outputs
        # =========================================================================
        # These outputs will be used by: rq_inspect (validation), rq_results (interpretation)

        log("[SAVE] Saving correlation test results...")

        # 3a. Save correlation results as CSV (wide format for validation)
        correlation_csv_path = RQ_DIR / "data" / "step05_intercept_slope_correlation.csv"
        correlation_df = pd.DataFrame([{
            'r': correlation_result['r'],
            'p_uncorrected': correlation_result['p_uncorrected'],
            'p_bonferroni': correlation_result['p_bonferroni'],
            'df': float(len(random_effects) - 2),  # N-2 for Pearson
            'alpha_corrected': 0.05 / 15,  # 0.05/15 = 0.0033
            'significant_uncorrected': correlation_result['significant_uncorrected'],
            'significant_bonferroni': correlation_result['significant_bonferroni']
        }])
        correlation_df.to_csv(correlation_csv_path, index=False, encoding='utf-8')
        log(f"[SAVED] {correlation_csv_path.name} (1 row, {len(correlation_df.columns)} columns)")

        # 3b. Save interpretation as plain text
        interpretation_path = RQ_DIR / "results" / "step05_correlation_interpretation.txt"
        with open(interpretation_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("INTERCEPT-SLOPE CORRELATION TEST RESULTS (RQ 5.13 Step 5)\n")
            f.write("="*80 + "\n\n")
            f.write(f"Correlation: r = {correlation_result['r']:.4f}\n")
            f.write(f"Direction: {'Negative' if correlation_result['r'] < 0 else 'Positive'}\n")
            f.write(f"Magnitude: {correlation_result['interpretation']}\n\n")
            f.write("-"*80 + "\n")
            f.write("SIGNIFICANCE TESTS (Decision D068 Dual P-Value Reporting)\n")
            f.write("-"*80 + "\n\n")
            f.write(f"Uncorrected p-value: p = {correlation_result['p_uncorrected']:.6f}\n")
            f.write(f"  Significant at alpha=0.05? {'YES' if correlation_result['significant_uncorrected'] else 'NO'}\n\n")
            f.write(f"Bonferroni-corrected p-value: p = {correlation_result['p_bonferroni']:.6f}\n")
            f.write(f"  Family size: 15 tests (Chapter 5)\n")
            f.write(f"  Corrected alpha: 0.05/15 = 0.0033\n")
            f.write(f"  Significant at corrected alpha? {'YES' if correlation_result['significant_bonferroni'] else 'NO'}\n\n")
            f.write("-"*80 + "\n")
            f.write("INTERPRETATION\n")
            f.write("-"*80 + "\n\n")
            if correlation_result['r'] < 0:
                f.write("Negative correlation suggests that participants with higher baseline\n")
                f.write("memory (positive random intercepts) tend to have slower forgetting rates\n")
                f.write("(negative random slopes, meaning less steep decline).\n\n")
            else:
                f.write("Positive correlation suggests that participants with higher baseline\n")
                f.write("memory (positive random intercepts) tend to have faster forgetting rates\n")
                f.write("(positive random slopes, meaning steeper decline).\n\n")

            if correlation_result['significant_bonferroni']:
                f.write("The correlation is statistically significant even after Bonferroni\n")
                f.write("correction, indicating a robust relationship between baseline ability\n")
                f.write("and forgetting rate that is unlikely due to chance.\n")
            elif correlation_result['significant_uncorrected']:
                f.write("The correlation is statistically significant without correction but\n")
                f.write("NOT significant after Bonferroni correction, suggesting the relationship\n")
                f.write("may not be robust to multiple testing concerns.\n")
            else:
                f.write("The correlation is NOT statistically significant, suggesting no strong\n")
                f.write("relationship between baseline ability and forgetting rate in this sample.\n")

        log(f"[SAVED] {interpretation_path.name}")

        # =========================================================================
        # STEP 4: Create Visualizations
        # =========================================================================
        # Create histogram and Q-Q plot for random slopes distribution

        log("[PLOT] Creating random slopes distribution visualizations...")

        # Set plot style
        sns.set_style("whitegrid")
        plt.rcParams['figure.dpi'] = 300

        # 4a. Histogram with normal overlay
        fig_hist, ax_hist = plt.subplots(figsize=(8, 6))

        slopes = random_effects['random_slope'].values

        # Plot histogram
        n, bins, patches = ax_hist.hist(slopes, bins=20, density=True, alpha=0.7,
                                         color='steelblue', edgecolor='black', label='Observed')

        # Overlay normal distribution
        mu, sigma = slopes.mean(), slopes.std()
        x = np.linspace(slopes.min(), slopes.max(), 100)
        normal_curve = stats.norm.pdf(x, mu, sigma)
        ax_hist.plot(x, normal_curve, 'r-', linewidth=2, label=f'Normal (mean={mu:.4f}, SD={sigma:.4f})')

        # Add mean reference line
        ax_hist.axvline(mu, color='black', linestyle='--', linewidth=1.5, label=f'Mean = {mu:.4f}')

        ax_hist.set_xlabel('Random Slope (Forgetting Rate)', fontsize=12)
        ax_hist.set_ylabel('Density', fontsize=12)
        ax_hist.set_title('Distribution of Random Slopes\n(Individual Differences in Forgetting Rate)', fontsize=14)
        ax_hist.legend(fontsize=10)
        ax_hist.grid(True, alpha=0.3)

        histogram_path = RQ_DIR / "plots" / "step05_random_slopes_histogram.png"
        fig_hist.tight_layout()
        fig_hist.savefig(histogram_path, dpi=300, bbox_inches='tight')
        plt.close(fig_hist)
        log(f"[SAVED] {histogram_path.name}")

        # 4b. Q-Q plot
        fig_qq, ax_qq = plt.subplots(figsize=(8, 6))

        stats.probplot(slopes, dist="norm", plot=ax_qq)
        ax_qq.set_title('Q-Q Plot: Random Slopes vs Normal Distribution', fontsize=14)
        ax_qq.set_xlabel('Theoretical Quantiles (Normal Distribution)', fontsize=12)
        ax_qq.set_ylabel('Observed Quantiles (Random Slopes)', fontsize=12)
        ax_qq.grid(True, alpha=0.3)

        qqplot_path = RQ_DIR / "plots" / "step05_random_slopes_qqplot.png"
        fig_qq.tight_layout()
        fig_qq.savefig(qqplot_path, dpi=300, bbox_inches='tight')
        plt.close(fig_qq)
        log(f"[SAVED] {qqplot_path.name}")

        log("[PLOT] Visualization complete")

        # =========================================================================
        # STEP 5: Run Validation Tool
        # =========================================================================
        # Tool: validate_correlation_test_d068
        # Validates: Decision D068 dual p-value reporting compliance
        # Checks: p_uncorrected and p_bonferroni columns present, values in valid ranges

        log("[VALIDATION] Running validate_correlation_test_d068...")

        validation_result = validate_correlation_test_d068(
            correlation_df=correlation_df,
            required_cols=None  # Uses default D068 spec
        )

        # Report validation results
        if validation_result['valid']:
            log(f"[VALIDATION] PASS - {validation_result['message']}")
            log(f"[VALIDATION] D068 compliant: {validation_result['d068_compliant']}")
        else:
            log(f"[VALIDATION] FAIL - {validation_result['message']}")
            if validation_result.get('missing_cols'):
                log(f"[VALIDATION] Missing columns: {validation_result['missing_cols']}")
            raise ValueError(f"Validation failed: {validation_result['message']}")

        log("[SUCCESS] Step 5 complete")
        log("")
        log("Summary:")
        log(f"  Correlation: r = {correlation_result['r']:.4f}")
        log(f"  Uncorrected p = {correlation_result['p_uncorrected']:.6f} ({'sig' if correlation_result['significant_uncorrected'] else 'ns'})")
        log(f"  Bonferroni p = {correlation_result['p_bonferroni']:.6f} ({'sig' if correlation_result['significant_bonferroni'] else 'ns'})")
        log(f"  Interpretation: {correlation_result['interpretation']}")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
