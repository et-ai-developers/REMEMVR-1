# Analysis Plan: RQ 5.2.7 - Domain-Based Clustering

**Research Question:** 5.2.7
**Created:** 2025-12-02
**Status:** Planning complete, ready for tool specification (rq_tools)

---

## ⚠️ WHEN DOMAIN EXCLUSION

**Exclusion Applied:** When domain (-O- tags) excluded from this analysis.

**Rationale:** RQ 5.2.1 discovered severe floor effect in When domain (77% item attrition, 6-9% floor).

**Impact on This Plan:**
- Clustering uses 4 variables (not 6): What intercept/slope, Where intercept/slope
- Source data: RQ 5.2.6 step04_random_effects.csv (200 rows: 100 UID × 2 domains)
- n_features=4 in BIC calculation (not 6)

---

## Overview

This RQ examines whether participants can be grouped into latent classes based on domain-specific forgetting trajectories across two episodic memory domains (What, Where). When domain excluded due to floor effect. The analysis uses K-means clustering on 4 clustering variables per participant: intercept and slope for each domain derived from RQ 5.2.6.

**Pipeline:** Clustering analysis only (no IRT/LMM - uses DERIVED data from RQ 5.2.6)

**Steps:** 7 total analysis steps
- Step 0: Load random effects from RQ 5.2.6 (DERIVED data dependency)
- Step 1: Standardize clustering features to z-scores
- Step 2: K-means model selection (K=1 to K=6 with BIC)
- Step 3: Fit final K-means model with optimal K
- Step 4: Validate cluster quality (silhouette, Davies-Bouldin, bootstrap stability)
- Step 5: Characterize clusters by domain-specific patterns
- Step 6: Prepare scatter plot matrix data (plot source CSV)

**Estimated Runtime:** Low-Medium (~10-15 minutes total)
- K-means iterations: ~5 minutes
- Bootstrap stability analysis (100 iterations): ~5 minutes
- Other steps: <5 minutes

**Key Decisions Applied:**
- K-means over LPA (justified in 1_concept.md): Exploratory analysis, interpretability, computational efficiency, sample size considerations (N=100 at lower bound for LPA)
- Cluster validation (Step 4): Silhouette score, Davies-Bouldin index, bootstrap Jaccard stability per rq_stats validation requirements
- GMM sensitivity analysis: If cluster quality fails (silhouette < 0.25 or non-spherical shapes), run GMM as alternative

**Critical Dependencies:**
- RQ 5.2.6 MUST complete Step 4 (extract random effects) before this RQ can run
- Expected input: results/ch5/5.2.6/data/step04_random_effects.csv (200 rows: 100 UID x 2 domains)

---

## Analysis Plan

### Step 0: Load Random Effects from RQ 5.2.6

**Dependencies:** None (first step, but depends on RQ 5.2.6 completion)
**Complexity:** Low (<1 minute - data loading only)

**Purpose:** Load domain-specific random effects (intercepts and slopes) from RQ 5.2.6 variance decomposition analysis.

**Input:**

**File:** results/ch5/5.2.6/data/step04_random_effects.csv
**Source:** Generated by RQ 5.2.6 Step 4 (domain-stratified LMM random effects extraction)
**Format:** CSV, long format with 200 rows (100 participants x 2 domains)
**Required Columns:**
  - `UID` (string, format: P### with leading zeros, e.g., P001)
  - `domain` (string, values: {what, where} - lowercase)
  - `Total_Intercept` (float, baseline theta at Day 0 for given domain)
  - `Total_Slope` (float, forgetting rate per day for given domain)

**Expected Dimensions:** 200 rows x 4 columns

**Filters:**
- No filters applied - all 100 participants x 2 domains required for clustering

**Dependency Circuit Breaker:**
- If results/ch5/5.2.6/data/step04_random_effects.csv does NOT exist -> QUIT with EXPECTATIONS ERROR
- Message: "RQ 5.2.6 must complete Step 4 (extract random effects) before RQ 5.2.7 can run"
- Action: User must execute RQ 5.2.6 first

**Processing:**

1. Check RQ 5.2.6 dependency file exists (circuit breaker if missing)
2. Load step04_random_effects.csv from RQ 5.2.6
3. Validate structure: 200 rows (100 UID x 2 domains), 4 columns (UID, domain, Total_Intercept, Total_Slope)
4. Pivot from long to wide format:
   - Rows: 100 UIDs
   - Columns: 4 clustering variables
     - Total_Intercept_What (What domain baseline)
     - Total_Slope_What (What domain forgetting rate)
     - Total_Intercept_Where (Where domain baseline)
     - Total_Slope_Where (Where domain forgetting rate)
5. Verify no missing values (all 100 participants must have all 4 variables)
6. Save pivoted data for Step 1 standardization

**Output:**

**File 1:** data/step00_random_effects_from_rq526.csv
**Format:** CSV, wide format (one row per participant)
**Columns:**
  - `UID` (string, participant identifier)
  - `Total_Intercept_What` (float, What domain baseline theta)
  - `Total_Slope_What` (float, What domain forgetting rate)
  - `Total_Intercept_Where` (float, Where domain baseline theta)
  - `Total_Slope_Where` (float, Where domain forgetting rate)
**Expected Rows:** 100 participants
**Expected Columns:** 5 (UID + 4 clustering variables)

**Validation Requirement:**

Validation tools MUST be used after data extraction tool execution. Specific validation tools will be determined by rq_tools based on data extraction requirements (likely tools.validation.validate_dataframe_structure).

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step00_random_effects_from_rq526.csv exists (exact path)
- Expected rows: 100 (all participants present, no data loss)
- Expected columns: 5 (UID + 4 clustering variables)
- Data types: UID (object/string), all 4 clustering variables (float64)

*Value Ranges:*
- Total_Intercept_* in [-3, 3] (typical IRT theta range for baseline ability)
- Total_Slope_* in [-2, 2] (typical forgetting rate range per day, negative = decline)
- No extreme outliers (|z-score| > 4 after standardization would indicate data error)

*Data Quality:*
- All 100 participants present (no missing UIDs from RQ 5.2.6)
- No NaN values (all 4 clustering variables must be non-null for all participants)
- No duplicate UIDs (each participant appears exactly once)
- Pivot validation: 100 UID x 2 domains (200 rows in source) -> 100 UID x 4 variables (wide output)

*Log Validation:*
- Required pattern: "Loaded 200 rows from RQ 5.2.6"
- Required pattern: "Pivoted to wide format: 100 rows x 4 clustering variables"
- Required pattern: "No missing values detected"
- Forbidden patterns: "ERROR", "NaN detected", "Missing domain", "Duplicate UID"
- Acceptable warnings: None expected for data loading

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Expected 100 rows, found 87 - missing participants")
- Log failure to logs/step00_load_random_effects.log
- Quit script immediately (do NOT proceed to Step 1)
- g_debug invoked to diagnose root cause (common causes: RQ 5.2.6 incomplete, participants excluded, data corruption)

---

### Step 1: Standardize Clustering Features

**Dependencies:** Step 0 (requires loaded random effects in wide format)
**Complexity:** Low (<1 minute - z-score transformation)

**Purpose:** Standardize all 4 clustering variables to z-scores (mean=0, SD=1) to ensure equal weighting across domains and parameters in K-means distance calculations.

**Input:**

**File:** data/step00_random_effects_from_rq526.csv
**Source:** Generated by Step 0 (pivoted random effects)
**Format:** CSV, wide format with 100 rows x 5 columns
**Required Columns:**
  - `UID` (string)
  - 4 clustering variables (Total_Intercept_What, Total_Slope_What, Total_Intercept_Where, Total_Slope_Where)

**Processing:**

1. Load step00_random_effects_from_rq526.csv
2. For each of 4 clustering variables:
   - Compute mean (mu) and standard deviation (sigma)
   - Transform to z-score: z = (x - mu) / sigma
   - Store z-scored variable with suffix `_z` (e.g., Total_Intercept_What_z)
3. Validate standardization:
   - Mean of each z-scored variable ~ 0 (tolerance: |mean| < 0.01)
   - SD of each z-scored variable ~ 1 (tolerance: |SD - 1| < 0.05)
   - Acceptable variance due to finite sample (N=100)
4. Check for extreme outliers:
   - Flag any z-score |z| > 3 (unusual but acceptable)
   - Flag any z-score |z| > 4 (data error - investigate)
   - Document flagged participants in log
5. Save standardized features for K-means clustering

**Output:**

**File 1:** data/step01_standardized_features.csv
**Format:** CSV, wide format
**Columns:**
  - `UID` (string)
  - 4 z-scored clustering variables:
    - `Total_Intercept_What_z` (float, standardized What baseline)
    - `Total_Slope_What_z` (float, standardized What slope)
    - `Total_Intercept_Where_z` (float, standardized Where baseline)
    - `Total_Slope_Where_z` (float, standardized Where slope)
**Expected Rows:** 100 participants
**Expected Columns:** 5 (UID + 4 z-scored variables)
**Note:** Original raw-scale variables NOT included in output (z-scored only for clustering)

**File 2:** data/step01_standardization_summary.txt
**Format:** Plain text report
**Contents:**
  - Mean and SD for each of 4 raw variables (before standardization)
  - Mean and SD for each of 4 z-scored variables (after standardization)
  - List of any extreme outliers (|z| > 3) with UID and variable name
  - Confirmation that standardization successful (mean ~ 0, SD ~ 1)

**Validation Requirement:**

Validation tools MUST be used after standardization tool execution. Specific validation tools will be determined by rq_tools (likely tools.validation.validate_standardization).

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step01_standardized_features.csv exists
- Expected rows: 100 (no data loss during standardization)
- Expected columns: 5 (UID + 4 z-scored variables)
- Data types: UID (object), all z-variables (float64)

*Value Ranges:*
- All z-scored variables: typically in [-3, 3] (99.7% of data per normal distribution)
- Outliers acceptable: |z| in (3, 4] (flag and document)
- Errors: |z| > 4 (investigate as potential data error)

*Data Quality:*
- No NaN values (standardization should not introduce missing data)
- No infinite values (division by zero would indicate zero variance - impossible for random effects)
- Standardization validation: For each z-variable:
  - mean in [-0.01, 0.01] (approximately zero)
  - SD in [0.95, 1.05] (approximately one, allowing for finite sample variance)

*Log Validation:*
- Required pattern: "Standardized 4 variables to z-scores"
- Required pattern: "Validation - PASS: All means within [-0.01, 0.01]"
- Required pattern: "Validation - PASS: All SDs within [0.95, 1.05]"
- Forbidden patterns: "ERROR", "NaN introduced", "Infinite value", "Zero variance"
- Acceptable warnings: "Outlier detected: UID=P042, Total_Slope_When_z=3.2" (document but proceed)

**Expected Behavior on Validation Failure:**
- Raise error with specific failure (e.g., "Total_Intercept_What_z: mean=0.15 (expected ~ 0)")
- Log failure to logs/step01_standardize_features.log
- Quit script immediately (do NOT proceed to Step 2)
- g_debug invoked to diagnose (common causes: incorrect formula, subset data, zero variance)

---

### Step 2: K-Means Model Selection (K=1 to K=6)

**Dependencies:** Step 1 (requires standardized clustering features)
**Complexity:** Medium (~5 minutes - 6 K-means models with multiple random initializations)

**Purpose:** Test K=1 to K=6 cluster solutions using K-means algorithm, compute inertia and BIC for each K, select optimal K as BIC minimum (or apply parsimony rule if �BIC < 2).

**Input:**

**File:** data/step01_standardized_features.csv
**Source:** Generated by Step 1 (z-scored clustering variables)
**Format:** CSV with 100 rows x 5 columns (UID + 4 z-variables)
**Clustering Matrix:** Extract 4 z-scored columns only (exclude UID column)
**Expected Shape:** 100 participants x 4 features (numpy array for sklearn)

**Processing:**

1. Load standardized features (100 x 4 matrix, z-scored)
2. For K in range(1, 7):  # Test K=1, 2, 3, 4, 5, 6
   - Fit K-means with:
     - n_clusters=K
     - random_state=42 (reproducibility)
     - n_init=50 (50 random initializations per K, select best by inertia)
     - max_iter=300 (allow convergence)
     - algorithm='lloyd' (standard K-means)
   - Extract inertia (within-cluster sum of squares)
   - Compute BIC:
     - BIC = n_samples * log(inertia / n_samples) + K * log(n_samples) * n_features
     - Where: n_samples=100, n_features=4
   - Store: K, inertia, BIC
3. Model selection logic:
   - Find K with minimum BIC (K_min_BIC)
   - Check parsimony rule: If multiple K have �BIC < 2 from minimum, select smallest K (more parsimonious)
   - Document selected K with justification
4. Plot elbow curve (inertia vs K) and BIC curve (BIC vs K) for diagnostic inspection
5. Save model selection results for Step 3 final fitting

**Output:**

**File 1:** data/step02_cluster_selection.csv
**Format:** CSV with 6 rows (K=1 to K=6), 3 columns
**Columns:**
  - `K` (int, number of clusters tested)
  - `inertia` (float, within-cluster sum of squares)
  - `BIC` (float, Bayesian Information Criterion)
**Expected Rows:** 6 (K=1, 2, 3, 4, 5, 6)
**Note:** Lower inertia = tighter clusters, Lower BIC = better model fit with parsimony penalty

**File 2:** data/step02_optimal_k_selection.txt
**Format:** Plain text report
**Contents:**
  - Selected K value (e.g., "Optimal K = 3")
  - Justification:
    - If clear BIC minimum: "K=3 has minimum BIC = 425.3"
    - If parsimony rule applied: "K=2 and K=3 have �BIC = 1.2 < 2, selecting K=2 (more parsimonious)"
  - BIC comparison table (K, BIC, �BIC from minimum)
  - Elbow curve interpretation (e.g., "Elbow visible at K=3")

**Validation Requirement:**

Validation tools MUST be used after K-means model selection tool execution. Specific validation tools will be determined by rq_tools (likely tools.validation.validate_dataframe_structure for cluster_selection.csv).

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step02_cluster_selection.csv exists
- Expected rows: 6 (K=1 to K=6 tested)
- Expected columns: 3 (K, inertia, BIC)
- Data types: K (int64), inertia (float64), BIC (float64)

*Value Ranges:*
- K in [1, 6] (exactly 6 models tested)
- inertia > 0 (must be positive, decreases monotonically as K increases)
- BIC: finite values (no NaN, no inf)
- Monotonicity check: inertia[K=1] > inertia[K=2] > ... > inertia[K=6] (must decrease with more clusters)

*Data Quality:*
- All 6 K values present (no missing models)
- No duplicate K values (each K tested exactly once)
- BIC minimum identified (at least one K has minimum BIC)
- Parsimony rule applied if needed (documented in log)

*Log Validation:*
- Required pattern: "Tested K=1 to K=6 models"
- Required pattern: "Optimal K selected: K=[value]"
- Required pattern: "BIC minimum at K=[value], BIC=[value]"
- Forbidden patterns: "ERROR", "K-means failed to converge", "NaN inertia", "BIC=inf"
- Acceptable warnings: "K=6 has �BIC=1.8 from K=5 (near parsimony threshold)" (document but proceed)

**Expected Behavior on Validation Failure:**
- Raise error with specific failure (e.g., "K-means failed to converge for K=4")
- Log failure to logs/step02_kmeans_model_selection.log
- Quit script immediately (do NOT proceed to Step 3)
- g_debug invoked to diagnose (common causes: initialization issues, degenerate clusters, data scaling errors)

---

### Step 3: Fit Final K-Means Model with Optimal K

**Dependencies:** Step 2 (requires selected optimal K)
**Complexity:** Low (~1 minute - single K-means fit with optimal K)

**Purpose:** Fit final K-means model using optimal K selected in Step 2, extract cluster assignments and cluster centers for interpretation and validation.

**Input:**

**File 1:** data/step01_standardized_features.csv
**Source:** Step 1 (z-scored clustering variables)
**Format:** 100 participants x 4 z-scored features

**File 2:** data/step02_optimal_k_selection.txt
**Source:** Step 2 (model selection results)
**Format:** Plain text with selected K value
**Extract:** Parse "Optimal K = [value]" to determine n_clusters parameter

**Processing:**

1. Load standardized features (100 x 4 z-scored matrix)
2. Read optimal K from step02_optimal_k_selection.txt (e.g., K=3)
3. Fit final K-means model:
   - n_clusters=K (from Step 2 selection)
   - random_state=42 (reproducibility)
   - n_init=50 (50 random initializations, select best)
   - max_iter=300 (allow convergence)
4. Extract cluster assignments:
   - labels_ attribute (array of length 100, values 0 to K-1)
   - Verify all cluster IDs consecutive (0, 1, 2, ..., K-1)
5. Extract cluster centers:
   - cluster_centers_ attribute (K x 4 matrix of z-scored centroids)
6. Compute cluster sizes:
   - Count participants per cluster
   - Check balance: No cluster < 10% of sample (N < 10 participants)
7. Merge cluster assignments with original UIDs
8. Save cluster assignments and centers for characterization and validation

**Output:**

**File 1:** data/step03_cluster_assignments.csv
**Format:** CSV with 100 rows (one per participant), 2 columns
**Columns:**
  - `UID` (string, participant identifier)
  - `cluster` (int, cluster assignment 0 to K-1)
**Expected Rows:** 100 participants
**Note:** Cluster IDs are integers starting at 0 (sklearn convention)

**File 2:** data/step03_cluster_centers.csv
**Format:** CSV with K rows (one per cluster), 5 columns
**Columns:**
  - `cluster` (int, cluster ID 0 to K-1)
  - 4 z-scored centroid coordinates:
    - `Total_Intercept_What_z` (float, What baseline centroid)
    - `Total_Slope_What_z` (float, What slope centroid)
    - `Total_Intercept_Where_z` (float, Where baseline centroid)
    - `Total_Slope_Where_z` (float, Where slope centroid)
**Expected Rows:** K (number of clusters from Step 2, typically 2-4)

**File 3:** data/step03_cluster_sizes.csv
**Format:** CSV with K rows, 3 columns
**Columns:**
  - `cluster` (int, cluster ID)
  - `N` (int, number of participants in cluster)
  - `percent` (float, percentage of total sample)
**Expected Rows:** K clusters
**Validation:** All N >= 10 (balance criterion), sum(N) = 100, sum(percent) ~ 100.0

**Validation Requirement:**

Validation tools MUST be used after K-means fitting tool execution. Specific validation tools will be determined by rq_tools (likely tools.validation.validate_cluster_assignment).

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step03_cluster_assignments.csv exists
  - Expected rows: 100 (all participants assigned)
  - Expected columns: 2 (UID, cluster)
  - Data types: UID (object), cluster (int64)
- data/step03_cluster_centers.csv exists
  - Expected rows: K (from Step 2 selection, typically 2-4)
  - Expected columns: 5 (cluster ID + 4 centroid coordinates)
  - Data types: cluster (int64), all centroids (float64)
- data/step03_cluster_sizes.csv exists
  - Expected rows: K
  - Expected columns: 3 (cluster, N, percent)

*Value Ranges:*
- cluster assignments in [0, K-1] (consecutive integers starting at 0)
- cluster centers (z-scored): typically in [-2, 2] (centroids less extreme than individual participants)
- cluster sizes: N in [10, 90] (balance requirement: no cluster < 10% of 100 participants)
- percent: in [10.0, 90.0], sum ~ 100.0 (within rounding tolerance)

*Data Quality:*
- All 100 participants assigned (no missing cluster assignments)
- Cluster IDs consecutive: {0, 1, ..., K-1} with no gaps (e.g., for K=3: {0, 1, 2} NOT {0, 2, 5})
- Cluster sizes balanced: min(N) >= 10 (no degenerate clusters)
- Cluster centers interpretable: All centroids within [-3, 3] z-score range

*Log Validation:*
- Required pattern: "Fitted K-means with K=[value] clusters"
- Required pattern: "All clusters assigned: 100 participants"
- Required pattern: "Cluster sizes balanced: min(N)=[value] >= 10"
- Required pattern: "Cluster IDs consecutive: [0, 1, ..., K-1]"
- Forbidden patterns: "ERROR", "Degenerate cluster", "Cluster size < 10", "Missing cluster ID"
- Acceptable warnings: "Cluster imbalance: cluster 0 has 45%, cluster 1 has 25%, cluster 2 has 30%" (document but acceptable if all >= 10%)

**Expected Behavior on Validation Failure:**
- Raise error with specific failure (e.g., "Cluster 2 has N=7 participants (< 10 minimum)")
- Log failure to logs/step03_fit_final_kmeans.log
- Quit script immediately (do NOT proceed to Step 4)
- g_debug invoked to diagnose (common causes: poor cluster quality, outliers dominating small cluster, inappropriate K selection)

---

### Step 4: Validate Cluster Quality

**Dependencies:** Step 3 (requires cluster assignments and centers)
**Complexity:** Medium (~5 minutes - bootstrap stability analysis with 100 iterations)

**Purpose:** Assess cluster quality using three metrics: (1) silhouette score (cohesion), (2) Davies-Bouldin index (separation), (3) bootstrap Jaccard stability. This step implements validation requirements from rq_stats review.

**Input:**

**File 1:** data/step01_standardized_features.csv
**Source:** Step 1 (z-scored features for distance calculations)
**Format:** 100 participants x 4 z-scored variables

**File 2:** data/step03_cluster_assignments.csv
**Source:** Step 3 (cluster labels for validation)
**Format:** 100 participants with cluster assignments

**Processing:**

1. **Silhouette Score (Cohesion and Separation)**
   - Compute silhouette coefficient for each participant:
     - a = mean intra-cluster distance (cohesion)
     - b = mean nearest-cluster distance (separation)
     - s = (b - a) / max(a, b) in [-1, 1]
   - Aggregate to global silhouette score (mean across all participants)
   - Interpretation:
     - s >= 0.50: Good cluster structure (target)
     - s >= 0.40: Acceptable cluster structure (minimum)
     - s < 0.40: Weak cluster structure (consider alternative K or GMM)
     - s < 0.25: Poor cluster structure (report continuous variation, not discrete clusters)

2. **Davies-Bouldin Index (Cluster Separation)**
   - For each cluster pair, compute similarity ratio:
     - R_ij = (s_i + s_j) / d_ij
     - Where s_i = average distance within cluster i, d_ij = distance between centroids i and j
   - DB = mean of max(R_ij) over all clusters
   - Interpretation:
     - DB < 1.0: Good separation (target)
     - DB < 1.5: Acceptable separation (minimum)
     - DB >= 1.5: Poor separation (overlapping clusters)

3. **Bootstrap Stability (Jaccard Coefficient)**
   - For 100 bootstrap iterations:
     - Resample 80 participants (80% subsample with replacement)
     - Fit K-means with same K and random_state
     - Compute Jaccard index: J = (number of pairs agreeing) / (number of pairs)
     - Agreement = both in same cluster (bootstrap vs original) OR both in different clusters
   - Aggregate bootstrap Jaccard:
     - Mean Jaccard across 100 iterations
     - 95% CI (2.5th and 97.5th percentiles)
   - Interpretation:
     - Jaccard > 0.75: Stable clusters (target)
     - Jaccard in [0.60, 0.75]: Moderately stable (acceptable with caution)
     - Jaccard < 0.60: Unstable clusters (report as tentative, interpret cautiously)

4. **Combined Decision Rule:**
   - GOOD: silhouette >= 0.50, DB < 1.0, Jaccard > 0.75 -> Report clusters as well-defined
   - ACCEPTABLE: silhouette >= 0.40, DB < 1.5, Jaccard > 0.60 -> Report clusters as acceptable
   - POOR: silhouette < 0.40 OR DB >= 1.5 OR Jaccard < 0.60 -> Report clusters as weak/tentative
   - FAIL: silhouette < 0.25 -> Recommend GMM sensitivity analysis or continuous interpretation

**Output:**

**File 1:** data/step04_cluster_validation.csv
**Format:** CSV with 5 rows (summary metrics), 3 columns
**Columns:**
  - `metric` (string, metric name)
  - `value` (float, metric value)
  - `interpretation` (string, quality assessment)
**Rows:**
  - Row 1: metric="silhouette_score", value=[0.0-1.0], interpretation=["Poor" | "Acceptable" | "Good"]
  - Row 2: metric="davies_bouldin_index", value=[0.0-inf], interpretation=["Good" | "Acceptable" | "Poor"]
  - Row 3: metric="jaccard_mean", value=[0.0-1.0], interpretation=["Unstable" | "Moderate" | "Stable"]
  - Row 4: metric="jaccard_ci_lower", value=[0.0-1.0], interpretation="95% CI lower bound"
  - Row 5: metric="jaccard_ci_upper", value=[0.0-1.0], interpretation="95% CI upper bound"

**File 2:** data/step04_validation_summary.txt
**Format:** Plain text report
**Contents:**
  - Overall cluster quality assessment (GOOD | ACCEPTABLE | POOR | FAIL)
  - Silhouette score: value, interpretation, threshold comparison
  - Davies-Bouldin index: value, interpretation, threshold comparison
  - Bootstrap Jaccard: mean, 95% CI, interpretation, threshold comparison
  - Recommendation:
    - If GOOD/ACCEPTABLE: "Proceed with cluster interpretation and characterization"
    - If POOR: "Interpret clusters cautiously - consider alternative K or GMM sensitivity analysis"
    - If FAIL: "Cluster structure not supported - run GMM sensitivity analysis or interpret as continuous variation"

**Validation Requirement:**

Validation tools MUST be used after cluster quality validation tool execution. Specific validation tools will be determined by rq_tools (likely tools.validation.validate_bootstrap_stability and tools.validation.validate_numeric_range).

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step04_cluster_validation.csv exists
  - Expected rows: 5 (silhouette, DB, Jaccard mean, Jaccard CI lower, Jaccard CI upper)
  - Expected columns: 3 (metric, value, interpretation)
  - Data types: metric (object), value (float64), interpretation (object)
- data/step04_validation_summary.txt exists
  - Plain text with overall assessment

*Value Ranges:*
- silhouette_score in [-1, 1] (typically [0, 1] for reasonable clusters)
- davies_bouldin_index > 0 (finite, typically [0.5, 3.0])
- jaccard_mean in [0, 1] (bootstrap stability)
- jaccard_ci_lower in [0, 1], jaccard_ci_lower <= jaccard_mean <= jaccard_ci_upper (valid CI)

*Data Quality:*
- All 5 metrics present (no missing values)
- Bootstrap CI valid: CI_lower < CI_upper
- Interpretations consistent with thresholds:
  - silhouette >= 0.50 -> "Good", [0.40, 0.50) -> "Acceptable", < 0.40 -> "Poor"
  - DB < 1.0 -> "Good", [1.0, 1.5) -> "Acceptable", >= 1.5 -> "Poor"
  - Jaccard > 0.75 -> "Stable", [0.60, 0.75] -> "Moderate", < 0.60 -> "Unstable"

*Log Validation:*
- Required pattern: "Computed silhouette score: [value]"
- Required pattern: "Computed Davies-Bouldin index: [value]"
- Required pattern: "Bootstrap stability (100 iterations): Jaccard mean=[value], 95% CI=[lower, upper]"
- Required pattern: "Overall cluster quality: [GOOD | ACCEPTABLE | POOR | FAIL]"
- Forbidden patterns: "ERROR", "Bootstrap failed", "NaN metric", "Infinite DB"
- Acceptable warnings: "Cluster quality POOR - consider GMM sensitivity" (document and proceed, leave GMM for user decision)

**Expected Behavior on Validation Failure:**
- Raise error with specific failure (e.g., "Bootstrap Jaccard computation failed: only 50/100 iterations succeeded")
- Log failure to logs/step04_validate_cluster_quality.log
- Quit script immediately (do NOT proceed to Step 5)
- g_debug invoked to diagnose (common causes: insufficient data for bootstrap, degenerate clusters, computational errors)

**Note on GMM Sensitivity Analysis:**
If cluster quality is POOR or FAIL, rq_stats validation recommended GMM as alternative. However, GMM implementation is left to user decision after reviewing cluster quality report. This step documents cluster quality; GMM sensitivity is optional downstream analysis.

---

### Step 5: Characterize Clusters by Domain-Specific Patterns

**Dependencies:** Step 4 (requires validated cluster assignments)
**Complexity:** Low (~1 minute - descriptive statistics and pattern labeling)

**Purpose:** Compute mean intercept and slope per cluster for each domain (What, Where, When), assign interpretive labels based on domain-specific patterns (e.g., "High What, Low Where/When"), create human-readable cluster characterizations.

**Input:**

**File 1:** data/step00_random_effects_from_rq526.csv
**Source:** Step 0 (original raw-scale random effects, NOT z-scored)
**Format:** 100 participants x 5 columns (UID + 4 variables in original theta scale)
**Note:** Use raw-scale values for interpretability (z-scores are for clustering only)

**File 2:** data/step03_cluster_assignments.csv
**Source:** Step 3 (cluster labels per participant)
**Format:** 100 participants x 2 columns (UID, cluster)

**Processing:**

1. Merge raw-scale random effects with cluster assignments on UID
2. For each cluster (0 to K-1):
   - Compute mean and SD for each of 4 variables:
     - Total_Intercept_What (What baseline)
     - Total_Slope_What (What forgetting rate)
     - Total_Intercept_Where (Where baseline)
     - Total_Slope_Where (Where forgetting rate)
   - Aggregate statistics: mean, SD, min, max per cluster per variable
3. Domain-specific pattern identification:
   - Compare cluster centroids across domains:
     - High baseline: mean intercept > 0 (above grand mean)
     - Low baseline: mean intercept < 0 (below grand mean)
     - Fast forgetting: mean slope < -0.5 (steep decline)
     - Slow forgetting: mean slope > -0.5 (shallow decline)
   - Identify domain-selective patterns:
     - Example: Cluster 1 = High What intercept, Low Where intercept
     - Example: Cluster 2 = Fast What forgetting, Slow Where forgetting
4. Assign interpretive labels per cluster:
   - Label format: "{Baseline pattern} / {Forgetting pattern}"
   - Examples:
     - "High Memory / Slow Forgetting" (high both intercepts, shallow both slopes)
     - "Domain-Selective What" (high What intercept, average Where)
     - "Fast Spatial Decline" (steep Where slope, moderate What slope)
5. Create cluster characterization report with:
   - Cluster ID
   - Sample size (N and percent)
   - Interpretive label
   - Domain-specific statistics (mean intercept/slope per domain)
   - Qualitative description (2-3 sentences per cluster)

**Output:**

**File 1:** data/step05_cluster_summary_statistics.csv
**Format:** CSV with K x 4 = K*4 rows (one row per cluster per variable), 6 columns
**Columns:**
  - `cluster` (int, cluster ID 0 to K-1)
  - `variable` (string, one of 4 clustering variables)
  - `mean` (float, cluster mean in raw theta scale)
  - `SD` (float, cluster standard deviation)
  - `min` (float, cluster minimum)
  - `max` (float, cluster maximum)
**Expected Rows:** K * 4 (e.g., if K=3: 12 rows)

**File 2:** data/step05_cluster_characterization.txt
**Format:** Plain text report
**Contents:** For each cluster:
  - Cluster ID and size (N, percent)
  - Interpretive label (e.g., "High Memory / Slow Forgetting")
  - Domain-specific means:
    - What: Intercept mean, Slope mean
    - Where: Intercept mean, Slope mean
    - When: Intercept mean, Slope mean
  - Qualitative description (2-3 sentences explaining pattern)
  - Example description:
    ```
    Cluster 1: High Memory / Slow Forgetting (N=42, 42%)

    This cluster exhibits high baseline memory across all three domains
    (What: 0.8, Where: 0.6, When: 0.5) with shallow forgetting rates
    (What: -0.3, Where: -0.4, When: -0.5). Participants show strong
    encoding and retention across all memory components.
    ```

**Validation Requirement:**

Validation tools MUST be used after cluster characterization tool execution. Specific validation tools will be determined by rq_tools (likely tools.validation.validate_cluster_summary_stats).

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step05_cluster_summary_statistics.csv exists
  - Expected rows: K * 4 (e.g., K=3 -> 12 rows)
  - Expected columns: 6 (cluster, variable, mean, SD, min, max)
  - Data types: cluster (int64), variable (object), mean/SD/min/max (float64)
- data/step05_cluster_characterization.txt exists
  - Plain text with K cluster descriptions

*Value Ranges:*
- cluster in [0, K-1] (consecutive IDs)
- variable in {Total_Intercept_What, Total_Slope_What, Total_Intercept_Where, Total_Slope_Where}
- mean (raw theta scale): typically in [-2, 2] for intercepts, [-1.5, 0] for slopes
- SD >= 0 (non-negative, zero SD indicates no within-cluster variance)
- min <= mean <= max (logical consistency)

*Data Quality:*
- All K clusters present (4 rows per cluster)
- All 4 variables present per cluster (K rows per variable)
- Summary statistics valid: min <= mean <= max, SD >= 0
- Characterizations interpretable (labels match numerical patterns)

*Log Validation:*
- Required pattern: "Computed summary statistics for K=[value] clusters"
- Required pattern: "Assigned interpretive labels to all [value] clusters"
- Required pattern: "Cluster characterization complete"
- Forbidden patterns: "ERROR", "Missing cluster", "NaN in summary statistics", "Negative SD"
- Acceptable warnings: "Cluster 2 has small within-cluster variance (SD < 0.1) for Total_Slope_What" (document but acceptable)

**Expected Behavior on Validation Failure:**
- Raise error with specific failure (e.g., "Cluster 1 statistics invalid: min > mean")
- Log failure to logs/step05_characterize_clusters.log
- Quit script immediately (do NOT proceed to Step 6)
- g_debug invoked to diagnose (common causes: merge error, aggregation bug, outlier contamination)

---

### Step 6: Prepare Scatter Plot Matrix Data

**Dependencies:** Step 5 (requires cluster assignments and characterizations)
**Complexity:** Low (~1 minute - data aggregation for plotting)

**Purpose:** Create plot source CSV for scatter plot matrix visualization (to be generated by rq_plots in Step 20). This step prepares plot-ready data with cluster assignments, cluster centers, and metadata for visualization.

**NOTE:** This is plot data preparation (analysis step), NOT plot generation. Per Option B architecture:
- THIS step (Step 6) creates plot source CSV in data/ folder
- rq_plots (Step 20) reads source CSV and generates PNG in plots/ folder
- NO data aggregation in rq_plots (visualization only)

**Input:**

**File 1:** data/step01_standardized_features.csv
**Source:** Step 1 (z-scored clustering variables for plotting)
**Format:** 100 participants x 5 columns (UID + 4 z-variables)

**File 2:** data/step03_cluster_assignments.csv
**Source:** Step 3 (cluster labels)
**Format:** 100 participants x 2 columns (UID, cluster)

**File 3:** data/step03_cluster_centers.csv
**Source:** Step 3 (cluster centroids)
**Format:** K clusters x 5 columns (cluster ID + 4 z-scored centroids)

**File 4:** data/step05_cluster_characterization.txt
**Source:** Step 5 (interpretive labels)
**Format:** Plain text with cluster labels
**Extract:** Parse cluster labels for annotation (e.g., "Cluster 0: High Memory / Slow Forgetting")

**Processing:**

1. Merge z-scored features with cluster assignments on UID
2. Create plot data matrix:
   - Rows: 100 participants (individual points in scatter plot)
   - Columns:
     - `UID` (participant identifier)
     - `cluster` (int, cluster assignment for color coding)
     - `cluster_label` (string, interpretive label for legend)
     - 4 z-scored variables (axes for pairwise scatter plots):
       - `Total_Intercept_What_z`
       - `Total_Slope_What_z`
       - `Total_Intercept_Where_z`
       - `Total_Slope_Where_z`
3. Create cluster centers annotation data:
   - Rows: K clusters (centroid markers in scatter plot)
   - Columns: Same as plot data (cluster ID, label, 4 z-coordinates)
4. Combine participant data and centroid data in single CSV:
   - First 100 rows: individual participants (point_type="participant")
   - Next K rows: cluster centroids (point_type="centroid")
   - Column `point_type` distinguishes participants from centroids
5. Add reference lines metadata (for plotting z=0 lines):
   - Grand mean reference (z=0 horizontal and vertical lines on all axes)

**Output:**

**File 1:** data/step06_scatter_plot_matrix_data.csv
**Format:** CSV with (100 + K) rows, 8 columns
**Columns:**
  - `UID` (string, participant ID or "Centroid_[cluster]")
  - `cluster` (int, cluster assignment 0 to K-1)
  - `cluster_label` (string, interpretive label from Step 5)
  - `point_type` (string, "participant" or "centroid")
  - 4 z-scored variables (float, axes for scatter plot matrix):
    - `Total_Intercept_What_z`
    - `Total_Slope_What_z`
    - `Total_Intercept_Where_z`
    - `Total_Slope_Where_z`
**Expected Rows:** 100 + K (participants + cluster centroids, e.g., 103 if K=3)
**Note:** rq_plots will create 4x4 pairwise scatter plot matrix (6 unique pairs) with:
  - Participant points colored by cluster
  - Cluster centroids marked with X or star
  - Reference lines at z=0 (grand mean)
  - Diagonal histograms or density plots

**Validation Requirement:**

Validation tools MUST be used after plot data preparation tool execution. Specific validation tools will be determined by rq_tools (likely tools.validation.validate_plot_data_completeness).

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step06_scatter_plot_matrix_data.csv exists
  - Expected rows: 100 + K (e.g., 103 if K=3)
  - Expected columns: 8 (UID, cluster, cluster_label, point_type, 4 z-variables)
  - Data types: UID (object), cluster (int64), cluster_label (object), point_type (object), all z-variables (float64)

*Value Ranges:*
- cluster in [0, K-1] (all clusters represented)
- point_type in {"participant", "centroid"} (categorical)
- All z-variables in [-4, 4] (typical z-score range, allowing for outliers)

*Data Quality:*
- All 100 participants present (no missing)
- All K centroids present (one per cluster)
- point_type distribution: 100 "participant" + K "centroid" rows
- All clusters represented: Each cluster ID has participants AND centroid
- No NaN values (all cells must have valid values for plotting)

*Log Validation:*
- Required pattern: "Plot data preparation complete: [100+K] rows created"
- Required pattern: "All [K] clusters represented with participants and centroids"
- Required pattern: "No missing values in plot data"
- Forbidden patterns: "ERROR", "NaN values detected", "Missing cluster", "Missing centroid"
- Acceptable warnings: None expected for plot data preparation

**Expected Behavior on Validation Failure:**
- Raise error with specific failure (e.g., "Cluster 2 has no centroid row")
- Log failure to logs/step06_prepare_scatter_plot_data.log
- Quit script immediately (rq_plots cannot proceed without valid plot data)
- g_debug invoked to diagnose (common causes: merge error, missing cluster centers, incomplete cluster assignments)

**Plot Specifications (for rq_plots Step 20):**

**Plot Description:** 4x4 scatter plot matrix showing pairwise relationships among 4 clustering variables, with points colored by cluster assignment and cluster centroids marked.

**Plotting Function (General):** Scatter plot matrix with cluster annotations
- rq_plots agent maps to appropriate tools/plots.py function (likely custom scatter matrix function)

**Visual Elements:**
- Points: Participant data colored by cluster (N=100 points per panel)
- Centroids: Cluster centers marked with large X or star symbols (K centroids per panel)
- Reference lines: z=0 horizontal and vertical lines (grand mean) on all axes
- Diagonal: Histograms or density plots showing distribution per variable
- Legend: Cluster labels (interpretive names from Step 5)

**Expected Output (rq_plots):**
- plots/step06_cluster_scatter_matrix.png (PNG file generated by rq_plots)
- Dimensions: 10x10 inches @ 300 DPI (high resolution for publication)

---

## Expected Outputs

### Data Files (ALL analysis inputs and outputs - intermediate and final)
- data/step00_random_effects_from_rq526.csv (100 rows x 5 cols: UID + 4 raw-scale clustering variables from RQ 5.2.6)
- data/step01_standardized_features.csv (100 rows x 5 cols: UID + 4 z-scored clustering variables)
- data/step01_standardization_summary.txt (text report: means, SDs, outliers)
- data/step02_cluster_selection.csv (6 rows: K=1-6 with inertia and BIC)
- data/step02_optimal_k_selection.txt (text report: selected K with justification)
- data/step03_cluster_assignments.csv (100 rows x 2 cols: UID, cluster)
- data/step03_cluster_centers.csv (K rows x 5 cols: cluster ID + 4 z-scored centroids)
- data/step03_cluster_sizes.csv (K rows x 3 cols: cluster ID, N, percent)
- data/step04_cluster_validation.csv (5 rows: silhouette, DB, Jaccard metrics)
- data/step04_validation_summary.txt (text report: overall quality assessment)
- data/step05_cluster_summary_statistics.csv (K*4 rows: cluster stats per variable)
- data/step05_cluster_characterization.txt (text report: interpretive cluster descriptions)
- data/step06_scatter_plot_matrix_data.csv (100+K rows x 8 cols: plot source data)

### Logs (ONLY execution logs - .log files capturing stdout/stderr)
- logs/step00_load_random_effects.log
- logs/step01_standardize_features.log
- logs/step02_kmeans_model_selection.log
- logs/step03_fit_final_kmeans.log
- logs/step04_validate_cluster_quality.log
- logs/step05_characterize_clusters.log
- logs/step06_prepare_scatter_plot_data.log

### Plots (EMPTY until rq_plots runs - Step 20 in workflow)
- plots/step06_cluster_scatter_matrix.png (created by rq_plots, NOT by analysis steps)
  - Generated by rq_plots reading data/step06_scatter_plot_matrix_data.csv
  - 4x4 scatter plot matrix colored by cluster with centroids marked

### Results (EMPTY until rq_results runs - Step 22 in workflow)
- results/summary.md (created by rq_results, NOT by analysis steps)
  - Human-readable summary of clustering results
  - Cluster characterizations with domain-specific patterns
  - Clinical/theoretical interpretation

---

## Expected Data Formats

### Clustering Variables (6 per participant)

**Raw Scale (from RQ 5.2.6):**
- Total_Intercept_What/Where/When: Baseline theta at Day 0 per domain (typical range: [-2, 2])
- Total_Slope_What/Where/When: Forgetting rate per day per domain (typical range: [-1.5, 0], negative = decline)

**Z-Scored (for clustering):**
- All 6 variables transformed to mean=0, SD=1
- Ensures equal weighting in K-means distance calculations
- Typical range: [-3, 3] (99.7% of data if normally distributed)

### Data Transformations

**Step 0 -> Step 1 (Pivot Long to Wide):**
- Input: 200 rows (100 UID x 2 domains in long format)
- Output: 100 rows (UID as rows, 4 domain-specific variables as columns)
- Transformation: pivot_wider(names_from=domain, values_from=c(Total_Intercept, Total_Slope))

**Step 1 -> Step 2 (Standardization):**
- Input: 100 x 4 raw-scale matrix
- Output: 100 x 4 z-scored matrix
- Transformation: For each column: z = (x - mean(x)) / sd(x)

**Step 3 -> Step 5 (Merge for Characterization):**
- Input 1: 100 rows (raw-scale variables)
- Input 2: 100 rows (cluster assignments)
- Output: 100 rows merged (UID, 4 variables, cluster)
- Transformation: left_join on UID, group_by cluster, summarize mean/SD/min/max per variable

### Column Naming Conventions

**Per names.md (v4.1):**
- Participant ID: `UID` (NOT uid, participant_id, or subject)
- Cluster assignment: `cluster` (int, 0-indexed per sklearn convention)
- Z-scored variables: suffix `_z` (e.g., Total_Intercept_What_z)
- Raw-scale variables: no suffix (e.g., Total_Intercept_What)

### File Formats

**CSV Files:**
- Encoding: UTF-8
- Line endings: LF (Unix-style)
- No index column (index=False in pandas.to_csv)
- Header row required (column names)
- Floating-point precision: 4 decimal places for z-scores, 3 for raw-scale

**Text Reports:**
- Plain text (UTF-8 encoding)
- Markdown formatting acceptable for structure
- Include section headers, bullet lists, numeric tables

---

## Cross-RQ Dependencies

### Dependency Type 2: DERIVED Data from Other RQs (Dependencies Exist)

**This RQ requires outputs from:**

- **RQ 5.2.6** (Domain-Specific Variance Decomposition)
  - File: results/ch5/5.2.6/data/step04_random_effects.csv
  - Used in: Step 0 (source of 6 clustering variables per participant)
  - Rationale: RQ 5.2.6 fits domain-stratified LMMs with random slopes, extracting intercept and slope estimates for each participant in each domain. This RQ clusters participants based on those domain-specific trajectories.

**Execution Order Constraint:**
1. RQ 5.2.6 must complete Step 4 (extract random effects) first
2. This RQ (5.2.7) executes second (uses RQ 5.2.6 outputs)

**Data Source Boundaries:**
- **RAW data:** None (this RQ uses only DERIVED data from RQ 5.2.6)
- **DERIVED data:** Random effects from RQ 5.2.6 (6 variables per participant)
- **Scope:** This RQ does NOT re-fit LMMs or re-extract random effects (uses RQ 5.2.6 estimates as fixed inputs)

**Validation:**
- Step 0: Check results/ch5/5.2.6/data/step04_random_effects.csv exists
  - Circuit breaker: EXPECTATIONS ERROR if absent
  - Message: "RQ 5.2.6 must complete Step 4 before RQ 5.2.7 can run"
  - Action: User must execute RQ 5.2.6 first

---

## Validation Requirements

### CRITICAL MANDATE

Every analysis step in this plan MUST use validation tools after analysis tool execution.

This is not optional. This is the core architectural principle preventing cascading failures observed in v3.0 (where analysis errors propagated undetected through 5+ downstream steps before discovery).

**Exact Specification Requirement:**

> "Validation tools MUST be used after analysis tool execution"

**Implementation:**
- rq_tools (Step 11 workflow) will read tool_inventory.md validation tools section
- rq_tools will specify BOTH analysis tool + validation tool per step in 3_tools.yaml
- rq_analysis (Step 12 workflow) will embed validation tool call AFTER analysis tool call in 4_analysis.yaml
- g_code (Step 14 workflow) will generate stepN_name.py scripts with validation function calls
- bash execution (Step 14 workflow) will run analysis -> validation -> error on validation failure

**Downstream Agent Requirements:**
- **rq_tools:** MUST specify validation tool for EVERY analysis step (no exceptions)
- **rq_analysis:** MUST embed validation tool call for EVERY analysis step (no exceptions)
- **g_code:** MUST generate code with validation function calls (no exceptions)
- **rq_inspect:** MUST verify validation ran successfully (checks logs/stepN_name.log for validation output)

---

## Summary

**Total Steps:** 7 (Step 0 through Step 6)

**Estimated Runtime:** Low-Medium (~10-15 minutes total)
- Step 0: <1 min (data loading)
- Step 1: <1 min (standardization)
- Step 2: ~5 min (K-means model selection, 6 models with 50 inits each)
- Step 3: <1 min (final K-means fit)
- Step 4: ~5 min (bootstrap stability, 100 iterations)
- Step 5: <1 min (cluster characterization)
- Step 6: <1 min (plot data preparation)

**Cross-RQ Dependencies:** RQ 5.2.6 (must complete Step 4 - extract random effects)

**Primary Outputs:**
- data/step03_cluster_assignments.csv (100 participants with cluster labels)
- data/step03_cluster_centers.csv (K cluster centroids in z-scored space)
- data/step04_cluster_validation.csv (quality metrics: silhouette, DB, Jaccard)
- data/step05_cluster_characterization.txt (interpretive cluster descriptions)
- data/step06_scatter_plot_matrix_data.csv (plot source data for rq_plots)

**Validation Coverage:** 100% (all 7 steps have validation requirements with substance criteria)

**Key Statistical Methods:**
- K-means clustering (K=1-6 with BIC selection)
- Cluster quality validation (silhouette, Davies-Bouldin, bootstrap Jaccard)
- Exploratory cluster characterization (domain-specific patterns)

**Expected Findings (per 1_concept.md hypothesis):**
- Optimal K: 2-4 latent profiles (determined empirically)
- Possible patterns:
  - Global high/average/low memory (uniform across both domains)
  - Domain-selective impairment (poor spatial only OR poor object memory only)
  - Dissociations between What vs Where (dual-process systems)
- Clinical interpretation: Identify participants with domain-specific memory deficits vs global impairment

---

**Next Steps (Workflow):**
1. User reviews and approves this plan (Step 10 user gate)
2. Workflow continues to Step 11: rq_tools reads this plan -> creates 3_tools.yaml
3. Workflow continues to Step 12: rq_analysis reads this plan + 3_tools.yaml -> creates 4_analysis.yaml
4. Workflow continues to Step 14: g_code reads 4_analysis.yaml -> generates step00-step06 Python scripts
5. Step 14: bash execution loop runs all 7 steps with per-step validation
6. Step 16: rq_inspect validates all outputs match this plan's substance criteria
7. Step 20: rq_plots generates scatter plot matrix PNG from step06 plot source CSV
8. Step 22: rq_results creates summary.md with cluster interpretations

---

**Version History:**
- v1.0 (2025-12-02): Initial plan created by rq_planner agent for RQ 5.2.7
  - K-means clustering on 6 domain-specific random effects (intercept + slope per What/Where/When)
  - Comprehensive cluster validation (silhouette, Davies-Bouldin, bootstrap stability)
  - Exploratory analysis with BIC model selection (K=1-6)
  - Option B architecture: plot data preparation in analysis steps, visualization in rq_plots
