#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step06
Step Name: Standardize Outcomes for Parallel LMM
RQ: ch5/5.2.5
Generated: 2025-11-30

PURPOSE:
Standardize all three measurement approaches (Full CTT, Purified CTT, IRT theta)
to z-scores to enable valid AIC comparison per Burnham & Anderson (2002).

CTT scores are proportions [0, 1], while IRT theta scores are on logit scale.
Direct AIC comparison requires IDENTICAL data scales. Z-score standardization
(mean=0, SD=1) puts all measurements on same scale without altering relative
information content.

**CRITICAL: When domain EXCLUDED** - Due to floor effect discovered in RQ 5.2.1
(77% item attrition, 6-9% floor). Only What and Where domains processed.

EXPECTED INPUTS:
  - data/step00_theta_scores.csv
    Columns: ['composite_ID', 'theta_what', 'theta_where']
    Format: IRT theta scores from RQ 5.2.1 (What/Where only)
    Expected rows: ~400 (100 participants x 4 tests)

  - data/step02_ctt_full_scores.csv
    Columns: ['composite_ID', 'UID', 'TEST', 'CTT_full_what', 'CTT_full_where']
    Format: Full CTT scores (all items) from Step 2 (What/Where only)
    Expected rows: ~400

  - data/step03_ctt_purified_scores.csv
    Columns: ['composite_ID', 'UID', 'TEST', 'CTT_purified_what', 'CTT_purified_where']
    Format: Purified CTT scores (retained items only) from Step 3 (What/Where only)
    Expected rows: ~400

  - data/step00_tsvr_mapping.csv
    Columns: ['composite_ID', 'UID', 'TSVR_hours']
    Format: TSVR time variable from RQ 5.1 Step 0
    Expected rows: ~400

EXPECTED OUTPUTS:
  - data/step06_standardized_outcomes.csv
    Columns: ['composite_ID', 'UID', 'TSVR_hours', 'domain', 'z_full_ctt', 'z_purified_ctt', 'z_irt_theta']
    Format: Z-scored outcomes in long format for parallel LMM fitting
    Expected rows: ~800 (400 composite_IDs x 2 domains - What/Where only)

VALIDATION CRITERIA:
  - mean(z) within ±0.01 of 0 for all columns
  - sd(z) within ±0.01 of 1 for all columns
  - Checks all 3 measurement types x 2 domains = 6 combinations

g_code REASONING:
- Approach: Z-score transformation (value - mean) / sd per measurement x domain
- Why this approach: AIC comparison assumes identical data scales (Burnham & Anderson 2002).
  CTT [0,1] vs IRT logit violates this. Z-scores normalize all to mean=0, SD=1.
- Data flow: Load 4 CSVs → merge on composite_ID → reshape to long format →
  group by measurement_type x domain → compute z-scores → validate mean=0, SD=1
- Expected performance: ~10-30 seconds (400 rows x 3 domains = 1200 rows, simple arithmetic)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib (pandas z-score transformation, NOT catalogued tool)
- Validation tool: tools.validation.validate_standardization
- Parameters: tolerance = ±0.01 for mean/SD validation (allows for sampling variation)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_standardization

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.2.5 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step06_standardize_outcomes.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step06_standardized_outcomes.csv
#   CORRECT: data/step03_theta_scores.csv
#   WRONG:   results/standardized_outcomes.csv  (wrong folder + no prefix)
#   WRONG:   data/outcomes.csv                  (missing step prefix)
#   WRONG:   logs/step06_outcomes.csv           (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 6: Standardize Outcomes for Parallel LMM")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: 4 CSVs with theta scores, full CTT, purified CTT, and TSVR
        # Purpose: Merge all measurement approaches for z-score standardization

        log("[LOAD] Loading input data...")

        # Load IRT theta scores (from RQ 5.1 Step 3 Pass 2)
        df_theta = pd.read_csv(RQ_DIR / "data/step00_theta_scores.csv")
        log(f"[LOADED] step00_theta_scores.csv ({len(df_theta)} rows, {len(df_theta.columns)} cols)")

        # Load full CTT scores (from Step 2)
        df_full_ctt = pd.read_csv(RQ_DIR / "data/step02_ctt_full_scores.csv")
        log(f"[LOADED] step02_ctt_full_scores.csv ({len(df_full_ctt)} rows, {len(df_full_ctt.columns)} cols)")

        # Load purified CTT scores (from Step 3)
        df_purified_ctt = pd.read_csv(RQ_DIR / "data/step03_ctt_purified_scores.csv")
        log(f"[LOADED] step03_ctt_purified_scores.csv ({len(df_purified_ctt)} rows, {len(df_purified_ctt.columns)} cols)")

        # Load TSVR mapping (from RQ 5.1 Step 0)
        df_tsvr = pd.read_csv(RQ_DIR / "data/step00_tsvr_mapping.csv")
        log(f"[LOADED] step00_tsvr_mapping.csv ({len(df_tsvr)} rows, {len(df_tsvr.columns)} cols)")

        # =========================================================================
        # STEP 2: Merge Data on composite_ID
        # =========================================================================
        # Tool: pandas merge operations
        # What it does: Combine all 4 DataFrames to create complete dataset
        # Expected output: Single DataFrame with composite_ID + UID + TSVR + all scores

        log("[MERGE] Merging all measurement approaches on composite_ID...")

        # Start with TSVR (includes UID which we need for grouping)
        df_merged = df_tsvr.copy()

        # Merge theta scores (What/Where only - no When)
        df_merged = df_merged.merge(
            df_theta[['composite_ID', 'theta_what', 'theta_where']],
            on='composite_ID',
            how='left'
        )

        # Merge full CTT scores (What/Where only - no When)
        df_merged = df_merged.merge(
            df_full_ctt[['composite_ID', 'CTT_full_what', 'CTT_full_where']],
            on='composite_ID',
            how='left'
        )

        # Merge purified CTT scores (What/Where only - no When)
        df_merged = df_merged.merge(
            df_purified_ctt[['composite_ID', 'CTT_purified_what', 'CTT_purified_where']],
            on='composite_ID',
            how='left'
        )

        log(f"[MERGED] All data merged ({len(df_merged)} rows, {len(df_merged.columns)} cols)")

        # Check for merge failures (NaN values)
        na_counts = df_merged.isna().sum()
        if na_counts.sum() > 0:
            log(f"[WARNING] Merge produced NaN values:")
            for col, count in na_counts[na_counts > 0].items():
                log(f"  - {col}: {count} NaN values")

        # =========================================================================
        # STEP 3: Reshape to Long Format
        # =========================================================================
        # Tool: pandas melt operation
        # What it does: Convert from wide format (1 row per composite_ID) to long format (3 rows per composite_ID, 1 per domain)
        # Expected output: Long DataFrame with columns: composite_ID, UID, TSVR_hours, domain, theta, ctt_full, ctt_purified

        log("[RESHAPE] Converting to long format (2 rows per composite_ID, 1 per domain)...")
        log("[INFO] When domain EXCLUDED per RQ 5.2.1 floor effect")

        # Create list to store long-format data
        rows = []

        for _, row in df_merged.iterrows():
            # Process each domain (What/Where only - When excluded)
            for domain in ['what', 'where']:
                rows.append({
                    'composite_ID': row['composite_ID'],
                    'UID': row['UID'],
                    'TSVR_hours': row['TSVR_hours'],
                    'domain': domain,
                    'theta_raw': row[f'theta_{domain}'],
                    'ctt_full_raw': row[f'CTT_full_{domain}'],
                    'ctt_purified_raw': row[f'CTT_purified_{domain}']
                })

        df_long = pd.DataFrame(rows)
        log(f"[RESHAPED] Long format created ({len(df_long)} rows = {len(df_merged)} composite_IDs x 2 domains)")

        # =========================================================================
        # STEP 4: Compute Z-Scores per Measurement Type x Domain
        # =========================================================================
        # Tool: scipy.stats.zscore or manual (value - mean) / sd
        # What it does: Standardize each measurement approach to mean=0, SD=1 within each domain
        # Expected output: 3 new columns (z_full_ctt, z_purified_ctt, z_irt_theta)

        log("[STANDARDIZE] Computing z-scores per measurement type x domain...")

        # Group by domain and compute z-scores for each measurement type
        # Z-score formula: (value - mean) / sd

        for domain in ['what', 'where']:
            domain_mask = df_long['domain'] == domain

            # IRT theta z-scores
            theta_vals = df_long.loc[domain_mask, 'theta_raw']
            theta_mean = theta_vals.mean()
            theta_std = theta_vals.std()
            df_long.loc[domain_mask, 'z_irt_theta'] = (theta_vals - theta_mean) / theta_std
            log(f"[STANDARDIZED] {domain} - IRT theta: mean={theta_mean:.4f}, SD={theta_std:.4f}")

            # Full CTT z-scores
            full_vals = df_long.loc[domain_mask, 'ctt_full_raw']
            full_mean = full_vals.mean()
            full_std = full_vals.std()
            df_long.loc[domain_mask, 'z_full_ctt'] = (full_vals - full_mean) / full_std
            log(f"[STANDARDIZED] {domain} - Full CTT: mean={full_mean:.4f}, SD={full_std:.4f}")

            # Purified CTT z-scores
            purified_vals = df_long.loc[domain_mask, 'ctt_purified_raw']
            purified_mean = purified_vals.mean()
            purified_std = purified_vals.std()
            df_long.loc[domain_mask, 'z_purified_ctt'] = (purified_vals - purified_mean) / purified_std
            log(f"[STANDARDIZED] {domain} - Purified CTT: mean={purified_mean:.4f}, SD={purified_std:.4f}")

        # =========================================================================
        # STEP 5: Save Standardized Outcomes
        # =========================================================================
        # Output: data/step06_standardized_outcomes.csv
        # Contains: composite_ID, UID, TSVR_hours, domain, z_full_ctt, z_purified_ctt, z_irt_theta
        # Purpose: Input for Step 7 parallel LMM fitting

        log("[SAVE] Saving standardized outcomes...")

        # Select final columns for output
        df_output = df_long[['composite_ID', 'UID', 'TSVR_hours', 'domain', 'z_full_ctt', 'z_purified_ctt', 'z_irt_theta']]

        output_path = RQ_DIR / "data/step06_standardized_outcomes.csv"
        df_output.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(df_output)} rows, {len(df_output.columns)} cols)")

        # =========================================================================
        # STEP 6: Run Validation Tool
        # =========================================================================
        # Tool: tools.validation.validate_standardization
        # Validates: mean ≈ 0, SD ≈ 1 for all z-score columns (tolerance ±0.01)
        # Threshold: Passes only if ALL 9 combinations (3 measurements x 3 domains) meet criteria

        log("[VALIDATION] Running validate_standardization...")

        validation_result = validate_standardization(
            df=df_output,
            column_names=['z_full_ctt', 'z_purified_ctt', 'z_irt_theta'],
            tolerance=0.01
        )

        # Report validation results
        # Expected: valid=True with mean/SD values for each column
        if validation_result['valid']:
            log("[VALIDATION] PASS - All z-scores meet standardization criteria (mean ≈ 0, SD ≈ 1)")
            log(f"[VALIDATION] Mean values: {validation_result['mean_values']}")
            log(f"[VALIDATION] SD values: {validation_result['sd_values']}")
        else:
            log(f"[VALIDATION] FAIL - {validation_result['message']}")
            log(f"[VALIDATION] Mean values: {validation_result['mean_values']}")
            log(f"[VALIDATION] SD values: {validation_result['sd_values']}")
            raise ValueError(f"Standardization validation failed: {validation_result['message']}")

        log("[SUCCESS] Step 6 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
