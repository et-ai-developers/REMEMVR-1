#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step03
Step Name: Compute Purified CTT Scores
RQ: ch5/5.2.5
Generated: 2025-11-30

PURPOSE:
Calculate Classical Test Theory (CTT) scores using ONLY IRT-retained items
(purified item set) per UID x Test x Domain. This step filters to items with
retained=True from Step 1 mapping, then computes mean dichotomized responses
per domain (What/Where/When). Compares to full CTT (Step 2) to test whether
IRT-informed item purification improves CTT-IRT convergence.

EXPECTED INPUTS:
  - data/step00_raw_scores.csv
    Columns: ['composite_ID', 'UID', 'TEST', 'TQ_*'] (105 TQ_* item columns)
    Format: Raw dichotomized item responses (0/1) for CTT computation
    Expected rows: ~400 (100 participants x 4 tests)

  - data/step01_item_mapping.csv
    Columns: ['item_name', 'domain', 'retained']
    Format: Item-to-domain mapping with retention status from RQ 5.1 purification
    Expected rows: ~50 items (all TQ_* items from dfData)

EXPECTED OUTPUTS:
  - data/step03_ctt_purified_scores.csv
    Columns: ['composite_ID', 'UID', 'TEST', 'CTT_purified_what', 'CTT_purified_where', 'CTT_purified_when']
    Format: Purified CTT scores (mean of retained items only) per domain
    Expected rows: ~400 (100 participants x 4 tests)

VALIDATION CRITERIA:
  - All CTT_purified_* scores in [0, 1] range (proportion correct)
  - No negative values (proportion cannot be negative)
  - No values > 1.0 (maximum proportion)
  - Correlation with full CTT expected r > 0.95 (high overlap)

g_code REASONING:
- Approach: Filter item_mapping to retained=True, extract retained item names
  per domain, select those columns from raw_scores, compute mean per composite_ID.
  This creates CTT scores using ONLY the high-quality items identified by
  RQ 5.1 IRT purification (a > 0.4, |b| < 3.0).

- Why this approach: Tests central hypothesis that IRT purification removes
  measurement noise (poorly discriminating items) rather than signal. If purified
  CTT correlates more strongly with IRT theta than full CTT, validates that
  removed items contributed error not construct variance.

- Data flow: step01_item_mapping (identifies retained items per domain) +
  step00_raw_scores (dichotomized responses) -> filter to retained items ->
  compute domain means -> step03_ctt_purified_scores (purified CTT).

- Expected performance: ~2 seconds (simple pandas filtering + groupby mean)

IMPLEMENTATION NOTES:
- Analysis tool: STDLIB (pandas filtering and mean computation, NOT catalogued tool)
- Validation tool: tools.validation.validate_numeric_range
- Parameters:
    - scoring_method: Mean of dichotomized responses (0/1) within domain
    - score_range: [0, 1] (proportion correct)
    - item_filter: retained == True (ONLY IRT-retained items)
- Item counts: Expected ~38 retained items from ~50 total (~75% retention rate)
  What: ~14 retained, Where: ~12 retained, When: ~12 retained
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_numeric_range

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.2.5 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step03_compute_purified_ctt.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step03_ctt_purified_scores.csv
#   CORRECT: logs/step03_compute_purified_ctt.log
#   WRONG:   data/ctt_purified_scores.csv             (missing step prefix)
#   WRONG:   results/step03_ctt_purified_scores.csv   (CSV in results/ folder)
#   WRONG:   logs/step03_purified_items.csv           (CSV in logs/ folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 03: Compute Purified CTT Scores")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Raw dichotomized item responses from dfData.csv (Step 0)
        #           and item mapping with retention status from RQ 5.1 (Step 1)
        # Purpose: Identify which items to include in purified CTT computation

        log("[LOAD] Loading input data...")

        # Load raw scores with all TQ_* item columns
        # Contains dichotomized responses (0/1) for ~105 VR items
        df_raw = pd.read_csv(RQ_DIR / "data/step00_raw_scores.csv")
        log(f"[LOADED] step00_raw_scores.csv ({len(df_raw)} rows, {len(df_raw.columns)} cols)")

        # Load item mapping with retention status
        # Identifies which items were retained by RQ 5.1 IRT purification (a > 0.4, |b| < 3.0)
        df_mapping = pd.read_csv(RQ_DIR / "data/step01_item_mapping.csv")
        log(f"[LOADED] step01_item_mapping.csv ({len(df_mapping)} rows)")

        # Report retention counts per domain
        retention_counts = df_mapping.groupby('domain')['retained'].value_counts().unstack(fill_value=0)
        log(f"[INFO] Item retention by domain:")
        log(f"  What:  {retention_counts.loc['what', True] if 'what' in retention_counts.index else 0} retained, {retention_counts.loc['what', False] if 'what' in retention_counts.index else 0} removed")
        log(f"  Where: {retention_counts.loc['where', True] if 'where' in retention_counts.index else 0} retained, {retention_counts.loc['where', False] if 'where' in retention_counts.index else 0} removed")
        log(f"  When:  {retention_counts.loc['when', True] if 'when' in retention_counts.index else 0} retained, {retention_counts.loc['when', False] if 'when' in retention_counts.index else 0} removed")

        total_retained = df_mapping['retained'].sum()
        total_items = len(df_mapping)
        retention_rate = total_retained / total_items
        log(f"[INFO] Overall retention: {total_retained}/{total_items} items ({retention_rate:.1%})")

        # =========================================================================
        # STEP 2: Compute Purified CTT Scores
        # =========================================================================
        # For each domain: Filter to ONLY retained items, compute mean per composite_ID
        # This creates CTT scores using high-quality items only (IRT-informed selection)

        log("[ANALYSIS] Computing purified CTT scores per domain...")

        # Initialize output DataFrame with composite_ID, UID, TEST
        df_output = df_raw[['composite_ID', 'UID', 'TEST']].copy()

        # Process each domain
        for domain in ['what', 'where', 'when']:
            # Get retained items for this domain
            retained_items = df_mapping[
                (df_mapping['domain'] == domain) &
                (df_mapping['retained'] == True)
            ]['item_name'].tolist()

            log(f"[INFO] Domain '{domain}': {len(retained_items)} retained items")

            # Select only retained item columns from raw data
            # Handle case where item might not be in raw data columns
            available_items = [item for item in retained_items if item in df_raw.columns]

            if len(available_items) < len(retained_items):
                missing_items = set(retained_items) - set(available_items)
                log(f"[WARNING] Domain '{domain}': {len(missing_items)} retained items not found in raw data: {missing_items}")

            # Compute mean of retained items per composite_ID
            # Mean of dichotomized responses (0/1) = proportion correct
            df_output[f'CTT_purified_{domain}'] = df_raw[available_items].mean(axis=1)

            # Report score statistics
            mean_score = df_output[f'CTT_purified_{domain}'].mean()
            sd_score = df_output[f'CTT_purified_{domain}'].std()
            min_score = df_output[f'CTT_purified_{domain}'].min()
            max_score = df_output[f'CTT_purified_{domain}'].max()

            log(f"[INFO] CTT_purified_{domain}: mean={mean_score:.3f}, SD={sd_score:.3f}, range=[{min_score:.3f}, {max_score:.3f}]")

        log("[DONE] Purified CTT computation complete")

        # =========================================================================
        # STEP 3: Save Purified CTT Scores
        # =========================================================================
        # Output will be used by Step 5 (correlation analysis) and Step 7 (parallel LMM)

        log("[SAVE] Saving purified CTT scores...")
        output_path = RQ_DIR / "data/step03_ctt_purified_scores.csv"
        df_output.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(df_output)} rows, {len(df_output.columns)} cols)")

        # Report output structure
        log(f"[INFO] Output columns: {df_output.columns.tolist()}")

        # =========================================================================
        # STEP 4: Validation
        # =========================================================================
        # Validate purified CTT scores are in valid proportion range [0, 1]
        # Tests: No negative values, no values > 1.0, scores are proportions

        log("[VALIDATION] Running validate_numeric_range for purified CTT scores...")

        validation_passed = True

        for domain in ['what', 'where', 'when']:
            col_name = f'CTT_purified_{domain}'

            result = validate_numeric_range(
                data=df_output[col_name],
                min_val=0.0,
                max_val=1.0,
                column_name=col_name
            )

            if result['valid']:
                log(f"[VALIDATION] {col_name}: PASS - {result['message']}")
            else:
                log(f"[VALIDATION] {col_name}: FAIL - {result['message']}")
                log(f"[VALIDATION] Out of range count: {result['out_of_range_count']}")
                if result['violations']:
                    log(f"[VALIDATION] Sample violations: {result['violations'][:5]}")
                validation_passed = False

        if not validation_passed:
            log("[ERROR] Validation failed - purified CTT scores out of valid range [0, 1]")
            sys.exit(1)

        log("[VALIDATION] All purified CTT scores validated successfully")

        log("[SUCCESS] Step 03 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
