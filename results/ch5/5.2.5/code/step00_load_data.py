#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step00
Step Name: Load Data Sources
RQ: ch5/5.2.5
Generated: 2025-11-30
Updated: 2025-12-03 (When domain excluded per RQ 5.2.1 floor effect)

PURPOSE:
Load IRT item parameters, theta scores, TSVR mapping from RQ 5.2.1 and raw
dichotomized scores from dfData.csv for CTT computation. Creates composite_ID
in raw data (UID_test format) and copies all source files to local data/ folder
for reference.

**CRITICAL: When domain EXCLUDED** - Due to floor effect discovered in RQ 5.2.1
(77% item attrition, 6-9% floor). This step filters out When items/columns.

EXPECTED INPUTS:
  - results/ch5/5.2.1/data/step02_purified_items.csv
    Columns: ['item_name', 'factor', 'a', 'b']
    Format: IRT purified items from RQ 5.2.1 Step 2 (retained items after purification)
    Expected rows: ~38 items (but we filter to What/Where only)

  - results/ch5/5.2.1/data/step03_theta_scores.csv
    Columns: ['composite_ID', 'theta_what', 'theta_where', 'theta_when']
    Format: IRT theta scores from RQ 5.2.1 Step 3 (Pass 2 calibration)
    Expected rows: ~400 (100 participants x 4 tests)
    NOTE: theta_when column will be DROPPED

  - results/ch5/5.2.1/data/step00_tsvr_mapping.csv
    Columns: ['composite_ID', 'UID', 'test', 'TSVR_hours']
    Format: TSVR time variable from RQ 5.2.1 Step 0 (actual hours since encoding)
    Expected rows: ~400

  - data/cache/dfData.csv
    Columns: ['UID', 'TEST', 'TQ_*']
    Format: Raw dichotomized item responses for CTT computation (TQ_* columns)
    Expected rows: ~400

EXPECTED OUTPUTS:
  - data/step00_irt_purified_items.csv
    Columns: ['item_name', 'factor', 'a', 'b']
    Format: Purified items for What/Where domains only (When excluded)
    Expected rows: ~34 items (What + Where only)

  - data/step00_theta_scores.csv
    Columns: ['composite_ID', 'theta_what', 'theta_where']
    Format: Theta scores for What/Where only (theta_when DROPPED)
    Expected rows: ~400

  - data/step00_tsvr_mapping.csv
    Columns: ['composite_ID', 'UID', 'test', 'TSVR_hours']
    Format: Local copy of RQ 5.2.1 TSVR mapping for reference
    Expected rows: ~400

  - data/step00_raw_scores.csv
    Columns: ['composite_ID', 'UID', 'TEST', 'TQ_*']
    Format: dfData with composite_ID added (UID_test format)
    Expected rows: ~400

VALIDATION CRITERIA:
  - All 4 output files exist
  - Each file size >= 100 bytes (not empty)
  - Validate columns using validate_data_columns for each file
  - theta_when column NOT present in theta_scores output
  - No 'when' factor items in purified_items output

g_code REASONING:
- Approach: Load prerequisite data from RQ 5.2.1 (purified items, theta scores,
  TSVR mapping) and raw item responses from dfData.csv. Verify RQ 5.2.1 completion
  by checking status.yaml. Create composite_ID in dfData for cross-file merging.
  **Filter out When domain** at this step so downstream steps work with What/Where only.

- Why this approach: RQ 5.2.5 is a methodological convergence analysis comparing
  CTT vs IRT. When domain excluded due to floor effect (77% item attrition).
  Filtering at step00 ensures all downstream steps work with clean What/Where data.

- Data flow: RQ 5.2.1 outputs -> Filter When -> Local data/ copies -> CTT (Steps 2-3)
  -> Correlation analysis (Step 5) -> LMM comparison (Step 7)

- Expected performance: <5 seconds (file I/O operations only, no computation)

IMPLEMENTATION NOTES:
- Analysis tool: STDLIB (pandas read_csv, to_csv, string operations)
- Validation tool: tools.validation.check_file_exists + validate_data_columns
- Parameters: composite_ID format = UID + '_' + TEST (e.g., A010_1)
- Cross-RQ dependency: Checks results/ch5/5.2.1/status.yaml for step03_theta_scores = success
- WHEN EXCLUSION: Filter purified_items to factor != 'when', drop theta_when column
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import traceback
import yaml

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tools
from tools.validation import check_file_exists, validate_data_columns

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.2.5
LOG_FILE = RQ_DIR / "logs" / "step00_load_data.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step00_raw_scores.csv
#   CORRECT: data/step00_theta_scores.csv
#   WRONG:   results/raw_scores.csv  (wrong folder + no prefix)
#   WRONG:   data/raw_scores.csv     (missing step prefix)
#   WRONG:   logs/step00_raw_scores.csv (CSV in logs folder)

# =============================================================================
# Input/Output Paths
# =============================================================================

# Cross-RQ dependencies (RQ 5.2.1 outputs)
RQ51_DIR = PROJECT_ROOT / "results" / "ch5" / "5.2.1"
RQ51_STATUS = RQ51_DIR / "status.yaml"
RQ51_PURIFIED_ITEMS = RQ51_DIR / "data" / "step02_purified_items.csv"
RQ51_THETA_SCORES = RQ51_DIR / "data" / "step03_theta_scores.csv"
RQ51_TSVR_MAPPING = RQ51_DIR / "data" / "step00_tsvr_mapping.csv"

# Raw data source
DFDATA_PATH = PROJECT_ROOT / "data" / "cache" / "dfData.csv"

# Local output paths (data/ folder)
OUTPUT_PURIFIED_ITEMS = RQ_DIR / "data" / "step00_irt_purified_items.csv"
OUTPUT_THETA_SCORES = RQ_DIR / "data" / "step00_theta_scores.csv"
OUTPUT_TSVR_MAPPING = RQ_DIR / "data" / "step00_tsvr_mapping.csv"
OUTPUT_RAW_SCORES = RQ_DIR / "data" / "step00_raw_scores.csv"

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 0: Load Data Sources")

        # =========================================================================
        # STEP 1: Check RQ 5.1 Completion Status
        # =========================================================================
        # Expected: RQ 5.1 Step 3 (theta scores) completed successfully
        # Purpose: Ensure prerequisite IRT analysis finished before using outputs

        log("[CHECK] Verifying RQ 5.1 completion status...")

        if not RQ51_STATUS.exists():
            raise FileNotFoundError(
                f"RQ 5.1 status.yaml not found at {RQ51_STATUS}. "
                "RQ 5.1 must be completed before running RQ 5.12."
            )

        with open(RQ51_STATUS, 'r', encoding='utf-8') as f:
            status = yaml.safe_load(f)

        # Check step03_irt_calibration_pass2 completion (RQ 5.1 uses nested analysis_steps structure)
        if 'analysis_steps' not in status or 'step03_irt_calibration_pass2' not in status['analysis_steps']:
            raise ValueError(
                "RQ 5.1 status.yaml missing analysis_steps.step03_irt_calibration_pass2. "
                "RQ 5.1 must complete Step 3 before RQ 5.12 can proceed."
            )

        step03_data = status['analysis_steps']['step03_irt_calibration_pass2']
        step03_status = step03_data.get('status', 'unknown')
        if step03_status != 'success':
            raise ValueError(
                f"RQ 5.1 step03_irt_calibration_pass2 status = {step03_status}, expected 'success'. "
                "RQ 5.1 must complete Step 3 successfully before RQ 5.12 can proceed."
            )

        log(f"[PASS] RQ 5.1 Step 3 completed successfully (status = {step03_status})")

        # =========================================================================
        # STEP 2: Load RQ 5.1 Outputs
        # =========================================================================
        # Expected: Purified items (~38 items), theta scores (~400 rows), TSVR mapping (~400 rows)
        # Purpose: IRT outputs for convergence analysis with CTT

        log("[LOAD] Loading RQ 5.1 outputs...")

        # Load purified items
        log(f"[LOAD] Reading {RQ51_PURIFIED_ITEMS}...")
        df_purified_items = pd.read_csv(RQ51_PURIFIED_ITEMS, encoding='utf-8')
        log(f"[LOADED] Purified items (raw): {len(df_purified_items)} items, {len(df_purified_items.columns)} columns")
        log(f"[INFO] Columns: {list(df_purified_items.columns)}")

        # FILTER: Exclude When domain items (floor effect in RQ 5.2.1)
        items_before = len(df_purified_items)
        df_purified_items = df_purified_items[df_purified_items['factor'] != 'when'].copy()
        items_after = len(df_purified_items)
        log(f"[FILTER] Excluded When domain: {items_before} -> {items_after} items ({items_before - items_after} removed)")

        # Load theta scores
        log(f"[LOAD] Reading {RQ51_THETA_SCORES}...")
        df_theta = pd.read_csv(RQ51_THETA_SCORES, encoding='utf-8')
        log(f"[LOADED] Theta scores (raw): {len(df_theta)} rows, {len(df_theta.columns)} columns")
        log(f"[INFO] Columns: {list(df_theta.columns)}")

        # FILTER: Drop theta_when column (When domain excluded)
        if 'theta_when' in df_theta.columns:
            df_theta = df_theta.drop(columns=['theta_when'])
            log(f"[FILTER] Dropped theta_when column (When domain excluded)")
        log(f"[INFO] Theta columns after filter: {list(df_theta.columns)}")

        # Load TSVR mapping
        log(f"[LOAD] Reading {RQ51_TSVR_MAPPING}...")
        df_tsvr = pd.read_csv(RQ51_TSVR_MAPPING, encoding='utf-8')
        log(f"[LOADED] TSVR mapping: {len(df_tsvr)} rows, {len(df_tsvr.columns)} columns")
        log(f"[INFO] Columns: {list(df_tsvr.columns)}")

        # =========================================================================
        # STEP 3: Load Raw Scores from dfData.csv
        # =========================================================================
        # Expected: Raw dichotomized item responses (0/1) for ~400 composite_IDs
        # Purpose: CTT score computation requires raw item-level responses

        log("[LOAD] Loading raw dichotomized scores from dfData.csv...")
        log(f"[LOAD] Reading {DFDATA_PATH}...")
        df_raw = pd.read_csv(DFDATA_PATH, encoding='utf-8')
        log(f"[LOADED] Raw scores: {len(df_raw)} rows, {len(df_raw.columns)} columns")

        # Count TQ_* columns (item responses)
        tq_columns = [col for col in df_raw.columns if col.startswith('TQ_')]
        log(f"[INFO] Found {len(tq_columns)} TQ_* item columns")

        # Verify required columns present
        required_cols = ['UID', 'TEST']
        missing_cols = [col for col in required_cols if col not in df_raw.columns]
        if missing_cols:
            raise ValueError(f"dfData.csv missing required columns: {missing_cols}")
        log(f"[PASS] Required columns present: {required_cols}")

        # =========================================================================
        # STEP 4: Create composite_ID in Raw Data
        # =========================================================================
        # Expected: composite_ID = UID + '_' + TEST (e.g., A010_1)
        # Purpose: Enable cross-file merging with RQ 5.1 theta scores

        log("[TRANSFORM] Creating composite_ID in raw data...")
        log("[INFO] Format: UID + '_' + TEST.astype(str) (e.g., A010_1)")

        df_raw['composite_ID'] = df_raw['UID'] + '_' + df_raw['TEST'].astype(str)
        log(f"[CREATED] composite_ID for {len(df_raw)} rows")
        log(f"[INFO] Example composite_IDs: {df_raw['composite_ID'].head(3).tolist()}")

        # =========================================================================
        # STEP 5: Verify Expected Columns
        # =========================================================================
        # Expected: All loaded DataFrames have columns specified in 4_analysis.yaml
        # Purpose: Catch schema mismatches early before downstream steps fail

        log("[VALIDATE] Verifying expected columns...")

        # Validate purified items columns
        expected_purified = ['item_name', 'factor', 'a', 'b']
        result_purified = validate_data_columns(df_purified_items, expected_purified)
        if not result_purified['valid']:
            raise ValueError(
                f"Purified items column mismatch. Missing: {result_purified['missing_columns']}"
            )
        log(f"[PASS] Purified items columns: {expected_purified}")

        # Validate theta scores columns (theta_when already dropped)
        expected_theta = ['composite_ID', 'theta_what', 'theta_where']
        result_theta = validate_data_columns(df_theta, expected_theta)
        if not result_theta['valid']:
            raise ValueError(
                f"Theta scores column mismatch. Missing: {result_theta['missing_columns']}"
            )
        log(f"[PASS] Theta scores columns: {expected_theta}")

        # Validate TSVR mapping columns
        expected_tsvr = ['composite_ID', 'UID', 'test', 'TSVR_hours']
        result_tsvr = validate_data_columns(df_tsvr, expected_tsvr)
        if not result_tsvr['valid']:
            raise ValueError(
                f"TSVR mapping column mismatch. Missing: {result_tsvr['missing_columns']}"
            )
        log(f"[PASS] TSVR mapping columns: {expected_tsvr}")

        # Validate raw scores columns (now includes composite_ID)
        expected_raw = ['composite_ID', 'UID', 'TEST'] + tq_columns[:3]  # Check first 3 TQ_* as sample
        result_raw = validate_data_columns(df_raw, expected_raw)
        if not result_raw['valid']:
            raise ValueError(
                f"Raw scores column mismatch. Missing: {result_raw['missing_columns']}"
            )
        log(f"[PASS] Raw scores columns include: composite_ID, UID, TEST, + {len(tq_columns)} TQ_* items")

        # =========================================================================
        # STEP 6: Copy Files to Local data/ Folder
        # =========================================================================
        # Expected: All 4 output files written to results/ch5/5.2.5/data/
        # Purpose: Self-contained analysis with local copies (prevents cross-RQ file issues)

        log("[SAVE] Copying files to local data/ folder...")

        # Save purified items
        log(f"[SAVE] Writing {OUTPUT_PURIFIED_ITEMS}...")
        df_purified_items.to_csv(OUTPUT_PURIFIED_ITEMS, index=False, encoding='utf-8')
        log(f"[SAVED] {OUTPUT_PURIFIED_ITEMS.name} ({len(df_purified_items)} rows)")

        # Save theta scores
        log(f"[SAVE] Writing {OUTPUT_THETA_SCORES}...")
        df_theta.to_csv(OUTPUT_THETA_SCORES, index=False, encoding='utf-8')
        log(f"[SAVED] {OUTPUT_THETA_SCORES.name} ({len(df_theta)} rows)")

        # Save TSVR mapping
        log(f"[SAVE] Writing {OUTPUT_TSVR_MAPPING}...")
        df_tsvr.to_csv(OUTPUT_TSVR_MAPPING, index=False, encoding='utf-8')
        log(f"[SAVED] {OUTPUT_TSVR_MAPPING.name} ({len(df_tsvr)} rows)")

        # Save raw scores with composite_ID
        log(f"[SAVE] Writing {OUTPUT_RAW_SCORES}...")
        df_raw.to_csv(OUTPUT_RAW_SCORES, index=False, encoding='utf-8')
        log(f"[SAVED] {OUTPUT_RAW_SCORES.name} ({len(df_raw)} rows)")

        # =========================================================================
        # STEP 7: Run Validation Tool
        # =========================================================================
        # Tool: check_file_exists
        # Validates: All 4 output files exist with minimum size >= 100 bytes
        # Threshold: 100 bytes ensures files are not empty (header + at least 1 data row)

        log("[VALIDATION] Running check_file_exists for all 4 output files...")

        output_files = [
            OUTPUT_PURIFIED_ITEMS,
            OUTPUT_THETA_SCORES,
            OUTPUT_TSVR_MAPPING,
            OUTPUT_RAW_SCORES
        ]

        validation_results = []
        for file_path in output_files:
            result = check_file_exists(file_path, min_size_bytes=100)
            validation_results.append(result)

            if result['valid']:
                log(f"[VALIDATION] {file_path.name}: PASS (size = {result['size_bytes']} bytes)")
            else:
                log(f"[VALIDATION] {file_path.name}: FAIL - {result['message']}")

        # Check if all validations passed
        all_valid = all(r['valid'] for r in validation_results)

        if not all_valid:
            failed_files = [f.name for f, r in zip(output_files, validation_results) if not r['valid']]
            raise FileNotFoundError(
                f"Validation failed for {len(failed_files)} files: {failed_files}. "
                "Step 0 data loading incomplete."
            )

        log(f"[VALIDATION] All {len(output_files)} output files validated successfully")

        log("[SUCCESS] Step 0 complete")
        log(f"[SUMMARY] Loaded 4 data sources from RQ 5.1 + dfData.csv")
        log(f"[SUMMARY] Created composite_ID in raw scores (UID_TEST format)")
        log(f"[SUMMARY] Saved 4 local copies to data/ folder")
        log(f"[SUMMARY] Files ready for CTT computation (Steps 1-3)")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
