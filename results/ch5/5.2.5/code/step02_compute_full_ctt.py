#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step02
Step Name: Compute Full CTT Scores
RQ: ch5/5.2.5
Generated: 2025-11-30

PURPOSE:
Calculate Classical Test Theory (CTT) scores using ALL items (full item set)
per domain. CTT scores are computed as the mean of dichotomized responses (0/1)
within each domain (What, Where, When), providing a baseline measurement
approach for comparison with IRT theta scores and purified CTT scores.

EXPECTED INPUTS:
  - data/step00_raw_scores.csv
    Columns: ['composite_ID', 'UID', 'TEST', 'TQ_*'] (105 TQ_* item columns)
    Format: Raw dichotomized item responses (0/1) from dfData.csv
    Expected rows: ~400 (100 participants x 4 tests)

  - data/step01_item_mapping.csv
    Columns: ['item_name', 'domain', 'retained']
    Format: Item-to-domain mapping with retention status from Step 1
    Expected rows: ~50 items (all VR items)

EXPECTED OUTPUTS:
  - data/step02_ctt_full_scores.csv
    Columns: ['composite_ID', 'UID', 'TEST', 'CTT_full_what', 'CTT_full_where', 'CTT_full_when']
    Format: CTT scores (proportion correct [0,1]) per domain using ALL items
    Expected rows: ~400 (100 participants x 4 tests)

VALIDATION CRITERIA:
  - All CTT_full_* scores in [0, 1] range (proportion cannot be negative or > 1)
  - No negative values
  - No values > 1.0
  - NaN acceptable ONLY if participant missing all items for domain (rare)

g_code REASONING:
- Approach: Compute domain-specific CTT scores by averaging dichotomized item
  responses (0/1) within each domain. Uses ALL items (not filtered by retention
  status) to establish baseline measurement quality before IRT purification.

- Why this approach: Classical Test Theory provides interpretable proportion-correct
  scores that serve as methodological baseline. Full item set (pre-purification)
  allows quantifying the value added by IRT-based item purification in Step 3.

- Data flow:
  1. Load raw item responses (step00_raw_scores.csv with 105 TQ_* columns)
  2. Load item-to-domain mapping (step01_item_mapping.csv)
  3. For each domain (what/where/when):
     - Select ALL items matching domain tag pattern (regardless of retained status)
     - Group by composite_ID (UID_TEST format)
     - Compute mean of 0/1 responses -> CTT_full_{domain} score
  4. Merge domain scores into single wide-format DataFrame
  5. Add UID and TEST columns extracted from composite_ID

- Expected performance: <10 seconds (simple pandas aggregation)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib (pandas operations, NOT catalogued tool)
- Validation tool: tools.validation.validate_numeric_range
- Domain tag patterns:
  - What: TQ_*-N-* (nominal content)
  - Where: TQ_*-U-* + TQ_*-D-* (up/down spatial)
  - When: TQ_*-O-* (temporal order)
- CTT scoring: Mean of dichotomized responses within domain (proportion correct)
- Score range: [0, 1] where 0 = all incorrect, 1 = all correct
- Missing data: NaN if participant has no valid responses for entire domain (rare)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_numeric_range

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/chX/rqY (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step02_compute_full_ctt.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step02_ctt_full_scores.csv
#   CORRECT: logs/step02_compute_full_ctt.log
#   WRONG:   results/ctt_full_scores.csv     (wrong folder + no prefix)
#   WRONG:   data/ctt_scores.csv             (missing step prefix)
#   WRONG:   logs/step02_scores.csv          (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 2: Compute Full CTT Scores")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Raw item responses (step00) + item-to-domain mapping (step01)
        # Purpose: Compute CTT scores using ALL items (full item set) per domain

        log("[LOAD] Loading input data...")

        # Load raw scores with dichotomized item responses
        df_raw = pd.read_csv(RQ_DIR / "data" / "step00_raw_scores.csv")
        log(f"[LOADED] step00_raw_scores.csv ({len(df_raw)} rows, {len(df_raw.columns)} cols)")

        # Load item-to-domain mapping
        df_mapping = pd.read_csv(RQ_DIR / "data" / "step01_item_mapping.csv")
        log(f"[LOADED] step01_item_mapping.csv ({len(df_mapping)} items)")

        # Verify required columns
        required_raw_cols = ['composite_ID', 'UID', 'TEST']
        missing_raw = [c for c in required_raw_cols if c not in df_raw.columns]
        if missing_raw:
            raise ValueError(f"Missing required columns in step00_raw_scores.csv: {missing_raw}")

        required_mapping_cols = ['item_name', 'domain']
        missing_mapping = [c for c in required_mapping_cols if c not in df_mapping.columns]
        if missing_mapping:
            raise ValueError(f"Missing required columns in step01_item_mapping.csv: {missing_mapping}")

        # Extract TQ_* item columns
        tq_cols = [c for c in df_raw.columns if c.startswith('TQ_')]
        log(f"[INFO] Found {len(tq_cols)} TQ_* item columns in raw data")

        # =========================================================================
        # STEP 2: Compute CTT Scores for Each Domain
        # =========================================================================
        # Tool: stdlib (pandas mean aggregation)
        # What it does: For each domain, compute mean of dichotomized responses (0/1)
        # Expected output: CTT_full_{domain} scores in [0, 1] range

        log("[ANALYSIS] Computing full CTT scores using ALL items per domain...")

        # Initialize results dictionary
        ctt_scores = {'composite_ID': df_raw['composite_ID'].values}

        # Process each domain
        domains = ['what', 'where', 'when']

        for domain in domains:
            log(f"[COMPUTE] Processing domain: {domain}")

            # Get items for this domain (ALL items, not filtered by retained status)
            domain_items = df_mapping[df_mapping['domain'] == domain]['item_name'].tolist()
            log(f"  - Domain '{domain}' has {len(domain_items)} items (full item set)")

            # Verify all domain items exist in raw data
            missing_items = [item for item in domain_items if item not in df_raw.columns]
            if missing_items:
                log(f"  [WARNING] {len(missing_items)} items from mapping not found in raw data: {missing_items[:5]}...")
                # Filter to existing items only
                domain_items = [item for item in domain_items if item in df_raw.columns]
                log(f"  - Using {len(domain_items)} items that exist in raw data")

            if len(domain_items) == 0:
                raise ValueError(f"No items found for domain '{domain}' in raw data")

            # Compute mean across domain items (proportion correct)
            # axis=1 computes row-wise mean (across items for each participant)
            df_raw[f'CTT_full_{domain}'] = df_raw[domain_items].mean(axis=1)

            # Count non-NaN values
            non_nan_count = df_raw[f'CTT_full_{domain}'].notna().sum()
            log(f"  - Computed CTT_full_{domain}: {non_nan_count}/{len(df_raw)} non-NaN scores")

            # Report score range
            score_min = df_raw[f'CTT_full_{domain}'].min()
            score_max = df_raw[f'CTT_full_{domain}'].max()
            score_mean = df_raw[f'CTT_full_{domain}'].mean()
            log(f"  - Score range: [{score_min:.3f}, {score_max:.3f}], mean = {score_mean:.3f}")

        log("[DONE] All domain CTT scores computed")

        # =========================================================================
        # STEP 3: Save Analysis Outputs
        # =========================================================================
        # These outputs will be used by: Step 5 (correlation analysis), Step 6 (standardization)

        log("[SAVE] Saving CTT full scores...")

        # Select output columns
        output_cols = ['composite_ID', 'UID', 'TEST', 'CTT_full_what', 'CTT_full_where', 'CTT_full_when']
        df_output = df_raw[output_cols].copy()

        # Save to CSV
        output_path = RQ_DIR / "data" / "step02_ctt_full_scores.csv"
        df_output.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] step02_ctt_full_scores.csv ({len(df_output)} rows, {len(df_output.columns)} cols)")

        # =========================================================================
        # STEP 4: Run Validation Tool
        # =========================================================================
        # Tool: tools.validation.validate_numeric_range
        # Validates: CTT scores in [0, 1] range (proportion correct)
        # Threshold: min=0.0, max=1.0 (no negative values, no values > 1)

        log("[VALIDATION] Running validate_numeric_range on CTT scores...")

        validation_passed = True

        for domain in domains:
            col_name = f'CTT_full_{domain}'

            # Run validation
            result = validate_numeric_range(
                data=df_output[col_name],
                min_val=0.0,
                max_val=1.0,
                column_name=col_name
            )

            # Check validation result
            if result['valid']:
                log(f"[VALIDATION] {col_name}: PASS (all values in [0, 1])")
            else:
                log(f"[VALIDATION] {col_name}: FAIL - {result['message']}")
                log(f"  - Out of range count: {result['out_of_range_count']}")
                if result['violations']:
                    log(f"  - First violations: {result['violations'][:5]}")
                validation_passed = False

        if not validation_passed:
            raise ValueError("Validation failed: CTT scores out of valid range [0, 1]")

        log("[VALIDATION] All CTT scores validated successfully")
        log("[SUCCESS] Step 2 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
