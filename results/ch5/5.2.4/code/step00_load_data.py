#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step00
Step Name: Load Data from RQ 5.1 and Master Dataset
RQ: results/ch5/rq11
Generated: 2025-11-29

PURPOSE:
Load IRT theta scores from RQ 5.1 and extract raw VR item data for CTT computation.
This step ensures both IRT and CTT use the SAME purified item set for fair comparison.

EXPECTED INPUTS:
  - results/ch5/5.2.1/data/step03_theta_scores.csv
    Columns: ['composite_ID', 'theta_what', 'theta_where', 'theta_when']
    Format: IRT theta scores from RQ 5.1 Pass 2 calibration
    Expected rows: 400 (100 participants x 4 test sessions)

  - results/ch5/5.2.1/data/step00_tsvr_mapping.csv
    Columns: ['composite_ID', 'UID', 'test', 'TSVR_hours']
    Format: Time since viewing retention interval in hours
    Expected rows: 400

  - results/ch5/5.2.1/data/step02_purified_items.csv
    Columns: ['item_name', 'factor', 'a', 'b']
    Format: Purified item list from RQ 5.1 (40-50% retention expected)
    Expected rows: 40-60 items

  - data/cache/dfData.csv
    Columns: ['UID', 'TEST', 'item_columns (variable)']
    Format: Raw master dataset with all VR item responses
    Expected rows: 400

EXPECTED OUTPUTS:
  - data/step00_irt_theta_loaded.csv
    Columns: ['composite_ID', 'theta_what', 'theta_where', 'theta_when']
    Format: IRT theta scores (local copy for RQ 5.11)
    Expected rows: 400

  - data/step00_tsvr_loaded.csv
    Columns: ['composite_ID', 'UID', 'test', 'TSVR_hours']
    Format: TSVR time variable (local copy for RQ 5.11)
    Expected rows: 400

  - data/step00_purified_items.csv
    Columns: ['item_name', 'factor', 'a', 'b']
    Format: Purified item list (local copy for RQ 5.11)
    Expected rows: 40-60

  - data/step00_raw_data_filtered.csv
    Columns: ['UID', 'TEST', 'item_columns (40-60 purified items)']
    Format: Raw data filtered to contain ONLY purified items
    Expected rows: 400

VALIDATION CRITERIA:
  - All 400 composite_IDs present (no data loss during loading)
  - Theta values in [-3, 3] (typical IRT ability range)
  - TSVR_hours in [0, 300] (hours since encoding, allowing scheduling delays)
  - Purified items count 40-60 (40-50% retention expected from RQ 5.1)
  - Raw data filtered to match purified items exactly (same item set for fair comparison)

g_code REASONING:
- Approach: Load IRT theta scores from RQ 5.1, load purified items list, filter raw data
  to retain ONLY items in purified list (ensures IRT-CTT comparison uses same items)
- Why this approach: CTT requires raw item responses, but must use same item set as IRT
  to ensure fair convergent validity comparison (Decision D068)
- Data flow: Load theta (RQ 5.1) → Load purified items (RQ 5.1) → Load raw master data →
  Filter raw data to purified items only → Save filtered data for Step 1 CTT computation
- Expected performance: ~5 seconds (stdlib operations only, no statistical computation)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib (pandas operations only)
- Validation tool: tools.validation.validate_data_format
- Parameters: N/A (pure data loading and filtering)
- Critical: Raw data MUST be filtered to exact purified item set (IRT-CTT item alignment)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_data_format

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/rq11 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step00_load_data.log"

# Input paths (from RQ 5.1)
RQ1_DIR = PROJECT_ROOT / "results" / "ch5" / "5.2.1" / "data"
THETA_INPUT = RQ1_DIR / "step03_theta_scores.csv"
TSVR_INPUT = RQ1_DIR / "step00_tsvr_mapping.csv"
PURIFIED_ITEMS_INPUT = RQ1_DIR / "step02_purified_items.csv"
RAW_DATA_INPUT = PROJECT_ROOT / "data" / "cache" / "dfData.csv"

# Output paths
DATA_DIR = RQ_DIR / "data"
THETA_OUTPUT = DATA_DIR / "step00_irt_theta_loaded.csv"
TSVR_OUTPUT = DATA_DIR / "step00_tsvr_loaded.csv"
PURIFIED_ITEMS_OUTPUT = DATA_DIR / "step00_purified_items.csv"
RAW_FILTERED_OUTPUT = DATA_DIR / "step00_raw_data_filtered.csv"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step05_lmm_model_comparison.csv
#   CORRECT: data/step03_theta_scores.csv
#   WRONG:   results/lmm_model_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv             (missing step prefix)
#   WRONG:   logs/step02_removed_items.csv     (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 00: Load Data from RQ 5.1 and Master Dataset")

        # =========================================================================
        # STEP 1: Load IRT Theta Scores from RQ 5.1
        # =========================================================================
        # Expected: 400 rows (100 participants x 4 test sessions)
        # Purpose: IRT ability estimates for convergent validity comparison

        log("[LOAD] Loading IRT theta scores from RQ 5.1...")
        irt_theta = pd.read_csv(THETA_INPUT)
        log(f"[LOADED] {THETA_INPUT.name} ({len(irt_theta)} rows, {len(irt_theta.columns)} cols)")
        log(f"  Columns: {irt_theta.columns.tolist()}")
        log(f"  Theta range - What: [{irt_theta['theta_what'].min():.2f}, {irt_theta['theta_what'].max():.2f}]")
        log(f"  Theta range - Where: [{irt_theta['theta_where'].min():.2f}, {irt_theta['theta_where'].max():.2f}]")
        log(f"  Theta range - When: [{irt_theta['theta_when'].min():.2f}, {irt_theta['theta_when'].max():.2f}]")

        # =========================================================================
        # STEP 2: Load TSVR Mapping from RQ 5.1
        # =========================================================================
        # Expected: 400 rows (time variable for LMM analysis)
        # Purpose: TSVR_hours will be used as time variable in parallel LMM models

        log("[LOAD] Loading TSVR mapping from RQ 5.1...")
        tsvr_data = pd.read_csv(TSVR_INPUT)
        log(f"[LOADED] {TSVR_INPUT.name} ({len(tsvr_data)} rows, {len(tsvr_data.columns)} cols)")
        log(f"  Columns: {tsvr_data.columns.tolist()}")
        log(f"  TSVR range: [{tsvr_data['TSVR_hours'].min():.2f}, {tsvr_data['TSVR_hours'].max():.2f}] hours")

        # =========================================================================
        # STEP 3: Load Purified Items List from RQ 5.1
        # =========================================================================
        # Expected: 40-60 items (40-50% retention from RQ 5.1 item purification)
        # Purpose: Ensures IRT and CTT use SAME item set for fair comparison

        log("[LOAD] Loading purified items list from RQ 5.1...")
        purified_items = pd.read_csv(PURIFIED_ITEMS_INPUT)
        log(f"[LOADED] {PURIFIED_ITEMS_INPUT.name} ({len(purified_items)} rows, {len(purified_items.columns)} cols)")
        log(f"  Columns: {purified_items.columns.tolist()}")

        # Count items per factor
        factor_counts = purified_items['factor'].value_counts().to_dict()
        log(f"  Items per factor: {factor_counts}")

        # =========================================================================
        # STEP 4: Load Raw Master Data
        # =========================================================================
        # Expected: 400 rows, ~200+ columns (all VR items + demographics)
        # Purpose: Extract raw item responses for CTT computation

        log("[LOAD] Loading raw master data...")
        raw_data = pd.read_csv(RAW_DATA_INPUT)
        log(f"[LOADED] {RAW_DATA_INPUT.name} ({len(raw_data)} rows, {len(raw_data.columns)} cols)")

        # =========================================================================
        # STEP 5: Filter Raw Data to Purified Items Only
        # =========================================================================
        # Critical: CTT must use SAME items as IRT for valid comparison
        # Strategy: Keep UID, TEST columns + only item columns in purified_items list

        log("[FILTER] Filtering raw data to purified items only...")

        # Get purified item names
        purified_item_names = purified_items['item_name'].tolist()
        log(f"  Purified item count: {len(purified_item_names)}")

        # Check which purified items exist in raw data
        available_items = [col for col in purified_item_names if col in raw_data.columns]
        missing_items = [col for col in purified_item_names if col not in raw_data.columns]

        log(f"  Items available in raw data: {len(available_items)}")
        if missing_items:
            log(f"  [WARNING] Items in purified list but NOT in raw data: {len(missing_items)}")
            log(f"    Missing items: {missing_items}")

        # Select columns: UID, TEST, + purified items
        base_cols = ['UID', 'TEST']
        cols_to_keep = base_cols + available_items

        raw_data_filtered = raw_data[cols_to_keep].copy()
        log(f"[FILTERED] Raw data filtered ({len(raw_data_filtered)} rows, {len(raw_data_filtered.columns)} cols)")
        log(f"  Base columns: {base_cols}")
        log(f"  Item columns: {len(available_items)}")

        # =========================================================================
        # STEP 5b: DICHOTOMIZE Item Scores (CRITICAL FOR CTT-IRT COMPARISON)
        # =========================================================================
        # Rule: 1 stays 1, all other values (<1) become 0
        # Reason: RQ 5.1 IRT used dichotomized data, CTT must use same for fair comparison
        # Reference: 1_concept.md line 135

        log("[DICHOTOMIZE] Applying dichotomization rule: 1=1, <1=0...")

        # Get item column names (exclude UID, TEST)
        item_cols = [col for col in raw_data_filtered.columns if col not in base_cols]

        # Check values BEFORE dichotomization
        unique_values_before = set()
        for col in item_cols:
            unique_values_before.update(raw_data_filtered[col].dropna().unique())
        log(f"  Unique values BEFORE dichotomization: {sorted(unique_values_before)}")

        # Apply dichotomization: 1 stays 1, everything else becomes 0
        for col in item_cols:
            raw_data_filtered[col] = (raw_data_filtered[col] == 1).astype(int)

        # Check values AFTER dichotomization
        unique_values_after = set()
        for col in item_cols:
            unique_values_after.update(raw_data_filtered[col].dropna().unique())
        log(f"  Unique values AFTER dichotomization: {sorted(unique_values_after)}")

        # Validate dichotomization success
        if unique_values_after != {0, 1}:
            raise ValueError(f"Dichotomization FAILED: Expected {{0, 1}}, got {unique_values_after}")

        log("[DICHOTOMIZE] SUCCESS: All item scores are now binary (0 or 1)")

        # =========================================================================
        # STEP 6: Save All Outputs
        # =========================================================================
        # Save local copies for RQ 5.11 analysis pipeline

        log("[SAVE] Saving output files...")

        # Save IRT theta scores
        irt_theta.to_csv(THETA_OUTPUT, index=False, encoding='utf-8')
        log(f"[SAVED] {THETA_OUTPUT.name} ({len(irt_theta)} rows, {len(irt_theta.columns)} cols)")

        # Save TSVR mapping
        tsvr_data.to_csv(TSVR_OUTPUT, index=False, encoding='utf-8')
        log(f"[SAVED] {TSVR_OUTPUT.name} ({len(tsvr_data)} rows, {len(tsvr_data.columns)} cols)")

        # Save purified items
        purified_items.to_csv(PURIFIED_ITEMS_OUTPUT, index=False, encoding='utf-8')
        log(f"[SAVED] {PURIFIED_ITEMS_OUTPUT.name} ({len(purified_items)} rows, {len(purified_items.columns)} cols)")

        # Save filtered raw data
        raw_data_filtered.to_csv(RAW_FILTERED_OUTPUT, index=False, encoding='utf-8')
        log(f"[SAVED] {RAW_FILTERED_OUTPUT.name} ({len(raw_data_filtered)} rows, {len(raw_data_filtered.columns)} cols)")

        # =========================================================================
        # STEP 7: Run Validation
        # =========================================================================
        # Validation: Check data formats, ranges, completeness

        log("[VALIDATION] Running data format validation...")

        # Validation 1: IRT theta format and ranges
        log("  [CHECK] IRT theta format...")
        theta_validation = validate_data_format(
            df=irt_theta,
            required_cols=['composite_ID', 'theta_what', 'theta_where', 'theta_when']
        )

        if not theta_validation['valid']:
            raise ValueError(f"IRT theta validation failed: {theta_validation['message']}")

        # Check theta ranges
        theta_min = min(irt_theta['theta_what'].min(), irt_theta['theta_where'].min(), irt_theta['theta_when'].min())
        theta_max = max(irt_theta['theta_what'].max(), irt_theta['theta_where'].max(), irt_theta['theta_when'].max())

        if theta_min < -3 or theta_max > 3:
            log(f"  [WARNING] Theta values outside typical range [-3, 3]: [{theta_min:.2f}, {theta_max:.2f}]")
        else:
            log(f"  [PASS] Theta values in expected range: [{theta_min:.2f}, {theta_max:.2f}]")

        # Validation 2: TSVR format and ranges
        log("  [CHECK] TSVR format...")
        tsvr_validation = validate_data_format(
            df=tsvr_data,
            required_cols=['composite_ID', 'UID', 'test', 'TSVR_hours']
        )

        if not tsvr_validation['valid']:
            raise ValueError(f"TSVR validation failed: {tsvr_validation['message']}")

        # Check TSVR ranges
        tsvr_min = tsvr_data['TSVR_hours'].min()
        tsvr_max = tsvr_data['TSVR_hours'].max()

        if tsvr_min < 0 or tsvr_max > 300:
            log(f"  [WARNING] TSVR_hours outside expected range [0, 300]: [{tsvr_min:.2f}, {tsvr_max:.2f}]")
        else:
            log(f"  [PASS] TSVR_hours in expected range: [{tsvr_min:.2f}, {tsvr_max:.2f}]")

        # Validation 3: Purified items format
        log("  [CHECK] Purified items format...")
        items_validation = validate_data_format(
            df=purified_items,
            required_cols=['item_name', 'factor', 'a', 'b']
        )

        if not items_validation['valid']:
            raise ValueError(f"Purified items validation failed: {items_validation['message']}")

        # Check item count
        item_count = len(purified_items)
        if item_count < 40 or item_count > 60:
            log(f"  [WARNING] Purified item count outside expected range [40, 60]: {item_count}")
        else:
            log(f"  [PASS] Purified item count in expected range: {item_count}")

        # Validation 4: Raw data filtered format
        log("  [CHECK] Raw data filtered format...")
        raw_validation = validate_data_format(
            df=raw_data_filtered,
            required_cols=['UID', 'TEST']
        )

        if not raw_validation['valid']:
            raise ValueError(f"Raw data validation failed: {raw_validation['message']}")

        # Check row count
        if len(raw_data_filtered) != 400:
            log(f"  [WARNING] Raw data row count not 400: {len(raw_data_filtered)}")
        else:
            log(f"  [PASS] Raw data row count: {len(raw_data_filtered)}")

        # Check item columns
        item_col_count = len(raw_data_filtered.columns) - 2  # Subtract UID, TEST
        if item_col_count < 40:
            raise ValueError(f"Too few item columns in filtered data: {item_col_count} (expected >= 40)")
        else:
            log(f"  [PASS] Item columns in filtered data: {item_col_count}")

        # Validation 5: Composite ID alignment
        log("  [CHECK] Composite ID alignment across files...")
        theta_ids = set(irt_theta['composite_ID'].unique())
        tsvr_ids = set(tsvr_data['composite_ID'].unique())

        if theta_ids != tsvr_ids:
            missing_in_tsvr = theta_ids - tsvr_ids
            missing_in_theta = tsvr_ids - theta_ids
            msg = f"Composite ID mismatch between theta and TSVR files\n"
            if missing_in_tsvr:
                msg += f"  In theta but not TSVR: {len(missing_in_tsvr)} IDs\n"
            if missing_in_theta:
                msg += f"  In TSVR but not theta: {len(missing_in_theta)} IDs"
            raise ValueError(msg)
        else:
            log(f"  [PASS] Composite IDs aligned: {len(theta_ids)} unique IDs")

        log("[VALIDATION] All validation checks passed")
        log("[SUCCESS] Step 00 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
