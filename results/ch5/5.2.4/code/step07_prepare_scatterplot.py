#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Updated 2025-12-03 for When domain exclusion)
# =============================================================================
"""
Step ID: step07
Step Name: prepare_scatterplot
RQ: results/ch5/5.2.4 (IRT-CTT Convergent Validity Comparison)
Generated: 2025-11-29
Updated: 2025-12-03 (When domain exclusion applied)

PURPOSE:
Create plot source CSV for scatterplots showing IRT vs CTT correlation per domain.
This data will be used by rq_plots agent to generate scatter plots visualizing
the convergent validity between IRT theta scores and CTT mean scores across
the two memory domains (What, Where - When excluded).

**CRITICAL: When domain is EXCLUDED due to floor effects discovered in RQ 5.2.1**

EXPECTED INPUTS:
  - data/step00_irt_theta_loaded.csv
    Columns: ['composite_ID', 'theta_what', 'theta_where'] (NO theta_when)
    Format: IRT theta scores from RQ 5.2.1 Pass 2 (wide format)
    Expected rows: 400 (100 UIDs x 4 tests)

  - data/step01_ctt_scores.csv
    Columns: ['composite_ID', 'domain', 'CTT_score'] (additional columns may be present)
    Format: CTT mean scores per domain (long format, When excluded)
    Expected rows: 800 (400 UID x test x 2 domains)

  - data/step02_correlations.csv
    Columns: ['domain', 'r'] (additional correlation stats may be present)
    Format: Pearson correlations between IRT and CTT per domain
    Expected rows: 3 (What, Where, Overall - NO When)

EXPECTED OUTPUTS:
  - data/step07_scatterplot_data.csv
    Columns: ['composite_ID', 'domain', 'IRT_score', 'CTT_score', 'r']
    Format: Long-format plot source data (one row per UID x test x domain)
    Expected rows: 800 (400 UID x test x 2 domains - When excluded)
    Purpose: Input for rq_plots agent to create IRT vs CTT scatter plots with
             correlation coefficient annotations

VALIDATION CRITERIA:
  - Both domains present (What, Where - NO When)
  - Exactly 800 rows (400 UID x test x 2 domains - When excluded)
  - No NaN in IRT_score or CTT_score columns
  - IRT_score in [-3, 3] (typical IRT ability range)
  - CTT_score in [0, 1] (proportion correct)

g_code REASONING:
- Approach: Reshape IRT theta from wide to long format, merge with CTT scores
            on composite_ID + domain, then join correlation coefficients for
            plot annotations.
- Why this approach: Option B architecture separates data preparation from
                     plotting. This step creates plot-ready CSV that rq_plots
                     can consume without additional transformations.
- Data flow: Wide IRT theta → Long IRT theta → Merge with CTT → Join with r → Save
- Expected performance: ~1 second (simple pandas operations on 400 rows)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib (pandas reshape and merge operations)
- Validation tool: tools.validation.validate_plot_data_completeness
- Parameters: Domain mapping (theta_what → What, theta_where → Where, theta_when → When)
- Key transformation: pd.melt() to reshape IRT theta from wide to long format
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_plot_data_completeness

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/chX/rqY (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step07_prepare_scatterplot.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step07_scatterplot_data.csv
#   CORRECT: logs/step07_prepare_scatterplot.log
#   WRONG:   data/scatterplot_data.csv             (missing step prefix)
#   WRONG:   results/step07_scatterplot_data.csv   (CSV in results folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 07: Prepare Scatterplot Data")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: IRT theta scores (wide format), CTT scores (long format), correlations
        # Purpose: Combine IRT and CTT scores for scatter plot visualization

        log("[LOAD] Loading input data...")

        # Load IRT theta scores (wide format: one row per composite_ID, columns per domain)
        irt_theta_path = RQ_DIR / "data" / "step00_irt_theta_loaded.csv"
        irt_theta = pd.read_csv(irt_theta_path)
        log(f"[LOADED] step00_irt_theta_loaded.csv ({len(irt_theta)} rows, {len(irt_theta.columns)} cols)")

        # Load CTT scores (long format: one row per composite_ID x domain)
        ctt_scores_path = RQ_DIR / "data" / "step01_ctt_scores.csv"
        ctt_scores = pd.read_csv(ctt_scores_path)
        log(f"[LOADED] step01_ctt_scores.csv ({len(ctt_scores)} rows, {len(ctt_scores.columns)} cols)")

        # Load correlations (one row per domain)
        correlations_path = RQ_DIR / "data" / "step02_correlations.csv"
        correlations = pd.read_csv(correlations_path)
        log(f"[LOADED] step02_correlations.csv ({len(correlations)} rows, {len(correlations.columns)} cols)")

        # =========================================================================
        # STEP 2: Reshape IRT Theta to Long Format
        # =========================================================================
        # Tool: pandas.melt()
        # What it does: Convert wide-format IRT theta (theta_what, theta_where, theta_when columns)
        #               to long format (composite_ID, domain, IRT_score rows)
        # Expected output: 1200 rows (400 composite_IDs x 3 domains)

        log("[RESHAPE] Converting IRT theta from wide to long format (When excluded)...")

        # Domain mapping: theta column names -> domain labels
        # theta_what -> what
        # theta_where -> where
        # Note: Use lowercase to match CTT domain values
        # theta_when -> EXCLUDED due to floor effects
        domain_mapping = {
            'theta_what': 'what',
            'theta_where': 'where'
            # 'theta_when': 'when' - EXCLUDED
        }

        # Reshape using pd.melt() - NO theta_when
        irt_long = pd.melt(
            irt_theta,
            id_vars=['composite_ID'],
            value_vars=['theta_what', 'theta_where'],  # NO theta_when
            var_name='theta_column',
            value_name='IRT_score'
        )

        # Map theta column names to domain labels
        irt_long['domain'] = irt_long['theta_column'].map(domain_mapping)

        # Drop intermediate theta_column
        irt_long = irt_long[['composite_ID', 'domain', 'IRT_score']]

        log(f"[RESHAPED] IRT theta long format: {len(irt_long)} rows, {len(irt_long.columns)} cols")
        log(f"[RESHAPED] Domains: {sorted(irt_long['domain'].unique().tolist())}")

        # =========================================================================
        # STEP 3: Merge IRT and CTT on composite_ID + domain
        # =========================================================================
        # Tool: pandas.merge()
        # What it does: Join IRT and CTT scores on matching composite_ID and domain
        # Expected output: 1200 rows (all composite_IDs x domains should match)

        log("[MERGE] Joining IRT and CTT scores on composite_ID + domain...")

        # Select only required columns from CTT scores
        ctt_subset = ctt_scores[['composite_ID', 'domain', 'CTT_score']].copy()

        # Merge IRT and CTT
        scatterplot_data = pd.merge(
            irt_long,
            ctt_subset,
            on=['composite_ID', 'domain'],
            how='inner'
        )

        log(f"[MERGED] Combined data: {len(scatterplot_data)} rows")

        # Check for any merge failures (should be 0 unmatched rows)
        n_irt_rows = len(irt_long)
        n_merged_rows = len(scatterplot_data)
        if n_merged_rows < n_irt_rows:
            log(f"[WARNING] Merge resulted in {n_irt_rows - n_merged_rows} unmatched IRT rows")

        # =========================================================================
        # STEP 4: Join with Correlation Coefficients
        # =========================================================================
        # Tool: pandas.merge()
        # What it does: Add correlation coefficient 'r' to each row for plot annotation
        # Expected output: Same 1200 rows, now with 'r' column added

        log("[JOIN] Adding correlation coefficients for plot annotations...")

        # Select only domain and r columns from correlations
        # Filter out 'Overall' domain (only need what, where for individual scatter plots - When excluded)
        correlations_subset = correlations[correlations['domain'].isin(['what', 'where'])][['domain', 'r']].copy()

        # Join correlations on domain
        scatterplot_data = pd.merge(
            scatterplot_data,
            correlations_subset,
            on='domain',
            how='left'
        )

        log(f"[JOINED] Data with correlations: {len(scatterplot_data)} rows, {len(scatterplot_data.columns)} cols")

        # =========================================================================
        # STEP 5: Sort and Finalize
        # =========================================================================
        # Sort by domain, then composite_ID for consistent ordering

        log("[SORT] Sorting by domain and composite_ID...")

        scatterplot_data = scatterplot_data.sort_values(['domain', 'composite_ID']).reset_index(drop=True)

        # Select final column order
        final_columns = ['composite_ID', 'domain', 'IRT_score', 'CTT_score', 'r']
        scatterplot_data = scatterplot_data[final_columns]

        log(f"[SORTED] Final data: {len(scatterplot_data)} rows, {len(scatterplot_data.columns)} cols")

        # =========================================================================
        # STEP 6: Save Output
        # =========================================================================
        # Output will be used by rq_plots agent to create scatter plots

        output_path = RQ_DIR / "data" / "step07_scatterplot_data.csv"
        log(f"[SAVE] Saving to {output_path}...")

        scatterplot_data.to_csv(output_path, index=False, encoding='utf-8')

        log(f"[SAVED] step07_scatterplot_data.csv ({len(scatterplot_data)} rows, {len(scatterplot_data.columns)} cols)")

        # Log summary statistics
        log("[SUMMARY] Data summary:")
        log(f"  - Domains: {sorted(scatterplot_data['domain'].unique().tolist())}")
        log(f"  - IRT_score range: [{scatterplot_data['IRT_score'].min():.2f}, {scatterplot_data['IRT_score'].max():.2f}]")
        log(f"  - CTT_score range: [{scatterplot_data['CTT_score'].min():.3f}, {scatterplot_data['CTT_score'].max():.3f}]")
        log(f"  - Correlation coefficients (r): {scatterplot_data.groupby('domain')['r'].first().to_dict()}")

        # =========================================================================
        # STEP 7: Run Validation (When EXCLUDED)
        # =========================================================================
        # Tool: validate_plot_data_completeness
        # Validates: Both domains present (When excluded), expected row count, no missing data,
        #            IRT scores in [-3, 3], CTT scores in [0, 1]

        log("[VALIDATION] Running validate_plot_data_completeness (When excluded)...")

        validation_result = validate_plot_data_completeness(
            plot_data=scatterplot_data,
            required_domains=['what', 'where'],  # NO 'when' - excluded
            required_groups=[],  # No group stratification for this plot
            domain_col='domain'
        )

        # Report validation results
        if isinstance(validation_result, dict):
            for key, value in validation_result.items():
                log(f"[VALIDATION] {key}: {value}")
        else:
            log(f"[VALIDATION] {validation_result}")

        # Check validation passed
        if isinstance(validation_result, dict) and not validation_result.get('valid', False):
            raise ValueError(f"Validation failed: {validation_result.get('message', 'Unknown error')}")

        # Additional validations specific to this step
        log("[VALIDATION] Additional checks...")

        # Check row count (800 = 400 x 2 domains, When excluded)
        expected_rows = 800  # When excluded: 400 x 2 domains
        actual_rows = len(scatterplot_data)
        if actual_rows != expected_rows:
            log(f"[WARNING] Expected {expected_rows} rows, got {actual_rows}")
        else:
            log(f"[PASS] Row count: {actual_rows} (matches expected - When excluded)")

        # Check for NaN values
        nan_irt = scatterplot_data['IRT_score'].isna().sum()
        nan_ctt = scatterplot_data['CTT_score'].isna().sum()
        if nan_irt > 0 or nan_ctt > 0:
            raise ValueError(f"Found NaN values: IRT_score={nan_irt}, CTT_score={nan_ctt}")
        else:
            log("[PASS] No NaN values in IRT_score or CTT_score")

        # Check IRT score range
        irt_min, irt_max = scatterplot_data['IRT_score'].min(), scatterplot_data['IRT_score'].max()
        if irt_min < -3 or irt_max > 3:
            log(f"[WARNING] IRT scores outside typical range [-3, 3]: [{irt_min:.2f}, {irt_max:.2f}]")
        else:
            log(f"[PASS] IRT scores in typical range: [{irt_min:.2f}, {irt_max:.2f}]")

        # Check CTT score range
        ctt_min, ctt_max = scatterplot_data['CTT_score'].min(), scatterplot_data['CTT_score'].max()
        if ctt_min < 0 or ctt_max > 1:
            raise ValueError(f"CTT scores outside [0, 1] range: [{ctt_min:.3f}, {ctt_max:.3f}]")
        else:
            log(f"[PASS] CTT scores in valid range: [{ctt_min:.3f}, {ctt_max:.3f}]")

        log("[SUCCESS] Step 07 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
