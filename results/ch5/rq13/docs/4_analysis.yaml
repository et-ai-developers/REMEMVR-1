# ============================================================================
# ANALYSIS RECIPE - COMPLETE SPECIFICATION
# ============================================================================
# Generated: 2025-11-27
# RQ: ch5/rq13
# Agent: rq_analysis v4.0.0
# Purpose: Self-contained recipe for g_code agent (reads ONLY this file)
# ============================================================================

metadata:
  rq_id: "ch5/rq13"
  rq_title: "Between-Person Variance in Forgetting Rates"
  total_steps: 5
  analysis_type: "LMM Variance Decomposition (DERIVED data from RQ 5.7)"
  generated_by: "rq_analysis v4.0.0"
  timestamp: "2025-11-27T00:00:00Z"
  critical_dependency: "RQ 5.7 must complete Steps 0-5 before this RQ can execute"

# ============================================================================
# ANALYSIS STEPS - COMPLETE SPECIFICATIONS
# ============================================================================

steps:
  # --------------------------------------------------------------------------
  # STEP 1: Load RQ 5.7 Dependencies
  # --------------------------------------------------------------------------
  - name: "step01_load_rq57_dependencies"
    step_number: "01"
    description: "Load saved LMM model object, theta scores, and TSVR mapping from RQ 5.7 to enable variance decomposition analysis"

    # STDLIB OPERATION (pandas, pickle - NOT catalogued tool)
    analysis_call:
      type: "stdlib"
      operations:
        - "Load pickle file: results/ch5/rq7/data/step05_lmm_all_bestmodel.pkl using pickle.load() or joblib.load()"
        - "Load CSV: results/ch5/rq7/data/step04_theta_scores_allitems.csv using pd.read_csv()"
        - "Load CSV: results/ch5/rq7/data/step00_tsvr_mapping.csv using pd.read_csv()"
        - "Validate model object is statsmodels MixedLMResults with random effects"
        - "Validate CSV files have expected columns and row counts"
        - "Extract model metadata: model_source, model_type, n_participants, n_observations, random_effects, converged"
        - "Save metadata to YAML: data/step01_model_metadata.yaml"

      input_files:
        - path: "results/ch5/rq7/data/step05_lmm_all_bestmodel.pkl"
          format: "Python pickle file (statsmodels MixedLMResults object)"
          description: "Fitted LMM with random intercepts and random slopes from RQ 5.7 Step 5"
          required_attributes: ["cov_re", "scale", "random_effects", "converged"]

        - path: "results/ch5/rq7/data/step04_theta_scores_allitems.csv"
          format: "CSV with UTF-8 encoding"
          required_columns: ["UID", "TEST", "theta", "SE"]
          expected_rows: 400
          description: "IRT theta scores from RQ 5.7 Step 4 (100 participants x 4 tests)"

        - path: "results/ch5/rq7/data/step00_tsvr_mapping.csv"
          format: "CSV with UTF-8 encoding"
          required_columns: ["UID", "TEST", "TSVR"]
          expected_rows: 400
          description: "Time Since VR in hours from RQ 5.7 Step 0 (Decision D070)"

      output_files:
        - path: "data/step01_model_metadata.yaml"
          format: "YAML with UTF-8 encoding"
          description: "Model metadata documenting loaded LMM model from RQ 5.7"
          expected_keys: ["model_source", "model_type", "n_participants", "n_observations", "random_effects", "converged"]

        - path: "logs/step01_load_dependencies.log"
          format: "Text log"
          description: "Loading confirmation messages, file sizes, row counts, validation checks"

      circuit_breaker_check:
        condition: "If ANY of the three required files from RQ 5.7 are missing"
        action: "EXPECTATIONS ERROR"
        message: |
          EXPECTATIONS ERROR: To perform Step 1 (Load RQ 5.7 Dependencies) I expect:
            - results/ch5/rq7/data/step05_lmm_all_bestmodel.pkl (saved LMM model)
            - results/ch5/rq7/data/step04_theta_scores_allitems.csv (theta scores)
            - results/ch5/rq7/data/step00_tsvr_mapping.csv (TSVR mapping)

          But missing: [list missing files]

          Action: RQ 5.7 must complete Steps 0-5 before RQ 5.13 can execute.
          Run RQ 5.7 workflow first, then retry RQ 5.13.

    # Validation call specification
    validation_call:
      module: "tools.validation"
      function: "validate_model_convergence"
      signature: "validate_model_convergence(lmm_result: MixedLMResults) -> Dict[str, Any]"

      input_files: []  # Validates Python object, not file

      parameters:
        lmm_result: "lmm_model"  # Variable name from analysis_call

      returns:
        type: "Dict[str, Any]"
        variable_name: "validation_result"

      criteria:
        - "Model converged attribute = True"
        - "No convergence warnings in model object"
        - "n_participants = 100 (all participants from RQ 5.7)"
        - "n_observations = 380-400 (allowing for minor data loss)"

      on_failure:
        action: "raise ValueError(validation_result['message'])"
        log_to: "logs/step01_load_dependencies.log"

      description: "Validate statsmodels LMM model converged successfully and has expected participant/observation counts"

    log_file: "logs/step01_load_dependencies.log"

  # --------------------------------------------------------------------------
  # STEP 2: Extract Variance Components from LMM
  # --------------------------------------------------------------------------
  - name: "step02_extract_variance_components"
    step_number: "02"
    description: "Extract variance-covariance matrix from random effects and residual variance to enable ICC computation"

    # STDLIB OPERATION (pandas, numpy - NOT catalogued tool)
    analysis_call:
      type: "stdlib"
      operations:
        - "Extract variance components from model.cov_re (random effects covariance matrix):"
        - "  - var_intercept: Variance of random intercepts (diagonal element [0,0])"
        - "  - var_slope: Variance of random slopes (diagonal element [1,1])"
        - "  - cov_int_slope: Covariance between intercepts and slopes (off-diagonal element [0,1])"
        - "Extract residual variance: var_residual = model.scale"
        - "Compute correlation between intercepts and slopes: cor_int_slope = cov_int_slope / sqrt(var_intercept * var_slope)"
        - "Create DataFrame with component names and estimates"
        - "Save to CSV: data/step02_variance_components.csv"

      input_files:
        - path: "data/step01_model_metadata.yaml"
          format: "YAML (model metadata from Step 1)"
          description: "Model metadata confirming model loaded successfully"
          source: "Step 1 output (stdlib loading operation)"

      output_files:
        - path: "data/step02_variance_components.csv"
          format: "CSV with UTF-8 encoding"
          columns:
            - {name: "component", type: "str", description: "Variance component name (var_intercept, var_slope, cov_int_slope, var_residual, cor_int_slope)"}
            - {name: "estimate", type: "float", description: "Variance/covariance/correlation value"}
          expected_rows: 5
          description: "LMM variance components extracted from random effects covariance matrix"

        - path: "logs/step02_variance_extraction.log"
          format: "Text log"
          description: "Extracted values, formulas used, validation checks"

      parameters:
        model_object: "lmm_model"  # From Step 1
        operations:
          - "Extract cov_re from model"
          - "Extract scale from model"
          - "Compute correlation from covariance"

    # Validation call specification
    validation_call:
      module: "tools.validation"
      function: "validate_variance_positivity"
      signature: "validate_variance_positivity(variance_df: DataFrame, component_col: str = 'component', value_col: str = 'variance') -> Dict[str, Any]"

      input_files:
        - path: "data/step02_variance_components.csv"
          variable_name: "variance_components"
          source: "analysis call output (step02_extract_variance)"

      parameters:
        variance_df: "variance_components"
        component_col: "component"
        value_col: "estimate"

      returns:
        type: "Dict[str, Any]"
        variable_name: "validation_result"

      criteria:
        - "var_intercept > 0 (variance must be positive)"
        - "var_slope > 0 (variance must be positive)"
        - "var_residual > 0 (variance must be positive)"
        - "cor_int_slope in [-1, 1] (correlation bounds)"
        - "No NaN values (all components must be estimated)"

      on_failure:
        action: "raise ValueError(validation_result['message'])"
        log_to: "logs/step02_variance_extraction.log"

      description: "Validate all LMM variance components > 0. Negative or zero variance indicates estimation issues."

    log_file: "logs/step02_variance_extraction.log"

  # --------------------------------------------------------------------------
  # STEP 3: Compute Intraclass Correlation Coefficients (ICC)
  # --------------------------------------------------------------------------
  - name: "step03_compute_icc"
    step_number: "03"
    description: "Quantify proportion of variance that is between-person (stable individual differences) vs within-person (measurement error) for both intercepts and slopes"

    # CATALOGUED TOOL (from 3_tools.yaml)
    analysis_call:
      type: "catalogued"
      module: "tools.analysis_lmm"
      function: "compute_icc_from_variance_components"
      signature: "compute_icc_from_variance_components(variance_components_df: DataFrame, slope_name: str = 'TSVR_hours', timepoint: float = 6.0) -> DataFrame"

      input_files:
        - path: "data/step02_variance_components.csv"
          required_columns: ["component", "estimate"]
          variable_name: "variance_components"
          expected_rows: 5
          description: "Variance components from Step 2"

      output_files:
        - path: "data/step03_icc_estimates.csv"
          variable_name: "icc_estimates"
          columns:
            - {name: "icc_type", type: "str", description: "ICC type (intercept, slope_simple, slope_conditional)"}
            - {name: "icc_value", type: "float", description: "ICC estimate in [0, 1]"}
            - {name: "interpretation", type: "str", description: "Low (<0.20), Moderate (0.20-0.40), Substantial (>=0.40)"}
          expected_rows: 3
          description: "Three ICC estimates with interpretations"

        - path: "results/step03_icc_summary.txt"
          variable_name: "icc_summary"
          description: "Plain text summary of ICC estimates with interpretations and implications"

        - path: "logs/step03_icc_computation.log"
          description: "Computation formulas, intermediate values, validation checks"

      parameters:
        variance_components_df: "variance_components"
        slope_name: "TSVR_hours"
        timepoint: 6.0

      returns:
        type: "DataFrame"
        variable_name: "icc_estimates"

      description: "Compute 3 ICC estimates: ICC_intercept (baseline stability), ICC_slope_simple (forgetting rate variance only), ICC_slope_conditional (accounting for intercept-slope correlation at Day 6)"

    # Validation call specification
    validation_call:
      module: "tools.validation"
      function: "validate_icc_bounds"
      signature: "validate_icc_bounds(icc_df: DataFrame, icc_col: str = 'icc_value') -> Dict[str, Any]"

      input_files:
        - path: "data/step03_icc_estimates.csv"
          variable_name: "icc_estimates"
          source: "analysis call output (compute_icc_from_variance_components return value)"

      parameters:
        icc_df: "icc_estimates"
        icc_col: "icc_value"

      returns:
        type: "Dict[str, Any]"
        variable_name: "validation_result"

      criteria:
        - "All ICC values in [0, 1] range (mathematical constraint)"
        - "No NaN values (all ICCs must be computed)"
        - "No infinite values (indicates computation error)"

      on_failure:
        action: "raise ValueError(validation_result['message'])"
        log_to: "logs/step03_icc_computation.log"

      description: "Validate ICC values in [0,1] range. ICCs outside this range indicate computation errors since ICC is a proportion of variance."

    log_file: "logs/step03_icc_computation.log"

  # --------------------------------------------------------------------------
  # STEP 4: Extract Individual Random Effects
  # --------------------------------------------------------------------------
  - name: "step04_extract_random_effects"
    step_number: "04"
    description: "Extract participant-specific random intercepts and slopes for descriptive statistics, visualization, and downstream clustering analysis (RQ 5.14)"

    # STDLIB OPERATION (pandas - NOT catalogued tool)
    analysis_call:
      type: "stdlib"
      operations:
        - "Extract random_effects dictionary from loaded LMM model"
        - "For each participant (UID):"
        - "  - Extract random intercept (Intercept column)"
        - "  - Extract random slope (slope column for time variable)"
        - "  - Compute total intercept = fixed intercept + random intercept"
        - "  - Compute total slope = fixed slope + random slope"
        - "Create DataFrame with one row per participant"
        - "Compute descriptive statistics for random slopes: mean, SD, min, max, Q1, median, Q3"
        - "Save to CSV: data/step04_random_effects.csv"
        - "Save descriptive statistics to TXT: results/step04_random_slopes_descriptives.txt"

      input_files:
        - path: "data/step01_model_metadata.yaml"
          format: "YAML (model metadata from Step 1)"
          description: "Model metadata confirming model loaded successfully"
          source: "Step 1 output"

      output_files:
        - path: "data/step04_random_effects.csv"
          format: "CSV with UTF-8 encoding"
          columns:
            - {name: "UID", type: "str", description: "Participant identifier (format: P###)"}
            - {name: "random_intercept", type: "float", description: "Deviation from population mean baseline"}
            - {name: "random_slope", type: "float", description: "Deviation from population mean forgetting rate"}
            - {name: "total_intercept", type: "float", description: "Fixed + random intercept"}
            - {name: "total_slope", type: "float", description: "Fixed + random slope"}
          expected_rows: 100
          description: "Individual random effects for all participants (REQUIRED INPUT for RQ 5.14)"

        - path: "results/step04_random_slopes_descriptives.txt"
          format: "Plain text"
          description: "Descriptive statistics for random slopes distribution (mean, SD, min, max, quartiles)"

        - path: "logs/step04_random_effects_extraction.log"
          format: "Text log"
          description: "Extraction confirmation, participant count, descriptive statistics, normality checks"

      parameters:
        model_object: "lmm_model"  # From Step 1
        operations:
          - "Extract random_effects from model"
          - "Compute total effects (fixed + random)"
          - "Compute descriptive statistics"

      critical_note: "The file data/step04_random_effects.csv is a REQUIRED INPUT for RQ 5.14 (K-means clustering). This file MUST be saved to enable downstream dependency."

    # Validation call specification
    validation_call:
      module: "tools.validation"
      function: "validate_data_columns"
      signature: "validate_data_columns(df: DataFrame, required_columns: List[str]) -> Dict[str, Any]"

      input_files:
        - path: "data/step04_random_effects.csv"
          variable_name: "random_effects"
          source: "analysis call output (step04_extract_random_effects)"

      parameters:
        df: "random_effects"
        required_columns: ["UID", "random_intercept", "random_slope", "total_intercept", "total_slope"]

      returns:
        type: "Dict[str, Any]"
        variable_name: "validation_result"

      criteria:
        - "All required columns present in DataFrame"
        - "Expected row count: exactly 100 (one per participant)"
        - "No NaN values in random_intercept or random_slope"
        - "No duplicate UIDs (each participant appears exactly once)"

      on_failure:
        action: "raise ValueError(validation_result['message'])"
        log_to: "logs/step04_random_effects_extraction.log"

      description: "Validate required columns exist and row count = 100 participants"

    log_file: "logs/step04_random_effects_extraction.log"

  # --------------------------------------------------------------------------
  # STEP 5: Test Intercept-Slope Correlation and Visualize Distribution
  # --------------------------------------------------------------------------
  - name: "step05_test_correlation_visualize"
    step_number: "05"
    description: "Test hypothesis that baseline ability and forgetting rate are correlated with Decision D068 dual p-value reporting, and visualize random slopes distribution with normality assessment"

    # CATALOGUED TOOL (from 3_tools.yaml)
    analysis_call:
      type: "catalogued"
      module: "tools.analysis_lmm"
      function: "test_intercept_slope_correlation_d068"
      signature: "test_intercept_slope_correlation_d068(random_effects_df: DataFrame, family_alpha: float = 0.05, n_tests: int = 15, intercept_col: str = 'Group Var', slope_col: str = 'Group x TSVR_hours Var') -> Dict"

      input_files:
        - path: "data/step04_random_effects.csv"
          required_columns: ["UID", "random_intercept", "random_slope"]
          variable_name: "random_effects"
          expected_rows: 100
          description: "Individual random effects from Step 4"

      output_files:
        - path: "results/step05_intercept_slope_correlation.csv"
          variable_name: "correlation_results"
          columns:
            - {name: "statistic", type: "str", description: "correlation, p_uncorrected, p_bonferroni, df, alpha_corrected"}
            - {name: "value", type: "float", description: "Statistic values"}
          expected_rows: 5
          description: "Correlation test results with dual p-values per Decision D068"

        - path: "results/step05_correlation_interpretation.txt"
          variable_name: "correlation_interpretation"
          description: "Plain text interpretation of correlation magnitude, direction, significance"

        - path: "plots/step05_random_slopes_histogram.png"
          variable_name: "histogram_file"
          format: "PNG image (800x600 @ 300 DPI)"
          description: "Histogram of random slopes with normal overlay and mean reference line"
          min_size_bytes: 10000

        - path: "plots/step05_random_slopes_qqplot.png"
          variable_name: "qqplot_file"
          format: "PNG image (800x600 @ 300 DPI)"
          description: "Q-Q plot assessing normality of random slopes distribution"
          min_size_bytes: 10000

        - path: "logs/step05_correlation_test.log"
          description: "Correlation test results, Bonferroni correction applied, plotting confirmation"

      parameters:
        random_effects_df: "random_effects"
        family_alpha: 0.05
        n_tests: 15
        intercept_col: "random_intercept"
        slope_col: "random_slope"

      returns:
        type: "Dict"
        variable_name: "correlation_results"

      description: "Test correlation between random intercepts and slopes with Decision D068 dual p-value reporting (uncorrected + Bonferroni). Tests hypothesis that baseline ability predicts forgetting rate."

    # Validation call specification
    validation_call:
      module: "tools.validation"
      function: "validate_correlation_test_d068"
      signature: "validate_correlation_test_d068(correlation_df: DataFrame, required_cols: List[str] = None) -> Dict[str, Any]"

      input_files:
        - path: "results/step05_intercept_slope_correlation.csv"
          variable_name: "correlation_results"
          source: "analysis call output (test_intercept_slope_correlation_d068 return value)"

      parameters:
        correlation_df: "correlation_results"
        required_cols: null  # Uses default D068 spec

      returns:
        type: "Dict[str, Any]"
        variable_name: "validation_result"

      criteria:
        - "BOTH p_uncorrected and p_bonferroni present (Decision D068 dual p-value requirement)"
        - "Correlation r in [-1, 1] range"
        - "P-values in [0, 1] range"
        - "Bonferroni correction correct: p_bonf = min(p_uncorr Ã— 15, 1.0)"
        - "Degrees of freedom correct: df = 98 (N-2 = 100-2)"
        - "Alpha threshold correct: alpha_corrected = 0.0033 (0.05/15)"

      on_failure:
        action: "raise ValueError(validation_result['message'])"
        log_to: "logs/step05_correlation_test.log"

      description: "Validate correlation test results include Decision D068 dual p-value reporting (uncorrected + Bonferroni)"

    log_file: "logs/step05_correlation_test.log"

# ============================================================================
# END OF ANALYSIS RECIPE
# ============================================================================
