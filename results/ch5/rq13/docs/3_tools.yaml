# 3_tools.yaml - Tool Catalog (Analysis + Validation Tools)
# Created by: rq_tools agent (Step 11)
# Consumed by: rq_analysis agent (Step 12)
# Architecture: Tool Catalog (Option A) - Each tool listed once, deduplication
# RQ: 5.13 (Between-Person Variance in Forgetting Rates)

analysis_tools:
  compute_icc_from_variance_components:
    module: "tools.analysis_lmm"
    function: "compute_icc_from_variance_components"
    signature: "compute_icc_from_variance_components(variance_components_df: DataFrame, slope_name: str = 'TSVR_hours', timepoint: float = 6.0) -> DataFrame"
    validation_tool: "validate_icc_bounds"

    input_files:
      - path: "data/step02_variance_components.csv"
        required_columns: ["component", "estimate"]
        expected_rows: 5
        data_types:
          component: "string (var_intercept, var_slope, cov_int_slope, var_residual, cor_int_slope)"
          estimate: "float (variance/covariance values)"

    output_files:
      - path: "data/step03_icc_estimates.csv"
        columns: ["icc_type", "icc_value", "interpretation"]
        description: "Three ICC estimates (intercept, slope_simple, slope_conditional) with interpretations"

    parameters:
      variance_components_df: "pd.DataFrame (variance components from Step 2)"
      slope_name: "str (default: 'TSVR_hours', name of slope component)"
      timepoint: "float (default: 6.0, Day 6 for conditional ICC)"

    description: "Compute 3 ICC estimates from LMM variance components: ICC_intercept (baseline stability), ICC_slope_simple (forgetting rate variance only), ICC_slope_conditional (accounting for intercept-slope correlation at Day 6)"
    source_reference: "tools_inventory.md section 'Module: tools.analysis_lmm' - compute_icc_from_variance_components"

  test_intercept_slope_correlation_d068:
    module: "tools.analysis_lmm"
    function: "test_intercept_slope_correlation_d068"
    signature: "test_intercept_slope_correlation_d068(random_effects_df: DataFrame, family_alpha: float = 0.05, n_tests: int = 15, intercept_col: str = 'random_intercept', slope_col: str = 'random_slope') -> Dict"
    validation_tool: "validate_correlation_test_d068"

    input_files:
      - path: "data/step04_random_effects.csv"
        required_columns: ["UID", "random_intercept", "random_slope"]
        expected_rows: 100
        data_types:
          UID: "string (participant identifier, format: P###)"
          random_intercept: "float (deviation from population mean baseline)"
          random_slope: "float (deviation from population mean forgetting rate)"

    output_files:
      - path: "results/step05_intercept_slope_correlation.csv"
        columns: ["statistic", "value"]
        description: "Correlation test results with dual p-values (uncorrected + Bonferroni) per Decision D068"

    parameters:
      random_effects_df: "pd.DataFrame (individual random effects from Step 4)"
      family_alpha: "float (default: 0.05, significance threshold)"
      n_tests: "int (default: 15, Chapter 5 family size for Bonferroni)"
      intercept_col: "str (default: 'Group Var', statsmodels naming)"
      slope_col: "str (default: 'Group x TSVR_hours Var', statsmodels naming)"

    description: "Test correlation between random intercepts and slopes with Decision D068 dual p-value reporting (uncorrected + Bonferroni). Tests hypothesis that baseline ability predicts forgetting rate."
    source_reference: "tools_inventory.md section 'Module: tools.analysis_lmm' - test_intercept_slope_correlation_d068"

validation_tools:
  validate_icc_bounds:
    module: "tools.validation"
    function: "validate_icc_bounds"
    signature: "validate_icc_bounds(icc_df: DataFrame, icc_col: str = 'icc_value') -> Dict[str, Any]"

    input_files:
      - path: "data/step03_icc_estimates.csv"
        required_columns: ["icc_type", "icc_value"]
        source: "analysis tool output (step03_compute_icc)"

    parameters:
      icc_col: "icc_value"

    criteria:
      - "All ICC values in [0, 1] range (mathematical constraint)"
      - "No NaN values (all ICCs must be computed)"
      - "No infinite values (indicates computation error)"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool (True if all criteria passed)"
        message: "str (human-readable explanation)"
        out_of_bounds: "List[Dict] (any ICCs outside [0,1])"
        icc_range: "Tuple[float, float] (min, max ICC values)"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step03_icc_computation.log"
      invoke: "g_debug (master invokes after error)"

    description: "Validate ICC values in [0,1] range. ICCs outside this range indicate computation errors since ICC is a proportion of variance."
    source_reference: "tools_inventory.md section 'Module: tools.validation' - validate_icc_bounds"

  validate_correlation_test_d068:
    module: "tools.validation"
    function: "validate_correlation_test_d068"
    signature: "validate_correlation_test_d068(correlation_df: DataFrame, required_cols: List[str] = None) -> Dict[str, Any]"

    input_files:
      - path: "results/step05_intercept_slope_correlation.csv"
        required_columns: ["statistic", "value"]
        source: "analysis tool output (step05_test_correlation)"

    parameters:
      required_cols: null  # Uses default D068 spec (p_uncorrected + one of p_bonferroni/p_holm/p_fdr)

    criteria:
      - "BOTH p_uncorrected and p_bonferroni present (Decision D068 dual p-value requirement)"
      - "Correlation r in [-1, 1] range"
      - "P-values in [0, 1] range"
      - "Bonferroni correction correct: p_bonf = min(p_uncorr Ã— 15, 1.0)"
      - "Degrees of freedom correct: df = 98 (N-2 = 100-2)"
      - "Alpha threshold correct: alpha_corrected = 0.0033 (0.05/15)"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        d068_compliant: "bool (True if dual p-values present)"
        missing_cols: "List[str] (any missing required columns)"
        message: "str"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step05_correlation_test.log"
      invoke: "g_debug (master invokes)"

    description: "Validate correlation test results include Decision D068 dual p-value reporting (uncorrected + Bonferroni). Ensures both significance criteria reported transparently."
    source_reference: "tools_inventory.md section 'Module: tools.validation' - validate_correlation_test_d068"

  check_file_exists:
    module: "tools.validation"
    function: "check_file_exists"
    signature: "check_file_exists(file_path: Union[str, Path], min_size_bytes: int = 0) -> Dict[str, Any]"

    input_files: []  # Validation checks file existence, not content

    parameters:
      files_to_check:
        - path: "data/step01_model_metadata.yaml"
          min_size_bytes: 100
        - path: "data/step02_variance_components.csv"
          min_size_bytes: 100
        - path: "data/step03_icc_estimates.csv"
          min_size_bytes: 100
        - path: "data/step04_random_effects.csv"
          min_size_bytes: 1000
        - path: "results/step03_icc_summary.txt"
          min_size_bytes: 100
        - path: "results/step04_random_slopes_descriptives.txt"
          min_size_bytes: 100
        - path: "results/step05_correlation_interpretation.txt"
          min_size_bytes: 100
        - path: "plots/step05_random_slopes_histogram.png"
          min_size_bytes: 10000
        - path: "plots/step05_random_slopes_qqplot.png"
          min_size_bytes: 10000

    criteria:
      - "All expected output files exist"
      - "Each file meets minimum size requirement (not empty/corrupted)"
      - "PNG files > 10KB (non-trivial plots)"
      - "CSV/TXT files > 100 bytes (contain data)"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        file_path: "str"
        size_bytes: "int"
        message: "str"

    behavior_on_failure:
      action: "raise FileNotFoundError"
      log_to: "logs/stepNN_name.log"
      invoke: "g_debug (master invokes)"

    description: "Validate that expected output files exist and meet minimum size requirements. Prevents missing outputs from propagating to downstream steps."
    source_reference: "tools_inventory.md section 'Module: tools.validation' - check_file_exists"

  validate_variance_positivity:
    module: "tools.validation"
    function: "validate_variance_positivity"
    signature: "validate_variance_positivity(variance_df: DataFrame, component_col: str = 'component', value_col: str = 'estimate') -> Dict[str, Any]"

    input_files:
      - path: "data/step02_variance_components.csv"
        required_columns: ["component", "estimate"]
        source: "analysis tool output (step02_extract_variance)"

    parameters:
      component_col: "component"
      value_col: "estimate"

    criteria:
      - "var_intercept > 0 (variance must be positive)"
      - "var_slope > 0 (variance must be positive)"
      - "var_residual > 0 (variance must be positive)"
      - "cor_int_slope in [-1, 1] (correlation bounds)"
      - "No NaN values (all components must be estimated)"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        message: "str"
        negative_components: "List[str]"
        variance_range: "Tuple[float, float]"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step02_variance_extraction.log"
      invoke: "g_debug (master invokes)"

    description: "Validate all LMM variance components > 0. Negative or zero variance indicates estimation issues (collinearity, convergence failure, model misspecification)."
    source_reference: "tools_inventory.md section 'Module: tools.validation' - validate_variance_positivity"

  validate_data_columns:
    module: "tools.validation"
    function: "validate_data_columns"
    signature: "validate_data_columns(df: DataFrame, required_columns: List[str]) -> Dict[str, Any]"

    input_files:
      - path: "data/step04_random_effects.csv"
        required_columns: ["UID", "random_intercept", "random_slope", "total_intercept", "total_slope"]
        source: "analysis tool output (step04_extract_random_effects)"

    parameters:
      required_columns: ["UID", "random_intercept", "random_slope", "total_intercept", "total_slope"]

    criteria:
      - "All required columns present in DataFrame"
      - "Case-sensitive column name matching"
      - "Column order irrelevant"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        missing_columns: "List[str]"
        existing_columns: "List[str]"
        n_required: "int"
        n_missing: "int"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step04_random_effects_extraction.log"
      invoke: "g_debug (master invokes)"

    description: "Validate that required columns exist in DataFrame. Simple column presence check for data quality."
    source_reference: "tools_inventory.md section 'Module: tools.validation' - validate_data_columns"

  validate_model_convergence:
    module: "tools.validation"
    function: "validate_model_convergence"
    signature: "validate_model_convergence(lmm_result: MixedLMResults) -> Dict[str, Any]"

    input_files: []  # Validates Python object, not file

    parameters:
      lmm_result: "MixedLMResults object (loaded from RQ 5.7 pickle file)"

    criteria:
      - "Model converged attribute = True"
      - "No convergence warnings in model object"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        message: "str"
        converged: "bool"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step01_load_dependencies.log"
      invoke: "g_debug (master invokes)"

    description: "Validate statsmodels LMM model converged successfully. Checks model.converged attribute to ensure optimization algorithm reached a solution."
    source_reference: "tools_inventory.md section 'Module: tools.validation' - validate_model_convergence"

summary:
  analysis_tools_count: 2
  validation_tools_count: 6
  total_unique_tools: 8
  mandatory_decisions_embedded: ["D068"]
  rq_specific_notes:
    - "RQ 5.13 is variance decomposition ONLY - no IRT calibration, no new LMM fitting"
    - "All analysis uses DERIVED data from RQ 5.7 (saved LMM model + theta scores + TSVR)"
    - "Decision D068 enforced via test_intercept_slope_correlation_d068 (dual p-values)"
    - "Step 1-2-4 use standard library (pandas, pickle) - no custom tools required"
    - "Validation coverage: 100% (all 5 steps have validation requirements)"
