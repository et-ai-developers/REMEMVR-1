#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step02
Step Name: extract_variance_components
RQ: results/ch5/rq13
Generated: 2025-11-30

PURPOSE:
Extract variance-covariance matrix from random effects and residual variance
to enable ICC computation and individual differences quantification. This step
decomposes the total variance in forgetting trajectories into: (1) between-person
variance in baseline ability (intercepts), (2) between-person variance in forgetting
rates (slopes), (3) covariance between baseline and rate (how they relate), and
(4) within-person residual variance (measurement error).

EXPECTED INPUTS:
  - data/step01_model_metadata.yaml
    Format: YAML metadata documenting loaded LMM model from RQ 5.7
    Expected keys: model_source, model_type, n_participants, n_observations,
                   random_effects, converged
    Source: Step 1 output (stdlib loading operation)

EXPECTED OUTPUTS:
  - data/step02_variance_components.csv
    Columns: ['component', 'estimate']
    Format: CSV with 5 rows (var_intercept, var_slope, cov_int_slope,
            var_residual, cor_int_slope)
    Expected rows: 5 (one per variance component)

  - logs/step02_variance_extraction.log
    Format: Text log with extraction details, formulas, validation checks

VALIDATION CRITERIA:
  - var_intercept > 0 (variance must be positive)
  - var_slope > 0 (variance must be positive)
  - var_residual > 0 (variance must be positive)
  - cor_int_slope in [-1, 1] (correlation bounds)
  - No NaN values (all components must be estimated)

g_code REASONING:
- Approach: Extract variance components directly from statsmodels MixedLMResults
  object attributes (cov_re for random effects covariance matrix, scale for
  residual variance)
- Why this approach: LMM model object contains all estimated variance components
  in standardized format. Direct extraction avoids re-computation and ensures
  consistency with original model fit.
- Data flow: Loaded model object (Step 1) -> Extract cov_re and scale ->
  Compute correlation from covariance -> Save to CSV
- Expected performance: ~seconds (pure data extraction, no computation)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib operations (pandas, numpy) - extract from model object
- Validation tool: tools.validation.validate_variance_positivity
- Parameters: Model object from Step 1 (lmm_model variable loaded via pickle)
- statsmodels structure: cov_re is DataFrame with random effects covariance,
  scale is float for residual variance
- Correlation formula: cor = cov / sqrt(var1 * var2)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import traceback
import pickle
import yaml

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_variance_positivity

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/rq13 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step02_variance_extraction.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step02_variance_components.csv
#   CORRECT: logs/step02_variance_extraction.log
#   WRONG:   results/variance_components.csv  (wrong folder + no prefix)
#   WRONG:   data/variance_components.csv     (missing step prefix)
#   WRONG:   logs/step02_variance.csv         (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 02: Extract Variance Components from LMM")

        # =========================================================================
        # STEP 1: Load Model Object from Step 1
        # =========================================================================
        # Expected: Model object saved in Step 1 (lmm_model variable in memory)
        # Purpose: Access variance-covariance matrix and residual variance

        log("[LOAD] Loading model metadata from Step 1...")

        # Read metadata to get model source path
        metadata_path = RQ_DIR / "data" / "step01_model_metadata.yaml"
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = yaml.safe_load(f)

        log(f"[LOADED] Model metadata: {metadata_path}")
        log(f"  Model source: {metadata['model_source']}")
        log(f"  Model type: {metadata['model_type']}")
        log(f"  Converged: {metadata['converged']}")
        log(f"  Participants: {metadata['n_participants']}")
        log(f"  Observations: {metadata['n_observations']}")

        # Load model object from pickle
        model_path = RQ_DIR.parent.parent / metadata['model_source'].replace('results/', '')
        log(f"[LOAD] Loading model object from {model_path}...")

        # Statsmodels pickle workaround: manually bypass patsy formula re-evaluation
        # The model object is valid for variance extraction even if formula fails to re-evaluate
        log("[INFO] Using statsmodels pickle workaround for patsy compatibility")

        lmm_model = None
        model_loaded = False

        # Monkey-patch statsmodels data class to skip formula re-evaluation
        try:
            from statsmodels.base import data
            original_setstate = data.ModelData.__setstate__

            def patched_setstate(self, d):
                """Skip formula re-evaluation that causes patsy errors"""
                try:
                    # Try normal __setstate__
                    original_setstate(self, d)
                except AttributeError as e:
                    if "'NoneType' object has no attribute 'f_locals'" in str(e):
                        # Expected error - skip formula re-evaluation
                        # Manually set attributes we need (frame, orig_endog, orig_exog)
                        self.__dict__.update({k: v for k, v in d.items() if k != 'formula'})
                        log("[INFO] Skipped patsy formula re-evaluation (not needed for variance extraction)")
                    else:
                        raise

            # Apply patch
            data.ModelData.__setstate__ = patched_setstate

            # Now load pickle
            with open(model_path, 'rb') as f:
                lmm_model = pickle.load(f)
                model_loaded = True
                log("[PASS] Model loaded successfully with patsy workaround")

            # Restore original __setstate__
            data.ModelData.__setstate__ = original_setstate

        except Exception as e:
            log(f"[ERROR] Failed to load model even with patsy workaround: {str(e)}")
            log("[FAIL] Cannot proceed without model object")
            import traceback
            log(traceback.format_exc())
            sys.exit(1)

        log(f"[LOADED] Model object successfully loaded")
        log(f"  Type: {type(lmm_model).__name__}")

        # =========================================================================
        # STEP 2: Extract Variance Components
        # =========================================================================
        # Tool: Direct extraction from statsmodels MixedLMResults attributes
        # What it does: Extracts variance-covariance matrix from random effects
        #               and residual variance from model fit
        # Expected output: 5 variance components with positive variances

        log("[ANALYSIS] Extracting variance components from LMM...")

        # Extract random effects covariance matrix
        # cov_re is a DataFrame with random effects covariance (intercept, slope)
        cov_re = lmm_model.cov_re
        log(f"[EXTRACT] Random effects covariance matrix shape: {cov_re.shape}")
        log(f"[EXTRACT] Covariance matrix:\n{cov_re}")

        # Extract variance components from diagonal and off-diagonal elements
        # Diagonal [0,0] = variance of random intercepts
        # Diagonal [1,1] = variance of random slopes
        # Off-diagonal [0,1] = covariance between intercepts and slopes
        var_intercept = cov_re.iloc[0, 0]
        var_slope = cov_re.iloc[1, 1]
        cov_int_slope = cov_re.iloc[0, 1]

        log(f"[EXTRACT] var_intercept (baseline variance): {var_intercept:.6f}")
        log(f"[EXTRACT] var_slope (forgetting rate variance): {var_slope:.6f}")
        log(f"[EXTRACT] cov_int_slope (intercept-slope covariance): {cov_int_slope:.6f}")

        # Extract residual variance
        # scale attribute contains residual variance (within-person error)
        var_residual = lmm_model.scale
        log(f"[EXTRACT] var_residual (within-person variance): {var_residual:.6f}")

        # Compute correlation from covariance
        # Formula: cor = cov / sqrt(var1 * var2)
        # This standardizes covariance to [-1, 1] range for interpretability
        cor_int_slope = cov_int_slope / np.sqrt(var_intercept * var_slope)
        log(f"[COMPUTE] cor_int_slope (standardized correlation): {cor_int_slope:.6f}")
        log(f"  Formula: cov_int_slope / sqrt(var_intercept * var_slope)")
        log(f"  = {cov_int_slope:.6f} / sqrt({var_intercept:.6f} * {var_slope:.6f})")
        log(f"  = {cov_int_slope:.6f} / {np.sqrt(var_intercept * var_slope):.6f}")

        log("[DONE] Variance components extracted")

        # =========================================================================
        # STEP 3: Save Variance Components to CSV
        # =========================================================================
        # Output: data/step02_variance_components.csv
        # Contains: 5 rows (one per variance component)
        # Columns: component (string), estimate (float)

        log("[SAVE] Creating variance components DataFrame...")

        # Create DataFrame with component names and estimates
        variance_components = pd.DataFrame({
            'component': [
                'var_intercept',
                'var_slope',
                'cov_int_slope',
                'var_residual',
                'cor_int_slope'
            ],
            'estimate': [
                var_intercept,
                var_slope,
                cov_int_slope,
                var_residual,
                cor_int_slope
            ]
        })

        log(f"[CREATED] Variance components DataFrame ({len(variance_components)} rows)")
        log(f"\n{variance_components.to_string(index=False)}")

        # Save to CSV
        output_path = RQ_DIR / "data" / "step02_variance_components.csv"
        variance_components.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path} ({len(variance_components)} rows, {len(variance_components.columns)} cols)")

        # =========================================================================
        # STEP 4: Run Validation Tool
        # =========================================================================
        # Tool: tools.validation.validate_variance_positivity
        # Validates: All variance components > 0, correlation in [-1, 1]
        # Threshold: Variance > 0 (strict positivity), |correlation| <= 1

        log("[VALIDATION] Running validate_variance_positivity...")

        # Filter to only variance components (exclude covariance and correlation)
        # Covariance and correlation can be negative, so we only validate variances
        variance_only = variance_components[
            variance_components['component'].str.contains('var_')
        ].copy()

        log(f"[VALIDATION] Checking {len(variance_only)} variance components (excluding cov/cor)")

        validation_result = validate_variance_positivity(
            variance_df=variance_only,
            component_col='component',
            value_col='estimate'
        )

        # Report validation results
        if validation_result['valid']:
            log(f"[VALIDATION] PASS - All variance components valid")
            log(f"  Message: {validation_result['message']}")
        else:
            log(f"[VALIDATION] FAIL - {validation_result['message']}")
            if 'negative_components' in validation_result and validation_result['negative_components']:
                log(f"  Negative/zero components: {validation_result['negative_components']}")
            raise ValueError(f"Validation failed: {validation_result['message']}")

        # Additional validation for correlation bounds (not in validate_variance_positivity)
        if abs(cor_int_slope) > 1.0:
            log(f"[VALIDATION] FAIL - Correlation out of bounds: {cor_int_slope}")
            raise ValueError(f"Correlation out of bounds: cor_int_slope = {cor_int_slope}, expected in [-1, 1]")
        else:
            log(f"[VALIDATION] PASS - Correlation in bounds: {cor_int_slope} in [-1, 1]")

        log("[SUCCESS] Step 02 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
