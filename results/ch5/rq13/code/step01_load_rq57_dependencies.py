#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step01
Step Name: Load RQ 5.7 Dependencies
RQ: results/ch5/rq13
Generated: 2025-11-30

PURPOSE:
Load saved LMM model object, theta scores, and TSVR mapping from RQ 5.7 to
enable variance decomposition analysis. This step is the entry point for RQ 5.13,
which decomposes variance in forgetting rates from RQ 5.7's best-fitting model.

EXPECTED INPUTS (from RQ 5.7):
  1. ../rq7/data/lmm_Log.pkl (relative from RQ directory)
     Absolute: results/ch5/rq7/data/lmm_Log.pkl
     Format: Python pickle file (statsmodels MixedLMResults object)
     Required Attributes: cov_re, scale, random_effects, converged
     Description: Best-fitting Logarithmic LMM (AIC=873.71) with random intercepts and slopes

  2. ../rq7/data/step03_theta_scores.csv (relative from RQ directory)
     Absolute: results/ch5/rq7/data/step03_theta_scores.csv
     Columns: [UID, test, Theta_All]
     Expected rows: ~400 (100 participants × 4 sessions)
     Description: IRT theta scores from RQ 5.7 Step 3 (Pass 2 purified, All factor)

  3. ../rq7/data/step04_lmm_input.csv (relative from RQ directory)
     Absolute: results/ch5/rq7/data/step04_lmm_input.csv
     Columns: [composite_ID, UID, test, Theta, SE, TSVR_hours, Days, Days_squared, log_Days_plus1]
     Expected rows: ~400 (100 participants × 4 sessions)
     Description: LMM input with TSVR_hours (time variable) from RQ 5.7 Step 4

EXPECTED OUTPUTS:
  1. data/step01_model_metadata.yaml
     Format: YAML with UTF-8 encoding
     Content: Model metadata (source, type, n_participants, n_observations, random_effects, converged)
     Purpose: Documents loaded LMM model for downstream steps
     Expected keys: model_source, model_type, n_participants, n_observations, random_effects, converged

  2. logs/step01_load_dependencies.log
     Format: Text log
     Content: Loading confirmation messages, file sizes, row counts, validation checks
     Purpose: Audit trail for debugging if validation fails

VALIDATION CRITERIA:
  1. Model converged attribute = True
  2. No convergence warnings in model object
  3. n_participants = 100 (all participants from RQ 5.7)
  4. n_observations = 380-400 (allowing for minor data loss)

g_code REASONING:
  Approach: Standard library file loading with pickle + pandas, followed by validation
  Why this approach: STDLIB OPERATION type means no catalogued tool - just basic Python
  Data flow: Load pickle (MixedLMResults) -> Load CSVs (DataFrames) -> Extract metadata
  Expected performance: ~1-2 seconds (file I/O only, no computation)

IMPLEMENTATION NOTES:
  Analysis tool: stdlib (pickle.load, pd.read_csv, yaml.dump)
  Validation tool: tools.validation.validate_model_convergence
  Key parameters: None (stdlib operations have no parameters)
  Critical: Circuit breaker check - QUIT with EXPECTATIONS ERROR if any dependency file missing
"""
# =============================================================================

import sys
import pickle
import yaml
from pathlib import Path
import pandas as pd
from typing import Dict, Any, Tuple
import traceback
from datetime import datetime

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool (if available)
try:
    from tools.validation import validate_model_convergence
    HAS_VALIDATION_TOOL = True
except ImportError:
    HAS_VALIDATION_TOOL = False

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/chX/rqY (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step01_load_dependencies.log"

# Create logs directory if it doesn't exist
LOG_FILE.parent.mkdir(parents=True, exist_ok=True)

# Relative paths from RQ directory to RQ 5.7 artifacts
# These are RELATIVE paths as specified in 4_analysis.yaml
# CHANGED: Using Lin+Log model instead of Log (Log model has singular covariance, ΔAIC=0.8)
RQ57_LMM_MODEL = RQ_DIR / "../rq7/data/lmm_Lin+Log.pkl"
RQ57_THETA_SCORES = RQ_DIR / "../rq7/data/step03_theta_scores.csv"
RQ57_LMM_INPUT = RQ_DIR / "../rq7/data/step04_lmm_input.csv"

# Output paths (relative to RQ directory)
METADATA_OUTPUT = RQ_DIR / "data" / "step01_model_metadata.yaml"
METADATA_OUTPUT.parent.mkdir(parents=True, exist_ok=True)

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step05_lmm_model_comparison.csv
#   CORRECT: logs/step01_load_dependencies.log
#   WRONG:   results/lmm_model_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv             (missing step prefix)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg: str) -> None:
    """Write message to both console and log file (UTF-8 encoding)."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    formatted_msg = f"[{timestamp}] {msg}"

    # Write to console (ASCII-safe)
    print(formatted_msg)

    # Write to log file (UTF-8)
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(formatted_msg + "\n")


# =============================================================================
# Validation Functions
# =============================================================================

def validate_files_exist() -> bool:
    """
    Circuit breaker check: Validate all three RQ 5.7 dependency files exist.

    If ANY file is missing, print EXPECTATIONS ERROR and return False.
    """
    missing_files = []

    if not RQ57_LMM_MODEL.exists():
        missing_files.append(f"../rq7/data/lmm_Lin+Log.pkl (resolved: {RQ57_LMM_MODEL})")
    if not RQ57_THETA_SCORES.exists():
        missing_files.append(f"../rq7/data/step03_theta_scores.csv (resolved: {RQ57_THETA_SCORES})")
    if not RQ57_LMM_INPUT.exists():
        missing_files.append(f"../rq7/data/step04_lmm_input.csv (resolved: {RQ57_LMM_INPUT})")

    if missing_files:
        error_msg = """
EXPECTATIONS ERROR: To perform Step 1 (Load RQ 5.7 Dependencies) I expect:
  - ../rq7/data/lmm_Log.pkl (best-fitting LMM model)
  - ../rq7/data/step03_theta_scores.csv (theta scores)
  - ../rq7/data/step04_lmm_input.csv (LMM input with TSVR_hours)

But missing:
"""
        for f in missing_files:
            error_msg += f"  - {f}\n"

        error_msg += """
Action: RQ 5.7 must complete Steps 1-5 before RQ 5.13 can execute.
Run RQ 5.7 workflow first, then retry RQ 5.13.
"""
        log(error_msg)
        print(error_msg, file=sys.stderr)
        return False

    return True


def validate_csv_columns(df: pd.DataFrame, expected_columns: list, file_path: str) -> bool:
    """
    Validate that CSV has expected columns.

    Args:
        df: Loaded dataframe
        expected_columns: List of required column names
        file_path: Path for error reporting

    Returns:
        True if columns match, False otherwise
    """
    actual_columns = df.columns.tolist()

    if actual_columns != expected_columns:
        error_msg = f"[ERROR] CSV column mismatch in {file_path}"
        error_msg += f"\n  Expected: {expected_columns}"
        error_msg += f"\n  Actual: {actual_columns}"
        log(error_msg)
        return False

    return True


def validate_csv_rows(df: pd.DataFrame, expected_range: Tuple[int, int], file_path: str) -> bool:
    """
    Validate that CSV has expected number of rows.

    Args:
        df: Loaded dataframe
        expected_range: Tuple of (min_rows, max_rows)
        file_path: Path for error reporting

    Returns:
        True if row count in range, False otherwise
    """
    min_rows, max_rows = expected_range
    actual_rows = len(df)

    if not (min_rows <= actual_rows <= max_rows):
        error_msg = f"[ERROR] CSV row count out of range in {file_path}"
        error_msg += f"\n  Expected: {min_rows}-{max_rows} rows"
        error_msg += f"\n  Actual: {actual_rows} rows"
        log(error_msg)
        return False

    return True


def validate_model_object(model_obj: Any) -> bool:
    """
    Validate that loaded object is a valid statsmodels MixedLMResults.

    Args:
        model_obj: Loaded model object

    Returns:
        True if valid MixedLMResults, False otherwise
    """
    if model_obj is None:
        log("[ERROR] Model object is None (pickle likely corrupted)")
        return False

    # Check for expected attributes of MixedLMResults
    required_attrs = ['cov_re', 'scale', 'random_effects']
    missing_attrs = []
    for attr in required_attrs:
        if not hasattr(model_obj, attr):
            missing_attrs.append(attr)

    if missing_attrs:
        log(f"[ERROR] Model missing required attributes: {missing_attrs}")
        return False

    return True


# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 01: Load RQ 5.7 Dependencies")
        log("=" * 70)

        # =====================================================================
        # STEP 1: Circuit Breaker Check (Validate all files exist)
        # =====================================================================
        log("[CHECK] Validating RQ 5.7 dependency files...")

        if not validate_files_exist():
            log("[FAIL] Circuit breaker: Missing RQ 5.7 dependency files")
            sys.exit(1)

        log("[PASS] All three RQ 5.7 dependency files exist")

        # =====================================================================
        # STEP 2: Load LMM Model (Pickle)
        # =====================================================================
        log("[LOAD] Loading LMM model from pickle file...")
        log(f"  File: {RQ57_LMM_MODEL}")
        log(f"  Size: {RQ57_LMM_MODEL.stat().st_size / 1024:.1f} KB")

        # Statsmodels pickle workaround: manually bypass patsy formula re-evaluation
        # The model object is valid for variance extraction even if formula fails to re-evaluate
        log("[INFO] Using statsmodels pickle workaround for patsy compatibility")

        lmm_model = None
        model_loaded = False

        # Monkey-patch statsmodels data class to skip formula re-evaluation
        try:
            from statsmodels.base import data
            original_setstate = data.ModelData.__setstate__

            def patched_setstate(self, d):
                """Skip formula re-evaluation that causes patsy errors"""
                try:
                    # Try normal __setstate__
                    original_setstate(self, d)
                except AttributeError as e:
                    if "'NoneType' object has no attribute 'f_locals'" in str(e):
                        # Expected error - skip formula re-evaluation
                        # Manually set attributes we need (frame, orig_endog, orig_exog)
                        self.__dict__.update({k: v for k, v in d.items() if k != 'formula'})
                        log("[INFO] Skipped patsy formula re-evaluation (not needed for variance extraction)")
                    else:
                        raise

            # Apply patch
            data.ModelData.__setstate__ = patched_setstate

            # Now load pickle
            with open(RQ57_LMM_MODEL, 'rb') as f:
                lmm_model = pickle.load(f)
                model_loaded = True
                log("[PASS] LMM model loaded successfully with patsy workaround")

            # Restore original __setstate__
            data.ModelData.__setstate__ = original_setstate

        except Exception as e:
            log(f"[ERROR] Failed to load model even with patsy workaround: {str(e)}")
            log("[FAIL] Cannot proceed without model object")
            import traceback
            log(traceback.format_exc())
            sys.exit(1)

        if not model_loaded or lmm_model is None:
            log("[ERROR] Model object is None after loading")
            log("[FAIL] Cannot proceed")
            sys.exit(1)

        # Validate model object
        if not validate_model_object(lmm_model):
            log("[FAIL] Loaded object is not a valid MixedLMResults model")
            sys.exit(1)

        log("[PASS] Successfully loaded model from pickle file")
        log(f"  Model type: {type(lmm_model).__name__}")

        # =====================================================================
        # STEP 3: Load Theta Scores CSV
        # =====================================================================
        log("[LOAD] Loading theta scores from CSV...")
        log(f"  File: {RQ57_THETA_SCORES}")

        try:
            df_theta = pd.read_csv(RQ57_THETA_SCORES, encoding='utf-8')
        except Exception as e:
            log(f"[ERROR] Failed to load theta scores CSV: {str(e)}")
            sys.exit(1)

        # Validate columns
        if not validate_csv_columns(df_theta, ['UID', 'test', 'Theta_All'], str(RQ57_THETA_SCORES)):
            log("[FAIL] Theta scores CSV columns invalid")
            sys.exit(1)

        # Validate row count
        if not validate_csv_rows(df_theta, (380, 400), str(RQ57_THETA_SCORES)):
            log("[FAIL] Theta scores CSV row count out of range")
            sys.exit(1)

        log(f"[PASS] Successfully loaded theta scores: {len(df_theta)} rows, {len(df_theta.columns)} columns")

        # =====================================================================
        # STEP 4: Load LMM Input CSV (TSVR mapping)
        # =====================================================================
        log("[LOAD] Loading LMM input with TSVR mapping from CSV...")
        log(f"  File: {RQ57_LMM_INPUT}")

        try:
            df_lmm_input = pd.read_csv(RQ57_LMM_INPUT, encoding='utf-8')
        except Exception as e:
            log(f"[ERROR] Failed to load LMM input CSV: {str(e)}")
            sys.exit(1)

        # Validate columns
        expected_lmm_cols = ['composite_ID', 'UID', 'test', 'Theta', 'SE',
                             'TSVR_hours', 'Days', 'Days_squared', 'log_Days_plus1']
        if not validate_csv_columns(df_lmm_input, expected_lmm_cols, str(RQ57_LMM_INPUT)):
            log("[FAIL] LMM input CSV columns invalid")
            sys.exit(1)

        # Validate row count
        if not validate_csv_rows(df_lmm_input, (380, 400), str(RQ57_LMM_INPUT)):
            log("[FAIL] LMM input CSV row count out of range")
            sys.exit(1)

        log(f"[PASS] Successfully loaded LMM input: {len(df_lmm_input)} rows, {len(df_lmm_input.columns)} columns")

        # =====================================================================
        # STEP 5: Extract Model Metadata
        # =====================================================================
        log("[EXTRACT] Extracting model metadata...")

        # Get model metadata
        n_groups = len(lmm_model.model.group_labels)
        n_observations = len(lmm_model.model.endog)  # Endogenous variable (y values)
        model_converged = lmm_model.converged

        # Extract random effects structure
        random_effects_structure = []
        if hasattr(lmm_model, 'random_effects') and lmm_model.random_effects:
            first_uid = list(lmm_model.random_effects.keys())[0]
            re_data = lmm_model.random_effects[first_uid]
            if hasattr(re_data, 'columns'):
                random_effects_structure = re_data.columns.tolist()
            elif hasattr(re_data, 'index'):
                random_effects_structure = re_data.index.tolist()

        # Build metadata dict
        metadata = {
            'model_source': 'results/ch5/rq7/data/lmm_Lin+Log.pkl',
            'model_formula': 'Lin+Log (Theta ~ Days + log(Days+1))',
            'model_type': type(lmm_model).__name__,
            'n_participants': n_groups,
            'n_observations': n_observations,
            'random_effects': random_effects_structure,
            'converged': model_converged,
            'loaded_timestamp': datetime.now().isoformat(),
        }

        log(f"[EXTRACT] Model metadata extracted:")
        for key, value in metadata.items():
            log(f"  {key}: {value}")

        # =====================================================================
        # STEP 6: Save Metadata to YAML
        # =====================================================================
        log(f"[SAVE] Saving model metadata to {METADATA_OUTPUT}...")

        try:
            with open(METADATA_OUTPUT, 'w', encoding='utf-8') as f:
                yaml.dump(metadata, f, default_flow_style=False, allow_unicode=True)
        except Exception as e:
            log(f"[ERROR] Failed to save metadata YAML: {str(e)}")
            sys.exit(1)

        log("[PASS] Successfully saved metadata.yaml")

        # =====================================================================
        # STEP 7: Validation (using tools.validation if available)
        # =====================================================================
        log("[VALIDATE] Running model convergence validation...")

        if HAS_VALIDATION_TOOL:
            try:
                validation_result = validate_model_convergence(lmm_model)

                # Log validation results
                if isinstance(validation_result, dict):
                    for key, value in validation_result.items():
                        log(f"[VALIDATION] {key}: {value}")

                    # Check for failures
                    if validation_result.get('converged') == False:
                        log("[FAIL] Model did not converge - RQ 5.7 may have issues")
                        sys.exit(1)
                else:
                    log(f"[VALIDATION] {validation_result}")

            except Exception as e:
                log(f"[WARN] Validation tool error: {str(e)}")
                log("[INFO] Falling back to manual validation checks...")

                # Manual checks
                if not model_converged:
                    log("[FAIL] Model did not converge (converged=False)")
                    sys.exit(1)

                if n_groups != 100:
                    log(f"[WARN] Expected 100 participants, got {n_groups}")

                if not (380 <= n_observations <= 400):
                    log(f"[WARN] Expected 380-400 observations, got {n_observations}")

                log("[PASS] Manual validation checks passed")
        else:
            # Validation tool not available, use manual checks
            log("[INFO] Validation tool not available, using manual checks...")

            if not model_converged:
                log("[FAIL] Model did not converge (converged=False)")
                sys.exit(1)

            if n_groups != 100:
                log(f"[WARN] Expected 100 participants, got {n_groups}")

            if not (380 <= n_observations <= 400):
                log(f"[WARN] Expected 380-400 observations, got {n_observations}")

            log("[PASS] Manual validation checks passed")

        # =====================================================================
        # SUMMARY
        # =====================================================================
        log("=" * 70)
        log("[SUCCESS] Step 01 complete: Successfully loaded RQ 5.7 dependencies")
        log(f"  Model: {n_groups} participants, {n_observations} observations, converged={model_converged}")
        log(f"  Theta scores: {len(df_theta)} rows")
        log(f"  LMM input: {len(df_lmm_input)} rows")
        log(f"  Metadata: {METADATA_OUTPUT}")
        log("=" * 70)

        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] Unexpected error: {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
