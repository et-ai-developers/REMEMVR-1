#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step02
Step Name: step02_fit_lmm
RQ: results/ch5/rq10
Generated: 2025-11-28

PURPOSE:
Fit Linear Mixed Model testing whether age effects on forgetting rate vary by
memory domain (3-way Age x Domain x Time interaction).

EXPECTED INPUTS:
  - data/step01_lmm_input.csv
    Columns: ['UID', 'composite_ID', 'test', 'domain', 'theta', 'TSVR_hours', 'log_TSVR', 'age', 'Age_c', 'mean_age']
    Format: Long-format LMM input with theta scores, TSVR, and grand-mean centered Age
    Expected rows: ~1200 (100 participants x 4 tests x 3 domains)

EXPECTED OUTPUTS:
  - data/step02_lmm_model.pkl
    Format: Pickled statsmodels MixedLMResults object
    Expected: Fitted LMM with 3-way Age x Domain x Time interaction

  - data/step02_lmm_summary.txt
    Format: Text summary of model (fixed effects table, random effects variance, fit statistics)
    Expected: Complete model summary with ~20 fixed effects terms

  - data/step02_fixed_effects.csv
    Columns: ['term', 'estimate', 'se', 'z', 'p', 'CI_lower', 'CI_upper']
    Format: Fixed effects coefficients table
    Expected rows: ~20 (including 3-way interaction terms)

VALIDATION CRITERIA:
  - Model converged (no convergence warnings)
  - No singular fit (random effects variance > 0)
  - All fixed effects have finite estimates (no NaN/Inf)
  - 3-way interaction terms present in fixed effects

g_code REASONING:
- Approach: Fit LMM using TSVR (actual hours since encoding) with 3-way interaction
- Why this approach: Tests whether age effects on forgetting vary by domain (key hypothesis)
- Data flow: Step01 LMM input -> Fit LMM with Age x Domain x Time -> Model object + summary + fixed effects table
- Expected performance: ~30-60 seconds (100 participants, ~1200 observations, complex interaction structure)

IMPLEMENTATION NOTES:
- Analysis tool: fit_lmm_trajectory_tsvr from tools.analysis_lmm
- Validation tool: validate_lmm_convergence from tools.validation
- Parameters: Complex formula with Age_c:domain:TSVR_hours and Age_c:domain:log_TSVR 3-way interactions
- Random effects: Random slopes for TSVR_hours per participant (captures individual forgetting rates)
- REML=False: ML estimation required for subsequent model comparison (LRT in step02c)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import pickle
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_lmm import fit_lmm_trajectory

# Import validation tool
from tools.validation import validate_lmm_convergence

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/rq10 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step02_fit_lmm.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step05_lmm_model_comparison.csv
#   CORRECT: data/step03_theta_scores.csv
#   WRONG:   results/lmm_model_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv             (missing step prefix)
#   WRONG:   logs/step02_removed_items.csv     (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 02: Fit LMM with 3-Way Age x Domain x Time Interaction")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Long-format LMM input from step01_prepare_lmm_input
        # Purpose: Theta scores merged with TSVR and grand-mean centered Age

        log("[LOAD] Loading LMM input data...")
        input_file = RQ_DIR / "data/step01_lmm_input.csv"
        lmm_input = pd.read_csv(input_file)
        log(f"[LOADED] {input_file.name} ({len(lmm_input)} rows, {len(lmm_input.columns)} cols)")

        # Log data structure for transparency
        log(f"[INFO] Columns: {list(lmm_input.columns)}")
        log(f"[INFO] Unique UIDs: {lmm_input['UID'].nunique()}")
        log(f"[INFO] Domains: {sorted(lmm_input['domain'].unique())}")
        log(f"[INFO] Tests: {sorted(lmm_input['test'].unique())}")
        log(f"[INFO] Age range: {lmm_input['age'].min():.1f} - {lmm_input['age'].max():.1f}")
        log(f"[INFO] Age_c range: {lmm_input['Age_c'].min():.2f} - {lmm_input['Age_c'].max():.2f}")

        # =========================================================================
        # STEP 2: Run Analysis Tool
        # =========================================================================
        # Tool: fit_lmm_trajectory_tsvr
        # What it does: Fit LMM using TSVR (actual hours) as time variable with complex interaction structure
        # Expected output: MixedLMResults object with converged model

        log("[ANALYSIS] Fitting LMM with 3-way Age x Domain x Time interaction...")
        log("[INFO] Formula: theta ~ TSVR_hours + log_TSVR + Age_c + domain + TSVR_hours:Age_c + log_TSVR:Age_c + TSVR_hours:domain + log_TSVR:domain + Age_c:domain + TSVR_hours:Age_c:domain + log_TSVR:Age_c:domain")
        log("[INFO] Random effects: ~TSVR_hours (random slopes per participant)")
        log("[INFO] REML=False (ML estimation for model comparison)")

        # Define formula (complex 3-way interaction structure)
        # Main effects: TSVR_hours, log_TSVR, Age_c, domain
        # 2-way interactions: TSVR:Age, log_TSVR:Age, TSVR:domain, log_TSVR:domain, Age:domain
        # 3-way interactions: TSVR:Age:domain, log_TSVR:Age:domain
        formula = "theta ~ TSVR_hours + log_TSVR + Age_c + domain + TSVR_hours:Age_c + log_TSVR:Age_c + TSVR_hours:domain + log_TSVR:domain + Age_c:domain + TSVR_hours:Age_c:domain + log_TSVR:Age_c:domain"

        # Call analysis tool
        # Note: Using fit_lmm_trajectory (not fit_lmm_trajectory_tsvr) because
        # our data is already merged and in the correct format
        lmm_model = fit_lmm_trajectory(
            data=lmm_input,           # Merged LMM input with all variables
            formula=formula,
            groups="UID",             # Grouping variable for random effects
            re_formula="~TSVR_hours", # Random slopes for TSVR_hours per participant
            reml=False                # ML estimation (required for LRT in step02c)
        )

        log("[DONE] LMM fitting complete")
        log(f"[INFO] Model converged: {lmm_model.converged}")
        log(f"[INFO] Number of observations: {lmm_model.nobs}")
        log(f"[INFO] Number of groups: {len(lmm_model.model.group_labels)}")
        log(f"[INFO] Log-likelihood: {lmm_model.llf:.2f}")
        log(f"[INFO] AIC: {lmm_model.aic:.2f}")
        log(f"[INFO] BIC: {lmm_model.bic:.2f}")

        # =========================================================================
        # STEP 3: Save Analysis Outputs
        # =========================================================================
        # These outputs will be used by: step02b (assumption validation), step02c (model selection), step03 (interaction extraction)

        # Output 1: Model object (pickle)
        model_file = RQ_DIR / "data/step02_lmm_model.pkl"
        log(f"[SAVE] Saving model object to {model_file.name}...")
        # IMPORTANT: Use statsmodels.save() method, NOT pickle.dump()
        # Reason: pickle.dump() causes patsy/eval errors on load
        lmm_model.save(str(model_file))
        log(f"[SAVED] {model_file.name} (statsmodels MixedLMResults object)")

        # Output 2: Model summary (text)
        summary_file = RQ_DIR / "data/step02_lmm_summary.txt"
        log(f"[SAVE] Saving model summary to {summary_file.name}...")
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(str(lmm_model.summary()))
        log(f"[SAVED] {summary_file.name} (full model summary with fixed effects table)")

        # Output 3: Fixed effects table (CSV)
        fixed_effects_file = RQ_DIR / "data/step02_fixed_effects.csv"
        log(f"[SAVE] Saving fixed effects table to {fixed_effects_file.name}...")

        # Extract fixed effects from model
        fe_summary = lmm_model.summary().tables[1]  # Fixed effects table
        fe_df = pd.DataFrame({
            'term': fe_summary.index,
            'estimate': lmm_model.params,
            'se': lmm_model.bse,
            'z': lmm_model.tvalues,
            'p': lmm_model.pvalues,
            'CI_lower': lmm_model.conf_int()[0],
            'CI_upper': lmm_model.conf_int()[1]
        })

        fe_df.to_csv(fixed_effects_file, index=False, encoding='utf-8')
        log(f"[SAVED] {fixed_effects_file.name} ({len(fe_df)} rows, {len(fe_df.columns)} cols)")
        log(f"[INFO] Fixed effects include {len(fe_df)} terms")

        # Log key interaction terms for verification
        interaction_terms = fe_df[fe_df['term'].str.contains(':Age_c:domain', na=False)]
        log(f"[INFO] 3-way interaction terms found: {len(interaction_terms)}")
        for _, row in interaction_terms.iterrows():
            log(f"  - {row['term']}: beta={row['estimate']:.4f}, p={row['p']:.4f}")

        # =========================================================================
        # STEP 4: Run Validation Tool
        # =========================================================================
        # Tool: validate_lmm_convergence
        # Validates: Model convergence status and warnings
        # Threshold: Model must have converged successfully

        log("[VALIDATION] Running validate_lmm_convergence...")
        validation_result = validate_lmm_convergence(
            lmm_result=lmm_model
        )

        # Report validation results
        # Expected: converged=True, no warnings
        if isinstance(validation_result, dict):
            for key, value in validation_result.items():
                log(f"[VALIDATION] {key}: {value}")
        else:
            log(f"[VALIDATION] {validation_result}")

        # Check validation passed
        if isinstance(validation_result, dict) and not validation_result.get('converged', False):
            log("[VALIDATION] [FAIL] Model did not converge")
            log(f"[VALIDATION] Message: {validation_result.get('message', 'Unknown error')}")
            sys.exit(1)

        log("[VALIDATION] [PASS] Model converged successfully")

        # Additional manual checks per validation criteria
        log("[VALIDATION] Checking additional criteria...")

        # Check 1: No singular fit (random effects variance > 0)
        re_variance = lmm_model.cov_re.iloc[0, 0]  # Random intercept variance
        if re_variance > 0:
            log(f"[VALIDATION] [PASS] Random effects variance > 0 ({re_variance:.4f})")
        else:
            log(f"[VALIDATION] [FAIL] Singular fit detected (random effects variance = {re_variance})")
            sys.exit(1)

        # Check 2: All fixed effects have finite estimates (no NaN/Inf)
        if fe_df['estimate'].isna().any() or fe_df['estimate'].isin([float('inf'), float('-inf')]).any():
            log("[VALIDATION] [FAIL] Fixed effects contain NaN or Inf values")
            sys.exit(1)
        log("[VALIDATION] [PASS] All fixed effects have finite estimates")

        # Check 3: 3-way interaction terms present
        required_interactions = ['TSVR_hours:Age_c:domain', 'log_TSVR:Age_c:domain']
        found_interactions = []
        for interaction in required_interactions:
            if fe_df['term'].str.contains(interaction, na=False).any():
                found_interactions.append(interaction)

        if len(found_interactions) == len(required_interactions):
            log(f"[VALIDATION] [PASS] All required 3-way interaction terms present: {found_interactions}")
        else:
            missing = set(required_interactions) - set(found_interactions)
            log(f"[VALIDATION] [FAIL] Missing 3-way interaction terms: {missing}")
            sys.exit(1)

        log("[SUCCESS] Step 02 complete")
        log("[INFO] Model ready for assumption validation (step02b) and model selection (step02c)")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
