#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step06
Step Name: Prepare Plot Data
RQ: results/ch5/rq8
Generated: 2025-11-28

PURPOSE:
Aggregate observed means and model predictions for piecewise vs continuous
visualization. Combines raw observed data, quadratic model predictions, and
piecewise model predictions into a single plot-ready CSV file for the two-panel
comparison visualization.

EXPECTED INPUTS:
  - data/step00_theta_tsvr.csv
    Columns: ['UID', 'test', 'TSVR_hours', 'theta']
    Format: Observed theta scores (domain-collapsed mean theta)
    Expected rows: ~400 (100 participants × 4 tests)

  - data/step02_quadratic_predictions.csv
    Columns: ['Time', 'predicted_theta', 'CI_lower', 'CI_upper']
    Format: Quadratic model predictions on 11-point grid (0, 24, 48, ..., 240 hours)
    Expected rows: 11 (prediction grid timepoints)

  - data/step03_piecewise_predictions.csv
    Columns: ['Segment', 'Days_within', 'TSVR_hours', 'predicted_theta', 'CI_lower', 'CI_upper']
    Format: Piecewise model predictions per segment (Early + Late)
    Expected rows: 18 (9 Early + 9 Late timepoints)

EXPECTED OUTPUTS:
  - plots/step06_piecewise_comparison_data.csv
    Columns: ['source', 'TSVR_hours', 'theta', 'CI_lower', 'CI_upper', 'Segment']
    Format: Combined plot data for two-panel piecewise vs continuous comparison
    Expected rows: 33 exactly (4 observed + 11 quadratic + 18 piecewise)

VALIDATION CRITERIA:
  - All 3 sources present (Observed, Quadratic, Piecewise)
  - Expected row count: 33 exactly
  - No NaN in critical columns (source, TSVR_hours, theta, CIs)
  - Segment can be NA for non-piecewise sources

g_code REASONING:
- Approach: Aggregate observed data by test session (4 timepoints), load
  precomputed model predictions from Steps 2-3, stack into single DataFrame
  with 'source' column to distinguish Observed vs Quadratic vs Piecewise.
- Why this approach: Plot rendering (rq_plots agent) requires single CSV with
  all data sources tagged. Separating aggregation (this step) from rendering
  (rq_plots) follows v4.X atomic agent design.
- Data flow: Observed data (Step 0) + Quadratic predictions (Step 2) +
  Piecewise predictions (Step 3) → Single plot CSV with source tags.
- Expected performance: ~seconds (simple aggregation + file I/O)

IMPLEMENTATION NOTES:
- Analysis tool: prepare_piecewise_plot_data from tools.plotting
- Validation tool: validate_plot_data_completeness from tools.validation
- Parameters: segment_values=['Early', 'Late'], factor_values=['Observed', 'Quadratic', 'Piecewise']
- NOTE: prepare_piecewise_plot_data designed for piecewise LMM with factors
  (e.g., Common/Congruent/Incongruent). RQ 5.8 has NO factor dimension
  (domain-collapsed). Use 'source' as pseudo-factor to distinguish data sources.
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback
import pickle

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.plotting import prepare_piecewise_plot_data

# Import validation tool
from tools.validation import validate_plot_data_completeness

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/chX/rqY (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step06_prepare_plot_data.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step05_lmm_model_comparison.csv
#   CORRECT: data/step03_theta_scores.csv
#   WRONG:   results/lmm_model_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv             (missing step prefix)
#   WRONG:   logs/step02_removed_items.csv     (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 6: Prepare Plot Data")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Observed theta scores (Step 0), Quadratic predictions (Step 2),
        #           Piecewise predictions (Step 3)
        # Purpose: Combine all data sources into single plot-ready CSV

        log("[LOAD] Loading observed theta scores from Step 0...")
        observed_data = pd.read_csv(RQ_DIR / "data" / "step00_theta_tsvr.csv", encoding='utf-8')
        log(f"[LOADED] step00_theta_tsvr.csv ({len(observed_data)} rows, {len(observed_data.columns)} cols)")
        log(f"[INFO] Observed data columns: {list(observed_data.columns)}")

        log("[LOAD] Loading quadratic model predictions from Step 2...")
        quadratic_preds = pd.read_csv(RQ_DIR / "data" / "step02_quadratic_predictions.csv", encoding='utf-8')
        log(f"[LOADED] step02_quadratic_predictions.csv ({len(quadratic_preds)} rows, {len(quadratic_preds.columns)} cols)")
        log(f"[INFO] Quadratic predictions columns: {list(quadratic_preds.columns)}")

        log("[LOAD] Loading piecewise model predictions from Step 3...")
        piecewise_preds = pd.read_csv(RQ_DIR / "data" / "step03_piecewise_predictions.csv", encoding='utf-8')
        log(f"[LOADED] step03_piecewise_predictions.csv ({len(piecewise_preds)} rows, {len(piecewise_preds.columns)} cols)")
        log(f"[INFO] Piecewise predictions columns: {list(piecewise_preds.columns)}")

        # =========================================================================
        # STEP 2: Aggregate Observed Data by Test Session
        # =========================================================================
        # Aggregate observed theta scores by test session (4 timepoints)
        # Compute mean theta and 95% CI per timepoint

        log("[AGGREGATE] Computing observed means per test session...")

        # Group by test session, compute mean theta and SEM
        observed_agg = observed_data.groupby('test')['theta'].agg(['mean', 'sem', 'count']).reset_index()

        # Compute 95% CI: mean ± 1.96 × SEM
        observed_agg['CI_lower'] = observed_agg['mean'] - 1.96 * observed_agg['sem']
        observed_agg['CI_upper'] = observed_agg['mean'] + 1.96 * observed_agg['sem']

        # Merge with TSVR_hours mapping (median TSVR per test session)
        tsvr_mapping = observed_data.groupby('test')['TSVR_hours'].median().reset_index()
        observed_agg = observed_agg.merge(tsvr_mapping, on='test', how='left')

        # Rename columns for consistency
        observed_agg = observed_agg.rename(columns={'mean': 'theta'})

        # Add source column
        observed_agg['source'] = 'Observed'
        observed_agg['Segment'] = None  # No segment for observed data

        # Select final columns
        observed_agg = observed_agg[['source', 'TSVR_hours', 'theta', 'CI_lower', 'CI_upper', 'Segment']]

        log(f"[AGGREGATED] Observed data: {len(observed_agg)} timepoints (4 test sessions)")
        log(f"[INFO] Observed aggregated shape: {observed_agg.shape}")

        # =========================================================================
        # STEP 3: Format Quadratic Predictions
        # =========================================================================
        # Quadratic predictions already on prediction grid (0, 24, 48, ..., 240)
        # Just add source column and rename to match output schema

        log("[FORMAT] Formatting quadratic model predictions...")

        quadratic_formatted = quadratic_preds.copy()
        quadratic_formatted['source'] = 'Quadratic'
        quadratic_formatted['Segment'] = None  # No segment for continuous model
        quadratic_formatted = quadratic_formatted.rename(columns={'Time': 'TSVR_hours'})

        # Select final columns (match observed_agg schema)
        quadratic_formatted = quadratic_formatted[['source', 'TSVR_hours', 'predicted_theta', 'CI_lower', 'CI_upper', 'Segment']]
        quadratic_formatted = quadratic_formatted.rename(columns={'predicted_theta': 'theta'})

        log(f"[FORMATTED] Quadratic predictions: {len(quadratic_formatted)} timepoints")
        log(f"[INFO] Quadratic formatted shape: {quadratic_formatted.shape}")

        # =========================================================================
        # STEP 4: Format Piecewise Predictions
        # =========================================================================
        # Piecewise predictions have Segment column (Early/Late)
        # Add source column and select matching schema

        log("[FORMAT] Formatting piecewise model predictions...")

        piecewise_formatted = piecewise_preds.copy()
        piecewise_formatted['source'] = 'Piecewise'
        piecewise_formatted = piecewise_formatted.rename(columns={'predicted_theta': 'theta'})

        # Select final columns (TSVR_hours already present from Step 3)
        piecewise_formatted = piecewise_formatted[['source', 'TSVR_hours', 'theta', 'CI_lower', 'CI_upper', 'Segment']]

        log(f"[FORMATTED] Piecewise predictions: {len(piecewise_formatted)} timepoints")
        log(f"[INFO] Piecewise formatted shape: {piecewise_formatted.shape}")

        # =========================================================================
        # STEP 5: Combine All Data Sources
        # =========================================================================
        # Stack Observed + Quadratic + Piecewise into single DataFrame
        # Expected: 4 observed + 11 quadratic + 18 piecewise = 33 rows

        log("[COMBINE] Stacking all data sources...")

        plot_data = pd.concat([observed_agg, quadratic_formatted, piecewise_formatted], ignore_index=True)

        log(f"[COMBINED] Total plot data: {len(plot_data)} rows")
        log(f"[INFO] Expected: 33 rows (4 observed + 11 quadratic + 18 piecewise)")
        log(f"[INFO] Source counts:")
        for source in plot_data['source'].unique():
            count = len(plot_data[plot_data['source'] == source])
            log(f"  - {source}: {count} rows")

        # =========================================================================
        # STEP 6: Save Combined Plot Data
        # =========================================================================
        # Save to plots/ folder (plot-ready CSV, not intermediate data)

        output_path = RQ_DIR / "plots" / "step06_piecewise_comparison_data.csv"
        log(f"[SAVE] Saving combined plot data to {output_path.name}...")

        plot_data.to_csv(output_path, index=False, encoding='utf-8')

        log(f"[SAVED] {output_path.name} ({len(plot_data)} rows, {len(plot_data.columns)} cols)")

        # =========================================================================
        # STEP 7: Run Validation Tool
        # =========================================================================
        # Validate all 3 sources present, expected row count, no NaN in critical columns

        log("[VALIDATION] Running validate_plot_data_completeness...")

        validation_result = validate_plot_data_completeness(
            plot_data=plot_data,
            required_domains=['Observed', 'Quadratic', 'Piecewise'],  # Using 'source' as domain
            required_groups=['Early', 'Late'],  # Segments (only Piecewise has these)
            domain_col='source',
            group_col='Segment'
        )

        # Report validation results
        if isinstance(validation_result, dict):
            for key, value in validation_result.items():
                log(f"[VALIDATION] {key}: {value}")
        else:
            log(f"[VALIDATION] {validation_result}")

        # Additional manual validation (expected row count)
        if len(plot_data) != 33:
            log(f"[WARNING] Expected 33 rows, got {len(plot_data)}")
        else:
            log("[VALIDATION] Row count correct: 33 rows")

        # Check for NaN in critical columns
        critical_cols = ['source', 'TSVR_hours', 'theta', 'CI_lower', 'CI_upper']
        nan_check = plot_data[critical_cols].isna().sum()
        if nan_check.any():
            log(f"[WARNING] NaN values found in critical columns: {nan_check[nan_check > 0].to_dict()}")
        else:
            log("[VALIDATION] No NaN in critical columns")

        # Check Segment can be NA for non-piecewise sources
        non_piecewise_segment = plot_data[plot_data['source'] != 'Piecewise']['Segment']
        if non_piecewise_segment.isna().all():
            log("[VALIDATION] Segment correctly NA for non-piecewise sources")
        else:
            log(f"[WARNING] Segment should be NA for non-piecewise sources: {non_piecewise_segment.value_counts()}")

        log("[SUCCESS] Step 6 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
