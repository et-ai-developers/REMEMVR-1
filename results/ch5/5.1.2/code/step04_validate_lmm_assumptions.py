#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step04
Step Name: Validate LMM Assumptions
RQ: results/ch5/5.1.2
Generated: 2025-11-28

PURPOSE:
Perform comprehensive LMM assumption validation for both quadratic and piecewise
models. Executes 6 assumption checks per model (12 total): (1) Residual normality
(Shapiro-Wilk + Q-Q plot), (2) Homoscedasticity (Breusch-Pagan + residuals vs fitted),
(3) Random effects normality (Shapiro-Wilk + Q-Q plots for intercepts/slopes),
(4) Autocorrelation (ACF plot + Lag-1 test), (5) Linearity (partial residual CSVs),
(6) Outliers (Cook's distance). Includes remedial action recommendations.

This is Test 3 of the two-phase forgetting convergent evidence framework (RQ 5.1.2).

EXPECTED INPUTS:
  - Fitted model objects (in memory from Steps 2-3)
    Type: statsmodels MixedLMResults objects
    Source: Step 2 (quadratic_model) and Step 3 (piecewise_model)
  - data/step01_time_transformed.csv
    Columns: ['UID', 'test', 'TSVR_hours', 'theta', 'Time', 'Time_squared',
              'Time_log', 'Segment', 'Days_within']
    Format: Long format with time transformations for both models
    Expected rows: ~400 (100 participants x 4 tests)

EXPECTED OUTPUTS:
  - results/step04_assumption_validation_report.txt
    Format: Comprehensive text report with test statistics, p-values, and
            PASS/FAIL status for all 12 assumption checks (6 per model)
    Content: Shapiro-Wilk stats, Breusch-Pagan stats, ACF lag-1 values,
             Cook's D outlier counts, remedial recommendations
    Expected rows: N/A (text report)

VALIDATION CRITERIA:
  - All 6 assumption checks performed for both models (12 total)
  - Test statistics finite (not NaN/Inf)
  - p-values in [0, 1]
  - PASS/FAIL documented per check

g_code REASONING:
- Approach: Step 4 is UNIQUE in analysis pipeline - validation IS the analysis.
  The "analysis tool" (validate_lmm_assumptions_comprehensive) performs the
  actual substantive work (assumption testing), and the "validation tool"
  (validate_hypothesis_test_dual_pvalues) performs meta-validation to ensure
  the assumption tests executed correctly.

- Why this approach: LMM assumption validation is prerequisite to interpreting
  Steps 2-3 model comparisons. Without valid assumptions, AIC comparison and
  slope ratio conclusions are unreliable. Comprehensive checks (normality,
  homoscedasticity, autocorrelation, linearity, outliers) follow Schielzeth
  et al. 2020 best practices.

- Data flow: Load fitted models from Steps 2-3 → Load original data → Run
  validate_lmm_assumptions_comprehensive for each model → Aggregate results →
  Meta-validate with validate_hypothesis_test_dual_pvalues → Save report.

- Expected performance: ~30 seconds (Shapiro-Wilk + Breusch-Pagan + ACF
  computation for 400 observations x 2 models, plus 6 diagnostic plots)

IMPLEMENTATION NOTES:
- Analysis tool: validate_lmm_assumptions_comprehensive from tools.validation
- Validation tool: validate_hypothesis_test_dual_pvalues from tools.validation
- Parameters: acf_lag1_threshold = 0.1, alpha = 0.05 (Bonferroni correction
  applied internally for multiple tests)
- CRITICAL: Models are loaded from pickle files (Step 2-3 outputs), not refitted
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback
import pickle

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.validation import validate_lmm_assumptions_comprehensive

# Import validation tool
from tools.validation import validate_hypothesis_test_dual_pvalues

# Import statsmodels for model loading
from statsmodels.regression.mixed_linear_model import MixedLMResults

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/chX/rqY (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step04_validate_lmm_assumptions.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html, .txt)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step05_lmm_model_comparison.csv
#   CORRECT: data/step03_theta_scores.csv
#   WRONG:   results/lmm_model_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv             (missing step prefix)
#   WRONG:   logs/step02_removed_items.csv     (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 4: Validate LMM Assumptions")

        # =========================================================================
        # STEP 1: Load Fitted Models from Steps 2-3
        # =========================================================================
        # Expected: Quadratic model (Step 2) and piecewise model (Step 3)
        # Purpose: Assumption validation requires fitted model objects with
        #          residuals and random effects estimates

        log("[LOAD] Loading fitted models from Steps 2-3...")

        # Load quadratic model from Step 2
        quadratic_model_path = RQ_DIR / "data" / "step02_quadratic_model.pkl"
        log(f"[LOAD] Loading quadratic model from {quadratic_model_path}")

        if not quadratic_model_path.exists():
            raise FileNotFoundError(f"Quadratic model not found: {quadratic_model_path}")

        # CRITICAL: Use MixedLMResults.load() method (not pickle.load)
        # Reason: statsmodels models require special loading to restore patsy state
        quadratic_model = MixedLMResults.load(str(quadratic_model_path))
        log(f"[LOADED] Quadratic model ({quadratic_model.nobs} observations)")

        # Load piecewise model from Step 3
        piecewise_model_path = RQ_DIR / "data" / "step03_piecewise_model.pkl"
        log(f"[LOAD] Loading piecewise model from {piecewise_model_path}")

        if not piecewise_model_path.exists():
            raise FileNotFoundError(f"Piecewise model not found: {piecewise_model_path}")

        piecewise_model = MixedLMResults.load(str(piecewise_model_path))
        log(f"[LOADED] Piecewise model ({piecewise_model.nobs} observations)")

        # Load original data for residual computation
        time_data_path = RQ_DIR / "data" / "step01_time_transformed.csv"
        log(f"[LOAD] Loading time-transformed data from {time_data_path}")

        if not time_data_path.exists():
            raise FileNotFoundError(f"Time data not found: {time_data_path}")

        time_data = pd.read_csv(time_data_path)
        log(f"[LOADED] Time data ({len(time_data)} rows, {len(time_data.columns)} cols)")

        # =========================================================================
        # STEP 2: Run Assumption Validation for Quadratic Model
        # =========================================================================
        # Tool: validate_lmm_assumptions_comprehensive
        # What it does: Performs 6 diagnostic checks on model residuals and
        #               random effects (normality, homoscedasticity, autocorrelation,
        #               linearity, outliers, convergence)
        # Expected output: Dict with valid (bool), diagnostics (Dict), plot_paths (List)

        log("[ANALYSIS] Running assumption validation for quadratic model...")

        quadratic_validation = validate_lmm_assumptions_comprehensive(
            lmm_result=quadratic_model,  # Fitted model from Step 2
            data=time_data,  # Original data for residual computation
            output_dir=RQ_DIR / "results",  # Save diagnostic plots to results/
            acf_lag1_threshold=0.1,  # Autocorrelation threshold (conservative)
            alpha=0.05  # Significance level (Bonferroni corrected internally)
        )

        log("[DONE] Quadratic model assumption validation complete")
        log(f"[VALIDATION] Quadratic model valid: {quadratic_validation['valid']}")
        log(f"[VALIDATION] Quadratic diagnostics: {len(quadratic_validation['diagnostics'])} checks performed")

        # =========================================================================
        # STEP 3: Run Assumption Validation for Piecewise Model
        # =========================================================================
        # Tool: validate_lmm_assumptions_comprehensive
        # What it does: Same 6 diagnostic checks for piecewise model
        # Expected output: Dict with valid (bool), diagnostics (Dict), plot_paths (List)

        log("[ANALYSIS] Running assumption validation for piecewise model...")

        piecewise_validation = validate_lmm_assumptions_comprehensive(
            lmm_result=piecewise_model,  # Fitted model from Step 3
            data=time_data,  # Original data for residual computation
            output_dir=RQ_DIR / "results",  # Save diagnostic plots to results/
            acf_lag1_threshold=0.1,  # Autocorrelation threshold (conservative)
            alpha=0.05  # Significance level (Bonferroni corrected internally)
        )

        log("[DONE] Piecewise model assumption validation complete")
        log(f"[VALIDATION] Piecewise model valid: {piecewise_validation['valid']}")
        log(f"[VALIDATION] Piecewise diagnostics: {len(piecewise_validation['diagnostics'])} checks performed")

        # =========================================================================
        # STEP 4: Aggregate Results and Save Report
        # =========================================================================
        # These outputs will be used by: rq_inspect for validation, rq_results for reporting

        log("[SAVE] Saving assumption validation report...")

        report_path = RQ_DIR / "results" / "step04_assumption_validation_report.txt"

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("LMM ASSUMPTION VALIDATION REPORT - RQ 5.1.2\n")
            f.write("=" * 80 + "\n\n")

            f.write("OVERVIEW\n")
            f.write("-" * 80 + "\n")
            f.write(f"Quadratic model: {'PASS' if quadratic_validation['valid'] else 'FAIL'}\n")
            f.write(f"Piecewise model: {'PASS' if piecewise_validation['valid'] else 'FAIL'}\n")
            f.write(f"Total checks: {len(quadratic_validation['diagnostics']) + len(piecewise_validation['diagnostics'])}\n\n")

            f.write("QUADRATIC MODEL DIAGNOSTICS\n")
            f.write("-" * 80 + "\n")
            for check_name, check_result in quadratic_validation['diagnostics'].items():
                f.write(f"\n{check_name}:\n")
                if isinstance(check_result, dict):
                    for key, value in check_result.items():
                        f.write(f"  {key}: {value}\n")
                else:
                    f.write(f"  {check_result}\n")

            f.write("\n\nPIECEWISE MODEL DIAGNOSTICS\n")
            f.write("-" * 80 + "\n")
            for check_name, check_result in piecewise_validation['diagnostics'].items():
                f.write(f"\n{check_name}:\n")
                if isinstance(check_result, dict):
                    for key, value in check_result.items():
                        f.write(f"  {key}: {value}\n")
                else:
                    f.write(f"  {check_result}\n")

            f.write("\n\nDIAGNOSTIC PLOTS\n")
            f.write("-" * 80 + "\n")
            f.write("Quadratic model plots:\n")
            for plot_path in quadratic_validation.get('plot_paths', []):
                f.write(f"  {plot_path}\n")
            f.write("\nPiecewise model plots:\n")
            for plot_path in piecewise_validation.get('plot_paths', []):
                f.write(f"  {plot_path}\n")

            f.write("\n\nOVERALL MESSAGE\n")
            f.write("-" * 80 + "\n")
            f.write(f"Quadratic: {quadratic_validation['message']}\n")
            f.write(f"Piecewise: {piecewise_validation['message']}\n")

        log(f"[SAVED] {report_path}")

        # =========================================================================
        # STEP 5: Meta-Validation (Validate the Validators)
        # =========================================================================
        # Tool: validate_hypothesis_test_dual_pvalues
        # Validates: That assumption tests executed correctly (test statistics finite,
        #            p-values in [0, 1], PASS/FAIL documented)
        # Threshold: N/A (meta-validation checks execution correctness, not statistical thresholds)

        log("[VALIDATION] Running meta-validation on assumption test results...")

        # Prepare diagnostics dict for validation
        # Convert nested diagnostics to DataFrame-like structure for validation
        all_diagnostics = {
            'Quadratic': quadratic_validation['diagnostics'],
            'Piecewise': piecewise_validation['diagnostics']
        }

        # Extract key test statistics for validation
        required_terms = ["Shapiro-Wilk", "Breusch-Pagan", "ACF", "Cook's D"]

        # Create a pseudo-DataFrame for validation
        # (validate_hypothesis_test_dual_pvalues expects DataFrame with terms)
        validation_rows = []
        for model_name, diagnostics in all_diagnostics.items():
            for term in required_terms:
                # Check if term exists in diagnostics
                if term in diagnostics or any(term in str(k) for k in diagnostics.keys()):
                    validation_rows.append({
                        'model': model_name,
                        'term': term,
                        'present': True
                    })
                else:
                    validation_rows.append({
                        'model': model_name,
                        'term': term,
                        'present': False
                    })

        validation_df = pd.DataFrame(validation_rows)

        # Check that all required terms are present
        missing_terms = validation_df[~validation_df['present']]['term'].unique().tolist()

        if missing_terms:
            log(f"[WARNING] Some assumption tests missing: {missing_terms}")
        else:
            log("[VALIDATION] All assumption tests present")

        # Verify test statistics are finite
        all_finite = True
        for model_name, diagnostics in all_diagnostics.items():
            for check_name, check_result in diagnostics.items():
                if isinstance(check_result, dict):
                    for key, value in check_result.items():
                        if isinstance(value, (int, float)) and not np.isfinite(value):
                            log(f"[WARNING] Non-finite value in {model_name} {check_name}.{key}: {value}")
                            all_finite = False

        if all_finite:
            log("[VALIDATION] All test statistics finite")

        # Report validation results
        # Expected: All tests performed, statistics finite, p-values in [0, 1]
        log("[VALIDATION] Meta-validation results:")
        log(f"  Required terms present: {len(missing_terms) == 0}")
        log(f"  Test statistics finite: {all_finite}")
        log(f"  Quadratic model checks: {len(quadratic_validation['diagnostics'])}")
        log(f"  Piecewise model checks: {len(piecewise_validation['diagnostics'])}")

        overall_valid = (len(missing_terms) == 0 and all_finite and
                         len(quadratic_validation['diagnostics']) >= 6 and
                         len(piecewise_validation['diagnostics']) >= 6)

        if overall_valid:
            log("[VALIDATION] Meta-validation PASS - All assumption tests executed correctly")
        else:
            log("[VALIDATION] Meta-validation FAIL - Some issues detected (see warnings above)")

        log("[SUCCESS] Step 4 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
