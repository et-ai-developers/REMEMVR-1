#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step00
Step Name: Load Dependencies from RQ 5.5.1
RQ: results/ch5/5.5.4
Generated: 2025-12-05

PURPOSE:
Load IRT theta scores, purified items list, TSVR mapping from RQ 5.5.1 and
raw responses from dfData.csv. Reshape theta scores to long format for
downstream IRT-CTT convergence analysis. This is a dependency loading step
that prepares inputs from a completed upstream RQ.

EXPECTED INPUTS:
  - results/ch5/5.5.1/data/step03_theta_scores.csv
    Columns: ['composite_ID', 'theta_source', 'theta_destination', 'se_source', 'se_destination']
    Format: Wide format with one row per participant-test (400 rows)
    Expected rows: 400 (100 participants x 4 tests)

  - results/ch5/5.5.1/data/step02_purified_items.csv
    Columns: ['item_tag', 'factor', 'a', 'b', 'retention_reason']
    Format: Item parameters from Pass 2 purification
    Expected rows: 25-32 items (typical retention rate 40-50%)

  - results/ch5/5.5.1/data/step00_tsvr_mapping.csv
    Columns: ['composite_ID', 'UID', 'test', 'TSVR_hours']
    Format: Time Since VR mapping per Decision D070
    Expected rows: 400 (100 participants x 4 tests)

  - data/cache/dfData.csv
    Columns: ['UID', 'test', ...item columns...]
    Format: Project-level master dataset
    Expected rows: 400 (100 participants x 4 tests)

EXPECTED OUTPUTS:
  - data/step00_irt_theta_from_rq551.csv
    Columns: ['composite_ID', 'UID', 'test', 'location_type', 'irt_theta', 'irt_se', 'TSVR_hours']
    Format: Long format with 800 rows (400 x 2 location types)
    Expected rows: 800

  - data/step00_purified_items_from_rq551.csv
    Columns: ['item_code', 'location_type']
    Format: Purified items list for CTT score computation
    Expected rows: 25-32

  - data/step00_raw_responses_filtered.csv
    Columns: ['composite_ID', 'UID', 'test', ...purified item columns...]
    Format: Raw VR responses filtered to purified items only
    Expected rows: 400

VALIDATION CRITERIA:
  - RQ 5.5.1 files exist (EXPECTATIONS ERROR if missing)
  - theta_long has 800 rows (400 x 2 location types)
  - raw_responses has 400 rows
  - purified_items count in [25, 32] range
  - irt_theta in [-3, 3] range
  - irt_se in [0.1, 1.5] range
  - TSVR_hours in [0, 168] range
  - No NaN in irt_theta, irt_se, TSVR_hours
  - location_type in {'source', 'destination'}
  - No duplicate composite_IDs in theta_long

g_code REASONING:
- Approach: Load upstream RQ outputs as-is, reshape wide theta to long format
- Why this approach: RQ 5.5.4 needs parallel IRT/CTT analyses, requiring both
  theta scores (from RQ 5.5.1 IRT) and raw responses (for CTT computation)
- Data flow: RQ 5.5.1 outputs → reshape theta → filter raw data → save for
  downstream CTT score computation and correlation analysis
- Expected performance: <5 seconds (lightweight data loading and reshaping)

IMPLEMENTATION NOTES:
- No analysis tools (stdlib operations only)
- Validation via inline checks (no validation tool function)
- Column name mapping: item_tag → item_code, factor → location_type
- composite_ID format: {UID}_{test} (e.g., "100_1" for UID=100, test=1)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 5.5.4/ (rqY)
#   parents[2] = ch5/ (chX)
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.5.4
LOG_FILE = RQ_DIR / "logs" / "step00_load_dependencies_from_rq551.log"

# Dependency paths from RQ 5.5.1
RQ_551_DIR = PROJECT_ROOT / "results" / "ch5" / "5.5.1"
THETA_SCORES_PATH = RQ_551_DIR / "data" / "step03_theta_scores.csv"
PURIFIED_ITEMS_PATH = RQ_551_DIR / "data" / "step02_purified_items.csv"
TSVR_MAPPING_PATH = RQ_551_DIR / "data" / "step00_tsvr_mapping.csv"

# Project-level data
DFDATA_PATH = PROJECT_ROOT / "data" / "cache" / "dfData.csv"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step00_irt_theta_from_rq551.csv
#   CORRECT: data/step00_purified_items_from_rq551.csv
#   WRONG:   results/irt_theta.csv  (wrong folder + no prefix)
#   WRONG:   data/theta.csv         (missing step prefix)
#   WRONG:   logs/step00_items.csv  (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 00: Load Dependencies from RQ 5.5.1")

        # =========================================================================
        # STEP 1: Check RQ 5.5.1 Files Exist (EXPECTATIONS ERROR if missing)
        # =========================================================================
        # Expected: RQ 5.5.1 has completed successfully with outputs available
        # Purpose: Validate dependencies before attempting to load data

        log("[CHECK] Verifying RQ 5.5.1 dependency files exist...")

        missing_files = []
        for filepath in [THETA_SCORES_PATH, PURIFIED_ITEMS_PATH, TSVR_MAPPING_PATH]:
            if not filepath.exists():
                missing_files.append(str(filepath))

        if missing_files:
            log("[ERROR] EXPECTATIONS ERROR: RQ 5.5.1 dependency files missing")
            for missing in missing_files:
                log(f"[MISSING] {missing}")
            log("[ACTION] Run RQ 5.5.1 first to generate required dependency files")
            sys.exit(1)

        log("[PASS] All RQ 5.5.1 dependency files exist")

        # Check dfData.csv exists
        if not DFDATA_PATH.exists():
            log(f"[ERROR] EXPECTATIONS ERROR: dfData.csv not found at {DFDATA_PATH}")
            log("[ACTION] Run data preparation pipeline to generate dfData.csv")
            sys.exit(1)

        log("[PASS] dfData.csv exists")

        # =========================================================================
        # STEP 2: Load RQ 5.5.1 Outputs
        # =========================================================================
        # Expected: Clean outputs from RQ 5.5.1 Pass 2 IRT calibration
        # Purpose: Import theta scores, purified items, TSVR mapping

        log("[LOAD] Loading RQ 5.5.1 outputs...")

        # Load theta scores (wide format: composite_ID, theta_source, theta_destination, se_source, se_destination)
        theta_scores_wide = pd.read_csv(THETA_SCORES_PATH, encoding='utf-8')
        log(f"[LOADED] {THETA_SCORES_PATH.name} ({len(theta_scores_wide)} rows, {len(theta_scores_wide.columns)} cols)")

        # Load purified items (item_tag, factor, a, b, retention_reason)
        purified_items = pd.read_csv(PURIFIED_ITEMS_PATH, encoding='utf-8')
        log(f"[LOADED] {PURIFIED_ITEMS_PATH.name} ({len(purified_items)} rows, {len(purified_items.columns)} cols)")

        # Load TSVR mapping (composite_ID, UID, test, TSVR_hours)
        tsvr_mapping = pd.read_csv(TSVR_MAPPING_PATH, encoding='utf-8')
        log(f"[LOADED] {TSVR_MAPPING_PATH.name} ({len(tsvr_mapping)} rows, {len(tsvr_mapping.columns)} cols)")

        # =========================================================================
        # STEP 3: Reshape Theta Scores to Long Format
        # =========================================================================
        # Expected: Wide format with 400 rows (composite_ID, theta_source, theta_destination)
        # Purpose: Convert to long format for downstream LMM analyses (800 rows)

        log("[RESHAPE] Converting theta scores from wide to long format...")

        # Split composite_ID into UID and test
        theta_scores_wide[['UID', 'test']] = theta_scores_wide['composite_ID'].str.split('_', expand=True)
        # UID is string format (e.g., "A010"), test is integer
        theta_scores_wide['test'] = theta_scores_wide['test'].astype(int)

        # Reshape to long format (source and destination as separate rows)
        theta_long_list = []

        for location_type, theta_col, se_col in [
            ('source', 'theta_source', 'se_source'),
            ('destination', 'theta_destination', 'se_destination')
        ]:
            df_subset = theta_scores_wide[['composite_ID', 'UID', 'test', theta_col, se_col]].copy()
            df_subset.rename(columns={theta_col: 'irt_theta', se_col: 'irt_se'}, inplace=True)
            df_subset['location_type'] = location_type
            theta_long_list.append(df_subset)

        theta_long = pd.concat(theta_long_list, ignore_index=True)
        log(f"[RESHAPED] Wide ({len(theta_scores_wide)} rows) -> Long ({len(theta_long)} rows)")

        # =========================================================================
        # STEP 4: Merge TSVR_hours into Theta Long Format
        # =========================================================================
        # Expected: TSVR mapping has composite_ID + TSVR_hours
        # Purpose: Add time variable for downstream trajectory analyses

        log("[MERGE] Adding TSVR_hours to theta long format...")

        theta_long = theta_long.merge(
            tsvr_mapping[['composite_ID', 'TSVR_hours']],
            on='composite_ID',
            how='left'
        )

        log(f"[MERGED] Theta long now has {len(theta_long)} rows with TSVR_hours")

        # =========================================================================
        # STEP 5: Prepare Purified Items List
        # =========================================================================
        # Expected: item_tag, factor columns (rename to match spec)
        # Purpose: Create clean item list for CTT score computation

        log("[PREPARE] Creating purified items list...")

        purified_items_list = purified_items[['item_tag', 'factor']].copy()
        purified_items_list.rename(columns={
            'item_tag': 'item_code',
            'factor': 'location_type'
        }, inplace=True)

        log(f"[PREPARED] Purified items list: {len(purified_items_list)} items")

        # =========================================================================
        # STEP 6: Load and Filter dfData to Purified Items
        # =========================================================================
        # Expected: dfData.csv with all VR item columns
        # Purpose: Extract raw responses for CTT score computation

        log("[LOAD] Loading dfData.csv and filtering to purified items...")

        df_data_raw = pd.read_csv(DFDATA_PATH, encoding='utf-8')
        log(f"[LOADED] {DFDATA_PATH.name} ({len(df_data_raw)} rows, {len(df_data_raw.columns)} cols)")

        # Get list of purified item columns
        purified_item_cols = purified_items_list['item_code'].tolist()

        # Check which purified items exist in dfData
        available_items = [col for col in purified_item_cols if col in df_data_raw.columns]
        missing_items = [col for col in purified_item_cols if col not in df_data_raw.columns]

        if missing_items:
            log(f"[WARNING] {len(missing_items)} purified items not found in dfData.csv:")
            for item in missing_items[:5]:  # Show first 5 missing items
                log(f"  - {item}")
            if len(missing_items) > 5:
                log(f"  ... and {len(missing_items) - 5} more")

        log(f"[FOUND] {len(available_items)} / {len(purified_item_cols)} purified items in dfData.csv")

        # Create composite_ID in dfData (columns are uppercase: UID, TEST)
        df_data_raw['composite_ID'] = df_data_raw['UID'].astype(str) + '_' + df_data_raw['TEST'].astype(str)
        # Rename TEST to test for consistency
        df_data_raw['test'] = df_data_raw['TEST']

        # Filter to UID, test, composite_ID, and available purified items
        keep_cols = ['composite_ID', 'UID', 'test'] + available_items
        raw_responses = df_data_raw[keep_cols].copy()

        log(f"[FILTERED] Raw responses: {len(raw_responses)} rows, {len(raw_responses.columns)} cols")

        # =========================================================================
        # STEP 7: Validation Checks
        # =========================================================================
        # Expected: Clean data meeting all validation criteria
        # Purpose: Ensure data quality before saving outputs

        log("[VALIDATION] Running validation checks...")

        validation_errors = []

        # Check theta_long row count (800 expected)
        if len(theta_long) != 800:
            validation_errors.append(f"theta_long has {len(theta_long)} rows (expected 800)")
        else:
            log("[PASS] theta_long has 800 rows")

        # Check raw_responses row count (400 expected)
        if len(raw_responses) != 400:
            validation_errors.append(f"raw_responses has {len(raw_responses)} rows (expected 400)")
        else:
            log("[PASS] raw_responses has 400 rows")

        # Check purified_items count in [25, 32] range
        n_purified = len(purified_items_list)
        if not (25 <= n_purified <= 32):
            validation_errors.append(f"purified_items has {n_purified} items (expected 25-32)")
        else:
            log(f"[PASS] purified_items has {n_purified} items (within 25-32 range)")

        # Check irt_theta range [-3, 3]
        theta_min = theta_long['irt_theta'].min()
        theta_max = theta_long['irt_theta'].max()
        if theta_min < -3 or theta_max > 3:
            validation_errors.append(f"irt_theta out of range [{theta_min:.2f}, {theta_max:.2f}] (expected [-3, 3])")
        else:
            log(f"[PASS] irt_theta in range [{theta_min:.2f}, {theta_max:.2f}]")

        # Check irt_se range [0.1, 1.5]
        se_min = theta_long['irt_se'].min()
        se_max = theta_long['irt_se'].max()
        if se_min < 0.1 or se_max > 1.5:
            validation_errors.append(f"irt_se out of range [{se_min:.2f}, {se_max:.2f}] (expected [0.1, 1.5])")
        else:
            log(f"[PASS] irt_se in range [{se_min:.2f}, {se_max:.2f}]")

        # Check TSVR_hours range [0, 360] - extended to 360h to account for participants tested beyond 1 week
        # This matches the TSVR_hours range extension applied in RQ 5.5.2
        tsvr_min = theta_long['TSVR_hours'].min()
        tsvr_max = theta_long['TSVR_hours'].max()
        if tsvr_min < 0 or tsvr_max > 360:
            validation_errors.append(f"TSVR_hours out of range [{tsvr_min:.2f}, {tsvr_max:.2f}] (expected [0, 360])")
        else:
            log(f"[PASS] TSVR_hours in range [{tsvr_min:.2f}, {tsvr_max:.2f}]")

        # Check for NaN values
        nan_cols = []
        for col in ['irt_theta', 'irt_se', 'TSVR_hours']:
            if theta_long[col].isna().any():
                nan_count = theta_long[col].isna().sum()
                nan_cols.append(f"{col} ({nan_count} NaN)")

        if nan_cols:
            validation_errors.append(f"NaN values found: {', '.join(nan_cols)}")
        else:
            log("[PASS] No NaN values in irt_theta, irt_se, TSVR_hours")

        # Check location_type values
        location_types = set(theta_long['location_type'].unique())
        expected_types = {'source', 'destination'}
        if location_types != expected_types:
            validation_errors.append(f"location_type has {location_types} (expected {expected_types})")
        else:
            log("[PASS] location_type in {'source', 'destination'}")

        # Check for duplicate composite_IDs
        duplicates = theta_long[theta_long.duplicated(subset=['composite_ID', 'location_type'], keep=False)]
        if len(duplicates) > 0:
            validation_errors.append(f"Found {len(duplicates)} duplicate composite_ID x location_type combinations")
        else:
            log("[PASS] No duplicate composite_IDs in theta_long")

        # Report validation results
        if validation_errors:
            log("[FAIL] Validation errors found:")
            for error in validation_errors:
                log(f"  - {error}")
            raise ValueError("Validation failed - see log for details")

        log("[PASS] All validation checks passed")

        # =========================================================================
        # STEP 8: Save Outputs
        # =========================================================================
        # Purpose: Write cleaned data for downstream CTT and correlation analyses

        log("[SAVE] Saving outputs...")

        # Save theta_long (800 rows)
        output_path_theta = RQ_DIR / "data" / "step00_irt_theta_from_rq551.csv"
        theta_long.to_csv(output_path_theta, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path_theta.name} ({len(theta_long)} rows, {len(theta_long.columns)} cols)")

        # Save purified_items_list (25-32 rows)
        output_path_items = RQ_DIR / "data" / "step00_purified_items_from_rq551.csv"
        purified_items_list.to_csv(output_path_items, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path_items.name} ({len(purified_items_list)} rows, {len(purified_items_list.columns)} cols)")

        # Save raw_responses (400 rows)
        output_path_responses = RQ_DIR / "data" / "step00_raw_responses_filtered.csv"
        raw_responses.to_csv(output_path_responses, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path_responses.name} ({len(raw_responses)} rows, {len(raw_responses.columns)} cols)")

        log("[SUCCESS] Step 00 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
