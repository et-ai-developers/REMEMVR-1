#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step02
Step Name: step02_compute_correlations
RQ: results/ch5/5.5.4
Generated: 2025-12-05

PURPOSE:
Compute Pearson correlations between IRT theta scores and CTT mean scores,
stratified by location type (Source, Destination, Overall). Apply Holm-Bonferroni
correction for multiple comparisons per Decision D068 dual p-value reporting.
Test convergence criteria: r > 0.70 (strong), r > 0.90 (exceptional).

EXPECTED INPUTS:
  - data/step00_irt_theta_from_rq551.csv
    Columns: ['composite_ID', 'location_type', 'irt_theta']
    Format: Long format IRT theta scores (800 rows = 400 participant-test x 2 location types)
    Expected rows: ~800

  - data/step01_ctt_scores.csv
    Columns: ['composite_ID', 'location_type', 'ctt_mean_score']
    Format: CTT mean scores per participant-test-location (800 rows)
    Expected rows: ~800

EXPECTED OUTPUTS:
  - data/step02_correlations.csv
    Columns: ['location_type', 'r', 'CI_lower', 'CI_upper', 'p_uncorrected', 'p_holm', 'n', 'threshold_0.70', 'threshold_0.90']
    Format: Pearson correlations with Holm-Bonferroni correction (3 rows: source, destination, overall)
    Expected rows: 3

VALIDATION CRITERIA:
  - p_uncorrected column present
  - p_holm correction present (Decision D068 dual p-value requirement)
  - p_holm >= p_uncorrected (correction cannot reduce p-value)
  - r in [-1, 1] range (correlation coefficient bounds)
  - Exactly 3 rows (source, destination, overall)
  - All location types present

g_code REASONING:
- Approach: Merge IRT and CTT scores on composite_ID + location_type, then compute
  Pearson correlations per location type (source, destination) plus overall (pooled).
  Apply Holm-Bonferroni correction for 3 comparisons.
- Why this approach: Tests measurement convergence between IRT (latent trait) and
  CTT (classical proportion correct) approaches for same construct. Stratification
  by location type tests whether convergence holds across both memory types.
- Data flow: Load IRT theta (800 rows) + CTT scores (800 rows) -> Merge on keys ->
  Compute correlations per location + overall -> Apply Holm correction -> Save
- Expected performance: ~seconds (simple correlation computation)

IMPLEMENTATION NOTES:
- Analysis tool: compute_pearson_correlations_with_correction from tools.analysis_ctt
- Validation tool: validate_correlation_test_d068 from tools.validation
- Parameters: irt_col='irt_theta', ctt_col='ctt_mean_score', factor_col='location_type',
  thresholds=[0.70, 0.90] (strong/exceptional convergence)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 5.5.4/
#   parents[2] = ch5/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_ctt import compute_pearson_correlations_with_correction

# Import validation tool
from tools.validation import validate_correlation_test_d068

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.5.4 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step02_compute_correlations.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step05_lmm_model_comparison.csv
#   CORRECT: data/step03_theta_scores.csv
#   WRONG:   results/lmm_model_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv             (missing step prefix)
#   WRONG:   logs/step02_removed_items.csv     (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 02: Compute Pearson Correlations between IRT and CTT Scores")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: IRT theta scores from RQ 5.5.1 Pass 2 calibration (800 rows)
        #           CTT mean scores computed in Step 1 (800 rows)
        # Purpose: Merge both measurement approaches for convergence analysis

        log("[LOAD] Loading IRT theta scores from RQ 5.5.1...")
        theta_long = pd.read_csv(RQ_DIR / "data" / "step00_irt_theta_from_rq551.csv", encoding='utf-8')
        log(f"[LOADED] step00_irt_theta_from_rq551.csv ({len(theta_long)} rows, {len(theta_long.columns)} cols)")

        log("[LOAD] Loading CTT mean scores from Step 1...")
        ctt_scores = pd.read_csv(RQ_DIR / "data" / "step01_ctt_scores.csv", encoding='utf-8')
        log(f"[LOADED] step01_ctt_scores.csv ({len(ctt_scores)} rows, {len(ctt_scores.columns)} cols)")

        # =========================================================================
        # STEP 2: Merge IRT and CTT Scores
        # =========================================================================
        # Tool: pandas merge on composite_ID + location_type
        # What it does: Combines IRT theta and CTT mean score for same participant-test-location
        # Expected output: 800 rows with both irt_theta and ctt_mean_score columns

        log("[MERGE] Merging IRT theta and CTT scores on composite_ID + location_type...")

        # Select required columns from each dataset
        theta_subset = theta_long[['composite_ID', 'location_type', 'irt_theta']].copy()
        ctt_subset = ctt_scores[['composite_ID', 'location_type', 'ctt_mean_score']].copy()

        # Merge on composite_ID + location_type
        merged_data = pd.merge(
            theta_subset,
            ctt_subset,
            on=['composite_ID', 'location_type'],
            how='inner',
            validate='one_to_one'  # Ensure no duplicates
        )

        log(f"[MERGED] Combined dataset: {len(merged_data)} rows")

        # Verify merge completeness
        if len(merged_data) != 800:
            log(f"[WARNING] Expected 800 rows after merge, got {len(merged_data)}")

        # Check for missing values
        missing_irt = merged_data['irt_theta'].isna().sum()
        missing_ctt = merged_data['ctt_mean_score'].isna().sum()
        if missing_irt > 0 or missing_ctt > 0:
            log(f"[WARNING] Missing values detected: IRT={missing_irt}, CTT={missing_ctt}")

        # =========================================================================
        # STEP 3: Run Analysis Tool - Compute Pearson Correlations
        # =========================================================================
        # Tool: compute_pearson_correlations_with_correction
        # What it does: Computes Pearson r per location type (source, destination)
        #               plus overall (all pooled), with Holm-Bonferroni correction
        # Expected output: 3 rows (source, destination, overall) with dual p-values

        log("[ANALYSIS] Running compute_pearson_correlations_with_correction...")
        log("[ANALYSIS] Parameters:")
        log("  irt_col: 'irt_theta'")
        log("  ctt_col: 'ctt_mean_score'")
        log("  factor_col: 'location_type'")
        log("  thresholds: [0.70, 0.90]")

        correlations = compute_pearson_correlations_with_correction(
            df=merged_data,
            irt_col='irt_theta',
            ctt_col='ctt_mean_score',
            factor_col='location_type',
            thresholds=[0.70, 0.90]
        )

        log("[DONE] Correlation analysis complete")
        log(f"[RESULT] Generated {len(correlations)} correlation results")
        log(f"[INFO] Output columns: {list(correlations.columns)}")

        # Rename 'factor' to 'location_type' for consistency with spec (tool outputs 'factor')
        if 'factor' in correlations.columns and 'location_type' not in correlations.columns:
            correlations = correlations.rename(columns={'factor': 'location_type'})
            log("[INFO] Renamed 'factor' -> 'location_type'")

        # =========================================================================
        # STEP 4: Save Analysis Outputs
        # =========================================================================
        # Output: step02_correlations.csv
        # Contains: Pearson correlations with dual p-values (uncorrected + Holm)
        # Columns: location_type, r, CI_lower, CI_upper, p_uncorrected, p_holm, n, threshold_0.70, threshold_0.90
        # These outputs will be used by: Step 5 (fixed effects comparison), rq_results (final report)

        output_path = RQ_DIR / "data" / "step02_correlations.csv"
        log(f"[SAVE] Saving {output_path.name}...")
        correlations.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] step02_correlations.csv ({len(correlations)} rows, {len(correlations.columns)} cols)")

        # Log correlation results for transparency
        log("[CORRELATIONS] Results summary:")
        for idx, row in correlations.iterrows():
            loc_type = row['location_type']
            r_val = row['r']
            ci_lower = row['CI_lower']
            ci_upper = row['CI_upper']
            p_uncorr = row['p_uncorrected']
            p_holm = row['p_holm']
            n_obs = row['n']
            log(f"  {loc_type}: r={r_val:.3f} [95% CI: {ci_lower:.3f}, {ci_upper:.3f}], p_uncorr={p_uncorr:.4f}, p_holm={p_holm:.4f}, n={n_obs}")

        # =========================================================================
        # STEP 5: Run Validation Tool - Validate D068 Compliance
        # =========================================================================
        # Tool: validate_correlation_test_d068
        # Validates: Decision D068 dual p-value reporting compliance
        # Criteria: p_uncorrected + p_holm columns present, p_holm >= p_uncorrected,
        #           r in [-1, 1], exactly 3 rows, all location types present

        log("[VALIDATION] Running validate_correlation_test_d068...")
        validation_result = validate_correlation_test_d068(
            correlation_df=correlations,
            required_cols=None  # Use default D068 columns
        )

        # Report validation results
        if validation_result['valid']:
            log("[VALIDATION] PASS - All D068 compliance checks passed")
            log(f"[VALIDATION] Message: {validation_result['message']}")
        else:
            log("[VALIDATION] FAIL - D068 compliance issues detected")
            log(f"[VALIDATION] Message: {validation_result['message']}")
            if validation_result.get('missing_cols'):
                log(f"[VALIDATION] Missing columns: {validation_result['missing_cols']}")
            raise ValueError(f"Validation failed: {validation_result['message']}")

        # Additional validation checks
        log("[VALIDATION] Additional checks...")

        # Check exactly 3 rows
        if len(correlations) != 3:
            raise ValueError(f"Expected 3 rows (source, destination, overall), got {len(correlations)}")
        log("[VALIDATION] Row count: 3 rows (PASS)")

        # Check r in [-1, 1]
        r_out_of_bounds = correlations[(correlations['r'] < -1) | (correlations['r'] > 1)]
        if len(r_out_of_bounds) > 0:
            raise ValueError(f"Correlation coefficient out of bounds: {r_out_of_bounds['r'].tolist()}")
        log("[VALIDATION] Correlation bounds: all r in [-1, 1] (PASS)")

        # Check p_holm >= p_uncorrected (correction cannot reduce p-value)
        violations = correlations[correlations['p_holm'] < correlations['p_uncorrected']]
        if len(violations) > 0:
            raise ValueError(f"Holm correction violation: p_holm < p_uncorrected for {violations['location_type'].tolist()}")
        log("[VALIDATION] Holm correction monotonicity: all p_holm >= p_uncorrected (PASS)")

        # Check all location types present
        expected_types = {'source', 'destination', 'Overall'}  # Note: Overall capitalized
        actual_types = set(correlations['location_type'].unique())
        missing_types = expected_types - actual_types
        if missing_types:
            raise ValueError(f"Missing location types: {missing_types}")
        log(f"[VALIDATION] Location types: {actual_types} (PASS)")

        log("[SUCCESS] Step 02 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
