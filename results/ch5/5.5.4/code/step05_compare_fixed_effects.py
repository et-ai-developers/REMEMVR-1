#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step05
Step Name: Compare Fixed Effects between IRT and CTT Models
RQ: results/ch5/5.5.4
Generated: 2025-12-05

PURPOSE:
Assess agreement between IRT-based and CTT-based LMM fixed effects using
Cohen's kappa and overall agreement percentage. This validates measurement
convergence: do IRT and CTT approaches yield equivalent statistical inferences
about location type effects on forgetting trajectories?

EXPECTED INPUTS:
  - data/step03_irt_lmm_model.pkl
    Format: Statsmodels MixedLMResults pickle object
    Contains: Fitted IRT-based LMM (theta ~ LocationType * log_TSVR)

  - data/step03_ctt_lmm_model.pkl
    Format: Statsmodels MixedLMResults pickle object
    Contains: Fitted CTT-based LMM (mean_score ~ LocationType * log_TSVR)

EXPECTED OUTPUTS:
  - data/step05_coefficient_comparison.csv
    Columns: term, irt_coef, irt_se, irt_p_uncorrected, irt_p_bonferroni,
             ctt_coef, ctt_se, ctt_p_uncorrected, ctt_p_bonferroni,
             sign_match, sig_match, agreement
    Expected rows: 4 (Intercept, LocationType, log_TSVR, interaction)
    Format: Fixed effects comparison with Decision D068 dual p-values

  - data/step05_agreement_metrics.csv
    Columns: cohens_kappa, kappa_threshold_met, overall_agreement_pct,
             agreement_threshold_met, n_terms, n_agreements
    Expected rows: 1
    Format: Cohen's kappa and overall agreement summary

VALIDATION CRITERIA:
  - cohens_kappa > 0.60: Substantial agreement per Landis & Koch (1977)
  - overall_agreement_pct >= 80%: High convergence between IRT and CTT
  - kappa in [-1, 1] range
  - agreement_pct in [0, 100] range
  - p_bonferroni >= p_uncorrected (correction cannot reduce p-values)

g_code REASONING:
- Approach: Extract fixed effects from both models, apply Bonferroni correction
  (factor=4 for family of 4 tests per Decision D068), classify effects by sign
  and significance, compute Cohen's kappa for agreement on classifications

- Why this approach: Cohen's kappa accounts for chance agreement (unlike raw
  percentage), making it a robust metric for assessing measurement convergence.
  Dual p-value reporting (uncorrected + Bonferroni) satisfies Decision D068
  while allowing transparent interpretation of multiple testing impact.

- Data flow: Load models -> Extract fixed effects tables -> Apply Bonferroni
  correction -> Classify (sign match + significance match) -> Compute kappa ->
  Save comparison table + summary metrics

- Expected performance: ~1 second (model loading + statsmodels table extraction)

IMPLEMENTATION NOTES:
- Analysis tool: tools.analysis_ctt.compute_cohens_kappa_agreement
- Validation tool: tools.validation.validate_icc_bounds
- Parameters: Bonferroni factor=4 (4 fixed effects tested), alpha=0.05
- Critical: Use MixedLMResults.load() for model loading (NOT pickle.load()
  which causes patsy errors per REMEMVR data conventions)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/ (5.5.4)
#   parents[2] = chX/ (ch5)
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_ctt import compute_cohens_kappa_agreement

# Import validation tool
from tools.validation import validate_icc_bounds

# Import statsmodels for model loading
from statsmodels.regression.mixed_linear_model import MixedLMResults

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.5.4 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step05_compare_fixed_effects.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step05_coefficient_comparison.csv
#   CORRECT: data/step05_agreement_metrics.csv
#   WRONG:   results/coefficient_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/coefficient_comparison.csv     (missing step prefix)
#   WRONG:   logs/step05_coefficients.csv        (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 5: Compare Fixed Effects between IRT and CTT Models")

        # =========================================================================
        # STEP 1: Load Fixed Effects Tables from Step 3 CSVs
        # =========================================================================
        # Note: Step 3 saved fixed effects as CSVs (step03_irt_coefficients.csv,
        #       step03_ctt_coefficients.csv) - use these directly instead of pickle
        #       to avoid random effects contamination

        log("[LOAD] Loading IRT fixed effects from CSV...")
        irt_coef_path = RQ_DIR / "data" / "step03_irt_coefficients.csv"
        irt_coef_df = pd.read_csv(irt_coef_path)
        log(f"[LOADED] IRT coefficients: {irt_coef_path.name} ({len(irt_coef_df)} rows)")

        log("[LOAD] Loading CTT fixed effects from CSV...")
        ctt_coef_path = RQ_DIR / "data" / "step03_ctt_coefficients.csv"
        ctt_coef_df = pd.read_csv(ctt_coef_path)
        log(f"[LOADED] CTT coefficients: {ctt_coef_path.name} ({len(ctt_coef_df)} rows)")

        # =========================================================================
        # STEP 2: Extract Fixed Effects as Series
        # =========================================================================
        # Convert CSV DataFrames to indexed Series for comparison

        log("[EXTRACT] Extracting fixed effects from IRT model...")
        irt_params = pd.Series(irt_coef_df['coef'].values, index=irt_coef_df['term'].values)
        irt_bse = pd.Series(irt_coef_df['std_err'].values, index=irt_coef_df['term'].values)
        irt_pvalues = pd.Series(irt_coef_df['p_value'].values, index=irt_coef_df['term'].values)

        log("[EXTRACT] Extracting fixed effects from CTT model...")
        ctt_params = pd.Series(ctt_coef_df['coef'].values, index=ctt_coef_df['term'].values)
        ctt_bse = pd.Series(ctt_coef_df['std_err'].values, index=ctt_coef_df['term'].values)
        ctt_pvalues = pd.Series(ctt_coef_df['p_value'].values, index=ctt_coef_df['term'].values)

        # Verify both models have same fixed effects structure
        if not all(irt_params.index == ctt_params.index):
            raise ValueError(
                f"Fixed effects mismatch between models!\n"
                f"IRT terms: {list(irt_params.index)}\n"
                f"CTT terms: {list(ctt_params.index)}"
            )

        log(f"[EXTRACT] Fixed effects structure validated: {len(irt_params)} terms")
        log(f"[EXTRACT] Terms: {list(irt_params.index)}")

        # =========================================================================
        # STEP 3: Apply Bonferroni Correction (Decision D068)
        # =========================================================================
        # Tool: Bonferroni formula p_bonf = min(p_uncorrected x n_tests, 1.0)
        # What it does: Adjusts p-values for multiple testing (family of 4 tests)
        # Expected output: p_bonferroni >= p_uncorrected for all terms

        n_tests = len(irt_params)
        log(f"[BONFERRONI] Applying Bonferroni correction (factor={n_tests})...")

        irt_p_bonferroni = np.minimum(irt_pvalues * n_tests, 1.0)
        ctt_p_bonferroni = np.minimum(ctt_pvalues * n_tests, 1.0)

        log(f"[BONFERRONI] Correction applied to both models")

        # =========================================================================
        # STEP 4: Classify Effects (Sign Match + Significance Match)
        # =========================================================================
        # Tool: Boolean classification logic
        # What it does: Determines if IRT and CTT models agree on effect direction
        #               and statistical significance
        # Expected output: Boolean arrays for sign_match, sig_match, agreement

        log("[CLASSIFY] Classifying effect agreements...")

        # Sign match: Same direction (both positive OR both negative)
        sign_match = (np.sign(irt_params) == np.sign(ctt_params)).values

        # Significance match: Both significant OR both non-significant (alpha=0.05)
        # Use Bonferroni-corrected p-values for significance classification
        alpha = 0.05
        irt_sig = (irt_p_bonferroni < alpha).values
        ctt_sig = (ctt_p_bonferroni < alpha).values
        sig_match = (irt_sig == ctt_sig)

        # Overall agreement: sign_match AND sig_match
        agreement = sign_match & sig_match

        n_agreements = agreement.sum()
        agreement_pct = (n_agreements / n_tests) * 100

        log(f"[CLASSIFY] Sign matches: {sign_match.sum()}/{n_tests}")
        log(f"[CLASSIFY] Significance matches: {sig_match.sum()}/{n_tests}")
        log(f"[CLASSIFY] Overall agreements: {n_agreements}/{n_tests} ({agreement_pct:.1f}%)")

        # =========================================================================
        # STEP 5: Create Coefficient Comparison Table
        # =========================================================================
        # Output: step05_coefficient_comparison.csv
        # Contains: Side-by-side comparison of IRT and CTT fixed effects with
        #           dual p-values and agreement classifications

        log("[BUILD] Creating coefficient comparison table...")

        comparison_df = pd.DataFrame({
            'term': irt_params.index,
            'irt_coef': irt_params.values,
            'irt_se': irt_bse.values,
            'irt_p_uncorrected': irt_pvalues.values,
            'irt_p_bonferroni': irt_p_bonferroni.values,
            'ctt_coef': ctt_params.values,
            'ctt_se': ctt_bse.values,
            'ctt_p_uncorrected': ctt_pvalues.values,
            'ctt_p_bonferroni': ctt_p_bonferroni.values,
            'sign_match': sign_match,
            'sig_match': sig_match,
            'agreement': agreement
        })

        # Save coefficient comparison
        comparison_output = RQ_DIR / "data" / "step05_coefficient_comparison.csv"
        comparison_df.to_csv(comparison_output, index=False, encoding='utf-8')
        log(f"[SAVED] {comparison_output.name} ({len(comparison_df)} rows, {len(comparison_df.columns)} cols)")

        # =========================================================================
        # STEP 6: Compute Cohen's Kappa for Agreement
        # =========================================================================
        # Tool: tools.analysis_ctt.compute_cohens_kappa_agreement
        # What it does: Computes kappa statistic accounting for chance agreement
        # Expected output: kappa in [-1, 1], interpretation string

        log("[KAPPA] Computing Cohen's kappa for agreement classification...")

        kappa_result = compute_cohens_kappa_agreement(
            classifications_1=irt_sig.tolist(),  # IRT significance classifications
            classifications_2=ctt_sig.tolist(),  # CTT significance classifications
            labels=irt_params.index.tolist()     # Effect names for reporting
        )

        kappa = kappa_result['kappa']
        kappa_interpretation = kappa_result['interpretation']

        log(f"[KAPPA] Cohen's kappa = {kappa:.3f} ({kappa_interpretation})")

        # =========================================================================
        # STEP 7: Create Agreement Metrics Summary
        # =========================================================================
        # Output: step05_agreement_metrics.csv
        # Contains: Cohen's kappa, threshold checks, overall agreement percentage
        #           Used for validation and results reporting

        log("[BUILD] Creating agreement metrics summary...")

        # Thresholds per RQ 5.5.4 convergence criteria
        kappa_threshold = 0.60  # Landis & Koch (1977) "substantial agreement"
        agreement_threshold = 80.0  # 80% overall agreement

        kappa_threshold_met = (kappa > kappa_threshold)
        agreement_threshold_met = (agreement_pct >= agreement_threshold)

        metrics_df = pd.DataFrame({
            'cohens_kappa': [kappa],
            'kappa_threshold_met': [kappa_threshold_met],
            'overall_agreement_pct': [agreement_pct],
            'agreement_threshold_met': [agreement_threshold_met],
            'n_terms': [n_tests],
            'n_agreements': [n_agreements]
        })

        # Save agreement metrics
        metrics_output = RQ_DIR / "data" / "step05_agreement_metrics.csv"
        metrics_df.to_csv(metrics_output, index=False, encoding='utf-8')
        log(f"[SAVED] {metrics_output.name} ({len(metrics_df)} rows, {len(metrics_df.columns)} cols)")

        # =========================================================================
        # STEP 8: Validate Agreement Metrics
        # =========================================================================
        # Tool: tools.validation.validate_icc_bounds (adapted for kappa)
        # Validates: kappa in [-1, 1], agreement_pct in [0, 100]
        # Threshold: kappa > 0.60 for substantial agreement

        log("[VALIDATION] Validating Cohen's kappa bounds...")

        # Create validation DataFrame (validate_icc_bounds expects DataFrame)
        kappa_validation_df = pd.DataFrame({'cohens_kappa': [kappa]})

        validation_result = validate_icc_bounds(
            icc_df=kappa_validation_df,
            icc_col='cohens_kappa'
        )

        if not validation_result['valid']:
            raise ValueError(f"Kappa validation failed: {validation_result['message']}")

        log(f"[VALIDATION] Kappa bounds valid: {validation_result['message']}")

        # Additional validation: agreement percentage in [0, 100]
        if not (0 <= agreement_pct <= 100):
            raise ValueError(f"Agreement percentage {agreement_pct:.1f}% outside [0, 100] range")

        log(f"[VALIDATION] Agreement percentage valid: {agreement_pct:.1f}% in [0, 100]")

        # Additional validation: Bonferroni formula correct
        irt_bonf_valid = (irt_p_bonferroni >= irt_pvalues).all()
        ctt_bonf_valid = (ctt_p_bonferroni >= ctt_pvalues).all()
        bonferroni_valid = irt_bonf_valid and ctt_bonf_valid
        if not bonferroni_valid:
            raise ValueError("Bonferroni correction failed: p_bonferroni < p_uncorrected detected")

        log("[VALIDATION] Bonferroni correction valid: p_bonferroni >= p_uncorrected for all terms")

        # =========================================================================
        # STEP 9: Final Summary
        # =========================================================================

        log("[SUMMARY] Fixed effects comparison complete:")
        log(f"  - Coefficient comparison: {comparison_output.name}")
        log(f"  - Agreement metrics: {metrics_output.name}")
        log(f"  - Cohen's kappa: {kappa:.3f} ({kappa_interpretation})")
        log(f"  - Overall agreement: {agreement_pct:.1f}% ({n_agreements}/{n_tests} terms)")
        log(f"  - Kappa threshold (>0.60): {'MET' if kappa_threshold_met else 'NOT MET'}")
        log(f"  - Agreement threshold (>=80%): {'MET' if agreement_threshold_met else 'NOT MET'}")

        log("[SUCCESS] Step 5 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
