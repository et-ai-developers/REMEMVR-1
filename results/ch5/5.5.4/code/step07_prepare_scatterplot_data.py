#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step07
Step Name: Prepare Scatterplot Data (IRT vs CTT)
RQ: results/ch5/5.5.4
Generated: 2025-12-05

PURPOSE:
Merge IRT theta scores and CTT mean scores into single dataset for scatterplot
visualization. Creates plot-ready data showing relationship between IRT-based
and CTT-based measurements of the same episodic memory construct (source vs
destination location memory).

EXPECTED INPUTS:
  - data/step00_irt_theta_from_rq551.csv
    Columns: ['composite_ID', 'UID', 'test', 'location_type', 'irt_theta', 'irt_se', 'TSVR_hours']
    Format: Long format IRT theta scores from RQ 5.5.1 Pass 2 calibration
    Expected rows: 800 (100 participants x 4 tests x 2 location types)

  - data/step01_ctt_scores.csv
    Columns: ['composite_ID', 'location_type', 'ctt_mean_score', 'n_items', 'TSVR_hours']
    Format: CTT mean scores (proportion correct) per location type
    Expected rows: 800 (100 participants x 4 tests x 2 location types)

EXPECTED OUTPUTS:
  - data/step07_scatterplot_data.csv
    Columns: ['UID', 'test', 'location_type', 'irt_theta', 'ctt_mean_score']
    Format: Merged IRT and CTT scores for scatterplot visualization
    Expected rows: 800 (100 participants x 4 tests x 2 location types)

VALIDATION CRITERIA:
  - Exactly 800 rows (100 participants x 4 tests x 2 location types)
  - No NaN in irt_theta or ctt_mean_score
  - Location balance: 400 source, 400 destination
  - irt_theta in [-3, 3] range
  - ctt_mean_score in [0, 1] range
  - 100 unique UIDs
  - 4 unique tests (1, 2, 3, 4 or T1-T4)

g_code REASONING:
- Approach: Simple pandas merge on composite_ID + location_type keys
- Why this approach: Both datasets share composite_ID (UID_test format) and
  location_type, enabling straightforward inner join with no data loss
- Data flow: Load IRT theta → Load CTT scores → Inner merge → Select columns
  → Sort for consistent ordering → Save
- Expected performance: ~1 second (lightweight CSV merge operation)

IMPLEMENTATION NOTES:
- Analysis tool: Stdlib operations (pandas merge, select, sort)
- Validation tool: Inline checks (row count, NaN detection, value ranges)
- Parameters: None (pure data reshaping)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 5.5.4/
#   parents[2] = ch5/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.5.4 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step07_prepare_scatterplot_data.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step07_scatterplot_data.csv
#   CORRECT: logs/step07_prepare_scatterplot_data.log
#   WRONG:   results/scatterplot_data.csv  (wrong folder + no prefix)
#   WRONG:   data/scatterplot_data.csv     (missing step prefix)
#   WRONG:   logs/step07_scatterplot.csv   (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 7: Prepare Scatterplot Data (IRT vs CTT)")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: IRT theta scores and CTT mean scores from prior steps
        # Purpose: Merge these two measurement approaches for correlation visualization

        log("[LOAD] Loading IRT theta scores from RQ 5.5.1...")
        # Load data/step00_irt_theta_from_rq551.csv
        # Expected columns: composite_ID, UID, test, location_type, irt_theta, irt_se, TSVR_hours
        # Expected rows: 800 (100 participants x 4 tests x 2 location types)
        theta_long = pd.read_csv(RQ_DIR / "data" / "step00_irt_theta_from_rq551.csv", encoding='utf-8')
        log(f"[LOADED] IRT theta scores ({len(theta_long)} rows, {len(theta_long.columns)} cols)")

        log("[LOAD] Loading CTT mean scores...")
        # Load data/step01_ctt_scores.csv
        # Expected columns: composite_ID, UID, test, location_type, ctt_mean_score, n_items, TSVR_hours
        # Expected rows: 800 (100 participants x 4 tests x 2 location types)
        ctt_scores = pd.read_csv(RQ_DIR / "data" / "step01_ctt_scores.csv", encoding='utf-8')
        log(f"[LOADED] CTT scores ({len(ctt_scores)} rows, {len(ctt_scores.columns)} cols)")

        # =========================================================================
        # STEP 2: Merge Data
        # =========================================================================
        # Tool: pandas merge (stdlib)
        # What it does: Inner join on composite_ID + location_type keys
        # Expected output: 800 rows with both irt_theta and ctt_mean_score columns

        log("[MERGE] Merging IRT theta and CTT scores on composite_ID + location_type...")
        # Merge on composite_ID and location_type (both datasets have these keys)
        # Inner join ensures only matching records retained (should be all 800)
        merged_data = pd.merge(
            theta_long[['composite_ID', 'UID', 'test', 'location_type', 'irt_theta']],
            ctt_scores[['composite_ID', 'location_type', 'ctt_mean_score']],
            on=['composite_ID', 'location_type'],
            how='inner'
        )
        log(f"[MERGED] Combined dataset ({len(merged_data)} rows, {len(merged_data.columns)} cols)")

        # Check for merge issues (should have same row count as inputs)
        if len(merged_data) != 800:
            log(f"[WARNING] Expected 800 rows after merge, got {len(merged_data)}")
            log(f"[WARNING] IRT input: {len(theta_long)} rows, CTT input: {len(ctt_scores)} rows")

        # =========================================================================
        # STEP 3: Select and Sort Columns
        # =========================================================================
        # These outputs will be used by: rq_plots agent for scatterplot generation

        log("[SELECT] Selecting final columns for scatterplot...")
        # Output columns: UID, test, location_type, irt_theta, ctt_mean_score
        # Purpose: Minimal set needed for scatterplot (x=irt_theta, y=ctt_mean_score, color=location_type)
        scatterplot_data = merged_data[['UID', 'test', 'location_type', 'irt_theta', 'ctt_mean_score']].copy()

        log("[SORT] Sorting by location_type, UID, test for consistent ordering...")
        # Sort for reproducible plot ordering
        # Primary: location_type (group source/destination together)
        # Secondary: UID (alphabetical participant order)
        # Tertiary: test (chronological test order)
        scatterplot_data.sort_values(by=['location_type', 'UID', 'test'], inplace=True)
        scatterplot_data.reset_index(drop=True, inplace=True)
        log(f"[SORTED] Final dataset ({len(scatterplot_data)} rows)")

        # =========================================================================
        # STEP 4: Save Output
        # =========================================================================
        # Output will be used by: rq_plots for scatterplot visualization

        log("[SAVE] Saving scatterplot data to data/step07_scatterplot_data.csv...")
        # Output: data/step07_scatterplot_data.csv
        # Contains: Merged IRT and CTT scores for scatterplot
        # Columns: UID, test, location_type, irt_theta, ctt_mean_score
        output_path = RQ_DIR / "data" / "step07_scatterplot_data.csv"
        scatterplot_data.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(scatterplot_data)} rows, {len(scatterplot_data.columns)} cols)")

        # =========================================================================
        # STEP 5: Validation
        # =========================================================================
        # Tool: Inline validation checks
        # Validates: Row count, NaN detection, value ranges, location balance
        # Thresholds: 800 rows, no NaN, irt_theta in [-3,3], ctt_mean_score in [0,1]

        log("[VALIDATION] Running inline validation checks...")

        # Check 1: Row count
        expected_rows = 800
        if len(scatterplot_data) != expected_rows:
            raise ValueError(f"Expected {expected_rows} rows, got {len(scatterplot_data)}")
        log(f"[VALIDATION] Row count: {len(scatterplot_data)} rows (expected {expected_rows}) [PASS]")

        # Check 2: No NaN in critical columns
        nan_irt = scatterplot_data['irt_theta'].isna().sum()
        nan_ctt = scatterplot_data['ctt_mean_score'].isna().sum()
        if nan_irt > 0 or nan_ctt > 0:
            raise ValueError(f"NaN values detected: irt_theta={nan_irt}, ctt_mean_score={nan_ctt}")
        log(f"[VALIDATION] No NaN in irt_theta or ctt_mean_score [PASS]")

        # Check 3: Location balance
        location_counts = scatterplot_data['location_type'].value_counts()
        log(f"[VALIDATION] Location type counts: {location_counts.to_dict()}")
        if location_counts.get('source', 0) != 400 or location_counts.get('destination', 0) != 400:
            raise ValueError(f"Expected 400 source + 400 destination, got {location_counts.to_dict()}")
        log(f"[VALIDATION] Location balance: 400 source, 400 destination [PASS]")

        # Check 4: IRT theta range
        irt_min = scatterplot_data['irt_theta'].min()
        irt_max = scatterplot_data['irt_theta'].max()
        log(f"[VALIDATION] IRT theta range: [{irt_min:.2f}, {irt_max:.2f}]")
        if irt_min < -3 or irt_max > 3:
            log(f"[WARNING] IRT theta outside typical [-3, 3] range")
        else:
            log(f"[VALIDATION] IRT theta in typical [-3, 3] range [PASS]")

        # Check 5: CTT mean score range
        ctt_min = scatterplot_data['ctt_mean_score'].min()
        ctt_max = scatterplot_data['ctt_mean_score'].max()
        log(f"[VALIDATION] CTT mean score range: [{ctt_min:.3f}, {ctt_max:.3f}]")
        if ctt_min < 0 or ctt_max > 1:
            raise ValueError(f"CTT mean score outside [0, 1] range: [{ctt_min}, {ctt_max}]")
        log(f"[VALIDATION] CTT mean score in [0, 1] range [PASS]")

        # Check 6: UID count
        n_uids = scatterplot_data['UID'].nunique()
        log(f"[VALIDATION] Unique UIDs: {n_uids}")
        if n_uids != 100:
            log(f"[WARNING] Expected 100 unique UIDs, got {n_uids}")
        else:
            log(f"[VALIDATION] 100 unique UIDs present [PASS]")

        # Check 7: Test count
        n_tests = scatterplot_data['test'].nunique()
        test_values = sorted(scatterplot_data['test'].unique())
        log(f"[VALIDATION] Unique tests: {n_tests} (values: {test_values})")
        if n_tests != 4:
            log(f"[WARNING] Expected 4 unique tests, got {n_tests}")
        else:
            log(f"[VALIDATION] 4 unique tests present [PASS]")

        log("[SUCCESS] Step 7 complete")
        log(f"[OUTPUT] Generated: {output_path}")
        log(f"[OUTPUT] Rows: {len(scatterplot_data)}, Columns: {list(scatterplot_data.columns)}")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
