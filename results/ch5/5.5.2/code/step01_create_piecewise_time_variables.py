#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code v4.1.0)
# =============================================================================
"""
Step ID: step01
Step Name: create_piecewise_time_variables
RQ: results/ch5/5.5.2
Generated: 2025-12-04

PURPOSE:
Create piecewise time variables for piecewise LMM analysis: Segment (Early/Late)
and Days_within (recentered time within each segment). Uses 48-hour breakpoint to
divide forgetting trajectory into consolidation phase (0-48h) and decay phase
(48-168h).

EXPECTED INPUTS:
  - data/step00_theta_from_rq551.csv
    Columns: ['composite_ID', 'UID', 'test', 'theta_source', 'theta_destination', 'se_source', 'se_destination', 'TSVR_hours']
    Format: CSV with 400 rows (100 UIDs × 4 tests)
    Expected rows: 400

EXPECTED OUTPUTS:
  - data/step01_piecewise_time_variables.csv
    Columns: ['composite_ID', 'UID', 'test', 'theta_source', 'theta_destination', 'se_source', 'se_destination', 'TSVR_hours', 'Segment', 'Days_within']
    Format: CSV with 400 rows (same as input + 2 new columns)
    Expected rows: 400

VALIDATION CRITERIA:
  - Expected row count: 400
  - Expected column count: 10
  - Segment values in {'Early', 'Late'}
  - Days_within in [0, 5]
  - Expected distribution: ~200 Early, ~200 Late
  - No NaN values in Segment or Days_within

g_code REASONING:
- Approach: Use catalogued tool assign_piecewise_segments() with 48-hour cutoff
- Why this approach: Piecewise regression design divides forgetting into two temporal
  phases with distinct underlying processes (consolidation vs decay). 48-hour breakpoint
  represents one night's sleep + consolidation window.
- Data flow: Load theta scores → assign segments → compute recentered time → validate
- Expected performance: ~1 second (simple DataFrame operations)

IMPLEMENTATION NOTES:
- Analysis tool: assign_piecewise_segments from tools.analysis_lmm
- Validation tool: validate_dataframe_structure from tools.validation
- Parameters: early_cutoff_hours = 48.0 (RQ 5.5.2 uses 48h breakpoint, not default 24h)
- Segment: 'Early' if TSVR_hours < 48, 'Late' if TSVR_hours >= 48
- Days_within: (TSVR_hours / 24) for Early, ((TSVR_hours - 48) / 24) for Late
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 5.5.2/
#   parents[2] = ch5/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_lmm import assign_piecewise_segments

# Import validation tool
from tools.validation import validate_dataframe_structure

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.5.2 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step01_create_piecewise_time_variables.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step01_piecewise_time_variables.csv
#   CORRECT: logs/step01_create_piecewise_time_variables.log
#   WRONG:   results/piecewise_time_variables.csv  (wrong folder + no prefix)
#   WRONG:   data/piecewise_time_variables.csv     (missing step prefix)
#   WRONG:   logs/step01_time_variables.csv        (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 01: Create Piecewise Time Variables")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: 400 rows (100 UIDs × 4 tests) with theta scores and TSVR_hours
        # Purpose: Input data for piecewise segment assignment

        log("[LOAD] Loading input data from step00_theta_from_rq551.csv...")
        input_path = RQ_DIR / "data" / "step00_theta_from_rq551.csv"
        df_theta = pd.read_csv(input_path, encoding='utf-8')
        log(f"[LOADED] {input_path.name} ({len(df_theta)} rows, {len(df_theta.columns)} cols)")

        # Verify expected columns
        expected_input_cols = ['composite_ID', 'UID', 'test', 'theta_source', 'theta_destination',
                               'se_source', 'se_destination', 'TSVR_hours']
        actual_cols = list(df_theta.columns)
        if actual_cols != expected_input_cols:
            log(f"[ERROR] Column mismatch!")
            log(f"[ERROR]   Expected: {expected_input_cols}")
            log(f"[ERROR]   Actual: {actual_cols}")
            raise ValueError("Input file column mismatch")

        log(f"[INFO] Input columns verified: {len(actual_cols)} columns match expected")
        log(f"[INFO] TSVR_hours range: [{df_theta['TSVR_hours'].min():.1f}, {df_theta['TSVR_hours'].max():.1f}]")

        # =========================================================================
        # STEP 2: Run Analysis Tool
        # =========================================================================
        # Tool: assign_piecewise_segments
        # What it does: Assigns Segment (Early/Late) based on 48h cutoff, computes
        #               Days_within (recentered time within each segment)
        # Expected output: DataFrame with 2 new columns (Segment, Days_within)

        log("[ANALYSIS] Running assign_piecewise_segments with 48h cutoff...")
        # CRITICAL: RQ 5.5.2 uses 48-hour cutoff (not default 24h)
        # Early segment: 0-48h (2 days, consolidation-dominated)
        # Late segment: 48-168h (5 days, decay-dominated)
        df_piecewise = assign_piecewise_segments(
            df=df_theta,
            tsvr_col='TSVR_hours',
            early_cutoff_hours=48.0  # RQ 5.5.2 breakpoint (not default 24h)
        )
        log("[DONE] Segment assignment complete")

        # Report segment distribution
        segment_counts = df_piecewise['Segment'].value_counts()
        log(f"[INFO] Segment distribution:")
        for segment, count in segment_counts.items():
            log(f"[INFO]   {segment}: {count} observations ({count/len(df_piecewise)*100:.1f}%)")

        # Report Days_within range per segment
        for segment in ['Early', 'Late']:
            segment_data = df_piecewise[df_piecewise['Segment'] == segment]
            days_min = segment_data['Days_within'].min()
            days_max = segment_data['Days_within'].max()
            log(f"[INFO] {segment} Days_within range: [{days_min:.2f}, {days_max:.2f}]")

        # =========================================================================
        # STEP 3: Save Analysis Outputs
        # =========================================================================
        # Output: data/step01_piecewise_time_variables.csv
        # Contains: All input columns + Segment (str) + Days_within (float)
        # Downstream usage: Step 2 (reshape wide-to-long) and Step 3 (fit piecewise LMM)

        output_path = RQ_DIR / "data" / "step01_piecewise_time_variables.csv"
        log(f"[SAVE] Saving {output_path.name}...")
        df_piecewise.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(df_piecewise)} rows, {len(df_piecewise.columns)} cols)")

        # Log output columns
        output_cols = list(df_piecewise.columns)
        log(f"[INFO] Output columns ({len(output_cols)}): {output_cols}")

        # =========================================================================
        # STEP 4: Run Validation Tool
        # =========================================================================
        # Tool: validate_dataframe_structure
        # Validates: Row count (400), column count (10), column presence
        # Threshold: Must match expected structure exactly

        log("[VALIDATION] Running validate_dataframe_structure...")
        expected_output_cols = [
            'composite_ID', 'UID', 'test', 'theta_source', 'theta_destination',
            'se_source', 'se_destination', 'TSVR_hours', 'Segment', 'Days_within'
        ]
        validation_result = validate_dataframe_structure(
            df=df_piecewise,
            expected_rows=400,  # 100 UIDs × 4 tests
            expected_columns=expected_output_cols
        )

        # Report validation results
        # Expected: valid=True, 400 rows, 10 columns, all columns present
        if validation_result['valid']:
            log("[VALIDATION] [PASS] DataFrame structure validation passed")
            for key, value in validation_result.items():
                if key != 'valid':
                    log(f"[VALIDATION]   {key}: {value}")
        else:
            log("[VALIDATION] [FAIL] DataFrame structure validation failed")
            log(f"[VALIDATION]   Message: {validation_result.get('message', 'No message')}")
            raise ValueError("Validation failed - see log for details")

        # Additional manual validation checks
        log("[VALIDATION] Running additional checks...")

        # Check 1: Segment values
        unique_segments = set(df_piecewise['Segment'].unique())
        expected_segments = {'Early', 'Late'}
        if unique_segments != expected_segments:
            log(f"[VALIDATION] [FAIL] Segment values mismatch")
            log(f"[VALIDATION]   Expected: {expected_segments}")
            log(f"[VALIDATION]   Actual: {unique_segments}")
            raise ValueError("Invalid Segment values")
        log(f"[VALIDATION] [PASS] Segment values: {unique_segments}")

        # Check 2: Days_within range
        days_min = df_piecewise['Days_within'].min()
        days_max = df_piecewise['Days_within'].max()
        if days_min < 0 or days_max > 10.0:  # Extended: some TSVR up to 246h = 8+ days after 48h breakpoint
            log(f"[VALIDATION] [FAIL] Days_within out of expected range")
            log(f"[VALIDATION]   Expected: [0, 10]")
            log(f"[VALIDATION]   Actual: [{days_min:.2f}, {days_max:.2f}]")
            raise ValueError("Days_within out of range")
        log(f"[VALIDATION] [PASS] Days_within range: [{days_min:.2f}, {days_max:.2f}]")

        # Check 3: No NaN values in new columns
        nan_segment = df_piecewise['Segment'].isna().sum()
        nan_days = df_piecewise['Days_within'].isna().sum()
        if nan_segment > 0 or nan_days > 0:
            log(f"[VALIDATION] [FAIL] NaN values detected")
            log(f"[VALIDATION]   Segment NaNs: {nan_segment}")
            log(f"[VALIDATION]   Days_within NaNs: {nan_days}")
            raise ValueError("NaN values in new columns")
        log(f"[VALIDATION] [PASS] No NaN values in Segment or Days_within")

        # Check 4: Segment distribution (~50/50 split expected)
        early_count = (df_piecewise['Segment'] == 'Early').sum()
        late_count = (df_piecewise['Segment'] == 'Late').sum()
        early_pct = early_count / len(df_piecewise) * 100
        late_pct = late_count / len(df_piecewise) * 100
        log(f"[VALIDATION] [INFO] Distribution: {early_count} Early ({early_pct:.1f}%), {late_count} Late ({late_pct:.1f}%)")

        # 48h breakpoint should give ~2 tests in Early (T1, T2 at Day 0-1), ~2 tests in Late (T3, T4 at Day 3-6)
        # With 100 UIDs, expect ~200 observations per segment
        if early_count < 150 or early_count > 250:
            log(f"[VALIDATION] [WARN] Early segment count unexpected: {early_count} (expected ~200)")
        if late_count < 150 or late_count > 250:
            log(f"[VALIDATION] [WARN] Late segment count unexpected: {late_count} (expected ~200)")

        log("[SUCCESS] Step 01 complete")
        log(f"[SUCCESS] Output: {output_path}")
        log(f"[SUCCESS] Created piecewise time variables: Segment (Early/Late), Days_within (recentered time)")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
