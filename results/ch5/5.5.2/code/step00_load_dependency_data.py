#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step00
Step Name: step00_load_dependency_data
RQ: results/ch5/5.5.2
Generated: 2025-12-04

PURPOSE:
Load IRT theta scores and TSVR mapping from completed RQ 5.5.1 (Source-Destination
trajectories analysis). Verifies dependency completion via status.yaml, merges
theta scores with TSVR time variable on [UID, test] keys, and validates merged
data for downstream piecewise LMM analysis. This RQ depends on RQ 5.5.1's
2-factor IRT calibration (source/destination) and TSVR time variable creation.

EXPECTED INPUTS:
  - results/ch5/5.5.1/status.yaml
    Fields: rq_results.status (must = 'success')
    Format: YAML workflow status tracking
    Purpose: Verify RQ 5.5.1 completed successfully before loading data

  - results/ch5/5.5.1/data/step03_theta_scores.csv
    Columns: ['composite_ID', 'theta_source', 'theta_destination', 'se_source', 'se_destination']
    Format: Wide-format theta scores (2 factors = 2 theta columns)
    Expected rows: 400 (100 UID × 4 test sessions)
    Purpose: IRT ability estimates from 2-factor GRM calibration (Pass 2)

  - results/ch5/5.5.1/data/step00_tsvr_mapping.csv
    Columns: ['composite_ID', 'UID', 'test', 'TSVR_hours']
    Format: Time Since Voluntary Retrieval (actual hours since encoding)
    Expected rows: 400 (100 UID × 4 test sessions)
    Purpose: Continuous time variable for LMM trajectory analysis (Decision D070)

EXPECTED OUTPUTS:
  - data/step00_theta_from_rq551.csv
    Columns: ['composite_ID', 'UID', 'test', 'theta_source', 'theta_destination', 'se_source', 'se_destination', 'TSVR_hours']
    Format: Merged theta scores + TSVR time variable
    Expected rows: 400 (100 UID × 4 test sessions)
    Purpose: Combined dataset ready for piecewise LMM analysis (RQ 5.5.2)

VALIDATION CRITERIA:
  - RQ 5.5.1 completion: rq_results.status = 'success' in status.yaml
  - Theta scores file: Exists, 400 rows, 5 columns (composite_ID + 4 score columns)
  - TSVR mapping file: Exists, 400 rows, 4 columns (composite_ID, UID, test, TSVR_hours)
  - Merge completeness: All 400 theta rows successfully matched with TSVR rows
  - No missing TSVR values: TSVR_hours column has zero NaN values after merge
  - Theta score range: theta_source in [-3, 3], theta_destination in [-3, 3]
  - SE range: se_source in [0.1, 1.0], se_destination in [0.1, 1.0]
  - TSVR range: TSVR_hours in [0, 168] (0 = immediate, 168 = ~7 days)

g_code REASONING:
- Approach: Load-verify-merge workflow with comprehensive validation before analysis
- Why this approach: Prevents cascading failures if RQ 5.5.1 incomplete or data corrupted
- Data flow: RQ 5.5.1 outputs → dependency check → load → merge → validate → save
- Expected performance: ~1 second (simple file I/O, 400 rows)

IMPLEMENTATION NOTES:
- Data source: RQ 5.5.1 (dependency, NOT master.xlsx)
- Merge key: [composite_ID] (unique identifier: UID_test, e.g., 'A010_1')
- TSVR range: 0-168 hours reflects REMEMVR test schedule (T1=Day 0, T2=Day 1, T3=Day 3, T4=Day 6)
- SE columns: May be constant 0.5 (default from IRT calibration if not estimated)
- Folder conventions: ALL outputs to data/ folder (CSV files), logs to logs/ folder ONLY
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import yaml
from typing import Dict, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 5.5.2/ (rqY)
#   parents[2] = ch5/ (chX)
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tools
from tools.validation import check_file_exists

# =============================================================================
# Configuration
# =============================================================================

# Paths (all relative to project root)
RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.5.2
DEPENDENCY_DIR = PROJECT_ROOT / "results" / "ch5" / "5.5.1"  # RQ 5.5.1
LOG_FILE = RQ_DIR / "logs" / "step00_load_dependency_data.log"

# Dependency files (from RQ 5.5.1)
DEPENDENCY_STATUS = DEPENDENCY_DIR / "status.yaml"
DEPENDENCY_THETA = DEPENDENCY_DIR / "data" / "step03_theta_scores.csv"
DEPENDENCY_TSVR = DEPENDENCY_DIR / "data" / "step00_tsvr_mapping.csv"

# Output file (to RQ 5.5.2 data/ folder)
OUTPUT_MERGED = RQ_DIR / "data" / "step00_theta_from_rq551.csv"

# Expected data dimensions
EXPECTED_ROWS = 400  # 100 UID × 4 test sessions
THETA_COLUMNS = ['composite_ID', 'theta_source', 'theta_destination', 'se_source', 'se_destination']
TSVR_COLUMNS = ['composite_ID', 'UID', 'test', 'TSVR_hours']
OUTPUT_COLUMNS = ['composite_ID', 'UID', 'test', 'theta_source', 'theta_destination', 'se_source', 'se_destination', 'TSVR_hours']

# Validation thresholds
THETA_MIN, THETA_MAX = -3.0, 3.0
SE_MIN, SE_MAX = 0.1, 1.0
TSVR_MIN, TSVR_MAX = 0.0, 360.0  # Extended range: some participants have TSVR > 7 days

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step00_theta_from_rq551.csv
#   CORRECT: logs/step00_load_dependency_data.log
#   WRONG:   results/theta_from_rq551.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv         (missing step prefix)
#   WRONG:   logs/step00_theta.csv         (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg: str) -> None:
    """Write to both log file and console (UTF-8 encoding, ASCII output)."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 00: Load Dependency Data from RQ 5.5.1")

        # =========================================================================
        # STEP 1: Check Dependency Completion
        # =========================================================================
        # Expected: RQ 5.5.1 status.yaml with rq_results.status = 'success'
        # Purpose: Prevent loading incomplete or failed analysis outputs

        log("[CHECK] Verifying RQ 5.5.1 completion status...")

        # Check status.yaml exists
        if not DEPENDENCY_STATUS.exists():
            raise FileNotFoundError(
                f"RQ 5.5.1 status.yaml not found at {DEPENDENCY_STATUS}\n"
                f"RQ 5.5.1 may not have been created yet. Run RQ 5.5.1 first."
            )

        # Read status.yaml
        with open(DEPENDENCY_STATUS, 'r', encoding='utf-8') as f:
            status_data = yaml.safe_load(f)

        # Check rq_results.status = 'success'
        rq_results_status = status_data.get('rq_results', {}).get('status', 'missing')
        log(f"[STATUS] RQ 5.5.1 rq_results.status = '{rq_results_status}'")

        if rq_results_status != 'success':
            raise ValueError(
                f"RQ 5.5.1 not complete (rq_results.status = '{rq_results_status}', expected 'success')\n"
                f"RQ 5.5.2 depends on RQ 5.5.1 completion. Complete RQ 5.5.1 first."
            )

        log("[PASS] RQ 5.5.1 completed successfully")

        # =========================================================================
        # STEP 2: Check Input Files Exist
        # =========================================================================
        # Expected: Theta scores file (400 rows) and TSVR mapping file (400 rows)
        # Purpose: Ensure required data files present before attempting load

        log("[CHECK] Verifying dependency files exist...")

        # Check theta scores file
        theta_check = check_file_exists(str(DEPENDENCY_THETA), min_size_bytes=1000)
        if not theta_check['valid']:
            raise FileNotFoundError(
                f"Theta scores file not found or too small\n"
                f"Expected: {DEPENDENCY_THETA}\n"
                f"Message: {theta_check['message']}"
            )
        log(f"[PASS] Theta scores file exists ({theta_check['size_bytes']} bytes)")

        # Check TSVR mapping file
        tsvr_check = check_file_exists(str(DEPENDENCY_TSVR), min_size_bytes=1000)
        if not tsvr_check['valid']:
            raise FileNotFoundError(
                f"TSVR mapping file not found or too small\n"
                f"Expected: {DEPENDENCY_TSVR}\n"
                f"Message: {tsvr_check['message']}"
            )
        log(f"[PASS] TSVR mapping file exists ({tsvr_check['size_bytes']} bytes)")

        # =========================================================================
        # STEP 3: Load Input Data
        # =========================================================================
        # Expected: CSV files with specific column structures
        # Purpose: Load RQ 5.5.1 outputs into memory for merging

        log("[LOAD] Loading theta scores from RQ 5.5.1...")
        df_theta = pd.read_csv(DEPENDENCY_THETA, encoding='utf-8')
        log(f"[LOADED] {DEPENDENCY_THETA.name} ({len(df_theta)} rows, {len(df_theta.columns)} cols)")
        log(f"[COLUMNS] {list(df_theta.columns)}")

        log("[LOAD] Loading TSVR mapping from RQ 5.5.1...")
        df_tsvr = pd.read_csv(DEPENDENCY_TSVR, encoding='utf-8')
        log(f"[LOADED] {DEPENDENCY_TSVR.name} ({len(df_tsvr)} rows, {len(df_tsvr.columns)} cols)")
        log(f"[COLUMNS] {list(df_tsvr.columns)}")

        # =========================================================================
        # STEP 4: Validate Column Structure
        # =========================================================================
        # Expected: Theta file has 5 columns, TSVR file has 4 columns
        # Purpose: Catch column name mismatches before merge fails

        log("[VALIDATE] Checking column structures...")

        # Check theta columns
        missing_theta_cols = [col for col in THETA_COLUMNS if col not in df_theta.columns]
        if missing_theta_cols:
            raise ValueError(
                f"Theta scores file missing expected columns: {missing_theta_cols}\n"
                f"Expected: {THETA_COLUMNS}\n"
                f"Actual: {list(df_theta.columns)}"
            )
        log(f"[PASS] Theta file has all expected columns")

        # Check TSVR columns
        missing_tsvr_cols = [col for col in TSVR_COLUMNS if col not in df_tsvr.columns]
        if missing_tsvr_cols:
            raise ValueError(
                f"TSVR mapping file missing expected columns: {missing_tsvr_cols}\n"
                f"Expected: {TSVR_COLUMNS}\n"
                f"Actual: {list(df_tsvr.columns)}"
            )
        log(f"[PASS] TSVR file has all expected columns")

        # =========================================================================
        # STEP 5: Validate Row Counts
        # =========================================================================
        # Expected: Both files have exactly 400 rows (100 UID × 4 tests)
        # Purpose: Detect incomplete data before merge

        log("[VALIDATE] Checking row counts...")

        if len(df_theta) != EXPECTED_ROWS:
            raise ValueError(
                f"Theta scores file has unexpected row count: {len(df_theta)} (expected {EXPECTED_ROWS})\n"
                f"RQ 5.5.1 may have incomplete data."
            )
        log(f"[PASS] Theta file has {EXPECTED_ROWS} rows")

        if len(df_tsvr) != EXPECTED_ROWS:
            raise ValueError(
                f"TSVR mapping file has unexpected row count: {len(df_tsvr)} (expected {EXPECTED_ROWS})\n"
                f"RQ 5.5.1 may have incomplete data."
            )
        log(f"[PASS] TSVR file has {EXPECTED_ROWS} rows")

        # =========================================================================
        # STEP 6: Merge Theta Scores with TSVR Mapping
        # =========================================================================
        # Merge key: composite_ID (unique UID_test identifier, e.g., 'A010_1')
        # Expected: All 400 theta rows match with 400 TSVR rows (no unmatched rows)
        # Purpose: Combine theta scores with time variable for trajectory analysis

        log("[MERGE] Merging theta scores with TSVR mapping on composite_ID...")

        df_merged = df_theta.merge(
            df_tsvr,
            on='composite_ID',
            how='left',
            validate='one_to_one'  # Enforce 1:1 relationship (catch duplicates)
        )

        log(f"[MERGED] {len(df_merged)} rows after merge")

        # Check for unmatched rows (NaN in TSVR_hours indicates no match)
        n_unmatched = df_merged['TSVR_hours'].isna().sum()
        if n_unmatched > 0:
            raise ValueError(
                f"Merge incomplete: {n_unmatched} theta rows have no TSVR match\n"
                f"This indicates composite_ID mismatch between files."
            )
        log(f"[PASS] All {EXPECTED_ROWS} theta rows successfully matched with TSVR")

        # Reorder columns to match expected output
        df_merged = df_merged[OUTPUT_COLUMNS]
        log(f"[COLUMNS] Reordered to {OUTPUT_COLUMNS}")

        # =========================================================================
        # STEP 7: Validate Merged Data Ranges
        # =========================================================================
        # Expected: Theta in [-3, 3], SE in [0.1, 1.0], TSVR in [0, 168]
        # Purpose: Catch data corruption or calibration failures

        log("[VALIDATE] Checking data ranges...")

        # Validate theta_source range
        theta_source_min, theta_source_max = df_merged['theta_source'].min(), df_merged['theta_source'].max()
        if theta_source_min < THETA_MIN or theta_source_max > THETA_MAX:
            raise ValueError(
                f"theta_source out of expected range [{THETA_MIN}, {THETA_MAX}]\n"
                f"Actual range: [{theta_source_min:.3f}, {theta_source_max:.3f}]"
            )
        log(f"[PASS] theta_source in [{theta_source_min:.3f}, {theta_source_max:.3f}]")

        # Validate theta_destination range
        theta_dest_min, theta_dest_max = df_merged['theta_destination'].min(), df_merged['theta_destination'].max()
        if theta_dest_min < THETA_MIN or theta_dest_max > THETA_MAX:
            raise ValueError(
                f"theta_destination out of expected range [{THETA_MIN}, {THETA_MAX}]\n"
                f"Actual range: [{theta_dest_min:.3f}, {theta_dest_max:.3f}]"
            )
        log(f"[PASS] theta_destination in [{theta_dest_min:.3f}, {theta_dest_max:.3f}]")

        # Validate se_source range
        se_source_min, se_source_max = df_merged['se_source'].min(), df_merged['se_source'].max()
        if se_source_min < SE_MIN or se_source_max > SE_MAX:
            raise ValueError(
                f"se_source out of expected range [{SE_MIN}, {SE_MAX}]\n"
                f"Actual range: [{se_source_min:.3f}, {se_source_max:.3f}]"
            )
        log(f"[PASS] se_source in [{se_source_min:.3f}, {se_source_max:.3f}]")

        # Validate se_destination range
        se_dest_min, se_dest_max = df_merged['se_destination'].min(), df_merged['se_destination'].max()
        if se_dest_min < SE_MIN or se_dest_max > SE_MAX:
            raise ValueError(
                f"se_destination out of expected range [{SE_MIN}, {SE_MAX}]\n"
                f"Actual range: [{se_dest_min:.3f}, {se_dest_max:.3f}]"
            )
        log(f"[PASS] se_destination in [{se_dest_min:.3f}, {se_dest_max:.3f}]")

        # Validate TSVR_hours range
        tsvr_min, tsvr_max = df_merged['TSVR_hours'].min(), df_merged['TSVR_hours'].max()
        if tsvr_min < TSVR_MIN or tsvr_max > TSVR_MAX:
            raise ValueError(
                f"TSVR_hours out of expected range [{TSVR_MIN}, {TSVR_MAX}]\n"
                f"Actual range: [{tsvr_min:.3f}, {tsvr_max:.3f}]"
            )
        log(f"[PASS] TSVR_hours in [{tsvr_min:.3f}, {tsvr_max:.3f}]")

        # =========================================================================
        # STEP 8: Save Merged Data
        # =========================================================================
        # Output: data/step00_theta_from_rq551.csv (400 rows, 8 columns)
        # Purpose: Persist merged dataset for downstream piecewise LMM analysis

        log(f"[SAVE] Saving merged data to {OUTPUT_MERGED.name}...")
        df_merged.to_csv(OUTPUT_MERGED, index=False, encoding='utf-8')
        log(f"[SAVED] {OUTPUT_MERGED.name} ({len(df_merged)} rows, {len(df_merged.columns)} cols)")

        # =========================================================================
        # STEP 9: Final Validation (File Exists Check)
        # =========================================================================
        # Expected: Output file exists with reasonable size (>10 KB for 400 rows)
        # Purpose: Confirm save operation succeeded

        log("[VALIDATE] Verifying output file exists...")
        output_check = check_file_exists(str(OUTPUT_MERGED), min_size_bytes=10000)
        if not output_check['valid']:
            raise FileNotFoundError(
                f"Output file write failed or file too small\n"
                f"Message: {output_check['message']}"
            )
        log(f"[PASS] Output file exists ({output_check['size_bytes']} bytes)")

        # =========================================================================
        # SUCCESS
        # =========================================================================

        log("[SUCCESS] Step 00 complete")
        log(f"[SUMMARY] Loaded {EXPECTED_ROWS} theta scores from RQ 5.5.1")
        log(f"[SUMMARY] Merged with TSVR mapping (0 unmatched rows)")
        log(f"[SUMMARY] Output: {OUTPUT_MERGED.relative_to(PROJECT_ROOT)}")
        log(f"[NEXT] Run step01_create_piecewise_time_variables.py")

        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
