#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code v4.1)
# =============================================================================
"""
Step ID: step04
Step Name: step04_extract_segment_location_slopes
RQ: results/ch5/5.5.2
Generated: 2025-12-04

PURPOSE:
Extract 4 segment-location slopes from piecewise LocationType × Segment interaction LMM.
Computes slopes for Source_Early, Source_Late, Destination_Early, Destination_Late
via linear combinations of model coefficients with delta method SE propagation.

EXPECTED INPUTS:
  - data/step03_piecewise_lmm_model.pkl
    Format: Pickled statsmodels MixedLMResults object
    Description: Fitted piecewise LMM with formula theta ~ Days_within * Segment * LocationType
    Expected: 8 fixed effects coefficients (intercept + 7 interactions)

  - data/step03_lmm_coefficients.csv
    Columns: ['term', 'estimate', 'SE', 'z_score', 'p_value', 'CI_lower', 'CI_upper']
    Expected rows: 8
    Description: Fixed effects table from piecewise LMM (for validation)

EXPECTED OUTPUTS:
  - data/step04_segment_location_slopes.csv
    Columns: ['Segment', 'LocationType', 'slope', 'SE', 'CI_lower', 'CI_upper', 'p_value']
    Expected rows: 4
    Description: Segment-location slopes with delta method SEs (Source_Early, Source_Late,
                 Destination_Early, Destination_Late)

VALIDATION CRITERIA:
  - 4 rows present (Source_Early, Source_Late, Destination_Early, Destination_Late)
  - slope values in [-2, 0] (negative forgetting slopes)
  - SE values in [0.01, 1.0] (positive, finite)
  - CI_lower < slope < CI_upper for all rows
  - p_value in [0, 1]
  - No NaN values

g_code REASONING:
- Approach: Extract slopes via linear combinations of model coefficients. Treatment coding
  with Source = reference for LocationType, Early = reference for Segment means:
  * Source_Early slope = β_Days_within
  * Source_Late slope = β_Days_within + β_Days_within:Segment[T.Late]
  * Destination_Early slope = β_Days_within + β_Days_within:LocationType[T.Destination]
  * Destination_Late slope = β_Days_within + β_Days_within:Segment[T.Late] +
                              β_Days_within:LocationType[T.Destination] +
                              β_Days_within:Segment[T.Late]:LocationType[T.Destination]

- Why this approach: Delta method required for SE propagation through linear combinations
  (SE of sum ≠ sum of SEs due to covariances). Uses variance-covariance matrix from LMM.

- Data flow: Load model → extract coefficients + vcov → compute 4 slopes via linear
  combinations → propagate SE via delta method → compute 95% CIs and p-values → save

- Expected performance: <1 second (no fitting, just coefficient extraction)

IMPLEMENTATION NOTES:
- Analysis tool: Custom stdlib implementation (no catalogued tool for 2-factor interaction slopes)
- Validation tool: tools.validation.validate_dataframe_structure
- Delta method: SE²(aX + bY) = a²Var(X) + b²Var(Y) + 2abCov(X,Y)
- Treatment coding: Source and Early are reference levels (coefficient = 0)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from scipy import stats
from statsmodels.regression.mixed_linear_model import MixedLMResults
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 5.5.2/
#   parents[2] = ch5/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_dataframe_structure

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.5.2 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step04_extract_segment_location_slopes.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step04_segment_location_slopes.csv
#   CORRECT: logs/step04_extract_segment_location_slopes.log
#   WRONG:   results/segment_location_slopes.csv  (wrong folder + no prefix)
#   WRONG:   data/slopes.csv                      (missing step prefix)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 4: Extract segment-location slopes")

        # =========================================================================
        # STEP 1: Load Fitted LMM Model
        # =========================================================================
        # Expected: Piecewise LMM fitted in Step 3 with 8 fixed effects coefficients
        # Purpose: Extract coefficients and variance-covariance matrix for slope calculations

        log("[LOAD] Loading fitted piecewise LMM model...")
        model_path = RQ_DIR / "data" / "step03_piecewise_lmm_model.pkl"

        # Load model using statsmodels method (NOT pickle.load - avoids patsy issues)
        lmm_model = MixedLMResults.load(str(model_path))
        log(f"[LOADED] Piecewise LMM model from {model_path.name}")

        # Extract coefficient names and values
        coef_names = lmm_model.fe_params.index.tolist()
        coef_values = lmm_model.fe_params.values
        coef_se = lmm_model.bse_fe.values

        log(f"[INFO] Model has {len(coef_names)} fixed effects coefficients")
        log(f"[INFO] Coefficients: {coef_names}")

        # =========================================================================
        # STEP 2: Extract Variance-Covariance Matrix
        # =========================================================================
        # Delta method requires full variance-covariance matrix to propagate SE
        # through linear combinations (accounts for covariances between coefficients)

        log("[EXTRACT] Extracting variance-covariance matrix...")
        vcov_full = lmm_model.cov_params()
        log(f"[EXTRACTED] Full variance-covariance matrix shape: {vcov_full.shape}")

        # Extract only fixed effects portion (first n_fe x n_fe)
        n_fe = len(coef_names)
        vcov_matrix = vcov_full.iloc[:n_fe, :n_fe].values  # Convert to numpy for matrix operations
        log(f"[EXTRACTED] Fixed effects vcov matrix shape: {vcov_matrix.shape}")

        # =========================================================================
        # STEP 3: Identify Coefficient Indices
        # =========================================================================
        # Treatment coding: Source = reference for LocationType, Early = reference for Segment
        # Need indices for: Days_within, Days_within:Segment[T.Late],
        #                   Days_within:LocationType[T.Destination],
        #                   Days_within:Segment[T.Late]:LocationType[T.Destination]

        log("[IDENTIFY] Identifying coefficient indices for slope calculations...")

        # Find coefficient indices (case-sensitive matching)
        idx_days_within = coef_names.index('Days_within')
        idx_days_segment_late = coef_names.index('Days_within:Segment[T.Late]')
        idx_days_location_dest = coef_names.index('Days_within:LocationType[T.Destination]')
        idx_days_segment_location = coef_names.index('Days_within:Segment[T.Late]:LocationType[T.Destination]')

        log(f"[FOUND] Days_within index: {idx_days_within}")
        log(f"[FOUND] Days_within:Segment[T.Late] index: {idx_days_segment_late}")
        log(f"[FOUND] Days_within:LocationType[T.Destination] index: {idx_days_location_dest}")
        log(f"[FOUND] Days_within:Segment[T.Late]:LocationType[T.Destination] index: {idx_days_segment_location}")

        # =========================================================================
        # STEP 4: Compute 4 Segment-Location Slopes
        # =========================================================================
        # Linear combinations based on treatment coding:
        # Source_Early = β_Days_within
        # Source_Late = β_Days_within + β_Days_within:Segment[T.Late]
        # Destination_Early = β_Days_within + β_Days_within:LocationType[T.Destination]
        # Destination_Late = sum of all 4 coefficients

        log("[COMPUTE] Computing segment-location slopes via linear combinations...")

        # Slope 1: Source_Early (reference level for both factors)
        slope_source_early = coef_values[idx_days_within]

        # Slope 2: Source_Late (Segment = Late, LocationType = Source)
        slope_source_late = coef_values[idx_days_within] + coef_values[idx_days_segment_late]

        # Slope 3: Destination_Early (Segment = Early, LocationType = Destination)
        slope_destination_early = coef_values[idx_days_within] + coef_values[idx_days_location_dest]

        # Slope 4: Destination_Late (both factors at non-reference levels)
        slope_destination_late = (coef_values[idx_days_within] +
                                  coef_values[idx_days_segment_late] +
                                  coef_values[idx_days_location_dest] +
                                  coef_values[idx_days_segment_location])

        log(f"[COMPUTED] Source_Early slope: {slope_source_early:.6f}")
        log(f"[COMPUTED] Source_Late slope: {slope_source_late:.6f}")
        log(f"[COMPUTED] Destination_Early slope: {slope_destination_early:.6f}")
        log(f"[COMPUTED] Destination_Late slope: {slope_destination_late:.6f}")

        # =========================================================================
        # STEP 5: Propagate Standard Errors via Delta Method
        # =========================================================================
        # Delta method for linear combinations: SE²(aX + bY + cZ) =
        #   a²Var(X) + b²Var(Y) + c²Var(Z) + 2abCov(X,Y) + 2acCov(X,Z) + 2bcCov(Y,Z)

        log("[DELTA] Applying delta method for SE propagation...")

        # SE 1: Source_Early (single coefficient, no combination)
        se_source_early = coef_se[idx_days_within]

        # SE 2: Source_Late (2-term sum)
        # Gradient: [1, 1] for [Days_within, Days_within:Segment[T.Late]]
        gradient_source_late = np.zeros(len(coef_names))
        gradient_source_late[idx_days_within] = 1.0
        gradient_source_late[idx_days_segment_late] = 1.0
        se_source_late = np.sqrt(gradient_source_late @ vcov_matrix @ gradient_source_late)

        # SE 3: Destination_Early (2-term sum)
        # Gradient: [1, 1] for [Days_within, Days_within:LocationType[T.Destination]]
        gradient_destination_early = np.zeros(len(coef_names))
        gradient_destination_early[idx_days_within] = 1.0
        gradient_destination_early[idx_days_location_dest] = 1.0
        se_destination_early = np.sqrt(gradient_destination_early @ vcov_matrix @ gradient_destination_early)

        # SE 4: Destination_Late (4-term sum)
        # Gradient: [1, 1, 1, 1] for all 4 coefficients
        gradient_destination_late = np.zeros(len(coef_names))
        gradient_destination_late[idx_days_within] = 1.0
        gradient_destination_late[idx_days_segment_late] = 1.0
        gradient_destination_late[idx_days_location_dest] = 1.0
        gradient_destination_late[idx_days_segment_location] = 1.0
        se_destination_late = np.sqrt(gradient_destination_late @ vcov_matrix @ gradient_destination_late)

        log(f"[DELTA] Source_Early SE: {se_source_early:.6f}")
        log(f"[DELTA] Source_Late SE: {se_source_late:.6f}")
        log(f"[DELTA] Destination_Early SE: {se_destination_early:.6f}")
        log(f"[DELTA] Destination_Late SE: {se_destination_late:.6f}")

        # =========================================================================
        # STEP 6: Compute 95% CIs and p-values
        # =========================================================================
        # 95% CI: slope ± 1.96 × SE
        # p-value: Two-tailed z-test (H0: slope = 0)

        log("[CI] Computing 95% confidence intervals and p-values...")

        # Define slope data for iteration
        slopes_data = [
            ('Early', 'Source', slope_source_early, se_source_early),
            ('Late', 'Source', slope_source_late, se_source_late),
            ('Early', 'Destination', slope_destination_early, se_destination_early),
            ('Late', 'Destination', slope_destination_late, se_destination_late),
        ]

        results = []
        for segment, location_type, slope, se in slopes_data:
            # 95% CI
            ci_lower = slope - 1.96 * se
            ci_upper = slope + 1.96 * se

            # Two-tailed z-test p-value
            z_score = slope / se
            p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))

            results.append({
                'Segment': segment,
                'LocationType': location_type,
                'slope': slope,
                'SE': se,
                'CI_lower': ci_lower,
                'CI_upper': ci_upper,
                'p_value': p_value
            })

            log(f"[CI] {location_type}_{segment}: slope={slope:.6f}, SE={se:.6f}, "
                f"95%CI=[{ci_lower:.6f}, {ci_upper:.6f}], p={p_value:.6f}")

        # =========================================================================
        # STEP 7: Save Segment-Location Slopes
        # =========================================================================
        # Output: 4 rows (Source_Early, Source_Late, Destination_Early, Destination_Late)
        # Columns: Segment, LocationType, slope, SE, CI_lower, CI_upper, p_value

        log("[SAVE] Saving segment-location slopes...")
        df_slopes = pd.DataFrame(results)
        output_path = RQ_DIR / "data" / "step04_segment_location_slopes.csv"
        df_slopes.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(df_slopes)} rows, {len(df_slopes.columns)} columns)")

        # =========================================================================
        # STEP 8: Run Validation Tool
        # =========================================================================
        # Tool: validate_dataframe_structure
        # Validates: 4 rows present, correct columns, no NaN values, expected ranges

        log("[VALIDATION] Running validate_dataframe_structure...")
        validation_result = validate_dataframe_structure(
            df=df_slopes,
            expected_rows=4,
            expected_columns=['Segment', 'LocationType', 'slope', 'SE', 'CI_lower', 'CI_upper', 'p_value']
        )

        # Report validation results
        if validation_result['valid']:
            log(f"[VALIDATION] PASS: {validation_result['message']}")
        else:
            log(f"[VALIDATION] FAIL: {validation_result['message']}")
            log(f"[VALIDATION] Failed checks: {validation_result.get('checks', {})}")
            raise ValueError(f"Validation failed: {validation_result['message']}")

        # Additional range checks (beyond structural validation)
        log("[VALIDATION] Running additional range checks...")

        # Check slope range [-2, 0]
        slope_min, slope_max = df_slopes['slope'].min(), df_slopes['slope'].max()
        if not (-2 <= slope_min and slope_max <= 0):
            log(f"[VALIDATION] WARNING: Slopes outside expected range [-2, 0]: min={slope_min:.3f}, max={slope_max:.3f}")

        # Check SE range [0.01, 1.0]
        se_min, se_max = df_slopes['SE'].min(), df_slopes['SE'].max()
        if not (0.01 <= se_min and se_max <= 1.0):
            log(f"[VALIDATION] WARNING: SEs outside expected range [0.01, 1.0]: min={se_min:.3f}, max={se_max:.3f}")

        # Check CI bounds
        ci_violations = df_slopes[(df_slopes['CI_lower'] >= df_slopes['slope']) |
                                  (df_slopes['slope'] >= df_slopes['CI_upper'])]
        if len(ci_violations) > 0:
            log(f"[VALIDATION] ERROR: {len(ci_violations)} rows with CI_lower >= slope or slope >= CI_upper")
            raise ValueError("Confidence interval bounds violated")

        # Check p-value range [0, 1]
        p_min, p_max = df_slopes['p_value'].min(), df_slopes['p_value'].max()
        if not (0 <= p_min and p_max <= 1):
            log(f"[VALIDATION] ERROR: p-values outside [0, 1]: min={p_min:.6f}, max={p_max:.6f}")
            raise ValueError("p-values outside valid range")

        # Check for NaN values
        nan_count = df_slopes.isna().sum().sum()
        if nan_count > 0:
            log(f"[VALIDATION] ERROR: {nan_count} NaN values found in output")
            raise ValueError("NaN values detected in output")

        log("[VALIDATION] All range checks passed")

        log("[SUCCESS] Step 4 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
