#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step01
Step Name: Prepare Piecewise LMM Input
RQ: results/ch5/rq6
Generated: 2025-11-25

PURPOSE:
Reshape theta scores to long format, merge TSVR timestamps, create piecewise
segment structure (Early 0-24h, Late 24-168h) with Days_within variable
centered at segment start.

EXPECTED INPUTS:
  - data/step00_theta_scores_from_rq5.csv
    Columns: ['composite_ID', 'theta_common', 'theta_congruent', 'theta_incongruent',
              'se_common', 'se_congruent', 'se_incongruent']
    Expected rows: 400 (wide format)

EXPECTED OUTPUTS:
  - data/step01_lmm_input_piecewise.csv
    Columns: ['UID', 'test', 'composite_ID', 'Congruence', 'theta', 'SE',
              'TSVR_hours', 'Segment', 'Days_within']
    Expected rows: 1200 (400 composite_IDs x 3 congruence types)
    Description: Long-format piecewise LMM input with segment structure

VALIDATION CRITERIA:
  - Row count: 1200 rows (400 x 3 congruence types)
  - Column count: 9 columns
  - TSVR merge complete: All 1200 rows have non-null TSVR_hours
  - Segment assignment: TSVR in [0, 24] -> Early; TSVR > 24 -> Late
  - No segment overlap: Each observation in exactly one segment
  - Days_within range: Early [0, 1]; Late [0, 6]
  - All congruence types present for each composite_ID
  - No missing data in critical columns

g_code REASONING:
- Approach: Wide-to-long reshape + TSVR merge + piecewise segmentation
- Why this approach: Piecewise LMM requires segment structure with Days_within
- Data flow: Wide theta -> long format -> TSVR merge -> segment assignment
- Expected performance: ~5 seconds (data reshaping + TSVR extraction)

IMPLEMENTATION NOTES:
- Analysis: INLINE implementation (reference: RQ 5.2 step00)
- Function: assign_piecewise_segments (to be extracted to tools/ later)
- Segments: Early (0-24h consolidation), Late (24-168h decay)
- Days_within: Centered at segment start (Early: TSVR/24, Late: (TSVR-24)/24)
- Treatment coding: Congruence reference='Common', Segment reference='Early'
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import traceback

# Add project root to path for imports
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import data extraction functions
from data.data import get_tag_value_for_participant, get_participant_list

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/rq6
LOG_FILE = RQ_DIR / "logs" / "step01_prepare_piecewise_input.log"

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Helper Functions (INLINE IMPLEMENTATION - to extract to tools/ later)
# =============================================================================

def extract_tsvr_hours(composite_id: str) -> float:
    """
    Extract TSVR_hours from master.xlsx for given composite_ID.

    Args:
        composite_id: Format "{UID}_{test}" (e.g., "001_1")

    Returns:
        TSVR value in hours (float)

    Raises:
        ValueError: If TSVR tag not found or invalid
    """
    # Parse composite_ID
    uid, test = composite_id.split('_')
    test_num = int(test)

    # Map test number to session label
    test_map = {1: 'T1', 2: 'T2', 3: 'T3', 4: 'T4'}
    test_label = test_map[test_num]

    # Build TSVR tag name: {UID}-RVR-{Test}-STA-X-TSVR
    tsvr_tag = f"{uid}-RVR-{test_label}-STA-X-TSVR"

    # Extract TSVR value
    tsvr_value = get_tag_value_for_participant(uid, tsvr_tag)

    if tsvr_value is None or pd.isna(tsvr_value):
        raise ValueError(f"TSVR not found for {composite_id} (tag: {tsvr_tag})")

    return float(tsvr_value)

def assign_piecewise_segments(df: pd.DataFrame) -> pd.DataFrame:
    """
    Assign piecewise segments and compute Days_within.

    Segments:
      - Early: TSVR_hours in [0, 24] hours (Days 0-1, consolidation phase)
      - Late: TSVR_hours in (24, 168] hours (Days 1-6, decay phase)

    Days_within (centered at segment start):
      - Early: Days_within = TSVR_hours / 24
      - Late: Days_within = (TSVR_hours - 24) / 24

    Args:
        df: DataFrame with TSVR_hours column

    Returns:
        DataFrame with Segment and Days_within columns added
    """
    df = df.copy()

    # Assign Segment
    df['Segment'] = pd.cut(
        df['TSVR_hours'],
        bins=[0, 24, 168],
        labels=['Early', 'Late'],
        include_lowest=True  # Include TSVR=0 in Early segment
    )

    # Compute Days_within (centered at segment start)
    df['Days_within'] = np.where(
        df['Segment'] == 'Early',
        df['TSVR_hours'] / 24,  # Early: 0-1 days
        (df['TSVR_hours'] - 24) / 24  # Late: 0-6 days (within Late segment)
    )

    return df

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 1: Prepare Piecewise LMM Input")

        # =========================================================================
        # STEP 1: Load Theta Scores
        # =========================================================================
        # Expected: 400 rows x 7 columns (wide format)
        # Purpose: Source data for reshaping

        log("[LOAD] Loading theta scores from Step 0...")

        input_path = RQ_DIR / "data" / "step00_theta_scores_from_rq5.csv"
        df_theta_wide = pd.read_csv(input_path, encoding='utf-8')

        log(f"[LOADED] {input_path.name} ({len(df_theta_wide)} rows, {len(df_theta_wide.columns)} cols)")

        # =========================================================================
        # STEP 2: Reshape Wide to Long
        # =========================================================================
        # Operation: Melt theta_common/congruent/incongruent into (Congruence, theta, SE)
        # Output: 1200 rows (400 composite_IDs x 3 congruence types)

        log("[RESHAPE] Reshaping wide to long format...")

        # Melt theta columns
        df_theta_long = df_theta_wide.melt(
            id_vars=['composite_ID'],
            value_vars=['theta_common', 'theta_congruent', 'theta_incongruent'],
            var_name='congruence_type',
            value_name='theta'
        )

        # Melt SE columns
        df_se_long = df_theta_wide.melt(
            id_vars=['composite_ID'],
            value_vars=['se_common', 'se_congruent', 'se_incongruent'],
            var_name='se_type',
            value_name='SE'
        )

        # Combine theta and SE
        df_long = df_theta_long.copy()
        df_long['SE'] = df_se_long['SE']

        # Clean up congruence type labels
        df_long['Congruence'] = df_long['congruence_type'].str.replace('theta_', '').str.capitalize()
        df_long = df_long.drop(columns=['congruence_type'])

        log(f"[RESHAPE] Long format created: {len(df_long)} rows")

        # =========================================================================
        # STEP 3: Parse composite_ID
        # =========================================================================
        # Operation: Extract UID and test from composite_ID (format: {UID}_{test})
        # Purpose: Needed for TSVR extraction

        log("[PARSE] Extracting UID and test from composite_ID...")

        df_long[['UID', 'test']] = df_long['composite_ID'].str.split('_', expand=True)
        df_long['test'] = df_long['test'].astype(int)

        log(f"[PARSE] UID and test extracted")

        # =========================================================================
        # STEP 4: Merge TSVR Hours
        # =========================================================================
        # Operation: Extract TSVR_hours from master.xlsx using data.py functions
        # Expected: All 1200 rows matched (no missing TSVR)

        log("[TSVR] Extracting TSVR timestamps from master.xlsx...")

        tsvr_values = []
        missing_count = 0

        for composite_id in df_long['composite_ID'].unique():
            try:
                tsvr = extract_tsvr_hours(composite_id)
                tsvr_values.append({'composite_ID': composite_id, 'TSVR_hours': tsvr})
            except Exception as e:
                log(f"[WARNING] TSVR extraction failed for {composite_id}: {e}")
                missing_count += 1

        if missing_count > 0:
            raise ValueError(
                f"TSVR extraction incomplete: {missing_count} composite_IDs missing TSVR"
            )

        # Merge TSVR into long format
        df_tsvr = pd.DataFrame(tsvr_values)
        df_long = df_long.merge(df_tsvr, on='composite_ID', how='left')

        # Check merge completeness
        if df_long['TSVR_hours'].isna().any():
            n_missing = df_long['TSVR_hours'].isna().sum()
            raise ValueError(
                f"TSVR merge incomplete: {n_missing} rows have missing TSVR_hours"
            )

        log(f"[TSVR] Merged TSVR_hours for all {len(df_long)} rows")

        # =========================================================================
        # STEP 5: Assign Piecewise Segments
        # =========================================================================
        # Operation: Create Segment (Early/Late) and Days_within variables
        # Early: TSVR in [0, 24] hours; Late: TSVR in (24, 168] hours

        log("[SEGMENT] Assigning piecewise segments...")

        df_piecewise = assign_piecewise_segments(df_long)

        # Verify no segment overlap
        if df_piecewise['Segment'].isna().any():
            n_missing = df_piecewise['Segment'].isna().sum()
            raise ValueError(
                f"Segment assignment incomplete: {n_missing} rows have missing Segment"
            )

        # Count observations per segment
        segment_counts = df_piecewise['Segment'].value_counts()
        log(f"[SEGMENT] Segment distribution:")
        for segment, count in segment_counts.items():
            log(f"  - {segment}: {count} rows")

        # =========================================================================
        # STEP 6: Apply Treatment Coding
        # =========================================================================
        # Set reference levels: Congruence='Common', Segment='Early'

        log("[CODING] Applying treatment coding...")

        # Convert to categorical with explicit reference levels
        df_piecewise['Congruence'] = pd.Categorical(
            df_piecewise['Congruence'],
            categories=['Common', 'Congruent', 'Incongruent'],
            ordered=False
        )

        df_piecewise['Segment'] = pd.Categorical(
            df_piecewise['Segment'],
            categories=['Early', 'Late'],
            ordered=False
        )

        log(f"[CODING] Treatment coding applied (Congruence ref='Common', Segment ref='Early')")

        # =========================================================================
        # STEP 7: Validate and Save
        # =========================================================================

        log("[VALIDATION] Validating piecewise input structure...")

        # Check row count
        expected_rows = 1200
        if len(df_piecewise) != expected_rows:
            raise ValueError(
                f"Row count incorrect: expected {expected_rows}, found {len(df_piecewise)}"
            )
        log(f"[PASS] Row count correct (1200)")

        # Check column count
        expected_cols = ['UID', 'test', 'composite_ID', 'Congruence', 'theta', 'SE',
                         'TSVR_hours', 'Segment', 'Days_within']
        if len(df_piecewise.columns) != len(expected_cols):
            raise ValueError(
                f"Column count incorrect: expected {len(expected_cols)}, "
                f"found {len(df_piecewise.columns)}"
            )
        log(f"[PASS] Column count correct (9)")

        # Check Days_within ranges
        early_days = df_piecewise[df_piecewise['Segment'] == 'Early']['Days_within']
        late_days = df_piecewise[df_piecewise['Segment'] == 'Late']['Days_within']

        if early_days.min() < 0 or early_days.max() > 1:
            raise ValueError(
                f"Early Days_within range invalid: [{early_days.min():.3f}, {early_days.max():.3f}], "
                f"expected [0, 1]"
            )

        if late_days.min() < 0 or late_days.max() > 6:
            raise ValueError(
                f"Late Days_within range invalid: [{late_days.min():.3f}, {late_days.max():.3f}], "
                f"expected [0, 6]"
            )

        log(f"[PASS] Days_within ranges valid (Early: [0, 1], Late: [0, 6])")

        # Check no missing data
        critical_cols = ['theta', 'SE', 'TSVR_hours', 'Segment', 'Days_within']
        if df_piecewise[critical_cols].isna().any().any():
            n_missing = df_piecewise[critical_cols].isna().sum().sum()
            raise ValueError(
                f"Missing data detected: {n_missing} NaN values in critical columns"
            )
        log(f"[PASS] No missing data")

        # Reorder columns for clarity
        df_piecewise = df_piecewise[expected_cols]

        # Save output
        output_path = RQ_DIR / "data" / "step01_lmm_input_piecewise.csv"
        df_piecewise.to_csv(output_path, index=False, encoding='utf-8')

        log(f"[SAVED] {output_path.name} ({len(df_piecewise)} rows, {len(df_piecewise.columns)} cols)")

        log("[SUCCESS] Step 1 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
