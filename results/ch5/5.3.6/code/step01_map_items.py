#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated for RQ 5.3.6)
# =============================================================================
"""
Step ID: step01
Step Name: Map Items by Paradigm (Retained vs Removed)
RQ: results/ch5/5.3.6
Generated: 2025-12-03

PURPOSE:
Map purified items to paradigms (IFR/ICR/IRE) and create retention summary
showing which items passed IRT purification (from RQ 5.3.1) and which were removed.
This enables paradigm-specific CTT scoring comparison (Full vs Purified).

EXPECTED INPUTS:
- data/step00_purified_items.csv
  Columns: ['item', 'domain', 'Discrimination', 'Difficulty_1']
  Format: CSV with purified items from RQ 5.3.1 (45 items retained after IRT purification)
  Expected rows: ~45

EXPECTED OUTPUTS:
- data/step01_item_mapping.csv
  Columns: ['item_name', 'paradigm', 'retained', 'a', 'b', 'exclusion_reason']
  Format: One row per item (45 rows - only retained items)
  Expected rows: ~45

- data/step01_retention_summary.csv
  Columns: ['paradigm', 'total_items', 'retained_items', 'removed_items', 'retention_rate']
  Format: 3 rows (IFR, ICR, IRE)
  Expected rows: 3

VALIDATION CRITERIA:
- All 3 paradigms present (IFR, ICR, IRE)
- retention_rate in [0, 1]
- retained_items + removed_items = total_items
- No NaN values

g_code REASONING:
- Approach: Map domain names to paradigm codes, count retention per paradigm
- Why this approach: Establishes item-paradigm mapping for downstream CTT scoring
- Data flow: Purified items (from RQ 5.3.1) → paradigm mapping → retention summary
- Expected performance: <1 second (simple data transformation)

IMPLEMENTATION NOTES:
- Analysis tool: Pure pandas (no external tool needed)
- Validation tool: Basic checks (all paradigms present, valid retention rates)
- Parameters: Domain mapping (free_recall→IFR, cued_recall→ICR, recognition→IRE)
- Original item counts: IFR=24, ICR=24, IRE=24 (from RQ 5.3.1 step00_irt_input.csv)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.3.6
LOG_FILE = RQ_DIR / "logs" / "step01_map_items.log"

# Domain to paradigm mapping
DOMAIN_TO_PARADIGM = {
    'free_recall': 'IFR',
    'cued_recall': 'ICR',
    'recognition': 'IRE'
}

# Original item counts from RQ 5.3.1 (before purification)
ORIGINAL_ITEM_COUNTS = {
    'IFR': 24,
    'ICR': 24,
    'IRE': 24
}

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step01_item_mapping.csv
#   CORRECT: data/step01_retention_summary.csv
#   WRONG:   results/item_mapping.csv  (wrong folder + no prefix)
#   WRONG:   data/item_mapping.csv     (missing step prefix)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 01: Map Items by Paradigm (Retained vs Removed)")

        # =========================================================================
        # STEP 1: Load Purified Items
        # =========================================================================
        # Expected: Purified items from RQ 5.3.1 IRT analysis (45 items retained)
        # Purpose: Map to paradigms and create retention summary

        log("[LOAD] Loading purified items...")
        input_path = RQ_DIR / "data" / "step00_purified_items.csv"
        df_purified = pd.read_csv(input_path)
        log(f"[LOADED] {input_path.name} ({len(df_purified)} rows, {len(df_purified.columns)} cols)")

        # Verify expected columns
        expected_cols = ['item', 'domain', 'Discrimination', 'Difficulty_1']
        if list(df_purified.columns) != expected_cols:
            log(f"[ERROR] Column mismatch. Expected: {expected_cols}, Got: {list(df_purified.columns)}")
            sys.exit(1)

        # =========================================================================
        # STEP 2: Map Domain to Paradigm
        # =========================================================================
        # Tool: pandas.map()
        # What it does: Converts domain names (free_recall, cued_recall, recognition)
        #               to paradigm codes (IFR, ICR, IRE)
        # Expected output: DataFrame with paradigm column added

        log("[ANALYSIS] Mapping domains to paradigms...")
        df_purified['paradigm'] = df_purified['domain'].map(DOMAIN_TO_PARADIGM)

        # Check for unmapped domains
        unmapped = df_purified['paradigm'].isna().sum()
        if unmapped > 0:
            log(f"[ERROR] {unmapped} items have unmapped domains")
            log(f"[ERROR] Unmapped domains: {df_purified[df_purified['paradigm'].isna()]['domain'].unique()}")
            sys.exit(1)

        log(f"[DONE] Mapped {len(df_purified)} items to paradigms")
        log(f"[INFO] Paradigm distribution: {df_purified['paradigm'].value_counts().to_dict()}")

        # =========================================================================
        # STEP 3: Create Item Mapping Table
        # =========================================================================
        # Output: data/step01_item_mapping.csv
        # Contains: One row per retained item with paradigm, IRT parameters
        # Columns: item_name, paradigm, retained, a, b, exclusion_reason

        log("[SAVE] Creating item mapping table...")
        df_mapping = pd.DataFrame({
            'item_name': df_purified['item'],
            'paradigm': df_purified['paradigm'],
            'retained': True,  # All items in this list were retained after purification
            'a': df_purified['Discrimination'],  # IRT discrimination parameter
            'b': df_purified['Difficulty_1'],    # IRT difficulty parameter
            'exclusion_reason': 'retained'       # All passed purification
        })

        output_path = RQ_DIR / "data" / "step01_item_mapping.csv"
        df_mapping.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(df_mapping)} rows, {len(df_mapping.columns)} cols)")

        # =========================================================================
        # STEP 4: Create Retention Summary
        # =========================================================================
        # Output: data/step01_retention_summary.csv
        # Contains: 3 rows (IFR, ICR, IRE) with retention statistics
        # Columns: paradigm, total_items, retained_items, removed_items, retention_rate

        log("[SAVE] Creating retention summary...")

        # Count retained items per paradigm
        retained_counts = df_purified['paradigm'].value_counts().to_dict()

        # Build summary table
        summary_data = []
        for paradigm in ['IFR', 'ICR', 'IRE']:
            total = ORIGINAL_ITEM_COUNTS[paradigm]
            retained = retained_counts.get(paradigm, 0)
            removed = total - retained
            retention_rate = retained / total if total > 0 else 0.0

            summary_data.append({
                'paradigm': paradigm,
                'total_items': total,
                'retained_items': retained,
                'removed_items': removed,
                'retention_rate': retention_rate
            })

        df_summary = pd.DataFrame(summary_data)
        output_path = RQ_DIR / "data" / "step01_retention_summary.csv"
        df_summary.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(df_summary)} rows, {len(df_summary.columns)} cols)")

        # Log retention statistics
        for _, row in df_summary.iterrows():
            log(f"[INFO] {row['paradigm']}: {row['retained_items']}/{row['total_items']} retained ({row['retention_rate']:.1%})")

        # =========================================================================
        # STEP 5: Run Validation
        # =========================================================================
        # Validates: All 3 paradigms present, retention rates valid, no NaNs
        # Threshold: retention_rate in [0, 1], retained + removed = total

        log("[VALIDATION] Validating outputs...")

        # Check 1: All 3 paradigms present
        paradigms = set(df_summary['paradigm'])
        if paradigms != {'IFR', 'ICR', 'IRE'}:
            log(f"[VALIDATION FAIL] Expected paradigms ['IFR', 'ICR', 'IRE'], got {paradigms}")
            sys.exit(1)
        log("[VALIDATION PASS] All 3 paradigms present")

        # Check 2: retention_rate in [0, 1]
        invalid_rates = df_summary[(df_summary['retention_rate'] < 0) | (df_summary['retention_rate'] > 1)]
        if len(invalid_rates) > 0:
            log(f"[VALIDATION FAIL] Invalid retention rates: {invalid_rates['retention_rate'].tolist()}")
            sys.exit(1)
        log("[VALIDATION PASS] All retention rates in [0, 1]")

        # Check 3: retained + removed = total
        df_summary['sum_check'] = df_summary['retained_items'] + df_summary['removed_items']
        mismatch = df_summary[df_summary['sum_check'] != df_summary['total_items']]
        if len(mismatch) > 0:
            log(f"[VALIDATION FAIL] retained + removed != total for: {mismatch['paradigm'].tolist()}")
            sys.exit(1)
        log("[VALIDATION PASS] retained + removed = total for all paradigms")

        # Check 4: No NaN values
        if df_mapping.isna().any().any():
            log("[VALIDATION FAIL] NaN values found in item mapping")
            log(f"[INFO] Columns with NaN: {df_mapping.columns[df_mapping.isna().any()].tolist()}")
            sys.exit(1)
        if df_summary.isna().any().any():
            log("[VALIDATION FAIL] NaN values found in retention summary")
            log(f"[INFO] Columns with NaN: {df_summary.columns[df_summary.isna().any()].tolist()}")
            sys.exit(1)
        log("[VALIDATION PASS] No NaN values in outputs")

        log("[SUCCESS] Step 01 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
