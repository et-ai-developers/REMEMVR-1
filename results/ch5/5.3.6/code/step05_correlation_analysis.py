#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated for RQ 5.3.6)
# =============================================================================
"""
Step ID: step05
Step Name: Correlation Analysis with Steiger's Z-Test
RQ: 5.3.6 - Purified CTT Effects (Paradigms)
Generated: 2025-12-04

PURPOSE:
Test if Purified CTT correlates more strongly with IRT theta than Full CTT does.
Uses Steiger's z-test for dependent correlations with Bonferroni correction (D068).

EXPECTED INPUTS:
- data/step02_ctt_full_scores.csv
  Columns: ['UID', 'test', 'CTT_full_IFR', 'CTT_full_ICR', 'CTT_full_IRE']
  Format: Wide format, one row per UID × test
  Expected rows: ~400 (100 participants × 4 tests)

- data/step03_ctt_purified_scores.csv
  Columns: ['UID', 'test', 'CTT_purified_IFR', 'CTT_purified_ICR', 'CTT_purified_IRE']
  Format: Wide format, one row per UID × test
  Expected rows: ~400

- data/step00_theta_scores.csv
  Columns: ['composite_ID', 'domain_name', 'theta']
  Format: Long format with domain names (free_recall, cued_recall, recognition)
  Expected rows: ~1200 (400 observations × 3 domains)

EXPECTED OUTPUTS:
- data/step05_correlation_analysis.csv
  Columns: ['paradigm', 'r_full', 'r_purified', 'delta_r', 'steiger_z',
            'p_uncorrected', 'p_bonferroni', 'normality_test_p', 'linearity_flag']
  Expected rows: 3 (one per paradigm: IFR, ICR, IRE)

- data/step05_steiger_assumptions_report.txt
  Plain text report documenting assumption checks

VALIDATION CRITERIA:
- All correlations in [-1, 1]
- All p-values in [0, 1]
- p_bonferroni capped at 1.0
- All 3 paradigms present
- BOTH p_uncorrected AND p_bonferroni columns present (Decision D068)

REASONING:
- Approach: For each paradigm, compute r(Full CTT, IRT) and r(Purified CTT, IRT),
  then use Steiger's z-test to test if r_purified > r_full
- Why this approach: Dependent correlations share the IRT theta variable, so
  standard Fisher's z is invalid. Steiger's method accounts for covariance.
- Data flow:
  1. Load 3 score files
  2. Reshape theta from long to wide (pivot on domain_name)
  3. Merge Full CTT + Purified CTT + IRT theta on UID + test
  4. For each paradigm, compute correlations and run Steiger's test
  5. Apply Bonferroni correction (3 comparisons)
- Expected performance: ~seconds (lightweight computation)

IMPLEMENTATION NOTES:
- Analysis tool: tools.analysis_ctt.compare_correlations_dependent
- Validation tool: tools.validation.validate_contrasts_d068
- Parameters: Bonferroni correction for 3 comparisons, one-tailed test
- Domain name mapping: free_recall→IFR, cued_recall→ICR, recognition→IRE
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback
from scipy import stats

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_ctt import compare_correlations_dependent

# Import validation tool
from tools.validation import validate_contrasts_d068

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.3.6
LOG_FILE = RQ_DIR / "logs" / "step05_correlation_analysis.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.log

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Helper Functions
# =============================================================================

def standardize_test_column(df: pd.DataFrame, test_col: str = 'test') -> pd.DataFrame:
    """
    Standardize test column to integer format.

    Handles both 'T1'/'T2'/etc and integer 1/2/etc formats.
    """
    df = df.copy()

    # Check if test column contains strings like 'T1', 'T2'
    if df[test_col].dtype == 'object':
        # Convert 'T1' -> 1, 'T2' -> 2, etc.
        df[test_col] = df[test_col].str.replace('T', '').astype(int)
    else:
        # Already integers
        df[test_col] = df[test_col].astype(int)

    return df

def create_composite_id(df: pd.DataFrame, uid_col: str = 'UID', test_col: str = 'test') -> pd.DataFrame:
    """Create composite_ID column from UID and test."""
    df = df.copy()
    df['composite_ID'] = df[uid_col] + '_' + df[test_col].astype(str)
    return df

def pivot_theta_long_to_wide(df_theta: pd.DataFrame) -> pd.DataFrame:
    """
    Pivot theta scores from long to wide format.

    Input: composite_ID, domain_name, theta (long format)
    Output: composite_ID, theta_IFR, theta_ICR, theta_IRE (wide format)

    Domain name mapping:
    - free_recall -> IFR (Immediate Free Recall)
    - cued_recall -> ICR (Immediate Cued Recall)
    - recognition -> IRE (Immediate Recognition)
    """
    # Map domain names to paradigm names
    domain_map = {
        'free_recall': 'IFR',
        'cued_recall': 'ICR',
        'recognition': 'IRE'
    }

    df = df_theta.copy()
    df['paradigm'] = df['domain_name'].map(domain_map)

    # Check if all domains mapped successfully
    unmapped = df[df['paradigm'].isna()]['domain_name'].unique()
    if len(unmapped) > 0:
        raise ValueError(f"Unmapped domain names: {unmapped}")

    # Pivot to wide format
    df_wide = df.pivot(index='composite_ID', columns='paradigm', values='theta')
    df_wide.columns = [f'theta_{col}' for col in df_wide.columns]
    df_wide = df_wide.reset_index()

    return df_wide

def check_multivariate_normality(data: pd.DataFrame, columns: List[str]) -> Tuple[float, str]:
    """
    Check multivariate normality using Mardia's test approximation.

    For practical purposes, we check univariate normality for each variable
    using Shapiro-Wilk test (n < 5000 recommended) and flag if violations exist.

    Returns:
        Tuple of (p_value, interpretation)
        p_value: Minimum p-value across variables (most violated)
        interpretation: String describing normality status
    """
    p_values = []

    for col in columns:
        # Shapiro-Wilk test for normality
        stat, p = stats.shapiro(data[col].dropna())
        p_values.append(p)

    min_p = min(p_values)

    if min_p > 0.05:
        interpretation = "All variables approximately normal (Shapiro-Wilk p > 0.05)"
    elif min_p > 0.01:
        interpretation = "Mild normality violations (Shapiro-Wilk p between 0.01-0.05)"
    else:
        interpretation = "Moderate normality violations (Shapiro-Wilk p < 0.01)"

    return float(min_p), interpretation

def check_linearity(x: pd.Series, y: pd.Series, threshold_r: float = 0.3) -> str:
    """
    Check linearity assumption via correlation strength.

    Simple heuristic: If |r| > threshold, assume linear relationship adequate.
    More sophisticated methods (e.g., residual plots) could be added.
    """
    r, _ = stats.pearsonr(x, y)

    if abs(r) > threshold_r:
        return "linear"
    else:
        return "nonlinear"

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 05: Correlation Analysis with Steiger's Z-Test")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        log("[LOAD] Loading input data...")

        # Load Full CTT scores
        ctt_full = pd.read_csv(RQ_DIR / "data/step02_ctt_full_scores.csv")
        log(f"[LOADED] Full CTT: {len(ctt_full)} rows, {len(ctt_full.columns)} cols")
        log(f"[INFO] Full CTT columns: {list(ctt_full.columns)}")
        log(f"[INFO] Full CTT test column dtype: {ctt_full['test'].dtype}")

        # Load Purified CTT scores
        ctt_purified = pd.read_csv(RQ_DIR / "data/step03_ctt_purified_scores.csv")
        log(f"[LOADED] Purified CTT: {len(ctt_purified)} rows, {len(ctt_purified.columns)} cols")
        log(f"[INFO] Purified CTT test column dtype: {ctt_purified['test'].dtype}")

        # Load IRT theta scores
        theta_long = pd.read_csv(RQ_DIR / "data/step00_theta_scores.csv")
        log(f"[LOADED] IRT theta (long format): {len(theta_long)} rows, {len(theta_long.columns)} cols")
        log(f"[INFO] Theta domain names: {theta_long['domain_name'].unique()}")

        # =========================================================================
        # STEP 2: Reshape Theta Scores from Long to Wide Format
        # =========================================================================
        log("[RESHAPE] Pivoting theta scores to wide format...")

        theta_wide = pivot_theta_long_to_wide(theta_long)
        log(f"[RESHAPED] Theta wide: {len(theta_wide)} rows, columns: {list(theta_wide.columns)}")

        # Validate theta wide format
        expected_theta_cols = ['composite_ID', 'theta_IFR', 'theta_ICR', 'theta_IRE']
        if not all(col in theta_wide.columns for col in expected_theta_cols):
            raise ValueError(f"Theta wide format missing expected columns: {expected_theta_cols}")

        # =========================================================================
        # STEP 3: Standardize Test Columns and Create Composite IDs
        # =========================================================================
        log("[STANDARDIZE] Standardizing test columns and creating composite IDs...")

        # Standardize test columns to integers
        ctt_full = standardize_test_column(ctt_full)
        ctt_purified = standardize_test_column(ctt_purified)

        # Create composite_ID for merging
        ctt_full = create_composite_id(ctt_full)
        ctt_purified = create_composite_id(ctt_purified)

        log(f"[INFO] Full CTT composite_ID sample: {ctt_full['composite_ID'].head(3).tolist()}")
        log(f"[INFO] Purified CTT composite_ID sample: {ctt_purified['composite_ID'].head(3).tolist()}")
        log(f"[INFO] Theta composite_ID sample: {theta_wide['composite_ID'].head(3).tolist()}")

        # =========================================================================
        # STEP 4: Merge All Three Score Types on composite_ID
        # =========================================================================
        log("[MERGE] Merging Full CTT, Purified CTT, and IRT theta scores...")

        # Merge Full CTT and Purified CTT
        merged = ctt_full.merge(
            ctt_purified[['composite_ID', 'CTT_purified_IFR', 'CTT_purified_ICR', 'CTT_purified_IRE']],
            on='composite_ID',
            how='inner'
        )
        log(f"[MERGED] After CTT merge: {len(merged)} rows")

        # Merge with theta scores
        merged = merged.merge(
            theta_wide,
            on='composite_ID',
            how='inner'
        )
        log(f"[MERGED] After theta merge: {len(merged)} rows")
        log(f"[INFO] Final columns: {list(merged.columns)}")

        # Validation: Check for missing data
        missing_count = merged.isnull().sum().sum()
        if missing_count > 0:
            log(f"[WARNING] {missing_count} missing values detected in merged data")
            log(f"[INFO] Missing by column:\n{merged.isnull().sum()}")

        # =========================================================================
        # STEP 5: Compute Correlations and Run Steiger's Z-Test for Each Paradigm
        # =========================================================================
        log("[ANALYSIS] Running Steiger's z-test for each paradigm...")

        paradigms = ['IFR', 'ICR', 'IRE']
        results = []
        assumptions_report = []

        assumptions_report.append("=" * 80)
        assumptions_report.append("STEIGER'S Z-TEST ASSUMPTION CHECKS")
        assumptions_report.append("RQ 5.3.6 Step 05: Correlation Analysis")
        assumptions_report.append("=" * 80)
        assumptions_report.append("")

        for paradigm in paradigms:
            log(f"[PARADIGM] Processing {paradigm}...")

            # Column names
            theta_col = f'theta_{paradigm}'
            full_col = f'CTT_full_{paradigm}'
            purified_col = f'CTT_purified_{paradigm}'

            # Extract data for this paradigm
            data_subset = merged[[theta_col, full_col, purified_col]].dropna()
            n = len(data_subset)
            log(f"[INFO] {paradigm}: N={n} observations after removing missing values")

            if n < 20:
                log(f"[WARNING] {paradigm}: Sample size ({n}) too small for Steiger's test (requires n>=20)")
                continue

            # Compute correlations
            r_full, p_full = stats.pearsonr(data_subset[theta_col], data_subset[full_col])
            r_purified, p_purified = stats.pearsonr(data_subset[theta_col], data_subset[purified_col])
            r_12, p_12 = stats.pearsonr(data_subset[full_col], data_subset[purified_col])

            log(f"[CORRELATION] {paradigm}: r(IRT, Full CTT) = {r_full:.3f}")
            log(f"[CORRELATION] {paradigm}: r(IRT, Purified CTT) = {r_purified:.3f}")
            log(f"[CORRELATION] {paradigm}: r(Full CTT, Purified CTT) = {r_12:.3f}")

            # Run Steiger's z-test
            # compare_correlations_dependent(r12, r13, r23, n)
            # r12 = correlation between var1 (IRT) and var2 (Full CTT)
            # r13 = correlation between var1 (IRT) and var3 (Purified CTT)
            # r23 = correlation between var2 (Full CTT) and var3 (Purified CTT)
            steiger_result = compare_correlations_dependent(
                r12=r_full,
                r13=r_purified,
                r23=r_12,
                n=n
            )

            z_stat = steiger_result['z_statistic']
            p_uncorrected = steiger_result['p_value']
            delta_r = steiger_result['r_difference']

            # One-tailed test: test if r_purified > r_full
            # Steiger's test returns two-tailed p-value, convert to one-tailed
            if delta_r > 0:
                p_uncorrected_onetailed = p_uncorrected / 2
            else:
                p_uncorrected_onetailed = 1 - (p_uncorrected / 2)

            log(f"[STEIGER] {paradigm}: z = {z_stat:.3f}, p_uncorrected (one-tailed) = {p_uncorrected_onetailed:.4f}")
            log(f"[STEIGER] {paradigm}: delta_r = {delta_r:.3f} (r_purified - r_full)")

            # Assumption checks
            assumptions_report.append(f"PARADIGM: {paradigm}")
            assumptions_report.append("-" * 40)

            # 1. Multivariate normality
            norm_cols = [theta_col, full_col, purified_col]
            normality_p, norm_interp = check_multivariate_normality(data_subset, norm_cols)
            assumptions_report.append(f"1. Multivariate Normality:")
            assumptions_report.append(f"   Shapiro-Wilk min p-value: {normality_p:.4f}")
            assumptions_report.append(f"   Interpretation: {norm_interp}")

            # 2. Linearity checks
            linearity_full = check_linearity(data_subset[theta_col], data_subset[full_col])
            linearity_purified = check_linearity(data_subset[theta_col], data_subset[purified_col])
            linearity_flag = "linear" if (linearity_full == "linear" and linearity_purified == "linear") else "nonlinear"

            assumptions_report.append(f"2. Linearity:")
            assumptions_report.append(f"   IRT vs Full CTT: {linearity_full} (r={r_full:.3f})")
            assumptions_report.append(f"   IRT vs Purified CTT: {linearity_purified} (r={r_purified:.3f})")
            assumptions_report.append(f"   Overall linearity flag: {linearity_flag}")

            assumptions_report.append("")

            # Store result
            results.append({
                'paradigm': paradigm,
                'r_full': r_full,
                'r_purified': r_purified,
                'delta_r': delta_r,
                'steiger_z': z_stat,
                'p_uncorrected': p_uncorrected_onetailed,
                'p_bonferroni': np.nan,  # Will compute after loop
                'normality_test_p': normality_p,
                'linearity_flag': linearity_flag
            })

        # Convert to DataFrame
        results_df = pd.DataFrame(results)

        # =========================================================================
        # STEP 6: Apply Bonferroni Correction
        # =========================================================================
        log("[CORRECTION] Applying Bonferroni correction for 3 comparisons...")

        n_comparisons = len(paradigms)
        results_df['p_bonferroni'] = results_df['p_uncorrected'] * n_comparisons
        results_df['p_bonferroni'] = results_df['p_bonferroni'].clip(upper=1.0)  # Cap at 1.0

        log(f"[CORRECTION] Bonferroni correction: p_bonferroni = min(p_uncorrected × {n_comparisons}, 1.0)")

        # Log results
        for _, row in results_df.iterrows():
            log(f"[RESULT] {row['paradigm']}: p_uncorrected={row['p_uncorrected']:.4f}, p_bonferroni={row['p_bonferroni']:.4f}")
            if row['p_bonferroni'] < 0.05:
                log(f"[RESULT] {row['paradigm']}: SIGNIFICANT at alpha=0.05 (Bonferroni-corrected)")
            else:
                log(f"[RESULT] {row['paradigm']}: NOT significant at alpha=0.05")

        # =========================================================================
        # STEP 7: Save Outputs
        # =========================================================================
        log("[SAVE] Saving correlation analysis results...")

        # Save correlation analysis results
        output_path = RQ_DIR / "data/step05_correlation_analysis.csv"
        results_df.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path} ({len(results_df)} rows, {len(results_df.columns)} cols)")

        # Save assumptions report
        report_path = RQ_DIR / "data/step05_steiger_assumptions_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(assumptions_report))
        log(f"[SAVED] {report_path}")

        # =========================================================================
        # STEP 8: Validate Output with D068 Compliance Check
        # =========================================================================
        log("[VALIDATION] Validating D068 compliance (dual p-value reporting)...")

        # Check that both p_uncorrected and p_bonferroni columns exist
        required_p_cols = ['p_uncorrected', 'p_bonferroni']
        missing_p_cols = [col for col in required_p_cols if col not in results_df.columns]

        if missing_p_cols:
            raise ValueError(f"D068 violation: Missing p-value columns: {missing_p_cols}")

        log(f"[VALIDATION] D068 compliance: BOTH p-value columns present")

        # Validate correlations in [-1, 1]
        for paradigm in paradigms:
            row = results_df[results_df['paradigm'] == paradigm].iloc[0]
            if not (-1 <= row['r_full'] <= 1):
                raise ValueError(f"{paradigm}: r_full={row['r_full']} out of range [-1, 1]")
            if not (-1 <= row['r_purified'] <= 1):
                raise ValueError(f"{paradigm}: r_purified={row['r_purified']} out of range [-1, 1]")

        log(f"[VALIDATION] All correlations in valid range [-1, 1]")

        # Validate p-values in [0, 1]
        for paradigm in paradigms:
            row = results_df[results_df['paradigm'] == paradigm].iloc[0]
            if not (0 <= row['p_uncorrected'] <= 1):
                raise ValueError(f"{paradigm}: p_uncorrected={row['p_uncorrected']} out of range [0, 1]")
            if not (0 <= row['p_bonferroni'] <= 1):
                raise ValueError(f"{paradigm}: p_bonferroni={row['p_bonferroni']} out of range [0, 1]")

        log(f"[VALIDATION] All p-values in valid range [0, 1]")

        # Validate p_bonferroni <= 1.0
        max_p_bonf = results_df['p_bonferroni'].max()
        if max_p_bonf > 1.0:
            raise ValueError(f"p_bonferroni exceeds 1.0: max={max_p_bonf}")

        log(f"[VALIDATION] p_bonferroni capped at 1.0 (max observed: {max_p_bonf:.4f})")

        # Validate all paradigms present
        if len(results_df) != len(paradigms):
            raise ValueError(f"Expected {len(paradigms)} paradigms, got {len(results_df)}")

        log(f"[VALIDATION] All {len(paradigms)} paradigms present in results")

        log("[SUCCESS] Step 05 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
