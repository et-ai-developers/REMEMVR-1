#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step03
Step Name: Compute Purified CTT Scores (Retained Items Only)
RQ: results/ch5/5.3.6
Generated: 2025-12-04

PURPOSE:
Extract raw response data for RETAINED items only and compute Purified CTT scores
for each paradigm (IFR, ICR, IRE). This follows 2-pass purification workflow where
Step 1 identified quality items, Step 2 computed Full CTT (all items), and now
Step 3 computes Purified CTT (retained items only).

EXPECTED INPUTS:
  - data/step01_item_mapping.csv
    Columns: ['item_name', 'paradigm', 'retained', 'a', 'b', 'exclusion_reason']
    Format: Item quality mapping from RQ 5.3.1 purification
    Expected rows: 40-80 items (12 IFR, 19 ICR, 14 IRE retained per user request)

  - data/cache/dfData.csv
    Columns: ['UID', 'TEST', 'TQ_*' (105 item response columns)]
    Format: Raw VR item responses (dichotomized: TQ >= 1 → 1, TQ < 1 → 0)
    Expected rows: 400 (100 participants × 4 test sessions)

EXPECTED OUTPUTS:
  - data/step03_ctt_purified_scores.csv
    Columns: ['UID', 'test', 'CTT_purified_IFR', 'CTT_purified_ICR', 'CTT_purified_IRE']
    Format: Mean proportion correct using RETAINED items per paradigm
    Expected rows: 400 (100 participants × 4 test sessions)

VALIDATION CRITERIA:
  - All CTT scores in [0, 1] (proportion correct)
  - No NaN values in CTT score columns
  - All 400 rows present (100 UIDs × 4 tests)
  - No duplicate UID × test combinations

g_code REASONING:
- Approach: Filter item_mapping to retained=True items, map item names to TQ_* columns
  in dfData.csv, extract responses, dichotomize (0/1), compute mean per UID×test×paradigm
- Why this approach: CTT scores = simple proportion correct (mean of binary responses),
  contrasts with IRT's latent trait modeling; retained items = quality-filtered items
  from 2-pass IRT purification (a > 0.4, |b| < 3.0)
- Data flow: item_mapping (retained items) → dfData (raw responses) → dichotomize →
  group by UID×test×paradigm → mean → wide format output
- Expected performance: ~2-5 seconds (400 rows × 45 retained items = 18,000 computations)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib operations (pandas group-by and aggregation)
- Validation tool: tools.validation.validate_numeric_range
- Parameters: paradigms=['IFR', 'ICR', 'IRE'], aggregation='mean', filter_retained=True
- Item name mapping: item_mapping.item_name (e.g., 'TQ_ICR-D-i1') maps directly to
  dfData column names (column names are already TQ_* format)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 5.3.6/ (RQ directory)
#   parents[2] = ch5/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_numeric_range

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.3.6
LOG_FILE = RQ_DIR / "logs" / "step03_compute_purified_ctt.log"

# Paradigms to process
PARADIGMS = ["IFR", "ICR", "IRE"]

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step05_lmm_model_comparison.csv
#   CORRECT: data/step03_theta_scores.csv
#   WRONG:   results/lmm_model_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv             (missing step prefix)
#   WRONG:   logs/step02_removed_items.csv     (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 03: Compute Purified CTT Scores (Retained Items Only)")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Item mapping with retained=True/False flag from Step 1
        # Purpose: Identify which items to include in Purified CTT computation

        log("[LOAD] Loading input data...")

        # Load item mapping (output from Step 1)
        item_mapping_path = RQ_DIR / "data" / "step01_item_mapping.csv"
        item_mapping = pd.read_csv(item_mapping_path, encoding='utf-8')
        log(f"[LOADED] step01_item_mapping.csv ({len(item_mapping)} rows, {len(item_mapping.columns)} cols)")

        # Filter to retained items ONLY
        retained_items = item_mapping[item_mapping['retained'] == True].copy()
        log(f"[FILTER] Retained items: {len(retained_items)} total")

        # Report retained counts per paradigm
        for paradigm in PARADIGMS:
            n_retained = len(retained_items[retained_items['paradigm'] == paradigm])
            log(f"         - {paradigm}: {n_retained} retained items")

        # Load raw response data (project-level cache)
        raw_data_path = PROJECT_ROOT / "data" / "cache" / "dfData.csv"
        raw_data = pd.read_csv(raw_data_path, encoding='utf-8')
        log(f"[LOADED] dfData.csv ({len(raw_data)} rows, {len(raw_data.columns)} cols)")

        # Note: dfData uses 'TEST' (all caps), we need 'test' (lowercase) for consistency
        if 'TEST' in raw_data.columns:
            raw_data['test'] = raw_data['TEST']
            log("[TRANSFORM] Renamed TEST -> test for consistency")

        # =========================================================================
        # STEP 2: Compute Purified CTT Scores per Paradigm
        # =========================================================================
        # Tool: Pandas group-by and aggregation (stdlib operations)
        # What it does: For each paradigm, extract retained items, dichotomize responses,
        #               compute mean proportion correct per UID × test
        # Expected output: DataFrame with CTT_purified_* columns

        log("[ANALYSIS] Computing Purified CTT scores by paradigm...")

        # Initialize output DataFrame with UID and test
        ctt_purified = raw_data[['UID', 'test']].drop_duplicates().sort_values(['UID', 'test']).reset_index(drop=True)
        log(f"[INIT] Output DataFrame initialized: {len(ctt_purified)} rows (UID × test combinations)")

        # Process each paradigm
        for paradigm in PARADIGMS:
            log(f"[PARADIGM] Processing {paradigm}...")

            # Get retained items for this paradigm
            paradigm_items = retained_items[retained_items['paradigm'] == paradigm]['item_name'].tolist()
            log(f"           Retained items: {len(paradigm_items)}")

            # Verify all item columns exist in raw_data
            missing_cols = [item for item in paradigm_items if item not in raw_data.columns]
            if len(missing_cols) > 0:
                log(f"[WARNING] Missing {len(missing_cols)} item columns in dfData: {missing_cols[:5]}...")
                # Filter to available columns only
                paradigm_items = [item for item in paradigm_items if item in raw_data.columns]
                log(f"           Using {len(paradigm_items)} available items")

            # Extract responses for retained items
            item_responses = raw_data[['UID', 'test'] + paradigm_items].copy()

            # Dichotomize: TQ >= 1 → 1, TQ < 1 → 0 (per REMEMVR coding)
            # Note: dfData may already be dichotomized, but apply logic to be safe
            for item in paradigm_items:
                item_responses[item] = (item_responses[item] >= 1).astype(int)

            # Compute mean across items per UID × test (proportion correct)
            ctt_col = f"CTT_purified_{paradigm}"
            item_responses[ctt_col] = item_responses[paradigm_items].mean(axis=1)

            # Merge into output DataFrame
            ctt_purified = ctt_purified.merge(
                item_responses[['UID', 'test', ctt_col]],
                on=['UID', 'test'],
                how='left'
            )

            # Report statistics
            mean_score = item_responses[ctt_col].mean()
            sd_score = item_responses[ctt_col].std()
            min_score = item_responses[ctt_col].min()
            max_score = item_responses[ctt_col].max()
            log(f"           Mean: {mean_score:.3f}, SD: {sd_score:.3f}, Range: [{min_score:.3f}, {max_score:.3f}]")

        log("[DONE] Purified CTT computation complete")

        # =========================================================================
        # STEP 3: Save Analysis Output
        # =========================================================================
        # These outputs will be used by: Step 4 (reliability assessment), Step 5 (correlation analysis)

        output_path = RQ_DIR / "data" / "step03_ctt_purified_scores.csv"
        log(f"[SAVE] Saving {output_path.name}...")

        # Verify column order
        expected_cols = ['UID', 'test', 'CTT_purified_IFR', 'CTT_purified_ICR', 'CTT_purified_IRE']
        ctt_purified = ctt_purified[expected_cols]

        ctt_purified.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(ctt_purified)} rows, {len(ctt_purified.columns)} cols)")

        # =========================================================================
        # STEP 4: Run Validation Tool
        # =========================================================================
        # Tool: tools.validation.validate_numeric_range
        # Validates: CTT scores in [0, 1], no NaN values, correct row count
        # Threshold: min=0.0, max=1.0 (proportion correct range)

        log("[VALIDATION] Running validate_numeric_range...")

        # Validate each paradigm CTT score column
        validation_passed = True
        for paradigm in PARADIGMS:
            col_name = f"CTT_purified_{paradigm}"
            validation_result = validate_numeric_range(
                data=ctt_purified[col_name],
                min_val=0.0,
                max_val=1.0,
                column_name=col_name
            )

            if validation_result['valid']:
                log(f"[VALIDATION] {col_name}: PASS (range [0, 1], {len(ctt_purified)} values)")
            else:
                log(f"[VALIDATION] {col_name}: FAIL - {validation_result['message']}")
                validation_passed = False

        # Additional validation checks
        # Check for NaN values
        nan_counts = ctt_purified[['CTT_purified_IFR', 'CTT_purified_ICR', 'CTT_purified_IRE']].isna().sum()
        if nan_counts.sum() > 0:
            log(f"[VALIDATION] FAIL - NaN values detected: {nan_counts.to_dict()}")
            validation_passed = False
        else:
            log(f"[VALIDATION] No NaN values detected: PASS")

        # Check row count (expect 400 rows: 100 UIDs × 4 tests)
        expected_rows = 400
        if len(ctt_purified) == expected_rows:
            log(f"[VALIDATION] Row count: PASS ({expected_rows} rows)")
        else:
            log(f"[VALIDATION] Row count: FAIL (expected {expected_rows}, got {len(ctt_purified)})")
            validation_passed = False

        # Check for duplicate UID × test combinations
        duplicates = ctt_purified.duplicated(subset=['UID', 'test']).sum()
        if duplicates == 0:
            log(f"[VALIDATION] No duplicate UID × test combinations: PASS")
        else:
            log(f"[VALIDATION] FAIL - {duplicates} duplicate UID × test combinations detected")
            validation_passed = False

        if validation_passed:
            log("[SUCCESS] Step 03 complete - All validations passed")
            sys.exit(0)
        else:
            log("[ERROR] Step 03 failed validation checks")
            sys.exit(1)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
