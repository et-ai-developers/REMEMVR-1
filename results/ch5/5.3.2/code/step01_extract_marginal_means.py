#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step01
Step Name: Extract Marginal Means at Day 3 Midpoint
RQ: results/ch5/5.3.2
Generated: 2025-11-24

PURPOSE:
Extract paradigm-specific marginal means (predicted theta) at Day 3 midpoint (72 hours).
These marginal means show the expected memory performance for each paradigm at the
midpoint of the retention interval, controlling for individual differences.

EXPECTED INPUTS:
  - data/step00_model_loaded.txt (confirmation from Step 0)
  - RQ 5.3 model object (loaded fresh for coefficient extraction)
  - RQ 5.3 LMM input data (for paradigm level names)

EXPECTED OUTPUTS:
  - data/step01_marginal_means.csv
    Columns: [paradigm, marginal_mean, SE, CI_lower, CI_upper]
    Expected rows: 3 (one per paradigm)

VALIDATION CRITERIA:
  - Exactly 3 rows (one per paradigm)
  - All 3 paradigms present: free_recall, cued_recall, recognition
  - No NaN values
  - marginal_mean in [-3, 3]
  - SE in [0.01, 1.0]
  - CI_lower < marginal_mean < CI_upper

g_code REASONING:
- Approach: Extract coefficients from fitted LMM, compute predicted values at Day 3
- Why this approach: Marginal means show paradigm differences at a meaningful timepoint
- Data flow: Fitted model -> coefficient extraction -> prediction at log(3) -> save
- Expected performance: ~1 second (coefficient extraction, not full prediction)

IMPLEMENTATION NOTES:
- Day 3 in log scale: log(3) = 1.099 (since model uses log_Days)
- Model reference level is Free_Recall (intercept = Free_Recall at time=0)
- For each paradigm: predicted = Intercept + paradigm_effect + time_effect * log(3)
- SE computed from model covariance matrix using delta method
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, Any
import traceback

# Add project root to path for imports
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Statsmodels for loading model
from statsmodels.regression.mixed_linear_model import MixedLMResults

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.3.2
RQ3_DIR = RQ_DIR.parent / "5.3.1"  # results/ch5/5.3.1 (dependency)
LOG_FILE = RQ_DIR / "logs" / "step01_extract_marginal_means.log"

# Timepoint for marginal means: Day 3 (72 hours)
# In the Log model, time variable is log(Days), where Day 3 = 3 days
# From fixed effects, the variable is "log_Days" so it's log of days
LOG_DAY_3 = np.log(3)  # = 1.099
Z_CRITICAL = 1.96  # For 95% CI

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        # Clear log file for fresh run
        LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
        with open(LOG_FILE, 'w', encoding='utf-8') as f:
            f.write("")

        log("[START] Step 01: Extract Marginal Means at Day 3 Midpoint")

        # =========================================================================
        # STEP 1: Verify Step 0 Complete
        # =========================================================================
        log("[CHECK] Verifying Step 0 completed...")

        confirmation_path = RQ_DIR / "data" / "step00_model_loaded.txt"
        if not confirmation_path.exists():
            raise FileNotFoundError(f"Step 0 not complete - missing {confirmation_path}")
        log("[PASS] Step 0 confirmation file exists")

        # =========================================================================
        # STEP 2: Load Model and Extract Coefficients
        # =========================================================================
        log("[LOAD] Loading fitted LMM model...")

        model_path = RQ3_DIR / "data" / "step05_lmm_fitted_model.pkl"
        lmm_model = MixedLMResults.load(str(model_path))
        log(f"[LOADED] Model with {len(lmm_model.params)} parameters")

        # Get fixed effect coefficients
        params = lmm_model.params
        log(f"[INFO] All parameters:\n{params}")

        # Get covariance matrix for SE calculation
        cov_params = lmm_model.cov_params()
        log(f"[INFO] Covariance matrix shape: {cov_params.shape}")

        # =========================================================================
        # STEP 3: Identify Coefficient Names - EXACT MATCHING
        # =========================================================================
        # The model was fit with Factor as the paradigm variable
        # Reference level: Free_Recall
        # Treatment contrasts: Cued_Recall and Recognition effects
        # IMPORTANT: Use exact string matching to avoid confusing:
        #   'log_Days' (fixed effect) with 'log_Days Var' (random effect variance)

        log("[PARSE] Identifying coefficient names (exact matching)...")

        coef_names = list(params.index)

        # Known coefficient names from the fitted model
        # These are the EXACT names as they appear in params.index
        intercept_idx = 'Intercept'
        log_days_idx = 'log_Days'  # EXACT - not 'log_Days Var'
        cued_recall_idx = "C(Factor, Treatment('Free_Recall'))[T.Cued_Recall]"
        recognition_idx = "C(Factor, Treatment('Free_Recall'))[T.Recognition]"
        cued_interaction_idx = "log_Days:C(Factor, Treatment('Free_Recall'))[T.Cued_Recall]"
        recog_interaction_idx = "log_Days:C(Factor, Treatment('Free_Recall'))[T.Recognition]"

        # Verify all expected coefficients exist
        expected_coefs = [intercept_idx, log_days_idx, cued_recall_idx, recognition_idx,
                         cued_interaction_idx, recog_interaction_idx]
        for coef in expected_coefs:
            if coef not in coef_names:
                raise ValueError(f"Expected coefficient not found: {coef}")

        log(f"[INFO] Intercept: {intercept_idx}")
        log(f"[INFO] log_Days: {log_days_idx}")
        log(f"[INFO] Cued_Recall effect: {cued_recall_idx}")
        log(f"[INFO] Recognition effect: {recognition_idx}")
        log(f"[INFO] Cued interaction: {cued_interaction_idx}")
        log(f"[INFO] Recognition interaction: {recog_interaction_idx}")

        # =========================================================================
        # STEP 4: Compute Marginal Means at Day 3
        # =========================================================================
        # For each paradigm: E[theta | paradigm, time=log(3)]
        # Free_Recall: Intercept + log_Days * log(3)
        # Cued_Recall: Intercept + Cued_effect + (log_Days + Cued_interaction) * log(3)
        # Recognition: Intercept + Recog_effect + (log_Days + Recog_interaction) * log(3)

        log(f"[COMPUTE] Computing marginal means at log(Day 3) = {LOG_DAY_3:.4f}...")

        # Extract coefficient values
        b_intercept = params[intercept_idx]
        b_log_days = params[log_days_idx]
        b_cued = params[cued_recall_idx]
        b_recog = params[recognition_idx]
        b_cued_int = params[cued_interaction_idx]
        b_recog_int = params[recog_interaction_idx]

        log(f"[INFO] b_intercept = {b_intercept:.4f}")
        log(f"[INFO] b_log_days = {b_log_days:.4f} (this is the Free_Recall slope)")
        log(f"[INFO] b_cued = {b_cued:.4f}")
        log(f"[INFO] b_recog = {b_recog:.4f}")
        log(f"[INFO] b_cued_int = {b_cued_int:.4f}")
        log(f"[INFO] b_recog_int = {b_recog_int:.4f}")

        # Compute marginal means
        mm_free_recall = b_intercept + b_log_days * LOG_DAY_3
        mm_cued_recall = b_intercept + b_cued + (b_log_days + b_cued_int) * LOG_DAY_3
        mm_recognition = b_intercept + b_recog + (b_log_days + b_recog_int) * LOG_DAY_3

        log(f"[RESULT] Free_Recall marginal mean = {mm_free_recall:.4f}")
        log(f"[RESULT] Cued_Recall marginal mean = {mm_cued_recall:.4f}")
        log(f"[RESULT] Recognition marginal mean = {mm_recognition:.4f}")

        # =========================================================================
        # STEP 5: Compute Standard Errors Using Delta Method
        # =========================================================================
        # SE = sqrt(gradient' * Cov * gradient)
        # gradient = d(prediction)/d(coefficients)

        log("[COMPUTE] Computing standard errors...")

        # Build gradient vectors for each prediction
        # Order of coefficients in cov_params matches params.index
        n_params = len(params)

        def compute_se(gradient_dict):
            """Compute SE from gradient and covariance matrix."""
            gradient = np.zeros(n_params)
            for coef_name, value in gradient_dict.items():
                if coef_name in coef_names:
                    idx = coef_names.index(coef_name)
                    gradient[idx] = value
            variance = gradient @ cov_params.values @ gradient
            return np.sqrt(variance)

        # Gradient for Free_Recall: d/d_intercept=1, d/d_log_days=log(3)
        grad_free = {intercept_idx: 1.0, log_days_idx: LOG_DAY_3}
        se_free = compute_se(grad_free)

        # Gradient for Cued_Recall
        grad_cued = {
            intercept_idx: 1.0,
            cued_recall_idx: 1.0,
            log_days_idx: LOG_DAY_3,
            cued_interaction_idx: LOG_DAY_3
        }
        se_cued = compute_se(grad_cued)

        # Gradient for Recognition
        grad_recog = {
            intercept_idx: 1.0,
            recognition_idx: 1.0,
            log_days_idx: LOG_DAY_3,
            recog_interaction_idx: LOG_DAY_3
        }
        se_recog = compute_se(grad_recog)

        log(f"[RESULT] Free_Recall SE = {se_free:.4f}")
        log(f"[RESULT] Cued_Recall SE = {se_cued:.4f}")
        log(f"[RESULT] Recognition SE = {se_recog:.4f}")

        # =========================================================================
        # STEP 6: Compute 95% Confidence Intervals
        # =========================================================================
        log("[COMPUTE] Computing 95% confidence intervals...")

        ci_free_lower = mm_free_recall - Z_CRITICAL * se_free
        ci_free_upper = mm_free_recall + Z_CRITICAL * se_free

        ci_cued_lower = mm_cued_recall - Z_CRITICAL * se_cued
        ci_cued_upper = mm_cued_recall + Z_CRITICAL * se_cued

        ci_recog_lower = mm_recognition - Z_CRITICAL * se_recog
        ci_recog_upper = mm_recognition + Z_CRITICAL * se_recog

        # =========================================================================
        # STEP 7: Create Output DataFrame
        # =========================================================================
        log("[CREATE] Building marginal means DataFrame...")

        # Use lowercase paradigm names to match the data
        marginal_means = pd.DataFrame({
            'paradigm': ['free_recall', 'cued_recall', 'recognition'],
            'marginal_mean': [mm_free_recall, mm_cued_recall, mm_recognition],
            'SE': [se_free, se_cued, se_recog],
            'CI_lower': [ci_free_lower, ci_cued_lower, ci_recog_lower],
            'CI_upper': [ci_free_upper, ci_cued_upper, ci_recog_upper]
        })

        log(f"[INFO] Marginal means DataFrame:\n{marginal_means.to_string()}")

        # =========================================================================
        # STEP 8: Validate Output
        # =========================================================================
        log("[VALIDATE] Checking output validity...")

        # Check for NaN
        if marginal_means.isnull().any().any():
            raise ValueError("Output contains NaN values")

        # Check marginal_mean range
        if not marginal_means['marginal_mean'].between(-3, 3).all():
            log(f"[WARNING] Some marginal means outside expected range [-3, 3]")
            log(f"[WARNING] Actual range: [{marginal_means['marginal_mean'].min():.4f}, {marginal_means['marginal_mean'].max():.4f}]")

        # Check SE range
        if not marginal_means['SE'].between(0.01, 1.0).all():
            log(f"[WARNING] Some SEs outside expected range [0.01, 1.0]")
            log(f"[WARNING] Actual range: [{marginal_means['SE'].min():.4f}, {marginal_means['SE'].max():.4f}]")

        # Check CI ordering
        for _, row in marginal_means.iterrows():
            if not (row['CI_lower'] < row['marginal_mean'] < row['CI_upper']):
                raise ValueError(f"CI ordering violated for {row['paradigm']}")

        log("[PASS] All validation checks passed")

        # =========================================================================
        # STEP 9: Save Output
        # =========================================================================
        output_path = RQ_DIR / "data" / "step01_marginal_means.csv"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        marginal_means.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path} ({len(marginal_means)} rows)")

        log("[SUCCESS] Step 01 complete - Marginal means extracted")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
