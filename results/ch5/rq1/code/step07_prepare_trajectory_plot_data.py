#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step07
Step Name: Prepare Trajectory Plot Data (Decision D069)
RQ: results/ch5/rq1
Generated: 2025-11-22

PURPOSE:
Aggregate analysis outputs for dual-scale trajectory visualization (Decision D069).
This step prepares plot-ready data on both:
- Theta scale (IRT ability units, -3 to +3 typical range)
- Probability scale (0 to 1, more interpretable for non-psychometricians)

EXPECTED INPUTS:
  - data/step04_lmm_input.csv
    Format: CSV with UTF-8 encoding
    Required columns: [test, TSVR_hours, domain, theta]
    Description: Long-format LMM input for aggregation

  - data/step03_item_parameters.csv
    Format: CSV with UTF-8 encoding
    Required columns: [domain, Discrimination, Difficulty_1]
    Description: Item parameters for domain-specific theta-to-probability conversion

  - data/step05_lmm_fitted_model.pkl
    Format: Pickle file
    Description: Fitted LMM model for generating predictions

EXPECTED OUTPUTS:
  - plots/step07_trajectory_theta_data.csv
    Columns: [time, test, domain, mean_theta, CI_lower, CI_upper, predicted_theta, n_obs]
    Expected rows: 12 (3 domains x 4 tests)
    Description: Observed mean theta with 95% CIs and LMM predictions per domain x test

  - plots/step07_trajectory_probability_data.csv
    Columns: [time, test, domain, mean_probability, CI_lower, CI_upper, predicted_probability, n_obs]
    Expected rows: 12 (3 domains x 4 tests)
    Description: Mean theta transformed to probability scale with predictions (Decision D069)

VALIDATION CRITERIA:
  - Both output files exist in plots/ folder
  - Each file has exactly 12 rows (3 domains x 4 tests)
  - All 3 domains present: what, where, when
  - All 4 tests present: 0, 1, 3, 6 (nominal days, NOT {1,2,3,4})
  - Domain x test combinations are unique
  - No NaN values in any column
  - Probability bounds: mean_probability in [0, 1], CI bounds in [0, 1]
  - CI validity: CI_upper > CI_lower for all rows
  - n_obs >= 80 per group (some missing acceptable from 100)
  - predicted_theta and predicted_probability columns present

g_code REASONING:
- Approach: Aggregate theta scores by domain x test, then convert to probability
- Why this approach: Decision D069 requires dual-scale plots for interpretability
- Data flow: LMM input -> aggregate by domain/test -> theta CSV
                       -> convert to probability -> probability CSV
- Expected performance: ~seconds (aggregation only)

IMPLEMENTATION NOTES:
- Analysis tool: tools.plotting.convert_theta_to_probability
- Uses domain-specific average discrimination and difficulty from item parameters
- 95% CI = mean +/- 1.96 * SE, where SE = std / sqrt(n)
- Representative time = median TSVR_hours per test
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, Any
import traceback
import pickle
import statsmodels.api as sm

# Add project root to path for imports
# parents[4] = REMEMVR/ (code -> rq1 -> ch5 -> results -> REMEMVR)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import plotting tools
from tools.plotting import convert_theta_to_probability

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/rq1
LOG_FILE = RQ_DIR / "logs" / "step07_prepare_trajectory_plot_data.log"

# Ensure directories exist
LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
(RQ_DIR / "plots").mkdir(parents=True, exist_ok=True)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg: str) -> None:
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        # Clear log file for fresh run
        with open(LOG_FILE, 'w', encoding='utf-8') as f:
            f.write("")

        log("[START] Step 07: Prepare Trajectory Plot Data (Decision D069)")
        log("=" * 60)

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================

        log("\n[LOAD] Loading input data...")

        # Load LMM input (step04 output)
        lmm_input_path = RQ_DIR / "data" / "step04_lmm_input.csv"
        if not lmm_input_path.exists():
            raise FileNotFoundError(f"Input file not found: {lmm_input_path}")
        df_lmm = pd.read_csv(lmm_input_path, encoding='utf-8')
        log(f"[LOADED] step04_lmm_input.csv ({len(df_lmm)} rows)")

        # Load item parameters (step03 output)
        item_params_path = RQ_DIR / "data" / "step03_item_parameters.csv"
        if not item_params_path.exists():
            raise FileNotFoundError(f"Input file not found: {item_params_path}")
        df_items = pd.read_csv(item_params_path, encoding='utf-8')
        log(f"[LOADED] step03_item_parameters.csv ({len(df_items)} items)")

        # Map test values from {1,2,3,4} to nominal days {0,1,3,6}
        # Test 1 = Day 0 (immediate), Test 2 = Day 1, Test 3 = Day 3, Test 4 = Day 6
        TEST_TO_DAYS = {1: 0, 2: 1, 3: 3, 4: 6}
        if df_lmm['test'].isin([1, 2, 3, 4]).all():
            df_lmm['test'] = df_lmm['test'].map(TEST_TO_DAYS)
            log(f"[MAPPED] Test values mapped: {{1,2,3,4}} -> {{0,1,3,6}}")

        # =========================================================================
        # STEP 2: Compute Domain-Specific IRT Parameters
        # =========================================================================
        # For probability conversion, we need average discrimination and difficulty
        # per domain from the item parameters

        log("\n[COMPUTE] Computing domain-specific IRT parameters...")

        # Identify domain column (may be 'factor' or 'domain')
        factor_col = 'factor' if 'factor' in df_items.columns else 'domain'

        # Check which difficulty column exists
        diff_col = None
        for col in ['Difficulty_1', 'Difficulty', 'b', 'difficulty']:
            if col in df_items.columns:
                diff_col = col
                break
        if diff_col is None:
            raise ValueError(f"No difficulty column found. Available: {df_items.columns.tolist()}")

        # Check which discrimination column exists
        disc_col = None
        for col in ['Discrimination', 'a', 'discrimination']:
            if col in df_items.columns:
                disc_col = col
                break
        if disc_col is None:
            raise ValueError(f"No discrimination column found. Available: {df_items.columns.tolist()}")

        log(f"  Using columns: factor={factor_col}, disc={disc_col}, diff={diff_col}")

        # Compute mean a and b per domain
        domain_params = df_items.groupby(factor_col).agg({
            disc_col: 'mean',
            diff_col: 'mean'
        }).reset_index()
        domain_params.columns = ['domain', 'mean_a', 'mean_b']

        log(f"[DONE] Domain parameters computed:")
        for _, row in domain_params.iterrows():
            log(f"  {row['domain']}: a={row['mean_a']:.3f}, b={row['mean_b']:.3f}")

        # =========================================================================
        # STEP 3: Aggregate Theta Scores by Domain x Test
        # =========================================================================

        log("\n[AGGREGATE] Computing mean theta by domain x test...")

        # Group by domain and test
        theta_agg = df_lmm.groupby(['domain', 'test']).agg({
            'theta': ['mean', 'std', 'count'],
            'TSVR_hours': 'median'  # Representative time for plotting
        }).reset_index()

        # Flatten column names
        theta_agg.columns = ['domain', 'test', 'mean_theta', 'std_theta', 'n_obs', 'time']

        # Compute 95% CI
        theta_agg['se'] = theta_agg['std_theta'] / np.sqrt(theta_agg['n_obs'])
        theta_agg['CI_lower'] = theta_agg['mean_theta'] - 1.96 * theta_agg['se']
        theta_agg['CI_upper'] = theta_agg['mean_theta'] + 1.96 * theta_agg['se']

        log(f"[DONE] Aggregation complete: {len(theta_agg)} groups")
        log(f"  Domains: {sorted(theta_agg['domain'].unique().tolist())}")
        log(f"  Tests: {sorted(theta_agg['test'].unique().tolist())}")

        # =========================================================================
        # STEP 3b: Generate LMM Predictions
        # =========================================================================
        # Re-fit the best model (Log) to generate predictions for plot overlay
        # Note: Cannot pickle statsmodels results reliably, so we re-fit here

        log("\n[PREDICT] Generating LMM model predictions...")

        # Prepare data for LMM (need log_Days and domain coding)
        df_lmm_pred = df_lmm.copy()
        df_lmm_pred['Days'] = df_lmm_pred['TSVR_hours'] / 24.0
        df_lmm_pred['log_Days'] = np.log(df_lmm_pred['Days'] + 1)

        # Re-fit the Log model (best model from step05)
        from statsmodels.regression.mixed_linear_model import MixedLM
        import statsmodels.formula.api as smf

        # Get UID from composite_ID
        if 'UID' not in df_lmm_pred.columns:
            df_lmm_pred['UID'] = df_lmm_pred['composite_ID'].str.split('_').str[0]

        # Fit Log model: Ability ~ log(Days) * Domain with random intercept + slope per UID
        log_model = smf.mixedlm(
            "theta ~ log_Days * C(domain, Treatment('what'))",
            data=df_lmm_pred,
            groups=df_lmm_pred['UID'],
            re_formula='~log_Days'
        )
        log_result = log_model.fit(method='powell', reml=False)
        log(f"  Model re-fitted: AIC = {log_result.aic:.2f}")

        # Generate predictions for each domain x test combination
        predicted_theta = []
        for _, row in theta_agg.iterrows():
            domain = row['domain']
            time_val = row['time']  # median TSVR_hours for this group

            # Create prediction data point
            days = time_val / 24.0
            log_days = np.log(days + 1)

            # Build prediction manually from fixed effects
            # Formula: intercept + log_Days*beta + domain effects + interactions
            fe = log_result.fe_params

            pred = fe['Intercept'] + fe['log_Days'] * log_days

            # Add domain effects (treatment coding with 'what' as reference)
            if domain == 'when':
                pred += fe["C(domain, Treatment('what'))[T.when]"]
                pred += fe["log_Days:C(domain, Treatment('what'))[T.when]"] * log_days
            elif domain == 'where':
                pred += fe["C(domain, Treatment('what'))[T.where]"]
                pred += fe["log_Days:C(domain, Treatment('what'))[T.where]"] * log_days
            # 'what' is reference, no additional terms

            predicted_theta.append(float(pred))

        theta_agg['predicted_theta'] = predicted_theta
        log(f"[DONE] LMM predictions generated for {len(theta_agg)} groups")

        # =========================================================================
        # STEP 4: Prepare Theta Scale Output
        # =========================================================================

        log("\n[PREPARE] Creating theta scale plot data...")

        # Select columns for theta output (including predicted_theta)
        df_theta_plot = theta_agg[['time', 'test', 'domain', 'mean_theta', 'CI_lower', 'CI_upper', 'predicted_theta', 'n_obs']].copy()

        # Sort for consistent output
        df_theta_plot = df_theta_plot.sort_values(['domain', 'test']).reset_index(drop=True)

        log(f"[DONE] Theta plot data prepared: {len(df_theta_plot)} rows")

        # =========================================================================
        # STEP 5: Convert to Probability Scale (Decision D069)
        # =========================================================================

        log("\n[CONVERT] Transforming to probability scale (Decision D069)...")

        # Create probability dataframe
        df_prob_plot = df_theta_plot[['time', 'test', 'domain', 'n_obs']].copy()

        # Convert each domain's theta values to probability using domain-specific params
        mean_probs = []
        ci_lowers = []
        ci_uppers = []
        predicted_probs = []

        for _, row in df_theta_plot.iterrows():
            domain = row['domain']

            # Get domain-specific parameters
            domain_row = domain_params[domain_params['domain'] == domain]
            if len(domain_row) == 0:
                log(f"  Warning: No parameters for domain '{domain}', using defaults (a=1, b=0)")
                a, b = 1.0, 0.0
            else:
                a = domain_row['mean_a'].values[0]
                b = domain_row['mean_b'].values[0]

            # Convert mean and CI bounds
            mean_prob = convert_theta_to_probability(row['mean_theta'], discrimination=a, difficulty=b)
            ci_lower_prob = convert_theta_to_probability(row['CI_lower'], discrimination=a, difficulty=b)
            ci_upper_prob = convert_theta_to_probability(row['CI_upper'], discrimination=a, difficulty=b)
            predicted_prob = convert_theta_to_probability(row['predicted_theta'], discrimination=a, difficulty=b)

            mean_probs.append(float(mean_prob))
            ci_lowers.append(float(ci_lower_prob))
            ci_uppers.append(float(ci_upper_prob))
            predicted_probs.append(float(predicted_prob))

        df_prob_plot['mean_probability'] = mean_probs
        df_prob_plot['CI_lower'] = ci_lowers
        df_prob_plot['CI_upper'] = ci_uppers
        df_prob_plot['predicted_probability'] = predicted_probs

        # Reorder columns
        df_prob_plot = df_prob_plot[['time', 'test', 'domain', 'mean_probability', 'CI_lower', 'CI_upper', 'predicted_probability', 'n_obs']]

        log(f"[DONE] Probability conversion complete")
        log(f"  Probability range: {df_prob_plot['mean_probability'].min():.3f} - {df_prob_plot['mean_probability'].max():.3f}")

        # =========================================================================
        # STEP 6: Save Outputs
        # =========================================================================

        log("\n[SAVE] Saving plot data files...")

        # Save theta scale data
        theta_output_path = RQ_DIR / "plots" / "step07_trajectory_theta_data.csv"
        df_theta_plot.to_csv(theta_output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {theta_output_path}")
        log(f"  Rows: {len(df_theta_plot)}, Columns: {list(df_theta_plot.columns)}")

        # Save probability scale data
        prob_output_path = RQ_DIR / "plots" / "step07_trajectory_probability_data.csv"
        df_prob_plot.to_csv(prob_output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {prob_output_path}")
        log(f"  Rows: {len(df_prob_plot)}, Columns: {list(df_prob_plot.columns)}")

        # =========================================================================
        # STEP 7: Validation
        # =========================================================================

        log("\n[VALIDATION] Running validation checks...")

        validation_errors = []

        # Check 1: Both output files exist
        if not theta_output_path.exists():
            validation_errors.append("Theta output file not created")
        if not prob_output_path.exists():
            validation_errors.append("Probability output file not created")
        if not validation_errors:
            log("  [PASS] Both output files exist")

        # Check 2: Exact row count (12 = 3 domains x 4 tests)
        if len(df_theta_plot) != 12:
            validation_errors.append(f"Theta data has {len(df_theta_plot)} rows, expected 12")
        if len(df_prob_plot) != 12:
            validation_errors.append(f"Probability data has {len(df_prob_plot)} rows, expected 12")
        if not any("rows" in e for e in validation_errors):
            log("  [PASS] Each file has exactly 12 rows")

        # Check 3: Domain coverage
        expected_domains = {'what', 'where', 'when'}
        actual_domains = set(df_theta_plot['domain'].unique())
        if actual_domains != expected_domains:
            validation_errors.append(f"Missing domains: {expected_domains - actual_domains}")
        else:
            log("  [PASS] All 3 domains present")

        # Check 4: Test coverage (nominal days 0, 1, 3, 6)
        expected_tests = {0, 1, 3, 6}
        actual_tests = set(df_theta_plot['test'].unique())
        if actual_tests != expected_tests:
            validation_errors.append(f"Test values mismatch: expected {expected_tests}, found {actual_tests}")
        else:
            log("  [PASS] All 4 tests present (nominal days 0, 1, 3, 6)")

        # Check 5: No duplicates
        dupe_check = df_theta_plot.groupby(['domain', 'test']).size()
        if dupe_check.max() > 1:
            validation_errors.append("Duplicate domain x test combinations found")
        else:
            log("  [PASS] Domain x test combinations are unique")

        # Check 6: No NaN values
        if df_theta_plot.isna().any().any():
            validation_errors.append(f"NaN values in theta data: {df_theta_plot.isna().sum().sum()}")
        if df_prob_plot.isna().any().any():
            validation_errors.append(f"NaN values in probability data: {df_prob_plot.isna().sum().sum()}")
        if not any("NaN" in e for e in validation_errors):
            log("  [PASS] No NaN values in any column")

        # Check 7: Probability bounds
        if df_prob_plot['mean_probability'].min() < 0 or df_prob_plot['mean_probability'].max() > 1:
            validation_errors.append("mean_probability out of [0, 1] range")
        if df_prob_plot['CI_lower'].min() < 0 or df_prob_plot['CI_upper'].max() > 1:
            validation_errors.append("Probability CI bounds out of [0, 1] range")
        if not any("probability" in e.lower() for e in validation_errors):
            log("  [PASS] Probability values in [0, 1] range")

        # Check 8: CI validity
        if (df_theta_plot['CI_upper'] <= df_theta_plot['CI_lower']).any():
            validation_errors.append("CI_upper <= CI_lower in theta data")
        if (df_prob_plot['CI_upper'] <= df_prob_plot['CI_lower']).any():
            validation_errors.append("CI_upper <= CI_lower in probability data")
        if not any("CI_upper" in e for e in validation_errors):
            log("  [PASS] CI_upper > CI_lower for all rows")

        # Check 9: n_obs reasonable
        if df_theta_plot['n_obs'].min() < 80:
            log(f"  [WARN] Some groups have n_obs < 80 (min={df_theta_plot['n_obs'].min()})")
        else:
            log("  [PASS] n_obs >= 80 per group")

        # Report validation result
        if validation_errors:
            log("\n[FAIL] Validation errors detected:")
            for err in validation_errors:
                log(f"  - {err}")
            raise ValueError(f"Validation failed: {validation_errors}")
        else:
            log("\n[PASS] All validation checks passed")

        # =========================================================================
        # STEP 8: Summary
        # =========================================================================

        log("\n" + "=" * 60)
        log("[SUCCESS] Step 07 complete: Trajectory plot data prepared")
        log("=" * 60)
        log("\nOutputs created:")
        log(f"  1. {theta_output_path}")
        log(f"  2. {prob_output_path}")
        log("\nDecision D069 implemented: Dual-scale trajectory data")
        log("  - Theta scale: IRT ability units for technical audience")
        log("  - Probability scale: 0-1 range for general audience")

        # Print summary table
        log("\n[SUMMARY] Theta scale data:")
        log(df_theta_plot.to_string(index=False))
        log("\n[SUMMARY] Probability scale data:")
        log(df_prob_plot.to_string(index=False))

        sys.exit(0)

    except Exception as e:
        log(f"\n[ERROR] {str(e)}")
        log("\n[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
