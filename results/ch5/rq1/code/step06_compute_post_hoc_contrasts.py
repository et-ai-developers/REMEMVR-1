#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step06
Step Name: Compute Post-Hoc Contrasts (Decision D068)
RQ: results/ch5/rq1
Generated: 2025-11-22

PURPOSE:
Compute pairwise domain contrasts with dual p-value reporting (Decision D068) and
effect sizes. This step implements dual reporting of:
- Uncorrected p-values (alpha = 0.05) for transparency
- Bonferroni-corrected p-values (alpha = 0.05/3 = 0.0167) for Type I error control

EXPECTED INPUTS:
  - data/step05_lmm_fitted_model.pkl
    Format: Python pickle (MixedLMResults object)
    Description: Best fitted LMM from Step 5 model comparison

EXPECTED OUTPUTS:
  - results/step06_post_hoc_contrasts.csv
    Columns: [comparison, beta, se, z, p_uncorrected, alpha_corrected, p_corrected,
              sig_uncorrected, sig_corrected]
    Expected rows: 3 (one per pairwise comparison)
    Description: Post-hoc contrasts with DUAL p-values (Decision D068)

  - results/step06_effect_sizes.csv
    Columns: [effect, f_squared, interpretation]
    Description: Cohen's f-squared effect sizes for all fixed effects

VALIDATION CRITERIA:
  - Exactly 3 pairwise contrasts computed (where-what, when-what, when-where)
  - Both uncorrected AND Bonferroni p-values present (Decision D068)
  - p_corrected = min(1.0, p_uncorrected * 3) for each row
  - Effect sizes computed for all effects
  - p_uncorrected in [0, 1]
  - z values finite (no NaN/Inf)

g_code REASONING:
- Approach: Load fitted LMM from Step 5, compute pairwise contrasts between domains,
  then compute effect sizes for all fixed effects
- Why this approach: Decision D068 requires dual p-value reporting for exploratory
  thesis work. Reviewers can assess robustness across both thresholds.
- Data flow: Pickle -> compute_contrasts_pairwise -> contrasts CSV
                    -> compute_effect_sizes_cohens -> effect sizes CSV
- Expected performance: ~seconds (no model fitting, just coefficient extraction)

IMPLEMENTATION NOTES:
- Analysis tool: tools.analysis_lmm.compute_contrasts_pairwise
- Secondary analysis: tools.analysis_lmm.compute_effect_sizes_cohens
- Validation tool: tools.validation.validate_lmm_convergence
- Parameters:
    - comparisons: ["where-what", "when-what", "when-where"]
    - family_alpha: 0.05
    - n_comparisons: 3
    - bonferroni_alpha: 0.0167 (0.05 / 3)
    - include_interactions: true (for effect sizes)
"""
# =============================================================================

import sys
import pickle
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, Any
import traceback

# Add project root to path for imports
# parents[4] = REMEMVR/ (code -> rq1 -> ch5 -> results -> REMEMVR)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tools
from tools.analysis_lmm import compute_contrasts_pairwise, compute_effect_sizes_cohens

# Import validation tool
from tools.validation import validate_lmm_convergence

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/rq1
LOG_FILE = RQ_DIR / "logs" / "step06_compute_post_hoc_contrasts.log"

# Ensure log directory exists
LOG_FILE.parent.mkdir(parents=True, exist_ok=True)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg: str) -> None:
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        # Clear log file for fresh run
        with open(LOG_FILE, 'w', encoding='utf-8') as f:
            f.write("")

        log("[START] Step 06: Compute Post-Hoc Contrasts (Decision D068)")
        log("=" * 60)

        # =========================================================================
        # STEP 1: Re-fit Best LMM Model (Log model)
        # =========================================================================
        # Note: Statsmodels pickle has patsy eval environment issues
        # So we re-fit the best model (Log) inline instead of loading from pickle

        log("\n[LOAD] Loading LMM input data and re-fitting best model...")

        import statsmodels.formula.api as smf

        # Load input data
        data_path = RQ_DIR / "data" / "step04_lmm_input.csv"
        if not data_path.exists():
            raise FileNotFoundError(f"Input file not found: {data_path}")

        df = pd.read_csv(data_path, encoding='utf-8')
        log(f"[LOADED] step04_lmm_input.csv ({len(df)} rows)")

        # Transform columns to match tool expectations
        # IMPORTANT: Use same transformations as step05 (log_Days = log(Days + 1))
        df['Days'] = df['TSVR_hours'] / 24.0
        df['log_Days'] = np.log(df['Days'] + 1)  # Match step05: +1 not +0.01
        df['Factor'] = df['domain'].str.capitalize()
        df['Ability'] = df['theta']

        # Fit Log model (best by AIC from Step 5)
        # IMPORTANT: Include random slope on log_Days to match step05
        log("[FIT] Re-fitting Log model (best by AIC)...")
        formula = "Ability ~ log_Days * C(Factor, Treatment('What'))"
        re_formula = "~log_Days"  # Random intercept + random slope on log_Days
        model = smf.mixedlm(formula, df, groups=df['UID'], re_formula=re_formula)
        best_result = model.fit(method='powell', reml=False)  # Use ML not REML to match AIC comparison

        log(f"[FITTED] Log model")
        log(f"  - AIC: {best_result.aic:.2f}")
        log(f"  - BIC: {best_result.bic:.2f}")
        log(f"  - Log-Likelihood: {best_result.llf:.2f}")
        log(f"  - Number of observations: {best_result.nobs}")
        log(f"  - Converged: {best_result.converged}")

        # =========================================================================
        # STEP 2: Validate Model Convergence
        # =========================================================================
        # Tool: tools.validation.validate_lmm_convergence
        # Validates: Model converged successfully before computing contrasts

        log("\n[VALIDATION] Running validate_lmm_convergence...")

        validation_result = validate_lmm_convergence(best_result)

        log(f"  - Converged: {validation_result['converged']}")
        log(f"  - Message: {validation_result['message']}")

        if not validation_result['converged']:
            raise ValueError(f"Model did not converge: {validation_result.get('warning', 'Unknown error')}")

        # =========================================================================
        # STEP 3: Compute Pairwise Contrasts (Decision D068)
        # =========================================================================
        # Tool: tools.analysis_lmm.compute_contrasts_pairwise
        # What it does: Extract coefficient differences between domain pairs with
        #               dual p-value reporting (uncorrected + Bonferroni)
        # Expected output: DataFrame with 3 rows (one per comparison)

        log("\n[ANALYSIS] Running compute_contrasts_pairwise...")
        log("  Implementing Decision D068: Dual p-value reporting")

        # Define pairwise comparisons
        # Note: The function expects comparisons in format "A-B" where A and B are factor levels
        # Treatment coding means level coefficient represents (level - reference)
        # IMPORTANT: Factor levels must be Capitalized to match model (What, When, Where)
        comparisons = ["Where-What", "When-What", "When-Where"]
        family_alpha = 0.05

        log(f"  - Comparisons: {comparisons}")
        log(f"  - Family-wise alpha: {family_alpha}")
        log(f"  - Number of comparisons: {len(comparisons)}")
        log(f"  - Bonferroni-corrected alpha: {family_alpha / len(comparisons):.4f}")

        contrasts_df = compute_contrasts_pairwise(
            lmm_result=best_result,
            comparisons=comparisons,
            family_alpha=family_alpha
        )

        log(f"\n[DONE] Contrasts computed: {len(contrasts_df)} comparisons")

        # =========================================================================
        # STEP 4: Save Contrasts Output
        # =========================================================================
        # Output: results/step06_post_hoc_contrasts.csv
        # Contains: Pairwise contrasts with dual p-values (Decision D068)
        # Columns: comparison, beta, se, z, p_uncorrected, alpha_corrected,
        #          p_corrected, sig_uncorrected, sig_corrected

        contrasts_output_path = RQ_DIR / "results" / "step06_post_hoc_contrasts.csv"
        contrasts_df.to_csv(contrasts_output_path, index=False, encoding='utf-8')

        log(f"\n[SAVE] Saved contrasts: {contrasts_output_path}")
        log(f"  - Rows: {len(contrasts_df)}")
        log(f"  - Columns: {list(contrasts_df.columns)}")

        # Log contrast details
        log("\n[RESULTS] Post-hoc contrast results:")
        for idx, row in contrasts_df.iterrows():
            sig_str = ""
            if row['sig_corrected']:
                sig_str = "*** (corrected)"
            elif row['sig_uncorrected']:
                sig_str = "* (uncorrected only)"
            else:
                sig_str = "ns"

            log(f"  {row['comparison']}: beta={row['beta']:.4f}, z={row['z']:.3f}, "
                f"p_uncorr={row['p_uncorrected']:.4f}, p_corr={row['p_corrected']:.4f} {sig_str}")

        # =========================================================================
        # STEP 5: Compute Effect Sizes (Cohen's f-squared)
        # =========================================================================
        # Tool: tools.analysis_lmm.compute_effect_sizes_cohens
        # What it does: Compute effect sizes for all fixed effects including interactions
        # Expected output: DataFrame with effect name, f_squared, and interpretation

        log("\n[ANALYSIS] Running compute_effect_sizes_cohens...")

        effect_sizes_df = compute_effect_sizes_cohens(
            lmm_result=best_result,
            include_interactions=True  # Include interaction terms per 4_analysis.yaml
        )

        log(f"[DONE] Effect sizes computed: {len(effect_sizes_df)} effects")

        # =========================================================================
        # STEP 6: Save Effect Sizes Output
        # =========================================================================
        # Output: results/step06_effect_sizes.csv
        # Contains: Cohen's f-squared effect sizes for all fixed effects
        # Columns: effect, f_squared, interpretation

        effect_sizes_output_path = RQ_DIR / "results" / "step06_effect_sizes.csv"
        effect_sizes_df.to_csv(effect_sizes_output_path, index=False, encoding='utf-8')

        log(f"\n[SAVE] Saved effect sizes: {effect_sizes_output_path}")
        log(f"  - Rows: {len(effect_sizes_df)}")
        log(f"  - Columns: {list(effect_sizes_df.columns)}")

        # Log effect size details
        log("\n[RESULTS] Effect sizes (Cohen's f-squared):")
        for idx, row in effect_sizes_df.iterrows():
            log(f"  {row['effect']}: f2={row['f_squared']:.4f} ({row['interpretation']})")

        # =========================================================================
        # STEP 7: Final Validation
        # =========================================================================
        # Validates: Output meets criteria from 4_analysis.yaml
        # - Exactly 3 pairwise contrasts computed
        # - Both uncorrected AND Bonferroni p-values present
        # - p_corrected = min(1.0, p_uncorrected * 3) for each row
        # - Effect sizes computed for all effects
        # - p_uncorrected in [0, 1]
        # - z values finite (no NaN/Inf)

        log("\n[VALIDATION] Running output validation checks...")

        validation_errors = []

        # Check 1: Exactly 3 pairwise contrasts
        if len(contrasts_df) != 3:
            validation_errors.append(f"Expected 3 contrasts, got {len(contrasts_df)}")
        else:
            log("  [PASS] Exactly 3 pairwise contrasts computed")

        # Check 2: Required columns present
        required_cols = ['comparison', 'beta', 'se', 'z', 'p_uncorrected',
                        'alpha_corrected', 'p_corrected', 'sig_uncorrected', 'sig_corrected']
        missing_cols = [c for c in required_cols if c not in contrasts_df.columns]
        if missing_cols:
            validation_errors.append(f"Missing columns: {missing_cols}")
        else:
            log("  [PASS] Both uncorrected AND Bonferroni p-values present")

        # Check 3: p_corrected calculation is correct
        # p_corrected should be min(1.0, p_uncorrected * k) where k=3
        for idx, row in contrasts_df.iterrows():
            expected_p_corr = min(1.0, row['p_uncorrected'] * 3)
            if not pd.isna(row['p_corrected']) and abs(row['p_corrected'] - expected_p_corr) > 0.0001:
                validation_errors.append(
                    f"p_corrected mismatch for {row['comparison']}: "
                    f"expected {expected_p_corr:.4f}, got {row['p_corrected']:.4f}"
                )
        if not any("p_corrected mismatch" in e for e in validation_errors):
            log("  [PASS] p_corrected = min(1.0, p_uncorrected * 3) verified")

        # Check 4: Effect sizes computed
        if len(effect_sizes_df) == 0:
            validation_errors.append("No effect sizes computed")
        else:
            log(f"  [PASS] Effect sizes computed for {len(effect_sizes_df)} effects")

        # Check 5: p_uncorrected in [0, 1]
        p_vals = contrasts_df['p_uncorrected']
        if p_vals.min() < 0 or p_vals.max() > 1:
            validation_errors.append(f"p_uncorrected out of range [0,1]: min={p_vals.min()}, max={p_vals.max()}")
        else:
            log("  [PASS] p_uncorrected in [0, 1]")

        # Check 6: z values finite
        z_vals = contrasts_df['z']
        if z_vals.isna().any() or not all(pd.notna(z_vals)):
            validation_errors.append("z values contain NaN")
        elif not all(abs(z_vals) < float('inf')):
            validation_errors.append("z values contain Inf")
        else:
            log("  [PASS] z values finite (no NaN/Inf)")

        # Report validation result
        if validation_errors:
            log("\n[FAIL] Validation errors detected:")
            for err in validation_errors:
                log(f"  - {err}")
            raise ValueError(f"Validation failed: {validation_errors}")
        else:
            log("\n[PASS] All validation checks passed")

        # =========================================================================
        # STEP 8: Summary
        # =========================================================================

        log("\n" + "=" * 60)
        log("[SUCCESS] Step 06 complete: Post-hoc contrasts computed")
        log("=" * 60)
        log("\nOutputs created:")
        log(f"  1. {contrasts_output_path}")
        log(f"  2. {effect_sizes_output_path}")
        log("\nDecision D068 implemented: Dual p-value reporting")
        log(f"  - Significant at uncorrected alpha=0.05: {contrasts_df['sig_uncorrected'].sum()}/{len(contrasts_df)}")
        log(f"  - Significant at corrected alpha=0.0167: {contrasts_df['sig_corrected'].sum()}/{len(contrasts_df)}")

        sys.exit(0)

    except Exception as e:
        log(f"\n[ERROR] {str(e)}")
        log("\n[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
