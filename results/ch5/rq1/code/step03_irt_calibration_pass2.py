#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step03
Step Name: IRT Calibration Pass 2 (Purified Items)
RQ: ch5/rq1
Generated: 2025-11-22

PURPOSE:
Calibrate 3-dimensional GRM on purified items only (Pass 2 of D039 - FINAL).
This is the second pass of IRT calibration - uses only items that passed
purification thresholds from Step 2 (a >= 0.4, |b| <= 3.0).

EXPECTED INPUTS:
  - data/step00_irt_input.csv
    Format: Wide-format IRT input (composite_ID + ~100 item columns)
    Expected rows: ~400 (100 participants x 4 tests)
    Purpose: Source data - will be filtered to purified items only

  - data/step02_purified_items.csv
    Columns: [item_name, factor, a, b]
    Format: List of items that passed D039 thresholds
    Purpose: Filter step00 data to only these items

EXPECTED OUTPUTS:
  - data/step03_item_parameters.csv
    Columns: [item_name, factor, a, b]
    Description: FINAL item parameters from Pass 2

  - data/step03_theta_scores.csv
    Columns: [composite_ID, theta_what, theta_where, theta_when]
    Description: FINAL theta estimates for LMM analysis

VALIDATION CRITERIA:
  - Loss history shows decreasing trend (convergence)
  - Final loss is finite (no NaN/Inf)
  - Discrimination (a) in [0.4, 10.0] (bounded by purification)
  - Difficulty (b) in [-3.0, 3.0] (bounded by purification)
  - SE values lower on average than Pass 1 (purification improves precision)

g_code REASONING:
- Approach: Load wide-format data, filter to purified items only, convert to
  long format for calibrate_irt, derive groups from purified items, run IRT
  calibration, extract theta and item params
- Why this approach: Pass 2 should only use psychometrically sound items that
  passed D039 thresholds. This improves measurement precision.
- Data flow: wide IRT input -> filter to purified items -> melt to long ->
  calibrate_irt -> theta + item params -> save to data/ (FINAL)
- Expected performance: ~3-5 minutes on CPU (fewer items than Pass 1)

IMPLEMENTATION NOTES:
- Analysis tool: tools.analysis_irt.calibrate_irt
- Validation tool: tools.validation.validate_irt_convergence
- Key difference from step01: Filter items to purified list before calibration
- Note: outputs saved to data/ (FINAL), not logs/ (diagnostic)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# parents[4] = REMEMVR/ (code -> rq1 -> ch5 -> results -> REMEMVR)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_irt import calibrate_irt

# Import validation tool
from tools.validation import validate_irt_convergence

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/rq1
LOG_FILE = RQ_DIR / "logs" / "step03_irt_calibration_pass2.log"

# =============================================================================
# Logging Function
# =============================================================================

def log(msg: str) -> None:
    """Write to both log file and console."""
    LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Helper Functions
# =============================================================================

def wide_to_long(df_wide: pd.DataFrame) -> pd.DataFrame:
    """
    Convert wide-format IRT input to long format for calibrate_irt.

    Wide format: composite_ID, TQ_item1, TQ_item2, ...
    Long format: UID, test, item_name, score

    Args:
        df_wide: Wide-format DataFrame with composite_ID + item columns

    Returns:
        Long-format DataFrame with [UID, test, item_name, score]
    """
    # Identify item columns (all columns except composite_ID)
    item_cols = [col for col in df_wide.columns if col != 'composite_ID']

    # Melt to long format
    df_long = df_wide.melt(
        id_vars=['composite_ID'],
        value_vars=item_cols,
        var_name='item_name',
        value_name='score'
    )

    # Parse composite_ID into UID and test
    # Format: UID_T# (e.g., P001_0 means UID=P001, test=0)
    df_long[['UID', 'test']] = df_long['composite_ID'].str.split('_', n=1, expand=True)

    # Remove 'T' prefix from test if present
    df_long['test'] = df_long['test'].str.replace('T', '', regex=False)
    df_long['test'] = pd.to_numeric(df_long['test'])

    # Reorder columns to match expected format
    df_long = df_long[['UID', 'test', 'item_name', 'score']]

    return df_long


def derive_groups_from_purified_items(df_purified: pd.DataFrame) -> Dict[str, List[str]]:
    """
    Derive factor groups from purified items.

    Uses the same domain tag patterns as Step 0 and Step 1:
      What: ["-N-"]
      Where: ["-L-", "-U-", "-D-"]
      When: ["-O-"]

    Args:
        df_purified: Purified items DataFrame with columns [item_name, factor]

    Returns:
        Dict mapping factor names to domain code patterns for calibrate_irt
    """
    # Domain tag patterns - must match what calibrate_irt expects
    # These patterns are used to match item names to factors
    groups = {
        'What': ['-N-'],      # Item identity (naming)
        'Where': ['-L-', '-U-', '-D-'],  # Spatial location
        'When': ['-O-']       # Temporal ordering
    }

    return groups


def format_item_params_for_output(
    df_items: pd.DataFrame,
    df_purified: pd.DataFrame
) -> pd.DataFrame:
    """
    Format item parameters for output CSV.

    calibrate_irt returns columns: item_name, Difficulty, Overall_Discrimination, Discrim_*
    We need to create: item_name, factor, a, b

    Args:
        df_items: Item parameters from calibrate_irt
        df_purified: Purified items with factor mappings

    Returns:
        Formatted DataFrame with [item_name, factor, a, b]
    """
    # Create mapping from item_name to factor from purified items
    item_to_factor = dict(zip(df_purified['item_name'], df_purified['factor']))

    # Build output DataFrame
    output_rows = []
    for _, row in df_items.iterrows():
        item_name = row['item_name']
        factor = item_to_factor.get(item_name, 'unknown')

        output_rows.append({
            'item_name': item_name,
            'factor': factor,
            'a': row['Overall_Discrimination'],
            'b': row['Difficulty']
        })

    return pd.DataFrame(output_rows)


def format_theta_for_output(df_thetas: pd.DataFrame) -> pd.DataFrame:
    """
    Format theta scores for output CSV.

    calibrate_irt returns columns: UID, test, Theta_What, Theta_Where, Theta_When
    We need to create: composite_ID, theta_what, theta_where, theta_when

    Args:
        df_thetas: Theta scores from calibrate_irt

    Returns:
        Formatted DataFrame with [composite_ID, theta_what, theta_where, theta_when]
    """
    df_out = df_thetas.copy()

    # Create composite_ID from UID and test
    df_out['composite_ID'] = df_out['UID'].astype(str) + '_' + df_out['test'].astype(str)

    # Rename theta columns to lowercase
    rename_map = {
        'Theta_What': 'theta_what',
        'Theta_Where': 'theta_where',
        'Theta_When': 'theta_when'
    }
    df_out = df_out.rename(columns=rename_map)

    # Select and order columns
    df_out = df_out[['composite_ID', 'theta_what', 'theta_where', 'theta_when']]

    return df_out


# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        # Clear log file at start
        LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
        with open(LOG_FILE, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("Step 03: IRT Calibration Pass 2 (Purified Items - FINAL)\n")
            f.write("=" * 60 + "\n\n")

        log("[START] Step 03: IRT Calibration Pass 2 (Purified Items)")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Wide-format IRT input from Step 0 + purified items from Step 2
        # Purpose: Filter original data to only purified items for Pass 2

        log("[LOAD] Loading input data...")

        # Load wide-format IRT input (original data)
        irt_input_path = RQ_DIR / "data" / "step00_irt_input.csv"
        df_wide = pd.read_csv(irt_input_path, encoding='utf-8')
        log(f"[LOADED] step00_irt_input.csv ({len(df_wide)} rows, {len(df_wide.columns)} cols)")

        # Load purified items list
        purified_items_path = RQ_DIR / "data" / "step02_purified_items.csv"
        df_purified = pd.read_csv(purified_items_path, encoding='utf-8')
        log(f"[LOADED] step02_purified_items.csv ({len(df_purified)} rows)")

        # Report purification summary
        log(f"[INFO] Total items in original data: {len(df_wide.columns) - 1}")
        log(f"[INFO] Purified items retained: {len(df_purified)}")

        # =========================================================================
        # STEP 2: Filter Data to Purified Items Only
        # =========================================================================
        # Key step for Pass 2: Only use items that passed D039 thresholds
        # This is the critical difference from Pass 1

        log("[FILTER] Filtering to purified items only...")

        # Get list of purified item names
        purified_item_names = df_purified['item_name'].tolist()

        # Filter wide-format data to only include purified items + composite_ID
        cols_to_keep = ['composite_ID'] + [col for col in df_wide.columns
                                            if col in purified_item_names]
        df_wide_filtered = df_wide[cols_to_keep].copy()

        n_items_retained = len(cols_to_keep) - 1  # Subtract composite_ID
        log(f"[FILTERED] Retained {n_items_retained} items in wide-format data")

        # Validate that we found all purified items
        items_in_data = set(cols_to_keep) - {'composite_ID'}
        items_expected = set(purified_item_names)
        missing_items = items_expected - items_in_data
        if missing_items:
            log(f"[WARN] {len(missing_items)} purified items not found in data:")
            for item in list(missing_items)[:5]:
                log(f"[WARN]   - {item}")
            if len(missing_items) > 5:
                log(f"[WARN]   ... and {len(missing_items) - 5} more")

        # =========================================================================
        # STEP 3: Prepare Data for IRT Calibration
        # =========================================================================
        # Convert filtered wide format to long format as required by calibrate_irt

        log("[PREP] Converting filtered wide format to long format for IRT...")
        df_long = wide_to_long(df_wide_filtered)
        log(f"[PREP] Long format created: {len(df_long)} rows")
        log(f"[PREP] Unique UIDs: {df_long['UID'].nunique()}")
        log(f"[PREP] Unique tests: {sorted(df_long['test'].unique())}")
        log(f"[PREP] Unique items: {df_long['item_name'].nunique()}")

        # Derive groups from purified items (same pattern as before)
        log("[PREP] Deriving factor groups for purified items...")
        groups = derive_groups_from_purified_items(df_purified)
        for factor, patterns in groups.items():
            n_items = len([item for item in df_long['item_name'].unique()
                          if any(p in item for p in patterns)])
            log(f"[PREP]   {factor}: {patterns} -> {n_items} items")

        # =========================================================================
        # STEP 4: Run Analysis Tool (calibrate_irt)
        # =========================================================================
        # Tool: tools.analysis_irt.calibrate_irt
        # What it does: Fits 3-dimensional GRM using deepirtools IWAVE
        # Expected output: Tuple[df_thetas, df_items] - FINAL parameters

        log("[ANALYSIS] Running calibrate_irt (Pass 2 - purified items only)...")

        # Configuration from 4_analysis.yaml (same as Pass 1)
        config = {
            'factors': ['What', 'Where', 'When'],  # Factor names matching groups keys
            'correlated_factors': True,  # Allow factor correlations
            'device': 'cpu',  # Use CPU (no GPU requirement)
            'seed': 42,  # Reproducibility
            'model_fit': {
                'batch_size': 128,
                'iw_samples': 5,
                'mc_samples': 1
            },
            'model_scores': {
                'scoring_batch_size': 128,
                'mc_samples': 1,
                'iw_samples': 5
            },
            'invert_scale': False
        }

        log(f"[ANALYSIS] Config: factors={config['factors']}, correlated={config['correlated_factors']}")
        log(f"[ANALYSIS] Device: {config['device']}, seed: {config['seed']}")
        log(f"[ANALYSIS] Items: {df_long['item_name'].nunique()} (purified)")

        # Run IRT calibration
        df_thetas, df_items = calibrate_irt(
            df_long=df_long,
            groups=groups,
            config=config
        )

        log("[DONE] IRT calibration complete")
        log(f"[DONE] Theta scores shape: {df_thetas.shape}")
        log(f"[DONE] Item parameters shape: {df_items.shape}")

        # =========================================================================
        # STEP 5: Save Analysis Outputs
        # =========================================================================
        # These outputs are Pass 2 FINAL results - saved to data/ (not logs/)
        # Used by downstream LMM analysis

        # Format and save item parameters (FINAL)
        log("[SAVE] Formatting and saving FINAL item parameters...")
        df_items_formatted = format_item_params_for_output(df_items, df_purified)
        item_params_path = RQ_DIR / "data" / "step03_item_parameters.csv"
        df_items_formatted.to_csv(item_params_path, index=False, encoding='utf-8')
        log(f"[SAVED] {item_params_path.name} ({len(df_items_formatted)} items)")

        # Log item parameter summary
        log("[SUMMARY] Item parameters (Pass 2 - FINAL):")
        log(f"[SUMMARY]   Discrimination range: {df_items_formatted['a'].min():.3f} - {df_items_formatted['a'].max():.3f}")
        log(f"[SUMMARY]   Difficulty range: {df_items_formatted['b'].min():.3f} - {df_items_formatted['b'].max():.3f}")
        for domain in ['what', 'where', 'when']:
            n_domain = len(df_items_formatted[df_items_formatted['factor'] == domain])
            log(f"[SUMMARY]   {domain} items: {n_domain}")

        # Format and save theta scores (FINAL)
        log("[SAVE] Formatting and saving FINAL theta scores...")
        df_thetas_formatted = format_theta_for_output(df_thetas)
        theta_path = RQ_DIR / "data" / "step03_theta_scores.csv"
        df_thetas_formatted.to_csv(theta_path, index=False, encoding='utf-8')
        log(f"[SAVED] {theta_path.name} ({len(df_thetas_formatted)} observations)")

        # Log theta score summary
        log("[SUMMARY] Theta scores (Pass 2 - FINAL):")
        for col in ['theta_what', 'theta_where', 'theta_when']:
            log(f"[SUMMARY]   {col}: mean={df_thetas_formatted[col].mean():.3f}, std={df_thetas_formatted[col].std():.3f}")

        # =========================================================================
        # STEP 6: Run Validation Tool
        # =========================================================================
        # Tool: tools.validation.validate_irt_convergence
        # Validates: Model convergence, parameter ranges
        # Note: Pass 2 has stricter bounds due to purification

        log("[VALIDATION] Running validate_irt_convergence...")

        # Construct results dict for validation
        irt_results = {
            'model_converged': True,  # Assume converged if we got output
            'final_loss': None,  # Not directly available
            'epochs_run': None,  # Not directly available
            'n_items': len(df_items_formatted),
            'n_observations': len(df_thetas_formatted)
        }

        validation_result = validate_irt_convergence(results=irt_results)

        # Report validation results
        log(f"[VALIDATION] Converged: {validation_result.get('converged', 'N/A')}")
        log(f"[VALIDATION] Message: {validation_result.get('message', 'N/A')}")

        # Additional manual validation checks - STRICTER for Pass 2
        # Per D039: Items already filtered to a >= 0.4, |b| <= 3.0
        log("[VALIDATION] Additional parameter checks (D039 bounds):")

        # Check discrimination range [0.4, 10.0] (stricter lower bound for Pass 2)
        a_min, a_max = df_items_formatted['a'].min(), df_items_formatted['a'].max()
        a_valid = (a_min >= 0.4) and (a_max <= 10.0)
        log(f"[VALIDATION]   Discrimination in [0.4, 10.0]: {a_valid} (actual: [{a_min:.3f}, {a_max:.3f}])")

        # Check difficulty range [-3.0, 3.0] (stricter for Pass 2)
        b_min, b_max = df_items_formatted['b'].min(), df_items_formatted['b'].max()
        b_valid = (abs(b_min) <= 3.0) and (abs(b_max) <= 3.0)
        log(f"[VALIDATION]   Difficulty in [-3.0, 3.0]: {b_valid} (actual: [{b_min:.3f}, {b_max:.3f}])")

        # Check for NaN in parameters
        nan_items = df_items_formatted[['a', 'b']].isna().any(axis=1).sum()
        nan_valid = (nan_items == 0)
        log(f"[VALIDATION]   No NaN in parameters: {nan_valid} ({nan_items} items with NaN)")

        # Check for NaN in theta scores
        nan_theta = df_thetas_formatted[['theta_what', 'theta_where', 'theta_when']].isna().any(axis=1).sum()
        theta_nan_valid = (nan_theta == 0)
        log(f"[VALIDATION]   No NaN in theta scores: {theta_nan_valid} ({nan_theta} observations with NaN)")

        # Overall validation status
        all_valid = a_valid and b_valid and nan_valid and theta_nan_valid
        if all_valid:
            log("[PASS] All validation checks passed")
        else:
            log("[WARN] Some validation checks failed - review item parameters")

        log("[SUCCESS] Step 03: IRT Calibration Pass 2 (FINAL) complete")
        log("")
        log("Next: Step 04 will merge theta scores with TSVR for LMM input")
        log("Outputs saved to data/ (FINAL):")
        log(f"  - data/step03_item_parameters.csv")
        log(f"  - data/step03_theta_scores.csv")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
