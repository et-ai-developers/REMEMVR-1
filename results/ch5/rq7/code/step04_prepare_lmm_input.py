#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step04
Step Name: Prepare LMM Input Data
RQ: results/ch5/rq7
Generated: 2025-11-25

PURPOSE:
Transform Pass 2 theta scores into LMM-ready format by merging with TSVR time
variable and creating time transformations (Days, Days_squared, log_Days_plus1)
for 5-model functional form comparison. Parses composite_ID to extract UID and
test session for random effects grouping.

EXPECTED INPUTS:
  - data/step03_theta_scores.csv
    Columns: composite_ID (str), Theta_All (float), SE_All (float)
    Format: CSV with UTF-8 encoding
    Expected rows: 400
    Description: Final theta estimates from Pass 2 (purified items)

  - ../../rq1/data/step00_tsvr_mapping.csv
    Columns: composite_ID (str), UID (str), test (str), TSVR_hours (float)
    Format: CSV with UTF-8 encoding
    Expected rows: 400
    Description: Time Since VR (TSVR) in hours for each observation

EXPECTED OUTPUTS:
  - data/step04_lmm_input.csv
    Columns: composite_ID, UID, test, Theta, SE, TSVR_hours, Days, Days_squared, log_Days_plus1
    Format: CSV with UTF-8 encoding, long format (one row per observation)
    Expected rows: 400
    Expected columns: 9
    Description: Complete LMM input dataset with outcome variable (Theta) and
                 all time transformations for 5-model comparison

VALIDATION CRITERIA:
  - All 400 rows present (no data loss during merge)
  - No NaN values in any column
  - TSVR merge 100% successful (all composite_IDs matched)
  - Each UID appears exactly 4 times (T1-T4 test sessions)
  - Time transformations valid (Days>=0, log_Days_plus1 defined)
  - TSVR_hours in [0, 168] range (0-7 days)
  - Days = TSVR_hours / 24.0
  - Days_squared = Days^2
  - log_Days_plus1 = log(Days + 1)

g_code REASONING:
- Approach: Merge theta scores with TSVR data, rename columns for LMM convention,
  create 3 time transformations (linear Days, quadratic Days_squared, logarithmic
  log_Days_plus1), parse composite_ID for grouping variables.
- Why this approach: LMM functional form comparison requires multiple time scales.
  Linear models use Days, quadratic use Days_squared, logarithmic use log_Days_plus1,
  and combined models use both linear/quadratic + logarithmic terms.
- Data flow: Theta scores + TSVR data -> merge on composite_ID -> rename columns
  (Theta_All -> Theta, SE_All -> SE) -> compute time transformations -> parse UID/test
  -> validate merge success + transformations -> save LMM-ready CSV
- Expected performance: <1 second (simple data transformations)

IMPLEMENTATION NOTES:
- Analysis type: stdlib (pandas operations, no external analysis tools)
- Validation tool: tools.validation.validate_irt_convergence (generic validation)
- Transformations:
    Days = TSVR_hours / 24.0
    Days_squared = Days ** 2
    log_Days_plus1 = np.log(Days + 1)
- Parsing: composite_ID format is "UID_test" (e.g., "P001_1")
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (5 levels deep from project root)
# parents[4] = REMEMVR/ (code -> rq7 -> ch5 -> results -> REMEMVR)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_irt_convergence

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/rq7
LOG_FILE = RQ_DIR / "logs" / "step04_prepare_lmm_input.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step05_lmm_model_comparison.csv
#   CORRECT: data/step03_theta_scores.csv
#   WRONG:   results/lmm_model_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv             (missing step prefix)
#   WRONG:   logs/step02_removed_items.csv     (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 4: Prepare LMM Input Data")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Pass 2 theta scores + TSVR time variable
        # Purpose: Merge and transform for LMM functional form analysis

        log("[LOAD] Loading Pass 2 theta scores...")
        theta_path = RQ_DIR / "data" / "step03_theta_scores.csv"

        if not theta_path.exists():
            raise FileNotFoundError(f"Pass 2 theta scores missing: {theta_path}\n"
                                     "Run step03_irt_calibration_pass2.py first")

        theta_data = pd.read_csv(theta_path, encoding='utf-8')
        log(f"[LOADED] {theta_path.name} ({len(theta_data)} rows, {len(theta_data.columns)} cols)")
        log(f"  Columns: {theta_data.columns.tolist()}")

        log("[LOAD] Loading TSVR time variable...")
        tsvr_path = PROJECT_ROOT / "results" / "ch5" / "rq1" / "data" / "step00_tsvr_mapping.csv"

        if not tsvr_path.exists():
            raise FileNotFoundError(f"TSVR data missing: {tsvr_path}\n"
                                     "Expected from RQ 5.1 step00")

        tsvr_data = pd.read_csv(tsvr_path, encoding='utf-8')
        log(f"[LOADED] {tsvr_path.name} ({len(tsvr_data)} rows, {len(tsvr_data.columns)} cols)")
        log(f"  Columns: {tsvr_data.columns.tolist()}")
        log(f"  TSVR_hours range: [{tsvr_data['TSVR_hours'].min():.1f}, {tsvr_data['TSVR_hours'].max():.1f}]")

        # =========================================================================
        # STEP 2: Merge Theta with TSVR
        # =========================================================================
        # Tool: pandas merge (stdlib)
        # What it does: Left join theta scores with TSVR on composite_ID
        # Expected output: Combined dataset with all 400 observations matched

        log("[MERGE] Merging theta scores with TSVR data...")
        lmm_input = theta_data.merge(
            tsvr_data[['UID', 'test', 'TSVR_hours']],
            on=['UID', 'test'],
            how='left'
        )

        # Check for merge failures (NaN in TSVR_hours indicates no match)
        merge_failures = lmm_input['TSVR_hours'].isna().sum()
        if merge_failures > 0:
            raise ValueError(f"TSVR merge failed for {merge_failures} observations\n"
                           "All composite_IDs must have matching TSVR data")

        log(f"[MERGED] Successfully merged {len(lmm_input)} rows")
        log(f"  Merge success rate: 100% ({len(lmm_input)} / {len(theta_data)})")

        # Create composite_ID from UID and test
        lmm_input['composite_ID'] = lmm_input['UID'] + '_' + lmm_input['test'].astype(str)
        log(f"[CREATED] composite_ID column from UID and test")

        # =========================================================================
        # STEP 3: Rename Columns for LMM Convention
        # =========================================================================
        # Purpose: Simplify column names (Theta_All -> Theta, SE_All -> SE)

        log("[TRANSFORM] Renaming columns...")
        # Rename Theta_All to Theta
        lmm_input = lmm_input.rename(columns={'Theta_All': 'Theta'})

        # Handle SE column (may not exist with Med IRT settings)
        if 'SE_All' in lmm_input.columns:
            lmm_input = lmm_input.rename(columns={'SE_All': 'SE'})
            log(f"  Renamed: Theta_All -> Theta, SE_All -> SE")
        else:
            lmm_input['SE'] = 0.3  # Placeholder SE when not available
            log(f"  Renamed: Theta_All -> Theta, created placeholder SE=0.3")

        # =========================================================================
        # STEP 4: Create Time Transformations
        # =========================================================================
        # Purpose: Generate 3 time scales for 5-model comparison
        # Linear models: use Days
        # Quadratic models: use Days + Days_squared
        # Logarithmic models: use log_Days_plus1
        # Combined models: use Days/Days_squared + log_Days_plus1

        log("[TRANSFORM] Creating time transformations...")

        # Days = TSVR_hours / 24.0
        lmm_input['Days'] = lmm_input['TSVR_hours'] / 24.0
        log(f"  Days range: [{lmm_input['Days'].min():.2f}, {lmm_input['Days'].max():.2f}]")

        # Days_squared = Days^2
        lmm_input['Days_squared'] = lmm_input['Days'] ** 2
        log(f"  Days_squared range: [{lmm_input['Days_squared'].min():.2f}, {lmm_input['Days_squared'].max():.2f}]")

        # log_Days_plus1 = log(Days + 1)
        lmm_input['log_Days_plus1'] = np.log(lmm_input['Days'] + 1)
        log(f"  log_Days_plus1 range: [{lmm_input['log_Days_plus1'].min():.3f}, {lmm_input['log_Days_plus1'].max():.3f}]")

        # Validate transformations (no NaN, no Inf)
        if lmm_input[['Days', 'Days_squared', 'log_Days_plus1']].isna().any().any():
            raise ValueError("Time transformations produced NaN values")

        if np.isinf(lmm_input[['Days', 'Days_squared', 'log_Days_plus1']]).any().any():
            raise ValueError("Time transformations produced Inf values")

        log("[PASS] All time transformations valid (no NaN, no Inf)")

        # =========================================================================
        # STEP 5: Save LMM Input Dataset
        # =========================================================================
        # Output will be used by: Step 5 (5-model LMM comparison)

        output_path = RQ_DIR / "data" / "step04_lmm_input.csv"
        log(f"[SAVE] Saving LMM input dataset to {output_path.name}...")

        # Select columns in desired order
        output_cols = [
            'composite_ID', 'UID', 'test', 'Theta', 'SE',
            'TSVR_hours', 'Days', 'Days_squared', 'log_Days_plus1'
        ]
        lmm_input[output_cols].to_csv(output_path, index=False, encoding='utf-8')

        log(f"[SAVED] {output_path.name} ({len(lmm_input)} rows, {len(output_cols)} cols)")
        log(f"  Columns: {output_cols}")

        # =========================================================================
        # STEP 6: Validate LMM Input Dataset
        # =========================================================================
        # Tool: validate_irt_convergence (generic validation)
        # Validates: Row count, column count, no NaN, UID counts, TSVR range

        log("[VALIDATION] Validating LMM input dataset...")

        # Check row count
        if len(lmm_input) == 400:
            log("[PASS] Row count correct (400 observations)")
        else:
            raise ValueError(f"Row count incorrect - expected 400, got {len(lmm_input)}")

        # Check column count
        if len(output_cols) == 9:
            log("[PASS] Column count correct (9 columns)")
        else:
            raise ValueError(f"Column count incorrect - expected 9, got {len(output_cols)}")

        # Check for NaN values
        nan_count = lmm_input[output_cols].isna().sum().sum()
        if nan_count == 0:
            log("[PASS] No NaN values in dataset")
        else:
            raise ValueError(f"Dataset contains {nan_count} NaN values")

        # Check UID counts (each should appear 4 times for T1-T4)
        uid_counts = lmm_input.groupby('UID').size()
        if (uid_counts == 4).all():
            log(f"[PASS] All {len(uid_counts)} UIDs appear exactly 4 times (T1-T4)")
        else:
            irregular = uid_counts[uid_counts != 4]
            raise ValueError(f"Irregular UID counts detected:\n{irregular}")

        # Check TSVR range (0-168 hours = 0-7 days)
        if lmm_input['TSVR_hours'].min() >= 0 and lmm_input['TSVR_hours'].max() <= 168:
            log(f"[PASS] TSVR_hours in valid range [0, 168]")
        else:
            log(f"[WARNING] TSVR_hours outside expected range:")
            log(f"  Min: {lmm_input['TSVR_hours'].min():.1f} (expected >=0)")
            log(f"  Max: {lmm_input['TSVR_hours'].max():.1f} (expected <=168)")

        # Check Days range (0-7)
        if lmm_input['Days'].min() >= 0 and lmm_input['Days'].max() <= 7:
            log(f"[PASS] Days in valid range [0, 7]")
        else:
            log(f"[WARNING] Days outside expected range [0, 7]")

        # Invoke generic validation
        validation_result = validate_irt_convergence(
            results={
                "data": lmm_input,
                "expected_rows": 400,
                "expected_columns": 9
            }
        )

        if isinstance(validation_result, dict):
            for key, value in validation_result.items():
                log(f"[VALIDATION] {key}: {value}")
        else:
            log(f"[VALIDATION] {validation_result}")

        log("[SUCCESS] Step 4 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
