#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step06
Step Name: AIC Model Selection
RQ: results/ch5/rq7
Generated: 2025-11-25

PURPOSE:
Compute AIC-based model selection metrics: delta AIC (difference from minimum),
Akaike weights (model probabilities), and cumulative weights (evidence accumulation).
Identify best-fitting model and categorize uncertainty level based on Akaike weight
of best model (>0.90: very strong, 0.60-0.90: strong, 0.30-0.60: moderate, <0.30: high).

EXPECTED INPUTS:
  - results/step05_model_comparison.csv
    Columns: model_name, AIC, BIC, log_likelihood, n_params, converged
    Format: CSV with UTF-8 encoding
    Expected rows: 5
    Description: Model fit statistics from Step 5

  - data/step05_model_fits.pkl
    Format: Python pickle dictionary
    Keys: 'Linear', 'Quadratic', 'Logarithmic', 'LinLog', 'QuadLog'
    Values: MixedLMResults objects
    Description: Fitted model objects for extracting best model

EXPECTED OUTPUTS:
  - results/step06_aic_comparison.csv
    Columns: model_name, AIC, delta_AIC, akaike_weight, cumulative_weight
    Format: CSV sorted by AIC ascending
    Expected rows: 5
    Description: Complete AIC comparison with model selection metrics

  - data/step06_best_model.pkl
    Format: Python pickle (MixedLMResults object)
    Description: Best-fitting model object saved separately for easy access

  - results/step06_best_model_summary.txt
    Format: Text summary
    Description: Best model details (name, AIC, weight, uncertainty category, coefficients)

VALIDATION CRITERIA:
  - Akaike weights sum to 1.0 (within [0.999, 1.001])
  - All weights in (0, 1) exclusive range
  - delta_AIC correct (best model = 0, others positive)
  - cumulative_weight monotonic increasing (ends at 1.0)
  - Best model identified (row 1 has delta_AIC = 0)

g_code REASONING:
- Approach: Compute delta AIC = AIC_i - AIC_min, then Akaike weights using
  w_i = exp(-0.5 * delta_AIC_i) / sum(exp(-0.5 * delta_AIC_j)), sort by AIC,
  compute cumulative weights, categorize uncertainty, save best model separately.
- Why this approach: Akaike weights provide probabilistic interpretation of
  model support (w_i = probability model i is best given data and candidate set).
  Delta AIC quantifies evidence strength (delta < 2: substantial, 2-7: positive,
  7-10: weak, >10: essentially none). Uncertainty categorization guides interpretation.
- Data flow: Model comparison table -> compute delta_AIC -> compute weights ->
  sort by AIC -> cumulative weights -> identify best model -> extract from pickle
  -> save best model + summary -> validation checks
- Expected performance: <1 second (simple arithmetic on 5 rows)

IMPLEMENTATION NOTES:
- Analysis type: stdlib (pandas arithmetic, pickle operations)
- Validation tool: tools.validation.validate_irt_convergence (generic validation)
- Formulas:
    delta_AIC_i = AIC_i - min(AIC)
    w_i = exp(-0.5 * delta_AIC_i) / sum(exp(-0.5 * delta_AIC_j))
    cumulative_weight = cumsum(w_i) after sorting by AIC
- Uncertainty categories:
    w_best > 0.90: "Very strong" (>90% probability best model is correct)
    0.60-0.90: "Strong" (60-90% probability)
    0.30-0.60: "Moderate" (30-60% probability, substantial uncertainty)
    <0.30: "High" (<30% probability, weak support)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import pickle
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (5 levels deep from project root)
# parents[4] = REMEMVR/ (code -> rq7 -> ch5 -> results -> REMEMVR)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_irt_convergence

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/rq7
LOG_FILE = RQ_DIR / "logs" / "step06_model_selection.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step05_lmm_model_comparison.csv
#   CORRECT: data/step03_theta_scores.csv
#   WRONG:   results/lmm_model_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv             (missing step prefix)
#   WRONG:   logs/step02_removed_items.csv     (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 6: AIC Model Selection")

        # =========================================================================
        # STEP 1: Load Model Comparison Results
        # =========================================================================
        # Expected: Model fit statistics from Step 5 + fitted model objects
        # Purpose: Compute AIC selection metrics and identify best model

        log("[LOAD] Loading model comparison table...")
        comparison_path = RQ_DIR / "results" / "step05_model_comparison.csv"

        if not comparison_path.exists():
            raise FileNotFoundError(f"Model comparison table missing: {comparison_path}\n"
                                     "Run step05_fit_5_candidate_lmms.py first")

        model_comparison = pd.read_csv(comparison_path, encoding='utf-8')
        log(f"[LOADED] {comparison_path.name} ({len(model_comparison)} models)")
        log(f"  Columns: {model_comparison.columns.tolist()}")
        log(f"  AIC range: [{model_comparison['AIC'].min():.2f}, {model_comparison['AIC'].max():.2f}]")

        log("[LOAD] Loading fitted model objects...")
        pickle_path = RQ_DIR / "data" / "step05_model_fits.pkl"

        if not pickle_path.exists():
            raise FileNotFoundError(f"Model fits pickle missing: {pickle_path}\n"
                                     "Run step05_fit_5_candidate_lmms.py first")

        with open(pickle_path, 'rb') as f:
            model_fits = pickle.load(f)

        log(f"[LOADED] {pickle_path.name} ({len(model_fits)} model objects)")

        # =========================================================================
        # STEP 2: Compute Delta AIC
        # =========================================================================
        # Formula: delta_AIC_i = AIC_i - min(AIC)
        # What it does: Quantifies evidence against model i relative to best model
        # Interpretation: <2 (substantial), 2-7 (positive), 7-10 (weak), >10 (none)

        log("[COMPUTE] Computing delta AIC...")
        aic_min = model_comparison['AIC'].min()
        model_comparison['delta_AIC'] = model_comparison['AIC'] - aic_min

        log(f"  Minimum AIC: {aic_min:.2f}")
        log(f"  delta_AIC range: [{model_comparison['delta_AIC'].min():.2f}, {model_comparison['delta_AIC'].max():.2f}]")

        # =========================================================================
        # STEP 3: Compute Akaike Weights
        # =========================================================================
        # Formula: w_i = exp(-0.5 * delta_AIC_i) / sum(exp(-0.5 * delta_AIC_j))
        # What it does: Converts AIC to model probabilities (evidence for model i)
        # Properties: Sum to 1.0, all in (0, 1), higher weight = better support

        log("[COMPUTE] Computing Akaike weights...")
        model_comparison['akaike_weight'] = np.exp(-0.5 * model_comparison['delta_AIC'])
        weight_sum = model_comparison['akaike_weight'].sum()
        model_comparison['akaike_weight'] = model_comparison['akaike_weight'] / weight_sum

        log(f"  Weight sum: {model_comparison['akaike_weight'].sum():.6f} (should be 1.0)")
        log(f"  Weight range: [{model_comparison['akaike_weight'].min():.4f}, {model_comparison['akaike_weight'].max():.4f}]")

        # =========================================================================
        # STEP 4: Sort by AIC and Compute Cumulative Weights
        # =========================================================================
        # Purpose: Order models by evidence strength, show cumulative evidence
        # Cumulative weight: Running sum of weights (monotonic increasing to 1.0)

        log("[SORT] Sorting by AIC ascending...")
        model_comparison = model_comparison.sort_values('AIC').reset_index(drop=True)

        log("[COMPUTE] Computing cumulative weights...")
        model_comparison['cumulative_weight'] = model_comparison['akaike_weight'].cumsum()

        log("")
        log("AIC Comparison (sorted by AIC):")
        log(model_comparison[['model_name', 'AIC', 'delta_AIC', 'akaike_weight', 'cumulative_weight']].to_string(index=False))
        log("")

        # =========================================================================
        # STEP 5: Identify Best Model
        # =========================================================================
        # Best model: Row with delta_AIC = 0 (minimum AIC)

        best_model_name = model_comparison.iloc[0]['model_name']
        best_model_aic = model_comparison.iloc[0]['AIC']
        best_model_weight = model_comparison.iloc[0]['akaike_weight']

        log(f"[BEST MODEL] {best_model_name}")
        log(f"  AIC: {best_model_aic:.2f}")
        log(f"  Akaike weight: {best_model_weight:.4f}")

        # Categorize uncertainty
        if best_model_weight > 0.90:
            uncertainty = "Very strong"
            interpretation = ">90% probability this is the best model"
        elif best_model_weight >= 0.60:
            uncertainty = "Strong"
            interpretation = "60-90% probability this is the best model"
        elif best_model_weight >= 0.30:
            uncertainty = "Moderate"
            interpretation = "30-60% probability - substantial uncertainty"
        else:
            uncertainty = "High"
            interpretation = "<30% probability - weak support, consider model averaging"

        log(f"  Uncertainty: {uncertainty} ({interpretation})")

        # =========================================================================
        # STEP 6: Save AIC Comparison Results
        # =========================================================================
        # Output will be used by: Step 7 (plot preparation), final RQ report

        # Save AIC comparison table
        output_path = RQ_DIR / "results" / "step06_aic_comparison.csv"
        log(f"[SAVE] Saving AIC comparison to {output_path.name}...")
        model_comparison[['model_name', 'AIC', 'delta_AIC', 'akaike_weight', 'cumulative_weight']].to_csv(
            output_path,
            index=False,
            encoding='utf-8'
        )
        log(f"[SAVED] {output_path.name}")

        # Save best model object separately
        best_model_obj = model_fits[best_model_name]
        best_model_path = RQ_DIR / "data" / "step06_best_model.pkl"
        log(f"[SAVE] Saving best model object to {best_model_path.name}...")
        with open(best_model_path, 'wb') as f:
            pickle.dump(best_model_obj, f)
        log(f"[SAVED] {best_model_path.name}")

        # Save best model summary text
        summary_path = RQ_DIR / "results" / "step06_best_model_summary.txt"
        log(f"[SAVE] Saving best model summary to {summary_path.name}...")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("BEST MODEL SUMMARY - AIC Model Selection\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"Best Model: {best_model_name}\n")
            f.write(f"AIC: {best_model_aic:.2f}\n")
            f.write(f"Akaike Weight: {best_model_weight:.4f}\n")
            f.write(f"Uncertainty: {uncertainty}\n")
            f.write(f"Interpretation: {interpretation}\n\n")
            f.write("=" * 80 + "\n")
            f.write("Model Coefficients:\n")
            f.write("=" * 80 + "\n\n")
            f.write(str(best_model_obj.summary()))
            f.write("\n\n")
            f.write("=" * 80 + "\n")
            f.write("All Models Comparison:\n")
            f.write("=" * 80 + "\n\n")
            f.write(model_comparison[['model_name', 'AIC', 'delta_AIC', 'akaike_weight', 'cumulative_weight']].to_string(index=False))
            f.write("\n")

        log(f"[SAVED] {summary_path.name}")

        # =========================================================================
        # STEP 7: Validate AIC Comparison
        # =========================================================================
        # Tool: validate_irt_convergence (generic validation)
        # Validates: Weights sum to 1.0, weights in (0,1), delta_AIC correct,
        #            cumulative_weight monotonic

        log("[VALIDATION] Validating AIC comparison metrics...")

        # Check weights sum to 1.0
        weight_sum = model_comparison['akaike_weight'].sum()
        if 0.999 <= weight_sum <= 1.001:
            log(f"[PASS] Akaike weights sum to 1.0 ({weight_sum:.6f})")
        else:
            raise ValueError(f"Akaike weights sum incorrect: {weight_sum:.6f} (expected 1.0)")

        # Check all weights in (0, 1)
        if (model_comparison['akaike_weight'] > 0).all() and (model_comparison['akaike_weight'] < 1).all():
            log("[PASS] All Akaike weights in (0, 1) exclusive")
        else:
            raise ValueError("Some Akaike weights outside (0, 1) range")

        # Check delta_AIC correct
        if model_comparison.iloc[0]['delta_AIC'] == 0.0:
            log("[PASS] Best model has delta_AIC = 0")
        else:
            raise ValueError(f"Best model delta_AIC incorrect: {model_comparison.iloc[0]['delta_AIC']} (expected 0)")

        if (model_comparison['delta_AIC'] >= 0).all():
            log("[PASS] All delta_AIC >= 0")
        else:
            raise ValueError("Some delta_AIC values negative")

        # Check cumulative_weight monotonic increasing
        if (model_comparison['cumulative_weight'].diff().dropna() >= 0).all():
            log("[PASS] cumulative_weight monotonic increasing")
        else:
            raise ValueError("cumulative_weight not monotonic increasing")

        # Check cumulative_weight ends at 1.0
        cum_weight_final = model_comparison.iloc[-1]['cumulative_weight']
        if 0.999 <= cum_weight_final <= 1.001:
            log(f"[PASS] cumulative_weight ends at 1.0 ({cum_weight_final:.6f})")
        else:
            raise ValueError(f"cumulative_weight final value incorrect: {cum_weight_final:.6f} (expected 1.0)")

        # Invoke generic validation
        validation_result = validate_irt_convergence(
            results={
                "data": model_comparison
            }
        )

        if isinstance(validation_result, dict):
            for key, value in validation_result.items():
                log(f"[VALIDATION] {key}: {value}")
        else:
            log(f"[VALIDATION] {validation_result}")

        log("[SUCCESS] Step 6 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
