# ============================================================================
# ANALYSIS RECIPE - COMPLETE SPECIFICATION
# ============================================================================
# Generated: 2025-11-25
# RQ: ch5/rq7
# Agent: rq_analysis v4.0.0
# Purpose: Self-contained recipe for g_code agent (reads ONLY this file)
# ============================================================================

metadata:
  rq_id: "ch5/rq7"
  total_steps: 5
  analysis_type: "IRT single-factor -> LMM 5-model comparison -> AIC selection"
  generated_by: "rq_analysis v4.0.0"
  timestamp: "2025-11-25T00:00:00Z"
  cross_rq_dependencies:
    - "RQ 5.1 Step 0 (requires step00_irt_input.csv and step00a_tsvr_data.csv)"

# ============================================================================
# ANALYSIS STEPS - COMPLETE SPECIFICATIONS
# ============================================================================

steps:
  # --------------------------------------------------------------------------
  # STEP 1: IRT Calibration with Omnibus Factor
  # --------------------------------------------------------------------------
  - name: "step01_irt_calibration_omnibus"
    step_number: "01"
    description: "Calibrate single-factor IRT model with 'All' omnibus dimension aggregating all What/Where/When items"

    # Analysis tool specification
    tool:
      module: "tools.analysis_irt"
      function: "calibrate_irt"
      signature: "calibrate_irt(df_long: DataFrame, groups: Dict[str, List[str]], config: dict) -> Tuple[DataFrame, DataFrame]"

    # Input specifications (COMPLETE)
    inputs:
      data_file:
        path: "../../rq1/data/step00_irt_input.csv"
        format: "CSV with UTF-8 encoding, wide format"
        columns:
          - {name: "composite_ID", type: "str", description: "UID_test format (e.g., P001_T1)"}
          - {name: "VR item response columns", type: "int", description: "Item response columns (VR-*-*-*-ANS format), values 0 or 1"}
        row_count: 400
        description: "Wide-format IRT input from RQ 5.1, reprocessed with single 'All' factor instead of What/Where/When"

    # Parameter values (COMPLETE - from 2_plan.md + MANDATORY IRT config)
    parameters:
      df_long: "irt_data"
      groups:
        All:
          - "all VR items from step00_irt_input.csv"
          - "includes What items (VR-*-*-N-ANS)"
          - "includes Where items (VR-*-*-U-ANS, VR-*-*-D-ANS)"
          - "includes When items (VR-*-*-T-ANS)"
      config:
        factors: ["All"]
        correlated_factors: false
        device: "cpu"
        seed: 42
        n_cats: 2
        model_fit:
          batch_size: 2048
          iw_samples: 100
          mc_samples: 1
        model_scores:
          scoring_batch_size: 2048
          mc_samples: 100
          iw_samples: 100
        max_iter: 200

    # Output specifications (COMPLETE)
    outputs:
      theta_scores:
        path: "data/step01_theta_scores.csv"
        format: "CSV with UTF-8 encoding"
        columns:
          - {name: "composite_ID", type: "str", description: "UID_test format"}
          - {name: "Theta_All", type: "float", description: "IRT ability estimate for omnibus 'All' factor"}
          - {name: "SE_All", type: "float", description: "Standard error of theta estimate"}
        row_count: 400
        description: "Person ability estimates from single-factor omnibus calibration"

      item_parameters:
        path: "logs/step01_item_parameters.csv"
        format: "CSV with UTF-8 encoding"
        columns:
          - {name: "item_name", type: "str", description: "Item identifier (VR-*-*-*-ANS format)"}
          - {name: "dimension", type: "str", description: "Factor name (all items = 'All')"}
          - {name: "a", type: "float", description: "Discrimination parameter (>0.0)"}
          - {name: "b", type: "float", description: "Difficulty parameter (unrestricted)"}
        row_count: "150-200"
        description: "IRT item parameters for all items assigned to 'All' factor"

      calibration_log:
        path: "logs/step01_calibration.log"
        format: "Text log file"
        description: "IRT convergence diagnostics and parameter summaries"

    # Validation tool specification (MANDATORY)
    validation:
      tool:
        module: "tools.validation"
        function_convergence: "validate_irt_convergence"
        function_parameters: "validate_irt_parameters"
        signature_convergence: "validate_irt_convergence(results: Dict[str, Any]) -> Dict[str, Any]"
        signature_parameters: "validate_irt_parameters(df_items: DataFrame, a_min: float, b_max: float, a_col: str, b_col: str) -> Dict[str, Any]"

      inputs:
        theta_scores:
          path: "data/step01_theta_scores.csv"
          description: "Output from calibrate_irt (theta scores)"
        item_parameters:
          path: "logs/step01_item_parameters.csv"
          description: "Output from calibrate_irt (item parameters)"

      criteria:
        - name: "Model convergence"
          check: "Convergence status = True, loss stabilized"
          severity: "CRITICAL"
        - name: "Theta range"
          check: "All Theta_All in [-4, 4]"
          severity: "CRITICAL"
        - name: "SE range"
          check: "All SE_All in [0.1, 1.5]"
          severity: "CRITICAL"
        - name: "Discrimination bounds"
          check: "All a > 0.0"
          severity: "CRITICAL"
        - name: "Single factor assignment"
          check: "All items have dimension = 'All'"
          severity: "CRITICAL"
        - name: "No missing values"
          check: "No NaN in theta scores or item parameters"
          severity: "CRITICAL"
        - name: "Complete data"
          check: "All 400 composite_IDs present"
          severity: "CRITICAL"

      on_failure:
        action: "QUIT"
        message: "IRT calibration failed validation - see logs/step01_calibration.log"

  # --------------------------------------------------------------------------
  # STEP 2: Prepare LMM Input Data
  # --------------------------------------------------------------------------
  - name: "step02_prepare_lmm_input"
    step_number: "02"
    description: "Transform IRT output to LMM-ready format with time variable transformations (TSVR hours, Days, Days^2, log(Days+1))"

    # This is stdlib operations (pandas/numpy data transformation)
    tool:
      type: "stdlib"
      operations:
        - "Read data/step01_theta_scores.csv (theta scores from Step 1)"
        - "Read ../../rq1/data/step00a_tsvr_data.csv (TSVR time variable from RQ 5.1)"
        - "Rename Theta_All -> Theta (simplifies LMM formula readability)"
        - "Rename SE_All -> SE"
        - "Merge theta scores with TSVR on composite_ID (left join, all theta scores kept)"
        - "Create Days = TSVR_hours / 24.0 (convert hours to days for interpretability)"
        - "Create Days_squared = Days^2 (quadratic term for polynomial models)"
        - "Create log_Days_plus1 = log(Days + 1) (logarithmic term, +1 prevents log(0))"
        - "Parse composite_ID: UID = composite_ID.split('_')[0], test = composite_ID.split('_')[1]"
        - "Save to data/step02_lmm_input.csv"

    # Input specifications
    inputs:
      theta_scores:
        path: "data/step01_theta_scores.csv"
        format: "CSV with UTF-8 encoding"
        columns:
          - {name: "composite_ID", type: "str"}
          - {name: "Theta_All", type: "float"}
          - {name: "SE_All", type: "float"}
        row_count: 400
        description: "Theta scores from Step 1"

      tsvr_data:
        path: "../../rq1/data/step00a_tsvr_data.csv"
        format: "CSV with UTF-8 encoding"
        columns:
          - {name: "composite_ID", type: "str"}
          - {name: "TSVR_hours", type: "float", description: "Actual hours since VR encoding"}
        row_count: 400
        description: "Time Since VR in hours (Decision D070)"

    # Output specifications
    outputs:
      lmm_input:
        path: "data/step02_lmm_input.csv"
        format: "CSV with UTF-8 encoding"
        columns:
          - {name: "composite_ID", type: "str"}
          - {name: "UID", type: "str", description: "Participant identifier"}
          - {name: "test", type: "str", description: "Test session (T1, T2, T3, T4)"}
          - {name: "Theta", type: "float", description: "Renamed from Theta_All"}
          - {name: "SE", type: "float", description: "Renamed from SE_All"}
          - {name: "TSVR_hours", type: "float", description: "Actual hours since encoding"}
          - {name: "Days", type: "float", description: "TSVR_hours / 24, for interpretability"}
          - {name: "Days_squared", type: "float", description: "Days^2, for quadratic models"}
          - {name: "log_Days_plus1", type: "float", description: "log(Days+1), for logarithmic models"}
        row_count: 400
        description: "Long-format LMM input with time transformations"

    # Validation (inline checks for stdlib operations)
    validation:
      type: "inline"
      criteria:
        - name: "Data preparation complete"
          check: "400 observations created"
          severity: "CRITICAL"
        - name: "TSVR merge successful"
          check: "400/400 composite_IDs matched (no missing TSVR values)"
          severity: "CRITICAL"
        - name: "No missing values"
          check: "No NaN in any column after transformations"
          severity: "CRITICAL"
        - name: "Theta range preserved"
          check: "Theta in [-4, 4] (inherited from Step 1)"
          severity: "CRITICAL"
        - name: "SE range preserved"
          check: "SE in [0.1, 1.5] (inherited from Step 1)"
          severity: "CRITICAL"
        - name: "TSVR range valid"
          check: "TSVR_hours in [0, 168] (0=encoding, 168=7 days max)"
          severity: "CRITICAL"
        - name: "Days range valid"
          check: "Days in [0, 7]"
          severity: "CRITICAL"
        - name: "Days_squared range valid"
          check: "Days_squared in [0, 49]"
          severity: "CRITICAL"
        - name: "log_Days_plus1 range valid"
          check: "log_Days_plus1 in [0, 2.2]"
          severity: "CRITICAL"
        - name: "No duplicates"
          check: "No duplicate composite_IDs"
          severity: "CRITICAL"
        - name: "Balanced observations"
          check: "Each UID appears exactly 4 times (T1-T4)"
          severity: "CRITICAL"

      on_failure:
        action: "QUIT"
        message: "Data preparation failed validation - see error details"

  # --------------------------------------------------------------------------
  # STEP 3: Fit 5 Candidate LMM Models
  # --------------------------------------------------------------------------
  - name: "step03_fit_5_candidate_lmms"
    step_number: "03"
    description: "Fit 5 candidate Linear Mixed Models representing different functional forms (Linear, Quadratic, Logarithmic, Lin+Log, Quad+Log)"

    # Analysis tool specification (called 5 times sequentially)
    tool:
      module: "tools.analysis_lmm"
      function: "fit_lmm_trajectory_tsvr"
      signature: "fit_lmm_trajectory_tsvr(theta_scores: DataFrame, tsvr_data: DataFrame, formula: str, groups: str, re_formula: str, reml: bool) -> MixedLMResults"

    # Input specifications
    inputs:
      lmm_input:
        path: "data/step02_lmm_input.csv"
        format: "CSV with UTF-8 encoding"
        columns:
          - {name: "composite_ID", type: "str"}
          - {name: "UID", type: "str"}
          - {name: "test", type: "str"}
          - {name: "Theta", type: "float"}
          - {name: "SE", type: "float"}
          - {name: "TSVR_hours", type: "float"}
          - {name: "Days", type: "float"}
          - {name: "Days_squared", type: "float"}
          - {name: "log_Days_plus1", type: "float"}
        row_count: 400
        description: "LMM input with time transformations from Step 2"

    # Parameter values (5 models fitted sequentially with REML=False)
    parameters:
      theta_scores: "lmm_input"
      tsvr_data: "lmm_input"
      groups: "UID"
      reml: false
      formulas:
        Linear:
          formula: "Theta ~ Days + (1 + Days | UID)"
          description: "Simple linear trace decay"
        Quadratic:
          formula: "Theta ~ Days + Days_squared + (1 + Days | UID)"
          description: "Two-phase consolidation (rapid then slow decay)"
        Logarithmic:
          formula: "Theta ~ log_Days_plus1 + (1 + log_Days_plus1 | UID)"
          description: "Ebbinghaus curve (logarithmic forgetting)"
        LinLog:
          formula: "Theta ~ Days + log_Days_plus1 + (1 + Days | UID)"
          description: "Flexible approximation combining linear and logarithmic"
        QuadLog:
          formula: "Theta ~ Days + Days_squared + log_Days_plus1 + (1 + Days | UID)"
          description: "Most flexible approximation (polynomial + logarithmic)"

    # Output specifications
    outputs:
      model_fits:
        path: "data/step03_model_fits.pkl"
        format: "Python pickle"
        description: "Dictionary of 5 fitted model objects (keys: Linear, Quadratic, Logarithmic, LinLog, QuadLog)"

      model_comparison:
        path: "results/step03_model_comparison.csv"
        format: "CSV with UTF-8 encoding"
        columns:
          - {name: "model_name", type: "str", description: "Linear, Quadratic, Logarithmic, LinLog, QuadLog"}
          - {name: "AIC", type: "float", description: "Akaike Information Criterion"}
          - {name: "BIC", type: "float", description: "Bayesian Information Criterion"}
          - {name: "log_likelihood", type: "float", description: "Log-likelihood at convergence"}
          - {name: "n_params", type: "int", description: "Number of estimated parameters"}
          - {name: "converged", type: "bool", description: "Convergence status"}
        row_count: 5
        description: "Model fit comparison table"

      lmm_fitting_log:
        path: "logs/step03_lmm_fitting.log"
        format: "Text log file"
        description: "LMM fitting diagnostics for all 5 models"

    # Validation tool specification (MANDATORY)
    validation:
      tool:
        module: "tools.validation"
        function: "validate_lmm_convergence"
        signature: "validate_lmm_convergence(lmm_result: MixedLMResults) -> Dict[str, Any]"

      inputs:
        model_comparison:
          path: "results/step03_model_comparison.csv"
          description: "Model comparison table from Step 3"
        lmm_fitting_log:
          path: "logs/step03_lmm_fitting.log"
          description: "LMM fitting diagnostics"

      criteria:
        - name: "All models converged"
          check: "All 5 models have converged=True"
          severity: "CRITICAL"
        - name: "No singular covariance"
          check: "No singular matrix warnings"
          severity: "CRITICAL"
        - name: "AIC/BIC finite"
          check: "All AIC/BIC values not NaN or Inf"
          severity: "CRITICAL"
        - name: "Log-likelihood reasonable"
          check: "All log_likelihood negative, not -Inf"
          severity: "CRITICAL"
        - name: "Parameter counts correct"
          check: "n_params matches model specification (Linear=4, QuadLog=7)"
          severity: "CRITICAL"

      on_failure:
        action: "QUIT"
        message: "LMM fitting failed validation - see logs/step03_lmm_fitting.log"

  # --------------------------------------------------------------------------
  # STEP 4: Model Selection via AIC
  # --------------------------------------------------------------------------
  - name: "step04_aic_model_selection"
    step_number: "04"
    description: "Select best-fitting model using AIC and compute Akaike weights to quantify relative evidence"

    # Analysis tool specification
    tool:
      module: "tools.analysis_lmm"
      function: "compare_lmm_models_by_aic"
      signature: "compare_lmm_models_by_aic(data: DataFrame, n_factors: int, reference_group: str, groups: str, save_dir: Path) -> Dict"

    # Input specifications
    inputs:
      model_comparison:
        path: "results/step03_model_comparison.csv"
        format: "CSV with UTF-8 encoding"
        columns:
          - {name: "model_name", type: "str"}
          - {name: "AIC", type: "float"}
          - {name: "BIC", type: "float"}
          - {name: "log_likelihood", type: "float"}
        row_count: 5
        description: "Model comparison table from Step 3"

      model_fits:
        path: "data/step03_model_fits.pkl"
        format: "Python pickle"
        description: "Dictionary of fitted model objects"

    # Parameter values
    parameters:
      data: "model_comparison"
      n_factors: 5
      reference_group: "Linear"
      groups: "model_name"
      save_dir: "results/"
      calculations:
        delta_AIC: "AIC_i - AIC_min"
        akaike_weight: "exp(-0.5 * delta_AIC_i) / sum(exp(-0.5 * delta_AIC_j))"
      uncertainty_thresholds:
        very_strong: ">0.90"
        strong: "0.60-0.90"
        moderate: "0.30-0.60"
        high: "<0.30"

    # Output specifications
    outputs:
      aic_comparison:
        path: "results/step04_aic_comparison.csv"
        format: "CSV with UTF-8 encoding, sorted by AIC ascending"
        columns:
          - {name: "model_name", type: "str"}
          - {name: "AIC", type: "float"}
          - {name: "delta_AIC", type: "float", description: "AIC_i - AIC_min (best model = 0)"}
          - {name: "akaike_weight", type: "float", description: "Relative probability (sums to 1.0)"}
          - {name: "cumulative_weight", type: "float", description: "Cumulative sum of weights"}
        row_count: 5
        description: "AIC comparison with Akaike weights, best model first"

      best_model:
        path: "data/step04_best_model.pkl"
        format: "Python pickle"
        description: "Best-fitting model object (lowest AIC)"

      best_model_summary:
        path: "results/step04_best_model_summary.txt"
        format: "Text file"
        description: "Summary of best model (AIC, Akaike weight, uncertainty category, fixed/random effects)"

    # Validation tool specification (MANDATORY)
    validation:
      tool:
        module: "tools.validation"
        function: "validate_akaike_weights"
        signature: "validate_akaike_weights(aic_comparison: DataFrame) -> Dict[str, Any]"

      inputs:
        aic_comparison:
          path: "results/step04_aic_comparison.csv"
          description: "AIC comparison table from Step 4"

      criteria:
        - name: "Akaike weights sum to 1.0"
          check: "Sum of akaike_weight in [0.999, 1.001] (floating-point precision)"
          severity: "CRITICAL"
        - name: "All weights in valid range"
          check: "All akaike_weight in (0, 1) exclusive"
          severity: "CRITICAL"
        - name: "delta_AIC correct"
          check: "Best model delta_AIC = 0, others positive"
          severity: "CRITICAL"
        - name: "Cumulative weight monotonic"
          check: "cumulative_weight monotonic increasing, ends at 1.0"
          severity: "CRITICAL"
        - name: "Best model identified"
          check: "Row 1 has lowest AIC (sorted correctly)"
          severity: "CRITICAL"

      on_failure:
        action: "QUIT"
        message: "AIC model selection failed validation - see logs/step04_model_selection.log"

  # --------------------------------------------------------------------------
  # STEP 5: Prepare Multi-Panel Plot Data
  # --------------------------------------------------------------------------
  - name: "step05_prepare_functional_form_plots"
    step_number: "05"
    description: "Create plot source CSVs for multi-panel visualization (5 model fits + observed data, dual-scale per Decision D069)"

    # Analysis tool specification
    tool:
      module: "tools.plotting"
      function: "convert_theta_to_probability"
      signature: "convert_theta_to_probability(theta: ndarray, discrimination: float, difficulty: float) -> ndarray"

    # Input specifications
    inputs:
      lmm_input:
        path: "data/step02_lmm_input.csv"
        format: "CSV with UTF-8 encoding"
        columns:
          - {name: "composite_ID", type: "str"}
          - {name: "UID", type: "str"}
          - {name: "test", type: "str"}
          - {name: "Theta", type: "float"}
          - {name: "SE", type: "float"}
          - {name: "Days", type: "float"}
        row_count: 400
        description: "Observed theta scores with time variable"

      model_fits:
        path: "data/step03_model_fits.pkl"
        format: "Python pickle"
        description: "Dictionary of 5 fitted model objects"

      aic_comparison:
        path: "results/step04_aic_comparison.csv"
        format: "CSV with UTF-8 encoding"
        columns:
          - {name: "model_name", type: "str"}
          - {name: "AIC", type: "float"}
          - {name: "delta_AIC", type: "float"}
          - {name: "akaike_weight", type: "float"}
        row_count: 5
        description: "AIC comparison for annotating best model"

    # Parameter values
    parameters:
      theta: "lmm_input.Theta"
      discrimination: 1.7
      difficulty: 0.0
      formula: "p = 1 / (1 + exp(-discrimination * theta))"
      operations:
        - "Group lmm_input by test session (T1, T2, T3, T4)"
        - "Compute mean Theta per test"
        - "Compute 95% CI using SE: CI_lower = mean - 1.96*SE_pooled, CI_upper = mean + 1.96*SE_pooled"
        - "For each of 5 models: create prediction grid (Days in [0, 7], 50 points)"
        - "Generate model predictions using fitted coefficients (population-level, random effects = 0)"
        - "Transform theta to probability: p = 1 / (1 + exp(-1.7 * theta))"
        - "Apply probability transform to observed means and all model predictions"
        - "Save theta-scale plot data to plots/step05_functional_form_theta_data.csv"
        - "Save probability-scale plot data to plots/step05_functional_form_probability_data.csv"

    # Output specifications
    outputs:
      theta_plot_data:
        path: "plots/step05_functional_form_theta_data.csv"
        format: "CSV with UTF-8 encoding"
        columns:
          - {name: "Days", type: "float", description: "Time variable for X-axis"}
          - {name: "observed_theta", type: "float", description: "Mean observed theta per test"}
          - {name: "CI_lower_theta", type: "float", description: "Lower 95% CI"}
          - {name: "CI_upper_theta", type: "float", description: "Upper 95% CI"}
          - {name: "pred_Linear", type: "float", description: "Linear model predictions"}
          - {name: "pred_Quadratic", type: "float", description: "Quadratic model predictions"}
          - {name: "pred_Logarithmic", type: "float", description: "Logarithmic model predictions"}
          - {name: "pred_LinLog", type: "float", description: "Lin+Log model predictions"}
          - {name: "pred_QuadLog", type: "float", description: "Quad+Log model predictions"}
          - {name: "best_model", type: "str", description: "Name of best model (for annotation)"}
        row_count: 54
        description: "Theta-scale plot data (4 observed + 50 predicted)"

      probability_plot_data:
        path: "plots/step05_functional_form_probability_data.csv"
        format: "CSV with UTF-8 encoding"
        columns:
          - {name: "Days", type: "float"}
          - {name: "observed_prob", type: "float", description: "Observed theta transformed to probability"}
          - {name: "CI_lower_prob", type: "float", description: "Lower 95% CI transformed"}
          - {name: "CI_upper_prob", type: "float", description: "Upper 95% CI transformed"}
          - {name: "pred_Linear_prob", type: "float", description: "Linear predictions transformed"}
          - {name: "pred_Quadratic_prob", type: "float"}
          - {name: "pred_Logarithmic_prob", type: "float"}
          - {name: "pred_LinLog_prob", type: "float"}
          - {name: "pred_QuadLog_prob", type: "float"}
          - {name: "best_model", type: "str"}
        row_count: 54
        description: "Probability-scale plot data (Decision D069 dual-scale requirement)"

    # Validation tool specification (MANDATORY)
    validation:
      tool:
        module: "tools.validation"
        function: "validate_probability_transform"
        signature: "validate_probability_transform(prob_data: DataFrame) -> Dict[str, Any]"

      inputs:
        probability_plot_data:
          path: "plots/step05_functional_form_probability_data.csv"
          description: "Probability-scale plot data from Step 5"

      criteria:
        - name: "Plot data complete"
          check: "54 rows created (4 observed + 50 predicted)"
          severity: "CRITICAL"
        - name: "Probability bounds valid"
          check: "All probability values in [0, 1]"
          severity: "CRITICAL"
        - name: "CI bounds valid"
          check: "CI_upper_prob > CI_lower_prob for all observed data"
          severity: "CRITICAL"
        - name: "No missing Days"
          check: "No NaN in Days column (time grid complete)"
          severity: "CRITICAL"
        - name: "Monotonicity preserved"
          check: "If theta1 > theta2, then prob1 > prob2 (transformation preserves order)"
          severity: "MODERATE"
        - name: "Best model annotated"
          check: "best_model column populated with model name from AIC comparison"
          severity: "CRITICAL"

      on_failure:
        action: "QUIT"
        message: "Plot data preparation failed validation - see logs/step05_prepare_plot_data.log"

# ============================================================================
# END OF ANALYSIS RECIPE
# ============================================================================
