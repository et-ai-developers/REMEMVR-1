#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step02
Step Name: Item Purification
RQ: ch5/5.1.1
Generated: 2025-11-25

PURPOSE:
Apply Decision D039 quality thresholds (|b| <= 3.0, a >= 0.4) to item parameters
from Pass 1 calibration, identifying high-quality items for Pass 2 re-calibration.
This purification improves measurement precision by excluding poorly discriminating
or extremely difficult items.

EXPECTED INPUTS:
  - logs/step01_item_parameters.csv
    Columns: item_name (str), dimension (str), a (float), b (float)
    Format: CSV with UTF-8 encoding
    Expected rows: 105 (all items from Pass 1)
    Description: Item parameters from Pass 1 single-factor calibration

EXPECTED OUTPUTS:
  - data/step02_purified_items.csv
    Columns: item_name (str), pass1_a (float), pass1_b (float), dimension (str)
    Format: CSV with UTF-8 encoding
    Expected rows: 40-60 (40-60% retention typical)
    Description: High-quality items passing thresholds (a>=0.4, |b|<=3.0)

  - logs/step02_purification_report.txt
    Format: Text report
    Description: Detailed report of excluded items with reasons (low a or extreme b)

VALIDATION CRITERIA:
  - All retained items: a >= 0.4
  - All retained items: |b| <= 3.0
  - Retention rate: 30-70% (expected 40-60%)
  - No NaN values in purified dataset

g_code REASONING:
- Approach: Filter item parameters based on Decision D039 thresholds. Items with
  poor discrimination (a<0.4) or extreme difficulty (|b|>3.0) are excluded.
- Why this approach: Poor items reduce measurement precision and model fit.
  Purification followed by re-calibration (Pass 2) yields more reliable theta
  estimates for downstream LMM analysis.
- Data flow: Pass 1 item parameters -> threshold filtering -> purified item list
  + exclusion report -> saved for Pass 2 input
- Expected performance: <1 second (simple filtering operation)

IMPLEMENTATION NOTES:
- Analysis tool: tools.analysis_irt.filter_items_by_quality
- Validation tool: tools.validation.validate_irt_parameters
- Parameters: a_threshold=0.4, b_threshold=3.0 (Decision D039)
- Thresholds: Applied to 'a' and 'b' columns from Pass 1 parameters
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (5 levels deep from project root)
# parents[4] = REMEMVR/ (code -> rq7 -> ch5 -> results -> REMEMVR)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_irt import filter_items_by_quality

# Import validation tool
from tools.validation import validate_irt_parameters

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.1.1
LOG_FILE = RQ_DIR / "logs" / "step02_purification_report.txt"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step05_lmm_model_comparison.csv
#   CORRECT: data/step03_theta_scores.csv
#   WRONG:   results/lmm_model_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv             (missing step prefix)
#   WRONG:   logs/step02_removed_items.csv     (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 2: Item Purification")

        # =========================================================================
        # STEP 1: Load Pass 1 Item Parameters
        # =========================================================================
        # Expected: Item parameters from Step 1 single-factor calibration
        # Purpose: Identify high-quality items for Pass 2 re-calibration

        log("[LOAD] Loading Pass 1 item parameters...")
        input_path = RQ_DIR / "logs" / "step01_item_parameters.csv"

        if not input_path.exists():
            raise FileNotFoundError(f"Step 1 output missing: {input_path}\n"
                                     "Run step01_irt_calibration_omnibus.py first")

        pass1_params = pd.read_csv(input_path, encoding='utf-8')
        log(f"[LOADED] {input_path.name} ({len(pass1_params)} items)")
        log(f"  Columns: {pass1_params.columns.tolist()}")
        log(f"  Discrimination range: [{pass1_params['a'].min():.3f}, {pass1_params['a'].max():.3f}]")
        log(f"  Difficulty range: [{pass1_params['b'].min():.3f}, {pass1_params['b'].max():.3f}]")

        # =========================================================================
        # STEP 2: Apply Quality Thresholds (Decision D039)
        # =========================================================================
        # Tool: filter_items_by_quality
        # What it does: Filters items based on a>=0.4 and |b|<=3.0 thresholds
        # Expected output: Purified item list + exclusion report

        log("[ANALYSIS] Applying Decision D039 thresholds (a>=0.4, |b|<=3.0)...")
        purified_items, removed_items = filter_items_by_quality(
            df_items=pass1_params,
            a_threshold=0.4,  # Minimum discrimination (items with a<0.4 excluded)
            b_threshold=3.0   # Maximum |difficulty| (items with |b|>3.0 excluded)
        )

        retention_rate = len(purified_items) / len(pass1_params) * 100
        log(f"[DONE] Purification complete")
        log(f"  Retained: {len(purified_items)} items ({retention_rate:.1f}%)")
        log(f"  Removed: {len(removed_items)} items ({100-retention_rate:.1f}%)")

        # =========================================================================
        # STEP 3: Save Purification Outputs
        # =========================================================================
        # These outputs will be used by: Step 3 (Pass 2 IRT calibration)

        # Save purified item list
        output_path = RQ_DIR / "data" / "step02_purified_items.csv"
        log(f"[SAVE] Saving purified items to {output_path.name}...")
        purified_items.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(purified_items)} items, {len(purified_items.columns)} cols)")

        # Save exclusion report
        log(f"[SAVE] Saving exclusion report to {LOG_FILE.name}...")
        log("")  # Blank line separator
        log("=" * 80)
        log("EXCLUSION REPORT - Items Removed from Analysis")
        log("=" * 80)
        log(f"\nTotal items removed: {len(removed_items)}")

        if len(removed_items) > 0:
            log("\nRemoved items by reason:")
            log(removed_items.to_string(index=False))
        else:
            log("\nNo items removed (all items passed quality thresholds)")

        log("\n" + "=" * 80)
        log(f"[SAVED] Exclusion report appended to {LOG_FILE.name}")

        # =========================================================================
        # STEP 4: Validate Purified Items
        # =========================================================================
        # Tool: validate_irt_parameters
        # Validates: All retained items meet thresholds, retention rate reasonable
        # Threshold: a>=0.4, |b|<=3.0, retention 30-70%

        log("[VALIDATION] Validating purified item parameters...")
        validation_result = validate_irt_parameters(
            df_items=purified_items,
            a_min=0.4,        # Minimum discrimination threshold
            b_max=3.0,        # Maximum |difficulty| threshold
            a_col='pass1_a',  # Column name for discrimination in purified dataset
            b_col='pass1_b'   # Column name for difficulty in purified dataset
        )

        # Report validation results
        if isinstance(validation_result, dict):
            for key, value in validation_result.items():
                log(f"[VALIDATION] {key}: {value}")
        else:
            log(f"[VALIDATION] {validation_result}")

        # Check retention rate (expected 30-70%)
        if retention_rate < 30:
            log(f"[WARNING] Low retention rate ({retention_rate:.1f}%) - fewer items than expected")
        elif retention_rate > 70:
            log(f"[WARNING] High retention rate ({retention_rate:.1f}%) - thresholds may be too lenient")
        else:
            log(f"[PASS] Retention rate within expected range ({retention_rate:.1f}%)")

        log("[SUCCESS] Step 2 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
