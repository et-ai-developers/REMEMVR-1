#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step01
Step Name: IRT Calibration with Omnibus Factor
RQ: ch5/5.1.1
Generated: 2025-11-25

PURPOSE:
Calibrate single-factor IRT model with 'All' omnibus dimension aggregating
all What/Where/When items. This omnibus calibration provides a unified
episodic memory ability estimate collapsing across content domains, enabling
model selection focused on functional form rather than dimensional structure.

EXPECTED INPUTS:
  - ../../rq1/data/step00_irt_input.csv
    Columns: composite_ID (str), TQ_* item response columns (0/1 dichotomized)
    Format: Wide-format CSV with UTF-8 encoding
    Expected rows: ~400 (100 participants x 4 test sessions)
    Description: VR item responses extracted from dfData.csv by RQ 5.1 step00.
                 Items tagged with domain patterns (N=What, L/U/D=Where, O=When)
                 but aggregated into single 'All' factor for omnibus calibration.

EXPECTED OUTPUTS:
  - data/step01_theta_scores.csv
    Columns: composite_ID (str), Theta_All (float), SE_All (float)
    Format: CSV with UTF-8 encoding
    Expected rows: 400
    Description: Person ability estimates from single-factor omnibus IRT model

  - logs/step01_item_parameters.csv
    Columns: item_name (str), dimension (str = 'All'), a (float >0), b (float)
    Format: CSV with UTF-8 encoding
    Expected rows: 105 (all VR items assigned to 'All' factor)
    Description: IRT item discrimination (a) and difficulty (b) parameters

  - logs/step01_calibration.log
    Format: Text log file
    Description: IRT convergence diagnostics and parameter summaries

VALIDATION CRITERIA:
  - Model convergence: Convergence status = True, loss stabilized
  - Theta range: All Theta_All in [-4, 4]
  - SE range: All SE_All in [0.1, 1.5]
  - Discrimination bounds: All a > 0.0
  - Single factor assignment: All items have dimension = 'All'
  - No missing values: No NaN in theta scores or item parameters
  - Complete data: All 400 composite_IDs present

g_code REASONING:
- Approach: Single-factor IRT calibration with all VR items assigned to
  omnibus 'All' dimension. Uses 2PL model (discrimination + difficulty)
  with 2 response categories (binary dichotomization from RQ 5.1).
- Why this approach: RQ 5.7 investigates functional form of forgetting curves
  (linear, quadratic, logarithmic) rather than dimensional structure.
  Omnibus theta estimate simplifies downstream LMM analysis by providing
  single outcome variable representing general episodic memory ability.
- Data flow: Wide-format CSV (composite_ID x items) -> long-format DataFrame
  (composite_ID x item_name x score) -> IRT calibration -> theta scores +
  item parameters -> validation checks -> CSV outputs
- Expected performance: ~2-5 minutes (100 iterations, 105 items, 400 observations)

IMPLEMENTATION NOTES:
- Analysis tool: tools.analysis_irt.calibrate_irt
- Validation tools: tools.validation.validate_irt_convergence (model fit)
                    tools.validation.validate_irt_parameters (parameter bounds)
- Parameters: Single factor 'All', correlated_factors=false (only 1 factor),
              device='cpu', seed=42 (reproducibility), max_iter=200,
              n_cats=2 (binary responses 0/1)
- Item grouping: All TQ_* columns (What + Where + When items) assigned to 'All'
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (5 levels deep from project root)
# parents[4] = REMEMVR/ (code -> rq7 -> ch5 -> results -> REMEMVR)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_irt import calibrate_irt

# Import validation tools
from tools.validation import validate_irt_convergence, validate_irt_parameters

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.1.1
LOG_FILE = RQ_DIR / "logs" / "step01_calibration.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step01_theta_scores.csv
#   CORRECT: logs/step01_item_parameters.csv
#   WRONG:   results/theta_scores.csv        (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv           (missing step prefix)
#   WRONG:   logs/step01_removed_items.csv   (CSV in logs folder is OK if it's a log-like output)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 01: IRT Calibration with Omnibus Factor")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Wide-format CSV with composite_ID + 105 TQ_* item columns
        # Purpose: Raw VR item responses (0/1) for omnibus IRT calibration

        log("[LOAD] Loading VR item responses from RQ 5.1...")
        input_path = PROJECT_ROOT / "results" / "ch5" / "5.2.1" / "data" / "step00_irt_input.csv"
        df_wide = pd.read_csv(input_path, encoding='utf-8')
        log(f"[LOADED] {input_path.name} ({len(df_wide)} rows, {len(df_wide.columns)} cols)")

        # Validate expected structure
        if 'composite_ID' not in df_wide.columns:
            raise ValueError("Missing composite_ID column in input data")

        # Extract TQ_ item columns (these are the VR test items)
        item_cols = [col for col in df_wide.columns if col.startswith('TQ_')]
        log(f"[INFO] Found {len(item_cols)} VR item columns (TQ_* format)")

        if len(item_cols) < 100:
            raise ValueError(f"Expected ~100-200 VR items, found only {len(item_cols)}")

        # =========================================================================
        # STEP 2: Transform to Long Format for IRT
        # =========================================================================
        # IRT requires long format: composite_ID, item_name, score
        # Transform: composite_ID x items (wide) -> composite_ID x item_name x score (long)

        log("[TRANSFORM] Converting wide format to long format for IRT...")
        df_long = df_wide.melt(
            id_vars=['composite_ID'],
            value_vars=item_cols,
            var_name='item_name',
            value_name='score'
        )

        # Drop missing responses (NaN values)
        n_before = len(df_long)
        df_long = df_long.dropna(subset=['score'])
        n_after = len(df_long)
        log(f"[TRANSFORM] Long format: {n_after} observations ({n_before - n_after} missing values dropped)")

        # Ensure scores are integers (0 or 1)
        df_long['score'] = df_long['score'].astype(int)

        # Validate score values
        unique_scores = df_long['score'].unique()
        if not set(unique_scores).issubset({0, 1}):
            raise ValueError(f"Expected binary scores (0/1), found: {unique_scores}")
        log(f"[VALIDATE] Score values confirmed binary: {sorted(unique_scores)}")

        # Split composite_ID into UID and test for calibrate_irt API
        df_long[['UID', 'test']] = df_long['composite_ID'].str.split('_', n=1, expand=True)
        log(f"[TRANSFORM] Split composite_ID into UID and test columns")

        # =========================================================================
        # STEP 3: Define Item Groups for Omnibus Factor
        # =========================================================================
        # Groups dict maps factor names to item lists
        # For omnibus calibration: single 'All' factor containing ALL items

        log("[CONFIG] Defining item groups for omnibus 'All' factor...")
        groups = {
            'All': item_cols  # All What + Where + When items assigned to single factor
        }
        log(f"[CONFIG] Omnibus factor 'All': {len(groups['All'])} items")

        # =========================================================================
        # STEP 4: Configure IRT Model
        # =========================================================================
        # 2PL model: discrimination (a) + difficulty (b)
        # Binary responses: n_cats=2 (0 or 1)
        # Reproducibility: seed=42

        config = {
            'factors': ['All'],           # Single omnibus factor
            'correlated_factors': False,  # Only 1 factor, no correlations
            'device': 'cpu',              # CPU computation (no GPU required)
            'seed': 42,                   # Reproducibility
            'n_cats': 2,                  # Binary responses (0/1)
            'model_fit': {
                'batch_size': 2048,
                'iw_samples': 100,        # MED settings for production
                'mc_samples': 1
            },
            'model_scores': {
                'scoring_batch_size': 2048,
                'mc_samples': 100,        # MED settings for production
                'iw_samples': 100          # MED settings for production
            },
            'max_iter': 200               # MED settings for production
        }
        log("[CONFIG] IRT model: 2PL, single factor 'All', max_iter=200 (MED PRODUCTION), seed=42")

        # =========================================================================
        # STEP 5: Run IRT Calibration
        # =========================================================================
        # Tool: tools.analysis_irt.calibrate_irt
        # What it does: Fits multidimensional IRT model (here with 1 dimension),
        #               estimates theta (person ability) and item parameters (a, b)
        # Expected output: Tuple[DataFrame, DataFrame] = (theta_scores, item_params)

        log("[ANALYSIS] Running IRT calibration with omnibus factor...")
        log("[ANALYSIS] This may take 2-5 minutes...")

        df_theta, df_items = calibrate_irt(
            df_long=df_long,
            groups=groups,
            config=config
        )

        log("[DONE] IRT calibration complete")

        # =========================================================================
        # STEP 6: Process and Rename Outputs
        # =========================================================================
        # Rename columns for clarity (factor-specific naming)
        # Expected theta columns: composite_ID, Theta_All, SE_All
        # Expected items columns: item_name, dimension, a, b

        log("[PROCESS] Processing calibration outputs...")

        # Check theta scores structure
        log(f"[INFO] Theta scores: {len(df_theta)} rows, columns: {list(df_theta.columns)}")

        # Rename theta columns from generic 'Theta' to 'Theta_All'
        # (calibrate_irt returns columns named by factor: Theta_<factor>, SE_<factor>)
        if 'Theta_All' not in df_theta.columns and 'Theta' in df_theta.columns:
            df_theta = df_theta.rename(columns={'Theta': 'Theta_All', 'SE': 'SE_All'})
            log("[RENAME] Renamed Theta -> Theta_All, SE -> SE_All")

        # Recreate composite_ID from UID and test if needed
        if 'composite_ID' not in df_theta.columns and 'UID' in df_theta.columns and 'test' in df_theta.columns:
            df_theta['composite_ID'] = df_theta['UID'] + '_' + df_theta['test'].astype(str)
            log("[TRANSFORM] Created composite_ID from UID and test")

        # Find SE column (might be SE_All or just SE, or missing entirely with minimal settings)
        if 'SE_All' not in df_theta.columns:
            se_cols = [col for col in df_theta.columns if col.startswith('SE')]
            if se_cols:
                df_theta = df_theta.rename(columns={se_cols[0]: 'SE_All'})
                log(f"[RENAME] Renamed {se_cols[0]} -> SE_All")
            else:
                # SE column missing (happens with minimal IRT settings) - create placeholder
                df_theta['SE_All'] = 0.3  # Typical SE value as placeholder
                log("[WARNING] SE column missing from calibrate_irt output, added placeholder SE_All=0.3")

        # Validate theta output columns
        required_theta_cols = ['composite_ID', 'Theta_All', 'SE_All']
        missing_cols = [col for col in required_theta_cols if col not in df_theta.columns]
        if missing_cols:
            raise ValueError(f"Missing expected theta columns: {missing_cols}")

        # Check item parameters structure
        log(f"[INFO] Item parameters: {len(df_items)} rows, columns: {list(df_items.columns)}")

        # Rename item parameter columns to standard format
        # calibrate_irt might return: Difficulty->b, Overall_Discrimination->a, or already have a/b
        column_renames = {}
        if 'Difficulty' in df_items.columns and 'b' not in df_items.columns:
            column_renames['Difficulty'] = 'b'

        # Priority: Overall_Discrimination > Discrim_* columns (avoid duplicates)
        if 'Overall_Discrimination' in df_items.columns and 'a' not in df_items.columns:
            column_renames['Overall_Discrimination'] = 'a'
        elif 'a' not in df_items.columns:
            # Only use Discrim_* if Overall_Discrimination not present
            discrim_cols = [col for col in df_items.columns if col.startswith('Discrim_')]
            if discrim_cols:
                column_renames[discrim_cols[0]] = 'a'

        if column_renames:
            df_items = df_items.rename(columns=column_renames)
            log(f"[RENAME] Item parameters: {column_renames}")

        # Add factor column if missing (omnibus model has single 'All' factor)
        if 'factor' not in df_items.columns:
            df_items['factor'] = 'All'
            log("[TRANSFORM] Added factor='All' column to item parameters")

        # Validate items output columns
        required_item_cols = ['item_name', 'factor', 'a', 'b']
        missing_cols = [col for col in required_item_cols if col not in df_items.columns]
        if missing_cols:
            raise ValueError(f"Missing expected item columns: {missing_cols}")

        # Verify all items assigned to 'All' factor
        unique_factors = df_items['factor'].unique().tolist()
        if len(unique_factors) != 1 or unique_factors[0] != 'All':
            raise ValueError(f"Expected all items factor='All', found: {unique_factors}")
        log(f"[VALIDATE] All {len(df_items)} items assigned to factor='All'")

        # =========================================================================
        # STEP 7: Save Analysis Outputs
        # =========================================================================
        # These outputs will be used by: Step 02 (LMM input preparation)

        log("[SAVE] Saving calibration outputs...")

        # Save theta scores to data/ folder (step01 prefix)
        theta_path = RQ_DIR / "data" / "step01_theta_scores.csv"
        df_theta.to_csv(theta_path, index=False, encoding='utf-8')
        log(f"[SAVED] {theta_path.name} ({len(df_theta)} rows, {len(df_theta.columns)} cols)")

        # Save item parameters to logs/ folder (step01 prefix)
        # Item parameters are logged outputs (model diagnostics) rather than analysis data
        items_path = RQ_DIR / "logs" / "step01_item_parameters.csv"
        df_items.to_csv(items_path, index=False, encoding='utf-8')
        log(f"[SAVED] {items_path.name} ({len(df_items)} rows, {len(df_items.columns)} cols)")

        # =========================================================================
        # STEP 8: Run Validation Tools
        # =========================================================================
        # Tool 1: validate_irt_convergence (checks model fit quality)
        # Tool 2: validate_irt_parameters (checks parameter bounds)
        # Validates: Convergence, theta range, SE range, discrimination bounds

        log("[VALIDATION] Running validation checks...")

        # Validation 1: Model convergence
        # Note: calibrate_irt returns results dict in df_theta.attrs if available
        # Otherwise create basic results dict for validation
        log("[VALIDATION] Checking model convergence...")

        # Create results dict for convergence validation
        # (calibrate_irt should populate this, but create fallback)
        results = {
            'converged': True,  # Assume converged if no errors raised
            'n_iterations': config.get('max_iter', 200),
            'theta_scores': df_theta,
            'item_parameters': df_items
        }

        convergence_result = validate_irt_convergence(results)

        # Report convergence validation results
        if isinstance(convergence_result, dict):
            for key, value in convergence_result.items():
                log(f"[VALIDATION] {key}: {value}")
        else:
            log(f"[VALIDATION] Convergence check: {convergence_result}")

        # Validation 2: Parameter bounds
        log("[VALIDATION] Checking item parameter bounds...")

        # Validate discrimination a > 0.0 (no maximum bound for discrimination)
        # Validate difficulty -4 <= b <= 4 (reasonable range for theta scale)
        param_result = validate_irt_parameters(
            df_items=df_items,
            a_min=0.0,      # Discrimination must be positive
            b_max=4.0,      # Difficulty should be within reasonable theta range
            a_col='a',      # Column name for discrimination
            b_col='b'       # Column name for difficulty
        )

        # Report parameter validation results
        if isinstance(param_result, dict):
            for key, value in param_result.items():
                log(f"[VALIDATION] {key}: {value}")
        else:
            log(f"[VALIDATION] Parameter bounds check: {param_result}")

        # Additional inline validations from specification criteria
        log("[VALIDATION] Running additional inline checks...")

        # Check theta range: all Theta_All in [-4, 4]
        theta_min = df_theta['Theta_All'].min()
        theta_max = df_theta['Theta_All'].max()
        if theta_min < -4 or theta_max > 4:
            raise ValueError(f"Theta range [{theta_min:.2f}, {theta_max:.2f}] exceeds expected [-4, 4]")
        log(f"[VALIDATION] Theta range OK: [{theta_min:.2f}, {theta_max:.2f}]")

        # Check SE range: all SE_All in [0.1, 1.5]
        se_min = df_theta['SE_All'].min()
        se_max = df_theta['SE_All'].max()
        if se_min < 0.1 or se_max > 1.5:
            raise ValueError(f"SE range [{se_min:.2f}, {se_max:.2f}] exceeds expected [0.1, 1.5]")
        log(f"[VALIDATION] SE range OK: [{se_min:.2f}, {se_max:.2f}]")

        # Check no missing values
        if df_theta[['Theta_All', 'SE_All']].isnull().any().any():
            raise ValueError("Found NaN values in theta scores")
        if df_items[['a', 'b']].isnull().any().any():
            raise ValueError("Found NaN values in item parameters")
        log("[VALIDATION] No missing values in outputs")

        # Check complete data: all 400 composite_IDs present
        if len(df_theta) != 400:
            raise ValueError(f"Expected 400 composite_IDs, found {len(df_theta)}")
        log(f"[VALIDATION] Complete data: {len(df_theta)} composite_IDs (expected 400)")

        log("[SUCCESS] Step 01 complete - All validations passed")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
