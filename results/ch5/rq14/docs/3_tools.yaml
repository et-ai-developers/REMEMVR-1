# 3_tools.yaml - Tool Catalog (Analysis + Validation Tools)
# Created by: rq_tools agent (Step 11)
# Consumed by: rq_analysis agent (Step 12)
# Architecture: Tool Catalog (Option A) - Each tool listed once, deduplication
# RQ: 5.14 - Latent Forgetting Profiles (K-means Clustering)

analysis_tools:
  load_random_effects:
    module: "pandas"
    function: "read_csv"
    signature: "read_csv(filepath_or_buffer: str, sep: str = ',', header: int = 0) -> pd.DataFrame"
    validation_tool: "validate_dataframe_structure"

    input_files:
      - path: "../rq13/data/random_effects_total.csv"
        required_columns: ["UID", "Total_Intercept", "Total_Slope"]
        expected_rows: "100"
        data_types:
          UID: "string (format: P###)"
          Total_Intercept: "float64"
          Total_Slope: "float64"

    output_files:
      - path: "data/step00_random_effects.csv"
        columns: ["UID", "Total_Intercept", "Total_Slope"]
        description: "Random effects from RQ 5.13 copied locally for clustering input"

    parameters:
      filepath_or_buffer: "../rq13/data/random_effects_total.csv"
      sep: ","
      header: 0

    description: "Load random effects (intercepts and slopes) from RQ 5.13 Total domain LMM for clustering analysis"
    source_reference: "Standard pandas operation (stdlib, not in tools_inventory.md)"

  standardize_clustering_variables:
    module: "sklearn.preprocessing"
    function: "StandardScaler"
    signature: "StandardScaler().fit_transform(X: np.ndarray) -> np.ndarray"
    validation_tool: "validate_standardization"

    input_files:
      - path: "data/step00_random_effects.csv"
        required_columns: ["Total_Intercept", "Total_Slope"]
        expected_rows: "100"
        data_types:
          Total_Intercept: "float64"
          Total_Slope: "float64"

    output_files:
      - path: "data/step01_clustering_input.csv"
        columns: ["UID", "z_intercept", "z_slope"]
        description: "Standardized clustering variables (z-scores, mean=0, SD=1)"
      - path: "data/step01_standardization_params.csv"
        columns: ["variable", "mean", "sd"]
        description: "Standardization parameters for unstandardizing cluster centers"

    parameters:
      with_mean: true
      with_std: true

    description: "Standardize intercepts and slopes to z-scores for equal weighting in K-means Euclidean distance metric"
    source_reference: "Standard sklearn operation (stdlib, not in tools_inventory.md)"

  determine_optimal_k:
    module: "sklearn.cluster"
    function: "KMeans"
    signature: "KMeans(n_clusters: int, random_state: int, n_init: int) -> KMeans"
    validation_tool: "validate_dataframe_structure"

    input_files:
      - path: "data/step01_clustering_input.csv"
        required_columns: ["z_intercept", "z_slope"]
        expected_rows: "100"
        data_types:
          z_intercept: "float64"
          z_slope: "float64"

    output_files:
      - path: "results/step02_model_selection.csv"
        columns: ["K", "inertia", "BIC", "silhouette", "gap", "gap_se"]
        description: "Model selection metrics for K=1 to K=6 clusters"
      - path: "results/step02_optimal_k.txt"
        description: "Selected optimal K with justification (BIC minimum constrained by silhouette >=0.5)"
      - path: "plots/step02_elbow_plot.png"
        description: "Elbow plot (inertia vs K)"
      - path: "plots/step02_bic_plot.png"
        description: "BIC plot with optimal K marked"
      - path: "plots/step02_silhouette_plot.png"
        description: "Silhouette score vs K with 0.5 threshold"
      - path: "plots/step02_gap_plot.png"
        description: "Gap statistic vs K with error bars"

    parameters:
      k_range: [1, 2, 3, 4, 5, 6]
      random_state: 42
      n_init: 50
      silhouette_threshold: 0.5
      min_cluster_size_pct: 0.10
      n_bootstrap_gap: 100

    description: "Test K=1 to K=6 clusters using BIC model selection constrained by silhouette >=0.5 and gap statistic validation"
    source_reference: "Custom analysis function using sklearn.cluster.KMeans (not in tools_inventory.md, novel for RQ 5.14)"

  fit_kmeans_final:
    module: "sklearn.cluster"
    function: "KMeans"
    signature: "KMeans(n_clusters: int, random_state: int, n_init: int) -> KMeans"
    validation_tool: "validate_cluster_assignment"

    input_files:
      - path: "data/step01_clustering_input.csv"
        required_columns: ["UID", "z_intercept", "z_slope"]
        expected_rows: "100"
      - path: "results/step02_optimal_k.txt"
        description: "Optimal K selected in Step 2"

    output_files:
      - path: "data/step03_cluster_assignments.csv"
        columns: ["UID", "cluster"]
        description: "Cluster assignments (0 to K-1) for each participant"
      - path: "data/step03_cluster_centers_standardized.csv"
        columns: ["cluster", "z_intercept", "z_slope"]
        description: "Cluster centers on standardized scale"
      - path: "results/step03_cluster_sizes.csv"
        columns: ["cluster", "n", "proportion"]
        description: "Cluster sizes (n participants, proportion of sample)"

    parameters:
      random_state: 42
      n_init: 50

    description: "Fit K-means with optimal K, extract cluster assignments and cluster centers"
    source_reference: "Standard sklearn operation (stdlib, not in tools_inventory.md)"

  bootstrap_cluster_stability:
    module: "custom"
    function: "bootstrap_stability_analysis"
    signature: "bootstrap_stability_analysis(data: pd.DataFrame, cluster_assignments: pd.Series, n_clusters: int, n_bootstrap: int, random_state: int) -> pd.DataFrame"
    validation_tool: "validate_bootstrap_stability"

    input_files:
      - path: "data/step01_clustering_input.csv"
        required_columns: ["UID", "z_intercept", "z_slope"]
        expected_rows: "100"
      - path: "data/step03_cluster_assignments.csv"
        required_columns: ["UID", "cluster"]
        expected_rows: "100"
      - path: "results/step02_optimal_k.txt"
        description: "Optimal K for bootstrap iterations"

    output_files:
      - path: "results/step04_bootstrap_stability.csv"
        columns: ["iteration", "jaccard"]
        description: "Jaccard similarity coefficients across 100 bootstrap iterations"
      - path: "results/step04_stability_summary.txt"
        description: "Mean Jaccard, 95% CI, stability rating, acceptance decision"
      - path: "plots/step04_bootstrap_histogram.png"
        description: "Histogram of Jaccard values with mean and 95% CI marked"

    parameters:
      n_bootstrap: 100
      random_state: 42
      stability_threshold: 0.75

    description: "Assess cluster stability via bootstrap resampling with Jaccard similarity, acceptance criterion mean Jaccard >=0.75"
    source_reference: "Custom analysis function (not in tools_inventory.md, novel for RQ 5.14)"

  characterize_clusters:
    module: "pandas"
    function: "groupby"
    signature: "DataFrame.groupby(by: str) -> DataFrameGroupBy"
    validation_tool: "validate_cluster_summary_stats"

    input_files:
      - path: "data/step00_random_effects.csv"
        required_columns: ["UID", "Total_Intercept", "Total_Slope"]
        expected_rows: "100"
      - path: "data/step03_cluster_assignments.csv"
        required_columns: ["UID", "cluster"]
        expected_rows: "100"
      - path: "data/step03_cluster_centers_standardized.csv"
        required_columns: ["cluster", "z_intercept", "z_slope"]
        expected_rows: "K"
      - path: "data/step01_standardization_params.csv"
        required_columns: ["variable", "mean", "sd"]
        expected_rows: "2"

    output_files:
      - path: "data/step05_cluster_centers_original.csv"
        columns: ["cluster", "intercept", "slope"]
        description: "Cluster centers unstandardized to original LMM random effect scale"
      - path: "results/step05_cluster_summary.csv"
        columns: ["cluster", "n", "intercept_mean", "intercept_sd", "intercept_min", "intercept_max", "slope_mean", "slope_sd", "slope_min", "slope_max", "label"]
        description: "Summary statistics per cluster with interpretive labels"
      - path: "results/step05_cluster_labels.txt"
        description: "Human-readable cluster labels with sizes and center positions"

    parameters:
      by: "cluster"

    description: "Unstandardize cluster centers, compute summary statistics per cluster, assign interpretive labels based on intercept and slope positions"
    source_reference: "Standard pandas operation (stdlib, not in tools_inventory.md)"

  prepare_scatter_plot_data:
    module: "pandas"
    function: "merge"
    signature: "pd.merge(left: pd.DataFrame, right: pd.DataFrame, on: str) -> pd.DataFrame"
    validation_tool: "validate_dataframe_structure"

    input_files:
      - path: "data/step00_random_effects.csv"
        required_columns: ["UID", "Total_Intercept", "Total_Slope"]
        expected_rows: "100"
      - path: "data/step03_cluster_assignments.csv"
        required_columns: ["UID", "cluster"]
        expected_rows: "100"
      - path: "data/step05_cluster_centers_original.csv"
        required_columns: ["cluster", "intercept", "slope"]
        expected_rows: "K"
      - path: "results/step05_cluster_summary.csv"
        required_columns: ["cluster", "label"]
        expected_rows: "K"

    output_files:
      - path: "plots/step06_cluster_scatter_data.csv"
        columns: ["Total_Intercept", "Total_Slope", "cluster", "is_center", "label"]
        description: "Combined participants + cluster centers for 2D scatter plot visualization (100 + K rows)"

    parameters:
      on: "UID"
      how: "left"

    description: "Merge participants with cluster assignments and append cluster centers for Option B scatter plot visualization"
    source_reference: "Standard pandas operation (stdlib, not in tools_inventory.md)"

validation_tools:
  validate_dataframe_structure:
    module: "tools.validation"
    function: "validate_dataframe_structure"
    signature: "validate_dataframe_structure(df: pd.DataFrame, expected_rows: Union[int, Tuple[int, int]], expected_columns: List[str], column_types: Optional[Dict[str, type]] = None) -> Dict[str, Any]"

    input_files:
      - path: "data/step00_random_effects.csv"
        required_columns: ["UID", "Total_Intercept", "Total_Slope"]
        source: "analysis tool output (load_random_effects)"

    parameters:
      expected_rows: 100
      expected_columns: ["UID", "Total_Intercept", "Total_Slope"]
      column_types:
        UID: "object"
        Total_Intercept: "float64"
        Total_Slope: "float64"

    criteria:
      - "Exactly 100 rows (one per participant)"
      - "All required columns present (UID, Total_Intercept, Total_Slope)"
      - "Column data types match expected (object for UID, float64 for intercept/slope)"
      - "No missing values in critical columns"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool (True if all checks passed)"
        message: "str (human-readable explanation)"
        checks: "Dict[str, bool] (individual check results)"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step00_get_data.log"
      invoke: "g_debug (master invokes after error)"

    description: "Validate random effects DataFrame has expected structure (100 rows, 3 columns, correct types)"
    source_reference: "tools_inventory.md lines 410-418"

  validate_standardization:
    module: "tools.validation"
    function: "validate_standardization"
    signature: "validate_standardization(df: pd.DataFrame, column_names: List[str], tolerance: float = 0.01) -> Dict[str, Any]"

    input_files:
      - path: "data/step01_clustering_input.csv"
        required_columns: ["z_intercept", "z_slope"]
        source: "analysis tool output (standardize_clustering_variables)"

    parameters:
      column_names: ["z_intercept", "z_slope"]
      tolerance: 0.01

    criteria:
      - "z_intercept mean approximately 0 (within tolerance 0.01)"
      - "z_intercept SD approximately 1 (within tolerance 0.01)"
      - "z_slope mean approximately 0 (within tolerance 0.01)"
      - "z_slope SD approximately 1 (within tolerance 0.01)"
      - "No NaN values introduced during standardization"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        message: "str"
        mean_values: "Dict[str, float]"
        sd_values: "Dict[str, float]"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step01_standardize.log"
      invoke: "g_debug (master invokes)"

    description: "Validate z-score standardization successful (mean=0, SD=1 for both intercepts and slopes)"
    source_reference: "tools_inventory.md lines 380-388"

  validate_cluster_assignment:
    module: "tools.validation"
    function: "validate_cluster_assignment"
    signature: "validate_cluster_assignment(cluster_labels: Union[np.ndarray, pd.Series], n_expected: int, min_cluster_size: int = 5) -> Dict[str, Any]"

    input_files:
      - path: "data/step03_cluster_assignments.csv"
        required_columns: ["cluster"]
        source: "analysis tool output (fit_kmeans_final)"

    parameters:
      n_expected: 100
      min_cluster_size: 10

    criteria:
      - "All 100 participants assigned to clusters (no missing assignments)"
      - "Cluster IDs are consecutive integers starting from 0"
      - "Each cluster has >= 10 participants (10% minimum per RQ 5.14 plan)"
      - "Cluster sizes sum to exactly 100"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        message: "str"
        cluster_sizes: "Dict[int, int]"
        n_clusters: "int"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step03_fit_kmeans.log"
      invoke: "g_debug (master invokes)"

    description: "Validate cluster assignments are complete, consecutive, and meet minimum cluster size constraint"
    source_reference: "tools_inventory.md lines 430-438"

  validate_bootstrap_stability:
    module: "tools.validation"
    function: "validate_bootstrap_stability"
    signature: "validate_bootstrap_stability(jaccard_values: Union[np.ndarray, List[float]], min_jaccard_threshold: float = 0.75) -> Dict[str, Any]"

    input_files:
      - path: "results/step04_bootstrap_stability.csv"
        required_columns: ["jaccard"]
        source: "analysis tool output (bootstrap_cluster_stability)"

    parameters:
      min_jaccard_threshold: 0.75

    criteria:
      - "All Jaccard values in [0, 1] range"
      - "Mean Jaccard >= 0.75 (stability threshold per RQ 5.14 plan)"
      - "95% CI computed successfully from bootstrap distribution"
      - "100 bootstrap iterations completed (no missing values)"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        message: "str"
        mean_jaccard: "float"
        ci_lower: "float"
        ci_upper: "float"
        above_threshold: "bool"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step04_bootstrap_stability.log"
      invoke: "g_debug (master invokes)"

    description: "Validate clustering stability meets acceptance criterion (mean Jaccard >= 0.75)"
    source_reference: "tools_inventory.md lines 440-448"

  validate_cluster_summary_stats:
    module: "tools.validation"
    function: "validate_cluster_summary_stats"
    signature: "validate_cluster_summary_stats(summary_df: pd.DataFrame, min_col: str = 'min', mean_col: str = 'mean', max_col: str = 'max', sd_col: str = 'sd', n_col: str = 'N') -> Dict[str, Any]"

    input_files:
      - path: "results/step05_cluster_summary.csv"
        required_columns: ["cluster", "n", "intercept_mean", "intercept_sd", "intercept_min", "intercept_max", "slope_mean", "slope_sd", "slope_min", "slope_max"]
        source: "analysis tool output (characterize_clusters)"

    parameters:
      min_col: "intercept_min"
      mean_col: "intercept_mean"
      max_col: "intercept_max"
      sd_col: "intercept_sd"
      n_col: "n"

    criteria:
      - "For each cluster: min <= mean <= max (intercept and slope)"
      - "For each cluster: SD >= 0 (intercept and slope)"
      - "For each cluster: N > 0"
      - "Cluster sizes sum to 100 participants"
      - "No NaN values in summary statistics"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        message: "str"
        failed_checks: "List[str]"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step05_characterize_clusters.log"
      invoke: "g_debug (master invokes)"

    description: "Validate cluster summary statistics are mathematically consistent (min <= mean <= max, SD >= 0)"
    source_reference: "tools_inventory.md lines 450-458"

summary:
  analysis_tools_count: 7
  validation_tools_count: 5
  total_unique_tools: 12
  mandatory_decisions_embedded: []
  rq_specific_notes: "First clustering RQ in thesis. Uses K-means with 4-part validation (BIC + silhouette >=0.5 + gap statistic + bootstrap stability Jaccard >=0.75). Novel methodology requires custom analysis functions for bootstrap and model selection (not in tools_inventory.md). All 5 required validation tools now exist in tools_inventory.md (added during Phase 22 tool migration)."
