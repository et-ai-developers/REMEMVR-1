#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step02
Step Name: Item Purification
RQ: ch5/5.3.1
Generated: 2025-11-24

PURPOSE:
Filter items by quality thresholds per Decision D039 (a >= 0.4, |b| <= 3.0).
This is the 2-pass IRT methodology where low-quality items are removed before
final calibration to ensure psychometrically sound ability estimates.

EXPECTED INPUTS:
  - logs/step01_pass1_item_params.csv
    Columns: [item, domain, Discrimination, Difficulty_1]
    Format: CSV with UTF-8 encoding
    Expected rows: ~102 (all paradigm items from Pass 1)

EXPECTED OUTPUTS:
  - data/step02_purified_items.csv
    Columns: [item, domain, Discrimination, Difficulty_1]
    Format: CSV with UTF-8 encoding
    Expected rows: 40-80 (40-80% retention expected)

  - logs/step02_removed_items.csv
    Columns: [item, domain, Discrimination, Difficulty_1, exclusion_reason]
    Format: CSV with UTF-8 encoding
    Expected rows: Variable (items that failed quality thresholds)

VALIDATION CRITERIA:
  - All retained items have Discrimination >= 0.4
  - All retained items have |Difficulty_1| <= 3.0
  - At least 10 items retained per paradigm factor
  - No paradigm completely eliminated

g_code REASONING:
- Approach: Use tools.analysis_irt.filter_items_by_quality to apply D039 thresholds
- Why this approach: 2-pass IRT purification is standard for removing psychometrically
  weak items before final calibration. Items with low discrimination (a < 0.4) don't
  differentiate well between ability levels. Items with extreme difficulty (|b| > 3.0)
  are too easy or too hard, providing little information for most participants.
- Data flow: Pass 1 item parameters -> filter by thresholds -> retained items + excluded items
- Expected performance: ~1-2 seconds (simple DataFrame filtering)

IMPLEMENTATION NOTES:
- Analysis tool: filter_items_by_quality from tools.analysis_irt
- Validation tool: validate_irt_parameters from tools.validation
- Parameters: a_threshold=0.4, b_threshold=3.0 (Decision D039)
- The input data has univariate format (Discrimination, Difficulty_1)
- Need to convert to 'a', 'b', 'factor' columns for filter_items_by_quality univariate path
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (5 levels deep from project root)
# parents[4] = REMEMVR/ (code -> rqY -> chX -> results -> REMEMVR)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.analysis_irt import filter_items_by_quality

# Import validation tool
from tools.validation import validate_irt_parameters

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.3.1
LOG_FILE = RQ_DIR / "logs" / "step02_purify_items.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step05_lmm_model_comparison.csv
#   CORRECT: data/step02_purified_items.csv
#   WRONG:   results/lmm_model_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv             (missing step prefix)
#   WRONG:   logs/step02_removed_items.csv     (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        # Clear log file for fresh run
        LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
        with open(LOG_FILE, 'w', encoding='utf-8') as f:
            f.write("")

        log("[START] Step 02: Item Purification")
        log("=" * 60)

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Pass 1 item parameters from IRT calibration
        # Purpose: Apply quality thresholds to filter out weak items

        log("[LOAD] Loading Pass 1 item parameters...")

        input_path = RQ_DIR / "logs" / "step01_pass1_item_params.csv"
        pass1_params = pd.read_csv(input_path, encoding='utf-8')

        log(f"[LOADED] {input_path.name} ({len(pass1_params)} rows, {len(pass1_params.columns)} cols)")
        log(f"[INFO] Columns: {list(pass1_params.columns)}")

        # Log summary statistics before purification
        log(f"[INFO] Discrimination range: {pass1_params['Discrimination'].min():.3f} - {pass1_params['Discrimination'].max():.3f}")
        log(f"[INFO] Difficulty range: {pass1_params['Difficulty_1'].min():.3f} - {pass1_params['Difficulty_1'].max():.3f}")

        # Items per paradigm before purification
        items_by_paradigm = pass1_params['domain'].value_counts().to_dict()
        log(f"[INFO] Items by paradigm (before): {items_by_paradigm}")

        # =========================================================================
        # STEP 2: Run Analysis Tool
        # =========================================================================
        # Tool: filter_items_by_quality
        # What it does: Applies Decision D039 thresholds to identify quality items
        # Thresholds: a >= 0.4 (discrimination), |b| <= 3.0 (difficulty)
        # Expected output: Tuple of (retained_items, removed_items) DataFrames

        log("[ANALYSIS] Running filter_items_by_quality...")
        log(f"[PARAMS] a_threshold=0.4, b_threshold=3.0 (Decision D039)")

        # The filter_items_by_quality function has two paths:
        # 1. Multivariate: expects 'Difficulty' + 'Discrim_*' columns
        # 2. Univariate: expects 'factor', 'a', 'b' columns
        #
        # Our data has 'domain', 'Discrimination', 'Difficulty_1' columns
        # We need to convert to univariate format for compatibility

        pass1_params_formatted = pass1_params.copy()
        pass1_params_formatted = pass1_params_formatted.rename(columns={
            'item': 'item_name',
            'domain': 'factor',
            'Discrimination': 'a',
            'Difficulty_1': 'b'
        })

        log(f"[INFO] Reformatted columns for univariate path: {list(pass1_params_formatted.columns)}")

        retained_items, removed_items = filter_items_by_quality(
            df_items=pass1_params_formatted,
            a_threshold=0.4,  # Decision D039: Minimum discrimination
            b_threshold=3.0   # Decision D039: Maximum |difficulty|
        )

        log("[DONE] Analysis complete")
        log(f"[RESULT] Retained: {len(retained_items)} items, Removed: {len(removed_items)} items")

        # =========================================================================
        # STEP 3: Save Analysis Outputs
        # =========================================================================
        # These outputs will be used by: Step 03 (IRT Pass 2 calibration)

        # Rename columns back to original format for consistency
        retained_items_output = retained_items.rename(columns={
            'item_name': 'item',
            'factor': 'domain',
            'a': 'Discrimination',
            'b': 'Difficulty_1'
        })

        # Ensure output has expected columns in right order
        output_columns = ['item', 'domain', 'Discrimination', 'Difficulty_1']
        available_cols = [c for c in output_columns if c in retained_items_output.columns]
        retained_items_output = retained_items_output[available_cols].copy()

        # Save retained items
        output_path = RQ_DIR / "data" / "step02_purified_items.csv"
        output_path.parent.mkdir(parents=True, exist_ok=True)

        log(f"[SAVE] Saving {output_path.name}...")
        retained_items_output.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(retained_items_output)} rows, {len(retained_items_output.columns)} cols)")

        # Save removed items (for diagnostic purposes)
        if len(removed_items) > 0:
            removed_items_output = removed_items.rename(columns={
                'item_name': 'item',
                'factor': 'domain',
                'a': 'Discrimination',
                'b': 'Difficulty_1'
            })
            removed_output_path = RQ_DIR / "logs" / "step02_removed_items.csv"
            log(f"[SAVE] Saving {removed_output_path.name}...")
            removed_items_output.to_csv(removed_output_path, index=False, encoding='utf-8')
            log(f"[SAVED] {removed_output_path.name} ({len(removed_items_output)} rows)")
        else:
            log("[INFO] No items removed - all items met quality thresholds")

        # =========================================================================
        # STEP 4: Run Validation Tool
        # =========================================================================
        # Tool: validate_irt_parameters
        # Validates: All retained items meet quality thresholds
        # Thresholds: a >= 0.4, |b| <= 3.0

        log("[VALIDATION] Running validate_irt_parameters...")

        # Prepare validation input - need 'a' and 'b' columns
        validation_df = retained_items_output.copy()
        validation_df['a'] = validation_df['Discrimination']
        validation_df['b'] = validation_df['Difficulty_1']

        validation_result = validate_irt_parameters(
            df_items=validation_df,
            a_min=0.4,           # Decision D039 threshold
            b_max=3.0,           # Decision D039 threshold
            a_col='a',           # Column name for discrimination
            b_col='b'            # Column name for difficulty
        )

        # Report validation results
        log(f"[VALIDATION] Valid: {validation_result['valid']}")
        log(f"[VALIDATION] Total items: {validation_result['total_items']}")
        log(f"[VALIDATION] Flagged items: {validation_result['n_flagged']}")

        if not validation_result['valid']:
            log("[WARNING] Some items still flagged after purification - checking details...")
            for item in validation_result['flagged_items']:
                log(f"  - {item['item_name']}: {item['reasons']}")
            raise ValueError(f"Validation failed: {validation_result['n_flagged']} items still flagged")

        # Additional validation: Check items per paradigm
        items_by_paradigm_after = retained_items_output['domain'].value_counts().to_dict()
        log(f"[VALIDATION] Items by paradigm (after): {items_by_paradigm_after}")

        # Check minimum items per paradigm
        min_items_per_paradigm = 10
        for paradigm, count in items_by_paradigm_after.items():
            if count < min_items_per_paradigm:
                log(f"[WARNING] Paradigm '{paradigm}' has only {count} items (minimum: {min_items_per_paradigm})")

        # Check no paradigm completely eliminated
        expected_paradigms = {'free_recall', 'cued_recall', 'recognition'}
        actual_paradigms = set(items_by_paradigm_after.keys())
        missing_paradigms = expected_paradigms - actual_paradigms
        if missing_paradigms:
            raise ValueError(f"CRITICAL: Paradigms completely eliminated: {missing_paradigms}")

        # Summary statistics
        log("=" * 60)
        log("[SUMMARY] Item Purification Results:")
        log(f"  - Original items: {len(pass1_params)}")
        log(f"  - Retained items: {len(retained_items_output)}")
        log(f"  - Removed items: {len(removed_items)}")
        log(f"  - Retention rate: {len(retained_items_output)/len(pass1_params)*100:.1f}%")
        log("=" * 60)

        log("[SUCCESS] Step 02 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
