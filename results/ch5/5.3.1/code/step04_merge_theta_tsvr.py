#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step04
Step Name: merge_theta_tsvr
RQ: results/ch5/5.3.1
Generated: 2025-11-24

PURPOSE:
Merge theta scores with TSVR time variable, reshape for LMM (Decision D070).
Creates time transformations (squared, log) needed for model comparison in Step 5.

EXPECTED INPUTS:
  - data/step03_theta_scores.csv
    Columns: [composite_ID, domain_name, theta]
    Format: Long-format theta scores from Pass 2 IRT calibration
    Expected rows: 1200 (400 composite_IDs x 3 paradigms)

  - data/step00_tsvr_mapping.csv
    Columns: [composite_ID, UID, test, TSVR_hours]
    Format: TSVR time variable mapping from RQ 5.1
    Expected rows: 400 (one per composite_ID)

EXPECTED OUTPUTS:
  - data/step04_lmm_input.csv
    Columns: [composite_ID, UID, test, TSVR_hours, TSVR_hours_sq, TSVR_hours_log, paradigm, theta]
    Format: Long-format LMM input with time transformations
    Expected rows: 1200 (400 composite_IDs x 3 paradigms)

VALIDATION CRITERIA:
  - [CRITICAL] Row count == 1200
  - [CRITICAL] No missing TSVR values
  - [CRITICAL] Three paradigms present: free_recall, cued_recall, recognition
  - [CRITICAL] 100 unique UIDs
  - [MODERATE] TSVR range: 0-200 hours (warn only - real data has natural variation)

g_code REASONING:
- Approach: Merge theta scores with TSVR on composite_ID, add time transformations
- Why this approach: LMM model comparison (Step 5) tests Linear, Quadratic, Log models
  which require TSVR_hours_sq and TSVR_hours_log terms
- Data flow: step03 (theta) + step00 (TSVR) -> step04 (LMM input)
- Expected performance: ~1 second (simple merge and column operations)

IMPLEMENTATION NOTES:
- Analysis: stdlib pandas operations (no catalogued tool)
- Validation: inline criteria checks with CRITICAL/MODERATE severity
- Decision D070: TSVR as continuous time (actual hours since encoding)
- Real TSVR data can exceed 200 hours (participants tested late) - this is expected
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (5 levels deep from project root)
# parents[4] = REMEMVR/ (code -> rqY -> chX -> results -> REMEMVR)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.3.1
LOG_FILE = RQ_DIR / "logs" / "step04_merge_theta_tsvr.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.log

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        # Clear log file for fresh run
        with open(LOG_FILE, 'w', encoding='utf-8') as f:
            f.write("")

        log("[START] Step 04: Merge Theta with TSVR for LMM Input")
        log(f"[INFO] RQ Directory: {RQ_DIR}")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Load theta scores from Pass 2 calibration and TSVR mapping
        # Purpose: Combine ability estimates with continuous time variable

        log("[LOAD] Loading input data...")

        # Load theta scores from step03
        # Expected: 1200 rows (400 composite_IDs x 3 paradigms: free_recall, cued_recall, recognition)
        theta_path = RQ_DIR / "data" / "step03_theta_scores.csv"
        df_theta = pd.read_csv(theta_path, encoding='utf-8')
        log(f"[LOADED] step03_theta_scores.csv ({len(df_theta)} rows, {len(df_theta.columns)} cols)")
        log(f"[INFO] Theta columns: {list(df_theta.columns)}")
        log(f"[INFO] Unique composite_IDs in theta: {df_theta['composite_ID'].nunique()}")
        log(f"[INFO] Unique paradigms (domain_name): {df_theta['domain_name'].unique().tolist()}")

        # Load TSVR mapping from step00
        # Expected: 400 rows (one per composite_ID)
        tsvr_path = RQ_DIR / "data" / "step00_tsvr_mapping.csv"
        df_tsvr = pd.read_csv(tsvr_path, encoding='utf-8')
        log(f"[LOADED] step00_tsvr_mapping.csv ({len(df_tsvr)} rows, {len(df_tsvr.columns)} cols)")
        log(f"[INFO] TSVR columns: {list(df_tsvr.columns)}")
        log(f"[INFO] TSVR range: {df_tsvr['TSVR_hours'].min():.2f} - {df_tsvr['TSVR_hours'].max():.2f} hours")

        # =========================================================================
        # STEP 2: Merge Theta with TSVR
        # =========================================================================
        # Merge on composite_ID to attach time variable to each theta observation
        # Result: 1200 rows (each theta score gets TSVR_hours from its composite_ID)

        log("[ANALYSIS] Merging theta scores with TSVR mapping...")

        # Merge on composite_ID
        df_merged = pd.merge(
            df_theta,
            df_tsvr,
            on='composite_ID',
            how='left'
        )
        log(f"[MERGED] Result: {len(df_merged)} rows")

        # Check for any failed merges (NaN in TSVR_hours would indicate missing mapping)
        missing_tsvr = df_merged['TSVR_hours'].isna().sum()
        if missing_tsvr > 0:
            log(f"[WARNING] {missing_tsvr} rows have missing TSVR_hours after merge")
        else:
            log("[INFO] All rows have valid TSVR_hours")

        # =========================================================================
        # STEP 3: Rename and Transform Columns
        # =========================================================================
        # - Rename domain_name to paradigm (more intuitive for LMM)
        # - Create time transformations for model comparison:
        #   - TSVR_hours_sq: For quadratic time models
        #   - TSVR_hours_log: For logarithmic time models

        log("[TRANSFORM] Renaming columns and creating time transformations...")

        # Rename domain_name to paradigm
        df_merged = df_merged.rename(columns={'domain_name': 'paradigm'})
        log("[INFO] Renamed 'domain_name' -> 'paradigm'")

        # Create squared time term (for quadratic models)
        df_merged['TSVR_hours_sq'] = df_merged['TSVR_hours'] ** 2
        log("[INFO] Created TSVR_hours_sq = TSVR_hours ** 2")

        # Create log-transformed time term (for logarithmic models)
        # Adding 1 to avoid log(0) for T1 observations (TSVR_hours ~ 0)
        df_merged['TSVR_hours_log'] = np.log(df_merged['TSVR_hours'] + 1)
        log("[INFO] Created TSVR_hours_log = log(TSVR_hours + 1)")

        # =========================================================================
        # STEP 4: Extract UID if not already present
        # =========================================================================
        # UID should be in TSVR mapping, but verify it's in merged result

        if 'UID' not in df_merged.columns:
            # Extract UID from composite_ID (format: UID_test)
            df_merged['UID'] = df_merged['composite_ID'].str.split('_').str[0]
            log("[INFO] Extracted UID from composite_ID")
        else:
            log("[INFO] UID column already present from TSVR mapping")

        log(f"[INFO] Unique UIDs: {df_merged['UID'].nunique()}")

        # =========================================================================
        # STEP 5: Organize Column Order
        # =========================================================================
        # Reorder columns for clear LMM input format

        column_order = [
            'composite_ID',
            'UID',
            'test',
            'TSVR_hours',
            'TSVR_hours_sq',
            'TSVR_hours_log',
            'paradigm',
            'theta'
        ]

        # Verify all columns exist
        missing_cols = [c for c in column_order if c not in df_merged.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")

        df_lmm_input = df_merged[column_order]
        log(f"[INFO] Final columns: {list(df_lmm_input.columns)}")

        # =========================================================================
        # STEP 6: Save LMM Input
        # =========================================================================
        # Save to data/ folder with step prefix

        output_path = RQ_DIR / "data" / "step04_lmm_input.csv"
        df_lmm_input.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path} ({len(df_lmm_input)} rows, {len(df_lmm_input.columns)} cols)")

        # =========================================================================
        # STEP 7: Inline Validation
        # =========================================================================
        # Validate output meets all criteria from 4_analysis.yaml
        # CRITICAL severity = must pass or fail
        # MODERATE severity = warn only (real data has natural variation)

        log("[VALIDATION] Running inline validation checks...")

        critical_failures = []
        moderate_warnings = []

        # Check 1 [CRITICAL]: Row count == 1200
        if len(df_lmm_input) != 1200:
            critical_failures.append(f"Row count: expected 1200, got {len(df_lmm_input)}")
        else:
            log("[PASS] Row count == 1200")

        # Check 2 [CRITICAL]: No missing TSVR
        if not df_lmm_input['TSVR_hours'].notna().all():
            missing = df_lmm_input['TSVR_hours'].isna().sum()
            critical_failures.append(f"Missing TSVR_hours: {missing} rows")
        else:
            log("[PASS] No missing TSVR values")

        # Check 3 [CRITICAL]: Three paradigms present
        expected_paradigms = {'free_recall', 'cued_recall', 'recognition'}
        actual_paradigms = set(df_lmm_input['paradigm'].unique())
        if actual_paradigms != expected_paradigms:
            critical_failures.append(f"Paradigms: expected {expected_paradigms}, got {actual_paradigms}")
        else:
            log(f"[PASS] Three paradigms present: {actual_paradigms}")

        # Check 4 [CRITICAL]: 100 unique UIDs
        unique_uids = df_lmm_input['UID'].nunique()
        if unique_uids != 100:
            critical_failures.append(f"Unique UIDs: expected 100, got {unique_uids}")
        else:
            log(f"[PASS] 100 unique UIDs")

        # Check 5 [MODERATE]: TSVR range valid (0-200 hours)
        # Real TSVR data has natural variation - participants tested late is normal
        # This is a warning, not a failure
        tsvr_min = df_lmm_input['TSVR_hours'].min()
        tsvr_max = df_lmm_input['TSVR_hours'].max()
        if not df_lmm_input['TSVR_hours'].between(0, 200).all():
            # Count how many are outside range
            outside_range = (~df_lmm_input['TSVR_hours'].between(0, 200)).sum()
            moderate_warnings.append(
                f"TSVR range: {outside_range} values outside 0-200 hours "
                f"(min={tsvr_min:.2f}, max={tsvr_max:.2f}). "
                f"This is expected with real participant data."
            )
        else:
            log(f"[PASS] TSVR range valid: {tsvr_min:.2f} - {tsvr_max:.2f} hours")

        # Report validation results
        # MODERATE warnings first (don't fail)
        if moderate_warnings:
            for warning in moderate_warnings:
                log(f"[WARN] {warning}")

        # CRITICAL failures (fail if any)
        if critical_failures:
            for failure in critical_failures:
                log(f"[FAIL] {failure}")
            raise ValueError(f"Step 04 validation failed (CRITICAL): {'; '.join(critical_failures)}")

        log("[VALIDATION] All CRITICAL checks passed")

        # =========================================================================
        # STEP 8: Summary Statistics
        # =========================================================================
        # Log summary for debugging and verification

        log("\n[SUMMARY] LMM Input Data Summary:")
        log(f"  Total rows: {len(df_lmm_input)}")
        log(f"  Unique participants (UID): {df_lmm_input['UID'].nunique()}")
        log(f"  Unique composite_IDs: {df_lmm_input['composite_ID'].nunique()}")
        log(f"  Test sessions: {sorted(df_lmm_input['test'].unique())}")
        log(f"  Paradigms: {df_lmm_input['paradigm'].unique().tolist()}")
        log(f"  TSVR_hours: min={tsvr_min:.2f}, max={tsvr_max:.2f}, mean={df_lmm_input['TSVR_hours'].mean():.2f}")
        log(f"  Theta: min={df_lmm_input['theta'].min():.3f}, max={df_lmm_input['theta'].max():.3f}, mean={df_lmm_input['theta'].mean():.3f}")

        # Paradigm-wise summary
        log("\n[SUMMARY] By Paradigm:")
        for paradigm in ['free_recall', 'cued_recall', 'recognition']:
            subset = df_lmm_input[df_lmm_input['paradigm'] == paradigm]
            log(f"  {paradigm}: n={len(subset)}, theta_mean={subset['theta'].mean():.3f}, theta_std={subset['theta'].std():.3f}")

        log("\n[SUCCESS] Step 04 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
