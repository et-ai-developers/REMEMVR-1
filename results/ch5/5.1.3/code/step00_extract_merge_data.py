#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step00
Step Name: extract_merge_data
RQ: results/ch5/rq9
Generated: 2025-11-28

PURPOSE:
Load theta scores from RQ 5.7 (All composite factor), merge with TSVR time
variable and Age from dfData.csv to create complete LMM input dataset for
age effects analysis.

EXPECTED INPUTS:
  - results/ch5/rq7/data/step03_theta_all.csv
    Columns: ['composite_ID', 'theta_all', 'se_all']
    Format: Theta scores from RQ 5.7 'All' composite factor
    Expected rows: ~400 (100 participants × 4 tests)

  - results/ch5/rq7/data/step00_tsvr_mapping.csv
    Columns: ['UID', 'TEST', 'TSVR']
    Format: Time Since VR (actual hours since encoding)
    Expected rows: ~400 (100 participants × 4 tests)

  - data/cache/dfData.csv
    Columns: ['UID', 'age']
    Format: Participant demographics
    Expected rows: ~100 (one row per participant)

EXPECTED OUTPUTS:
  - data/step00_lmm_input_raw.csv
    Columns: ['composite_ID', 'UID', 'TEST', 'TSVR_hours', 'theta', 'se_all', 'age']
    Format: Merged dataset ready for LMM analysis
    Expected rows: 400 (100 participants × 4 tests)

VALIDATION CRITERIA:
  - All 3 source files exist and are not empty (>1KB each)
  - All 7 required columns present in merged output
  - Zero NaN values tolerated (complete cases only)

g_code REASONING:
- Approach: Load theta scores from RQ 5.7, merge with TSVR time variable
  (actual hours since encoding per Decision D070), then merge with Age from
  dfData. Parse composite_ID to extract UID and TEST for merge keys.

- Why this approach: RQ 5.9 analyzes Age × Time interaction effects on
  forgetting trajectories. Requires theta (memory outcome), TSVR_hours (time
  predictor per D070), and Age (individual differences predictor).

- Data flow:
  1. Load theta_all from RQ 5.7 (composite factor scores)
  2. Parse composite_ID → extract UID and TEST
  3. Merge with TSVR mapping on (UID, TEST) → adds time variable
  4. Merge with dfData on UID → adds age variable
  5. Validate no missing Age (all UIDs must have demographics)
  6. Rename columns for analysis clarity

- Expected performance: ~1 second (simple pandas merges, no computation)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib (pandas read_csv, merge, rename operations)
- Validation tools:
  - check_file_exists: Verify cross-RQ dependencies (RQ 5.7 outputs)
  - validate_data_format: Check merged data has all required columns
  - check_missing_data: Ensure no NaN values after merge
- Parameters: Left joins on (UID, TEST) and UID, zero NaN tolerance
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tools
from tools.validation import check_file_exists, validate_data_format, check_missing_data

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/chX/rqY (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step00_extract_merge_data.log"

# Input file paths (cross-RQ dependencies from RQ 5.7)
THETA_FILE = PROJECT_ROOT / "results/ch5/rq7/data/step03_theta_scores.csv"
TSVR_FILE = PROJECT_ROOT / "results/ch5/rq7/data/step04_lmm_input.csv"
AGE_FILE = PROJECT_ROOT / "data/cache/dfData.csv"

# Output file path
OUTPUT_FILE = RQ_DIR / "data" / "step00_lmm_input_raw.csv"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step00_lmm_input_raw.csv
#   CORRECT: logs/step00_extract_merge_data.log
#   WRONG:   results/lmm_input_raw.csv  (wrong folder + no prefix)
#   WRONG:   data/lmm_input.csv          (missing step prefix)
#   WRONG:   logs/step00_merged.csv      (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 00: extract_merge_data")

        # =========================================================================
        # VALIDATION 1: Check Cross-RQ Dependencies Exist
        # =========================================================================
        # Before attempting data load, verify all source files exist
        # RQ 5.9 depends on RQ 5.7 outputs - user must execute RQ 5.7 first

        log("[VALIDATION] Checking cross-RQ dependencies...")

        # Check theta file from RQ 5.7
        theta_check = check_file_exists(THETA_FILE, min_size_bytes=1000)
        if not theta_check['valid']:
            error_msg = (
                f"Cross-RQ dependency error: {theta_check['message']}\n"
                f"File: {THETA_FILE}\n"
                f"Required: RQ 5.7 must complete successfully before running RQ 5.9\n"
                f"Action: Execute RQ 5.7 first to generate theta scores"
            )
            log(f"[ERROR] {error_msg}")
            raise FileNotFoundError(error_msg)
        log(f"[PASS] Theta file exists: {THETA_FILE} ({theta_check['size_bytes']} bytes)")

        # Check TSVR file from RQ 5.7
        tsvr_check = check_file_exists(TSVR_FILE, min_size_bytes=1000)
        if not tsvr_check['valid']:
            error_msg = (
                f"Cross-RQ dependency error: {tsvr_check['message']}\n"
                f"File: {TSVR_FILE}\n"
                f"Required: RQ 5.7 Step 0 must complete to generate TSVR mapping\n"
                f"Action: Execute RQ 5.7 first"
            )
            log(f"[ERROR] {error_msg}")
            raise FileNotFoundError(error_msg)
        log(f"[PASS] TSVR file exists: {TSVR_FILE} ({tsvr_check['size_bytes']} bytes)")

        # Check dfData file (participant demographics)
        age_check = check_file_exists(AGE_FILE, min_size_bytes=1000)
        if not age_check['valid']:
            error_msg = (
                f"Data dependency error: {age_check['message']}\n"
                f"File: {AGE_FILE}\n"
                f"Required: dfData.csv must exist in data/cache/\n"
                f"Action: Run data preparation pipeline to generate dfData.csv"
            )
            log(f"[ERROR] {error_msg}")
            raise FileNotFoundError(error_msg)
        log(f"[PASS] Age file exists: {AGE_FILE} ({age_check['size_bytes']} bytes)")

        log("[VALIDATION] All 3 source files exist and are not empty")

        # =========================================================================
        # STEP 1: Load Theta + TSVR from RQ 5.7
        # =========================================================================
        # Expected: Complete LMM input from RQ 5.7 with Theta, SE, and TSVR
        # Purpose: Memory outcome variable + time variable for LMM

        log("[LOAD] Loading theta scores + TSVR from RQ 5.7 step04...")
        lmm_df = pd.read_csv(TSVR_FILE, encoding='utf-8')
        log(f"[LOADED] LMM data: {len(lmm_df)} rows, {len(lmm_df.columns)} columns")
        log(f"[INFO] Columns: {lmm_df.columns.tolist()}")

        # Extract UID and TEST from composite_ID, then select needed columns
        log("[TRANSFORM] Extracting UID and TEST from composite_ID...")
        lmm_df[['UID', 'TEST']] = lmm_df['composite_ID'].str.split('_', expand=True)

        # Select and rename columns
        lmm_df = lmm_df[['composite_ID', 'UID', 'TEST', 'Theta', 'SE', 'TSVR_hours']].copy()
        lmm_df = lmm_df.rename(columns={'Theta': 'theta_all', 'SE': 'se_all', 'TSVR_hours': 'TSVR'})

        log(f"[INFO] UID range: {lmm_df['UID'].min()} - {lmm_df['UID'].max()}")
        log(f"[INFO] TEST values: {sorted(lmm_df['TEST'].unique())}")
        log(f"[INFO] TSVR range: {lmm_df['TSVR'].min():.2f} - {lmm_df['TSVR'].max():.2f} hours")
        log(f"[INFO] Theta range: {lmm_df['theta_all'].min():.2f} - {lmm_df['theta_all'].max():.2f}")

        # Rename for consistency with merge logic below
        merged_df = lmm_df

        # =========================================================================
        # STEP 2: Load Age from dfData
        # =========================================================================
        # Expected: Participant demographics with Age variable
        # Purpose: Individual differences predictor for Age × Time interaction

        log("[LOAD] Loading Age from dfData.csv...")
        age_df = pd.read_csv(AGE_FILE, encoding='utf-8')
        log(f"[LOADED] Demographics: {len(age_df)} rows, {len(age_df.columns)} columns")
        log(f"[INFO] Demographics columns: {age_df.columns.tolist()}")

        # Select only UID and age columns, then get unique UID (age is constant per participant)
        age_df = age_df[['UID', 'age']].drop_duplicates(subset='UID').copy()
        log(f"[INFO] Unique participants with age: {len(age_df)}")
        log(f"[INFO] Age range: {age_df['age'].min():.1f} - {age_df['age'].max():.1f} years")

        # =========================================================================
        # STEP 3: Merge with Age on UID
        # =========================================================================
        # Purpose: Add individual differences predictor to dataset
        # Merge type: Left join (keep all theta+TSVR observations, add age)

        log("[MERGE] Merging combined data with Age on UID...")
        merged_df = merged_df.merge(
            age_df,
            on='UID',
            how='left'
        )
        log(f"[MERGED] Final data: {len(merged_df)} rows, {len(merged_df.columns)} columns")

        # Validate: Check for any missing Age values (critical error)
        age_missing = merged_df['age'].isna().sum()
        if age_missing > 0:
            missing_uids = merged_df[merged_df['age'].isna()]['UID'].unique()
            error_msg = (
                f"Age missing for {age_missing} rows across {len(missing_uids)} UIDs\n"
                f"Missing UIDs: {missing_uids}\n"
                f"All participants must have Age data in dfData.csv\n"
                f"Check dfData.csv completeness"
            )
            log(f"[ERROR] {error_msg}")
            raise ValueError(error_msg)
        log("[PASS] No missing Age values - all participants have demographics")

        # =========================================================================
        # STEP 4: Rename Columns for Analysis
        # =========================================================================
        # Purpose: Standardize column names for LMM analysis
        # theta_all -> theta (outcome variable)
        # TSVR -> TSVR_hours (explicit units for clarity)

        log("[TRANSFORM] Renaming columns for analysis clarity...")
        merged_df = merged_df.rename(columns={
            'theta_all': 'theta',
            'TSVR': 'TSVR_hours'
        })
        log("[INFO] Column renames applied: theta_all -> theta, TSVR -> TSVR_hours")

        # =========================================================================
        # STEP 5: Select Final Columns and Reorder
        # =========================================================================
        # Purpose: Create clean, analysis-ready dataset with only required columns
        # Order: Identifiers, predictors, outcome, uncertainty

        log("[TRANSFORM] Selecting final columns...")
        final_columns = ['composite_ID', 'UID', 'TEST', 'TSVR_hours', 'theta', 'se_all', 'age']
        merged_df = merged_df[final_columns].copy()
        log(f"[INFO] Final columns: {merged_df.columns.tolist()}")
        log(f"[INFO] Final dimensions: {len(merged_df)} rows × {len(merged_df.columns)} columns")

        # =========================================================================
        # STEP 6: Save Merged Data
        # =========================================================================
        # These outputs will be used by: Step 01 (prepare predictors) and Step 02 (fit LMM)

        log(f"[SAVE] Saving merged data to {OUTPUT_FILE}...")
        merged_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8')
        log(f"[SAVED] {OUTPUT_FILE.name} ({len(merged_df)} rows, {len(merged_df.columns)} columns)")

        # Summary statistics for validation
        log("[SUMMARY] Dataset overview:")
        log(f"  N participants: {merged_df['UID'].nunique()}")
        log(f"  N tests per participant: {merged_df.groupby('UID')['TEST'].nunique().mode().values[0]}")
        log(f"  Total observations: {len(merged_df)}")
        log(f"  Age range: {merged_df['age'].min():.1f} - {merged_df['age'].max():.1f} years")
        log(f"  TSVR range: {merged_df['TSVR_hours'].min():.2f} - {merged_df['TSVR_hours'].max():.2f} hours")
        log(f"  Theta range: {merged_df['theta'].min():.3f} - {merged_df['theta'].max():.3f}")

        # =========================================================================
        # VALIDATION 2: Check Output Data Format
        # =========================================================================
        # Tool: validate_data_format
        # Validates: All 7 required columns present in merged output
        # Purpose: Ensure merge operations produced expected structure

        log("[VALIDATION] Validating merged data format...")
        format_result = validate_data_format(
            df=merged_df,
            required_cols=['composite_ID', 'UID', 'TEST', 'TSVR_hours', 'theta', 'se_all', 'age']
        )

        if not format_result['valid']:
            error_msg = f"Data format validation failed: {format_result['message']}"
            log(f"[ERROR] {error_msg}")
            raise ValueError(error_msg)
        log(f"[PASS] {format_result['message']}")

        # =========================================================================
        # VALIDATION 3: Check for Missing Data
        # =========================================================================
        # Tool: check_missing_data
        # Validates: Zero NaN values tolerated (all columns complete)
        # Purpose: Ensure complete cases for LMM analysis (no missing predictors/outcomes)

        log("[VALIDATION] Checking for missing data...")
        missing_result = check_missing_data(df=merged_df)

        if missing_result['has_missing']:
            missing_cols = {col: count for col, count in missing_result['missing_by_column'].items() if count > 0}
            error_msg = (
                f"Missing data detected: {missing_result['total_missing']} NaN values "
                f"({missing_result['percent_missing']:.2f}% of all cells)\n"
                f"Missing by column: {missing_cols}\n"
                f"RQ 5.9 requires complete cases (zero NaN tolerance)\n"
                f"Check merge logic and source data completeness"
            )
            log(f"[ERROR] {error_msg}")
            raise ValueError(error_msg)
        log(f"[PASS] No missing data - all {missing_result['total_cells']} cells complete")

        log("[SUCCESS] Step 00 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
