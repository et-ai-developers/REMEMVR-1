#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated for RQ 5.3.7 Step 03)
# =============================================================================
"""
Step ID: step03
Step Name: Compute Intraclass Correlation Coefficients (ICC) Per Paradigm
RQ: results/ch5/5.3.7
Generated: 2025-12-04

PURPOSE:
Compute three types of ICC estimates per paradigm (Free Recall, Cued Recall, Recognition)
to quantify the proportion of variance attributable to between-person differences.

ICC Types:
1. ICC_intercept: Between-person variance in baseline memory ability
   Formula: var_intercept / (var_intercept + var_residual)

2. ICC_slope_simple: Between-person variance in forgetting rates
   Formula: var_slope / (var_slope + var_residual)

3. ICC_slope_conditional: Between-person variance at specific timepoint (Day 6)
   Formula: [var_int + 2*cov*T + var_slope*T²] / [var_int + 2*cov*T + var_slope*T² + var_res]
   where T = log(TSVR_hours + 1) for Day 6 (~144 hours) ≈ log(145) ≈ 4.976

EXPECTED INPUTS:
  - data/step02_variance_components.csv
    Columns: ['paradigm', 'component', 'estimate']
    Format: 15 rows (5 variance components × 3 paradigms)
    Paradigms: free_recall, cued_recall, recognition
    Components per paradigm:
      - var_intercept: Random intercept variance
      - var_slope: Random slope variance
      - cov_int_slope: Intercept-slope covariance
      - corr_int_slope: Intercept-slope correlation (not used in ICC)
      - var_residual: Residual/within-person variance

EXPECTED OUTPUTS:
  - data/step03_icc_estimates.csv
    Columns: ['paradigm', 'icc_type', 'icc_value', 'interpretation']
    Format: 9 rows (3 ICC types × 3 paradigms)
    Expected rows: ~0.1-0.7 ICC range (memory data typically moderate to high)

  - data/step03_icc_summary.txt
    Format: TXT, plain-text interpretation report
    Content: ICC values by paradigm, interpretation relative to thresholds

VALIDATION CRITERIA:
  - All ICC values in [0, 1] range (ICC is a proportion by definition)
  - All 9 rows present (3 paradigms × 3 ICC types)
  - No NaN values (valid variance components should yield valid ICCs)
  - Interpretation labels match thresholds:
    * Low: ICC < 0.20
    * Moderate: 0.20 <= ICC < 0.40
    * Substantial: ICC >= 0.40

REASONING:
- ICC quantifies "clustering" of observations within participants
- Higher ICC = more between-person variance = better reliability for individual differences
- Conditional ICC at Day 6 captures between-person variance at delayed timepoint
  (most relevant for retention research - immediate memory less interesting)
- Day 6 timepoint chosen to match typical neuropsych assessment delays

IMPLEMENTATION NOTES:
- Time transformation: log(TSVR_hours + 1) per RQ 5.3.1 model
- Day 6 = ~144 hours → log(145) ≈ 4.976
- Variance components extracted from Step 02 paradigm-stratified LMMs
- Function handles component name mapping from step02 format to expected format
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.3.7
LOG_FILE = RQ_DIR / "logs" / "step03_compute_icc.log"

# ICC interpretation thresholds (per 4_analysis.yaml specification)
ICC_THRESHOLDS = {
    'low': 0.20,
    'moderate': 0.40
}

# Day 6 timepoint for conditional ICC
# TSVR ~144 hours → log(TSVR_hours + 1) = log(145) ≈ 4.976
DAY_6_TSVR_HOURS = 144.0
LOG_TSVR_DAY6 = np.log(DAY_6_TSVR_HOURS + 1)

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step03_icc_estimates.csv
#   WRONG:   results/icc_estimates.csv  (wrong folder + no prefix)
#   WRONG:   data/icc_estimates.csv     (missing step prefix)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# ICC Computation Functions
# =============================================================================

def interpret_icc(icc: float) -> str:
    """
    Interpret ICC value using thresholds from 4_analysis.yaml.

    Thresholds:
    - Low: ICC < 0.20
    - Moderate: 0.20 <= ICC < 0.40
    - Substantial: ICC >= 0.40
    """
    if icc < ICC_THRESHOLDS['low']:
        return "Low"
    elif icc < ICC_THRESHOLDS['moderate']:
        return "Moderate"
    else:
        return "Substantial"

def compute_icc_for_paradigm(
    paradigm: str,
    variance_components: pd.DataFrame,
    timepoint_log_tsvr: float
) -> list:
    """
    Compute 3 types of ICC for a single paradigm.

    Parameters
    ----------
    paradigm : str
        Paradigm name (free_recall, cued_recall, recognition)
    variance_components : DataFrame
        Variance components with columns ['paradigm', 'component', 'estimate']
    timepoint_log_tsvr : float
        Log-transformed TSVR value for conditional ICC (e.g., log(145) ≈ 4.976)

    Returns
    -------
    list
        List of 3 dicts, one per ICC type
    """
    # Filter to this paradigm
    paradigm_data = variance_components[variance_components['paradigm'] == paradigm]

    # Extract variance components
    components = {row['component']: row['estimate']
                  for _, row in paradigm_data.iterrows()}

    var_intercept = components.get('var_intercept', 0.0)
    var_slope = components.get('var_slope', 0.0)
    cov_int_slope = components.get('cov_int_slope', 0.0)
    var_residual = components.get('var_residual', 0.0)

    results = []

    # ICC 1: Intercept-only ICC (between-person variance in baseline)
    total_var_intercept = var_intercept + var_residual
    if total_var_intercept > 0:
        icc_intercept = var_intercept / total_var_intercept
    else:
        icc_intercept = 0.0

    results.append({
        'paradigm': paradigm,
        'icc_type': 'intercept',
        'icc_value': icc_intercept,
        'interpretation': interpret_icc(icc_intercept)
    })

    # ICC 2: Simple slope ICC (between-person variance in slopes)
    total_var_slope = var_slope + var_residual
    if total_var_slope > 0:
        icc_slope_simple = var_slope / total_var_slope
    else:
        icc_slope_simple = 0.0

    results.append({
        'paradigm': paradigm,
        'icc_type': 'slope_simple',
        'icc_value': icc_slope_simple,
        'interpretation': interpret_icc(icc_slope_simple)
    })

    # ICC 3: Conditional ICC at specific timepoint
    # Formula: [var_int + 2*cov*T + var_slope*T²] / [var_int + 2*cov*T + var_slope*T² + var_res]
    T = timepoint_log_tsvr

    var_at_time = (var_intercept +
                   2 * T * cov_int_slope +
                   (T ** 2) * var_slope)

    total_var_at_time = var_at_time + var_residual

    if total_var_at_time > 0:
        icc_conditional = var_at_time / total_var_at_time
    else:
        icc_conditional = 0.0

    # Ensure ICC is in valid [0, 1] range (negative variance estimates can occur)
    icc_conditional = max(0.0, min(1.0, icc_conditional))

    results.append({
        'paradigm': paradigm,
        'icc_type': 'slope_conditional',
        'icc_value': icc_conditional,
        'interpretation': interpret_icc(icc_conditional)
    })

    return results

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 03: Compute Intraclass Correlation Coefficients (ICC) Per Paradigm")

        # =========================================================================
        # STEP 1: Load Variance Components
        # =========================================================================
        # Expected: 15 rows (5 components × 3 paradigms)
        # Purpose: Variance components from paradigm-stratified LMMs (Step 02)

        log("[LOAD] Loading variance components from step02...")
        variance_components = pd.read_csv(RQ_DIR / "data" / "step02_variance_components.csv")
        log(f"[LOADED] {len(variance_components)} variance components from {variance_components['paradigm'].nunique()} paradigms")

        # Validate structure
        expected_paradigms = {'free_recall', 'cued_recall', 'recognition'}
        actual_paradigms = set(variance_components['paradigm'].unique())

        if actual_paradigms != expected_paradigms:
            raise ValueError(f"Expected paradigms {expected_paradigms}, got {actual_paradigms}")

        log(f"[VALIDATE] All 3 paradigms present: {sorted(actual_paradigms)}")

        # =========================================================================
        # STEP 2: Compute ICC for Each Paradigm
        # =========================================================================
        # Tool: Custom ICC computation function
        # What it does: Computes 3 ICC types from variance components
        # Expected output: 9 rows (3 ICC types × 3 paradigms)

        log(f"[ANALYSIS] Computing ICC estimates (timepoint: Day 6, log_TSVR={LOG_TSVR_DAY6:.3f})...")

        all_icc_results = []

        for paradigm in sorted(expected_paradigms):
            log(f"[ICC] Computing ICC for {paradigm}...")

            paradigm_iccs = compute_icc_for_paradigm(
                paradigm=paradigm,
                variance_components=variance_components,
                timepoint_log_tsvr=LOG_TSVR_DAY6
            )

            all_icc_results.extend(paradigm_iccs)

            # Log results for this paradigm
            for result in paradigm_iccs:
                log(f"  {result['icc_type']:20s}: {result['icc_value']:.3f} ({result['interpretation']})")

        log("[DONE] ICC computation complete")

        # =========================================================================
        # STEP 3: Save ICC Estimates to CSV
        # =========================================================================
        # Output: data/step03_icc_estimates.csv (9 rows × 4 columns)

        log("[SAVE] Saving ICC estimates...")
        icc_df = pd.DataFrame(all_icc_results)

        output_csv = RQ_DIR / "data" / "step03_icc_estimates.csv"
        icc_df.to_csv(output_csv, index=False, encoding='utf-8')
        log(f"[SAVED] {output_csv} ({len(icc_df)} rows, {len(icc_df.columns)} cols)")

        # =========================================================================
        # STEP 4: Generate ICC Summary Report
        # =========================================================================
        # Output: data/step03_icc_summary.txt

        log("[SUMMARY] Generating ICC interpretation report...")

        summary_lines = []
        summary_lines.append("="*80)
        summary_lines.append("INTRACLASS CORRELATION COEFFICIENT (ICC) SUMMARY")
        summary_lines.append("RQ 5.3.7 - Paradigm-Stratified Variance Decomposition")
        summary_lines.append("="*80)
        summary_lines.append("")
        summary_lines.append("INTERPRETATION THRESHOLDS:")
        summary_lines.append("  Low:         ICC < 0.20")
        summary_lines.append("  Moderate:    0.20 <= ICC < 0.40")
        summary_lines.append("  Substantial: ICC >= 0.40")
        summary_lines.append("")
        summary_lines.append("TIMEPOINT FOR CONDITIONAL ICC:")
        summary_lines.append(f"  Day 6 (TSVR ~{DAY_6_TSVR_HOURS:.0f} hours)")
        summary_lines.append(f"  Log-transformed: log({DAY_6_TSVR_HOURS + 1:.0f}) = {LOG_TSVR_DAY6:.3f}")
        summary_lines.append("")
        summary_lines.append("-"*80)

        for paradigm in sorted(expected_paradigms):
            paradigm_data = icc_df[icc_df['paradigm'] == paradigm]

            summary_lines.append(f"\nPARADIGM: {paradigm.upper()}")
            summary_lines.append("-"*80)

            for _, row in paradigm_data.iterrows():
                icc_type = row['icc_type']
                icc_value = row['icc_value']
                interpretation = row['interpretation']

                # Format ICC type name
                if icc_type == 'intercept':
                    type_name = "ICC Intercept (baseline memory)"
                elif icc_type == 'slope_simple':
                    type_name = "ICC Slope Simple (forgetting rate)"
                else:
                    type_name = f"ICC Conditional at Day 6 (memory at {DAY_6_TSVR_HOURS:.0f}h)"

                summary_lines.append(f"  {type_name:50s}: {icc_value:.3f} ({interpretation})")

            summary_lines.append("")

        summary_lines.append("-"*80)
        summary_lines.append("\nINTERPRETATION NOTES:")
        summary_lines.append("- ICC quantifies proportion of variance between participants")
        summary_lines.append("- Higher ICC = more individual differences = better reliability")
        summary_lines.append("- Conditional ICC at Day 6 most relevant for retention research")
        summary_lines.append("- Intercept ICC captures individual differences in baseline encoding")
        summary_lines.append("- Slope ICC captures individual differences in forgetting rates")
        summary_lines.append("")
        summary_lines.append("="*80)

        summary_text = "\n".join(summary_lines)

        output_txt = RQ_DIR / "data" / "step03_icc_summary.txt"
        with open(output_txt, 'w', encoding='utf-8') as f:
            f.write(summary_text)
        log(f"[SAVED] {output_txt}")

        # =========================================================================
        # STEP 5: Run Validation
        # =========================================================================
        # Validation: All ICC values in [0, 1], no NaN, correct structure

        log("[VALIDATION] Validating ICC estimates...")

        validation_errors = []

        # Check 1: Exactly 9 rows (3 paradigms × 3 ICC types)
        if len(icc_df) != 9:
            validation_errors.append(f"Expected 9 rows, got {len(icc_df)}")

        # Check 2: All ICC values in [0, 1]
        invalid_iccs = icc_df[(icc_df['icc_value'] < 0) | (icc_df['icc_value'] > 1)]
        if len(invalid_iccs) > 0:
            validation_errors.append(f"Found {len(invalid_iccs)} ICC values outside [0, 1] range")

        # Check 3: No NaN values
        nan_count = icc_df['icc_value'].isna().sum()
        if nan_count > 0:
            validation_errors.append(f"Found {nan_count} NaN ICC values")

        # Check 4: All paradigms present
        if set(icc_df['paradigm'].unique()) != expected_paradigms:
            validation_errors.append(f"Missing paradigms in ICC results")

        # Check 5: All ICC types present per paradigm
        for paradigm in expected_paradigms:
            paradigm_types = set(icc_df[icc_df['paradigm'] == paradigm]['icc_type'])
            expected_types = {'intercept', 'slope_simple', 'slope_conditional'}
            if paradigm_types != expected_types:
                validation_errors.append(f"Missing ICC types for {paradigm}: expected {expected_types}, got {paradigm_types}")

        # Check 6: Interpretation labels match thresholds
        for _, row in icc_df.iterrows():
            expected_interp = interpret_icc(row['icc_value'])
            if row['interpretation'] != expected_interp:
                validation_errors.append(
                    f"Interpretation mismatch for {row['paradigm']} {row['icc_type']}: "
                    f"expected '{expected_interp}', got '{row['interpretation']}'"
                )

        if validation_errors:
            log("[VALIDATION] FAILED with errors:")
            for error in validation_errors:
                log(f"  - {error}")
            raise ValueError(f"Validation failed: {validation_errors}")

        log("[VALIDATION] PASSED - All criteria met:")
        log(f"  [PASS] Row count: {len(icc_df)} rows (3 paradigms x 3 ICC types)")
        log(f"  [PASS] ICC bounds: All values in [0, 1] (range: {icc_df['icc_value'].min():.3f} to {icc_df['icc_value'].max():.3f})")
        log(f"  [PASS] No NaN values: {nan_count} NaN found")
        log(f"  [PASS] All paradigms present: {sorted(icc_df['paradigm'].unique())}")
        log(f"  [PASS] All ICC types present per paradigm")
        log(f"  [PASS] Interpretation labels match thresholds")

        log("[SUCCESS] Step 03 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
