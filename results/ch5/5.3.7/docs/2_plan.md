# Analysis Plan for RQ 5.3.7: Paradigm-Specific Variance Decomposition

**Created by:** rq_planner agent
**Date:** 2025-12-02
**Status:** Ready for rq_tools (Step 11 workflow)

---

## Overview

This RQ examines individual differences in forgetting trajectories stratified by retrieval paradigm (Free Recall, Cued Recall, Recognition). The goal is to decompose variance into between-person (trait-like individual differences) versus within-person (measurement error/state fluctuation) components per paradigm, quantified via Intraclass Correlation Coefficients (ICC).

**Research Question:** What proportion of variance in forgetting rate is between-person versus within-person for each retrieval paradigm (Free Recall, Cued Recall, Recognition)?

**Analysis Approach:** Linear Mixed Models (LMM) variance decomposition with paradigm-stratified models. This RQ does NOT perform new IRT calibration - it uses existing theta scores from RQ 5.3.1 and fits separate LMMs per paradigm to isolate paradigm-specific variance components.

**Pipeline:** LMM variance decomposition ONLY (no IRT, no TSVR merge)

**Total Steps:** 7 analysis steps (Step 0 through Step 6)

**Estimated Runtime:** Medium (~30-60 minutes total)
- Step 0: Low (<5 min - data loading and validation)
- Step 1: Low (<5 min - model metadata loading)
- Step 2: High (30-45 min - 3 paradigm-stratified LMM fits with random slopes)
- Step 3: Low (<5 min - ICC computation)
- Step 4: Low (<5 min - random effects extraction)
- Step 5: Low (<5 min - correlation tests)
- Step 6: Low (<5 min - ICC comparison and barplot data preparation)

**Key Decisions Applied:**
- **Decision D068:** Dual p-value reporting (uncorrected + Bonferroni) for intercept-slope correlation tests
- **Practice Effects:** Acknowledged in concept.md - ICC values interpreted as lower bounds of trait-like stability
- **Convergence Contingency:** Bates et al. (2015) parsimonious model selection if random slopes fail to converge

**Critical Dependency:** RQ 5.3.1 must complete Steps 1-5 (IRT calibration, LMM fitting with Paradigm x Time interaction, model selection via AIC) before this RQ can execute.

**Critical Output:** data/step04_random_effects.csv (300 rows: 100 participants x 3 paradigms) is REQUIRED for downstream RQ 5.3.8 (Paradigm-Based Clustering).

---

## Analysis Plan

### Step 0: Load Theta Scores from RQ 5.3.1

**Purpose:** Load paradigm-specific theta scores from RQ 5.3.1 and validate data structure.

**Dependencies:** None (first step)

**Complexity:** Low (<5 minutes - data loading and validation)

**Input:**

**File 1:** results/ch5/5.3.1/data/step04_lmm_input.csv
**Source:** Generated by RQ 5.3.1 Step 4 (TSVR merge and reshape to long format)
**Format:** CSV, long format (one row per observation)
**Columns:**
  - `composite_ID` (string, format: {UID}_{test}, e.g., "P001_T1")
  - `UID` (string, participant identifier, e.g., "P001")
  - `test` (string, test session identifier: T1, T2, T3, T4 for Days 0, 1, 3, 6)
  - `TSVR_hours` (float, actual time since encoding in hours per Decision D070)
  - `paradigm` (string, categorical: IFR, ICR, IRE)
  - `theta` (float, IRT ability estimate per paradigm dimension)
  - `se` (float, standard error of theta estimate)
**Expected Rows:** 1200 rows (100 participants x 4 tests x 3 paradigms)
**Expected Columns:** 7 columns (composite_ID, UID, test, TSVR_hours, paradigm, theta, se)

**Data Quality Requirements:**
- All 3 paradigms present (IFR, ICR, IRE)
- All 4 test sessions present per participant-paradigm combination (T1, T2, T3, T4)
- No missing theta values (complete data required for variance decomposition)
- theta values in scientifically reasonable range ([-4, 4])
- se values positive and reasonable ([0.1, 1.5])

**Processing:**

1. Read results/ch5/5.3.1/data/step04_lmm_input.csv
2. Validate structure:
   - Expected row count: 1200 rows
   - Expected column count: 7 columns
   - Required columns present: composite_ID, UID, test, TSVR_hours, paradigm, theta, se
3. Validate paradigm categories: Exactly 3 unique values (IFR, ICR, IRE)
4. Validate test categories: Exactly 4 unique values (T1, T2, T3, T4)
5. Check for missing data:
   - No NaN values in theta column
   - No NaN values in TSVR_hours column
   - No NaN values in paradigm column
6. Validate value ranges:
   - theta in [-4, 4] (IRT ability range)
   - se in [0.1, 1.5] (reasonable standard errors)
   - TSVR_hours in [0, 200] hours (0 = encoding, ~168 hours = 1 week, some variation)
7. Verify balanced design:
   - All 100 participants have exactly 12 rows (4 tests x 3 paradigms)
   - No missing participant-test-paradigm combinations

**Output:**

**File 1:** data/step00_theta_scores_validated.csv
**Format:** CSV, same structure as input after validation
**Columns:** composite_ID, UID, test, TSVR_hours, paradigm, theta, se (7 columns)
**Expected Rows:** 1200 rows
**Purpose:** Validated copy of input data for subsequent steps

**File 2:** data/step00_validation_summary.txt
**Format:** TXT, validation report
**Contents:**
  - Data source confirmation (RQ 5.3.1 dependency verified)
  - Row count: 1200
  - Column count: 7
  - Paradigm categories: IFR (400 rows), ICR (400 rows), IRE (400 rows)
  - Test sessions: T1 (300 rows), T2 (300 rows), T3 (300 rows), T4 (300 rows)
  - Participant count: 100 unique UIDs
  - Missing data: 0 NaN values
  - Theta range: [min, max] (should be within [-4, 4])
  - SE range: [min, max] (should be within [0.1, 1.5])
  - TSVR range: [min, max] hours
  - Balanced design: PASS/FAIL (all participants have 12 rows)

**Validation Requirement:**

Validation tools MUST be used after data loading. Specific validation tools will be determined by rq_tools based on data validation requirements (structure checks, missing data checks, value range checks). The rq_analysis agent will embed validation tool calls after the data loading tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step00_theta_scores_validated.csv: 1200 rows x 7 columns (composite_ID: object, UID: object, test: object, TSVR_hours: float64, paradigm: object, theta: float64, se: float64)
- data/step00_validation_summary.txt exists (>500 bytes indicating complete validation report)

*Value Ranges:*
- theta in [-4, 4] (outside = IRT calibration problem in RQ 5.3.1)
- se in [0.1, 1.5] (above 1.5 = unreliable, below 0.1 = suspiciously precise)
- TSVR_hours in [0, 200] (reasonable time range for 4 test sessions)

*Data Quality:*
- All 1200 rows present (no data loss)
- All 3 paradigms present (IFR, ICR, IRE with 400 rows each)
- All 4 test sessions present (T1, T2, T3, T4 with 300 rows each)
- No NaN values in theta, TSVR_hours, or paradigm columns
- All 100 participants present (balanced design check)

*Log Validation:*
- Required pattern: "Data validation PASSED: 1200 rows loaded"
- Required pattern: "Paradigm categories verified: IFR, ICR, IRE"
- Required pattern: "Balanced design verified: 100 participants x 12 observations each"
- Forbidden patterns: "ERROR", "VALIDATION FAILED", "Missing paradigm", "NaN values detected"
- Acceptable warnings: None expected for data loading

**Expected Behavior on Validation Failure:**
- If RQ 5.3.1 dependency file missing -> QUIT with "EXPECTATIONS ERROR: RQ 5.3.1 must complete before RQ 5.3.7"
- If row count != 1200 -> QUIT with "VALIDATION FAILED: Expected 1200 rows, found {N}"
- If paradigm categories != 3 -> QUIT with "VALIDATION FAILED: Expected 3 paradigms (IFR, ICR, IRE), found {list}"
- If NaN values detected -> QUIT with "VALIDATION FAILED: Missing data in {column}"
- If value ranges violated -> QUIT with "VALIDATION FAILED: theta/se/TSVR_hours out of range"
- Log error to logs/step00_load_theta_scores.log
- g_debug invoked by master to diagnose RQ 5.3.1 output issues

---

### Step 1: Load RQ 5.3.1 Best-Fitting Model Metadata

**Purpose:** Load the best-fitting LMM model metadata from RQ 5.3.1 to understand the functional form and paradigm structure used in the original analysis.

**Dependencies:** Step 0 (requires validated theta scores)

**Complexity:** Low (<5 minutes - metadata loading and inspection)

**Input:**

**File 1:** results/ch5/5.3.1/data/step05_lmm_fitted_model.pkl
**Source:** Generated by RQ 5.3.1 Step 5 (LMM model fitting with Paradigm x Time interaction)
**Format:** Pickle file containing fitted statsmodels MixedLM object
**Contents:** Best-fitting LMM model (likely Log model based on Chapter 5 pattern from RQ 5.1.1)
**Purpose:** Understand functional form (Linear, Quadratic, Logarithmic, etc.) for paradigm-stratified models

**File 2:** results/ch5/5.3.1/data/step05_model_comparison.csv
**Source:** Generated by RQ 5.3.1 Step 5 (AIC comparison of 5 candidate models)
**Format:** CSV with model comparison results
**Columns:** model_name, AIC, delta_AIC, akaike_weight, converged
**Purpose:** Identify which functional form was selected as best (used to specify same form for paradigm-stratified models)

**Processing:**

1. Load results/ch5/5.3.1/data/step05_lmm_fitted_model.pkl using pickle
2. Extract model metadata:
   - Functional form (Time transformation: Linear, Log, Quadratic, etc.)
   - Formula specification
   - Convergence status (model.converged attribute)
   - Random effects structure (random slopes by UID present?)
3. Load results/ch5/5.3.1/data/step05_model_comparison.csv
4. Identify best model by minimum AIC
5. Verify consistency:
   - Best model from comparison table matches loaded .pkl file
   - Model converged successfully (converged = True)
   - Paradigm factor present in model formula
6. Document functional form to use in Step 2 (paradigm-stratified models)

**Output:**

**File 1:** data/step01_model_metadata.yaml
**Format:** YAML metadata file
**Contents:**
  - Source: RQ 5.3.1 Step 5
  - Best model: {model_name} (e.g., "Log" or "Quadratic+Log")
  - Formula: {model_formula_string}
  - Converged: {True/False}
  - AIC: {value}
  - Random effects: {structure description}
  - Paradigm factor: {present/absent}
  - Time variable: TSVR_hours (per Decision D070)
  - Functional form for Step 2: {transformation to apply}
**Purpose:** Document RQ 5.3.1 model structure for paradigm-stratified replication

**File 2:** data/step01_paradigm_categories.csv
**Format:** CSV
**Columns:** paradigm (string), N_observations (int)
**Expected Rows:** 3 rows (IFR, ICR, IRE)
**Contents:**
  - IFR, 400
  - ICR, 400
  - IRE, 400
**Purpose:** Verify paradigm balance before stratified modeling

**Validation Requirement:**

Validation tools MUST be used after model metadata loading. Specific validation tools will be determined by rq_tools based on model inspection requirements (convergence checks, formula parsing, paradigm structure validation). The rq_analysis agent will embed validation tool calls after the metadata extraction tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step01_model_metadata.yaml exists (>200 bytes indicating complete metadata)
- data/step01_paradigm_categories.csv: 3 rows x 2 columns (paradigm: object, N_observations: int64)

*Value Ranges:*
- N_observations = 400 for all 3 paradigms (balanced design)
- AIC finite and positive (model fit index)
- Converged = True (boolean, model must have converged in RQ 5.3.1)

*Data Quality:*
- Exactly 3 paradigm categories (IFR, ICR, IRE)
- All paradigms have 400 observations (balanced)
- Formula string contains "paradigm" factor
- Time variable is TSVR_hours (Decision D070 compliance)

*Log Validation:*
- Required pattern: "Model loaded successfully from RQ 5.3.1"
- Required pattern: "Best model: {model_name}, AIC: {value}"
- Required pattern: "Converged: True"
- Required pattern: "Paradigm factor verified in formula"
- Forbidden patterns: "ERROR", "Converged: False", "Model loading failed"
- Acceptable warnings: None expected for metadata loading

**Expected Behavior on Validation Failure:**
- If model file missing -> QUIT with "EXPECTATIONS ERROR: RQ 5.3.1 Step 5 must complete (model file missing)"
- If model not converged -> QUIT with "VALIDATION FAILED: RQ 5.3.1 model did not converge"
- If paradigm factor missing -> QUIT with "VALIDATION FAILED: Paradigm factor not in model formula"
- If paradigm counts != 400 each -> QUIT with "VALIDATION FAILED: Unbalanced paradigm design"
- Log error to logs/step01_load_model_metadata.log
- g_debug invoked to diagnose RQ 5.3.1 model issues

---

### Step 2: Fit Paradigm-Stratified LMMs with Random Slopes

**Purpose:** Fit three separate LMMs (one per paradigm: IFR, ICR, IRE) with random slopes per participant to isolate paradigm-specific variance components. Use the same functional form identified as best in RQ 5.3.1 to ensure consistency.

**Dependencies:** Step 0 (theta scores), Step 1 (model metadata)

**Complexity:** High (30-45 minutes - 3 LMM fits with random slopes, convergence challenges possible)

**Input:**

**File 1:** data/step00_theta_scores_validated.csv
**Source:** Step 0 output
**Format:** CSV, long format
**Columns:** composite_ID, UID, test, TSVR_hours, paradigm, theta, se
**Expected Rows:** 1200 rows total

**File 2:** data/step01_model_metadata.yaml
**Source:** Step 1 output
**Contents:** Functional form to apply (e.g., "Log" means log(TSVR_hours + 1))
**Purpose:** Specify time transformation for paradigm-stratified models

**Processing:**

1. Load validated theta scores and model metadata
2. Extract functional form from metadata (e.g., "Log", "Quadratic", "Linear+Log")
3. Stratify data by paradigm (create 3 subsets):
   - IFR: 400 rows (100 participants x 4 tests)
   - ICR: 400 rows (100 participants x 4 tests)
   - IRE: 400 rows (100 participants x 4 tests)
4. Create time transformations per functional form:
   - If "Linear": Time = TSVR_hours
   - If "Log": Time = log(TSVR_hours + 1) (add 1 to handle TSVR=0 at encoding)
   - If "Quadratic": Time = TSVR_hours, Time_squared = TSVR_hours^2
   - If "Linear+Log": Time = TSVR_hours, Time_log = log(TSVR_hours + 1)
   - If "Quadratic+Log": Time, Time_squared, Time_log
5. For each paradigm (IFR, ICR, IRE):
   a. Fit LMM with random slopes and intercepts:
      - Formula: theta ~ {time_terms} + (1 + {time_term} | UID)
      - Example (Log model): theta ~ log(TSVR_hours + 1) + (1 + log(TSVR_hours + 1) | UID)
      - Random effects: Random intercept + random slope by participant
      - Correlation: Allow intercept-slope covariance (correlated random effects)
      - Method: REML=False (for AIC comparability, though not used here)
   b. Check convergence:
      - If converged: Extract variance components
      - If not converged: Apply Bates et al. (2015) contingency plan (see Convergence Contingency below)
   c. Extract variance components from model.cov_re attribute:
      - var_intercept: Between-person variance in baseline ability
      - var_slope: Between-person variance in forgetting rate
      - cov_int_slope: Covariance between intercept and slope
      - var_residual: Within-person residual variance
   d. Compute correlation: corr_int_slope = cov_int_slope / sqrt(var_intercept * var_slope)
6. Save variance components for all 3 paradigms

**Convergence Contingency Plan (per 1_concept.md):**

If any paradigm-specific model fails to converge with random slopes:
1. Try alternative optimizers (bobyqa, nlminb)
2. Use likelihood ratio test (LRT) to compare random slopes vs intercept-only:
   - Fit intercept-only model: theta ~ {time_terms} + (1 | UID)
   - Compare via LRT (chi-squared test on -2 x log-likelihood difference)
3. If LRT p < 0.05: Retain slopes with simplified correlation structure (uncorrelated random effects)
4. If LRT p >= 0.05: Use random intercepts-only model
5. **Note:** If random slopes cannot be estimated for a paradigm, ICC_slope cannot be computed; report this limitation in results

**Output:**

**File 1:** data/step02_variance_components.csv
**Format:** CSV with variance component estimates
**Columns:**
  - paradigm (string: IFR, ICR, IRE)
  - component (string: var_intercept, var_slope, cov_int_slope, corr_int_slope, var_residual)
  - estimate (float: variance/covariance/correlation value)
**Expected Rows:** 15 rows (3 paradigms x 5 components each)
**Example:**
  - IFR, var_intercept, 0.35
  - IFR, var_slope, 0.12
  - IFR, cov_int_slope, -0.08
  - IFR, corr_int_slope, -0.39
  - IFR, var_residual, 0.48
  - [... repeat for ICR and IRE]

**File 2:** data/step02_lmm_ifr_model.pkl
**Format:** Pickle file containing fitted statsmodels MixedLM object for IFR paradigm
**Purpose:** Saved model for diagnostics and downstream use

**File 3:** data/step02_lmm_icr_model.pkl
**Format:** Pickle file for ICR paradigm

**File 4:** data/step02_lmm_ire_model.pkl
**Format:** Pickle file for IRE paradigm

**File 5:** data/step02_model_summaries.txt
**Format:** TXT with concatenated model summaries for all 3 paradigms
**Contents:**
  - IFR model summary (fixed effects, random effects, convergence status, AIC)
  - ICR model summary
  - IRE model summary
  - Convergence notes (which optimizer used, any contingency plan applied)

**Validation Requirement:**

Validation tools MUST be used after LMM fitting. Specific validation tools will be determined by rq_tools based on LMM validation requirements (convergence checks, variance positivity, random effects structure, residual diagnostics per 1_concept.md Section "Validation Procedures"). The rq_analysis agent will embed validation tool calls after the LMM fitting tool call for this step.

**LMM Assumption Validation (per 1_concept.md - MANDATORY):**

For each of the 3 paradigm-stratified models, perform 6 assumption checks:

1. **Residual Normality:** Q-Q plot + Shapiro-Wilk test (accept if p > 0.01)
2. **Homoscedasticity:** Residuals vs fitted plot; Levene's test by test session
3. **Random Effects Normality:** Q-Q plot of random intercept and slope estimates
4. **Independence:** ACF plot of residuals (no significant autocorrelation)
5. **Linearity:** Residuals vs Time predictor (no systematic patterns)
6. **Outliers:** Cook's distance < 4/N threshold (N=100 participants per paradigm)

**Remedial Actions (if assumptions violated):**
- If normality violated: Report robust standard errors; note impact on variance component estimates
- If heteroscedasticity: Consider variance function by test session
- If outliers detected: Sensitivity analysis excluding influential participants; report ICC with and without outliers
- Document all violations and their potential impact on ICC interpretation

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step02_variance_components.csv: 15 rows x 3 columns (paradigm: object, component: object, estimate: float64)
- data/step02_lmm_ifr_model.pkl exists (>1KB indicating saved model object)
- data/step02_lmm_icr_model.pkl exists
- data/step02_lmm_ire_model.pkl exists
- data/step02_model_summaries.txt exists (>2KB indicating complete summaries for 3 models)

*Value Ranges:*
- var_intercept > 0 (all 3 paradigms - negative variance impossible)
- var_slope > 0 (all 3 paradigms - negative variance impossible)
- var_residual > 0 (all 3 paradigms - negative variance impossible)
- corr_int_slope in [-1, 1] (correlation bounds)
- cov_int_slope unrestricted (can be negative if negative correlation)

*Data Quality:*
- All 15 rows present (3 paradigms x 5 components)
- All 3 paradigms have estimates for all 5 components
- No NaN values in estimate column
- At least 2 out of 3 models converged (if all 3 fail, analysis cannot proceed)

*Log Validation:*
- Required pattern (per paradigm): "Model converged: True" OR "Applied convergence contingency: {method}"
- Required pattern: "Variance components extracted for {paradigm}"
- Required pattern: "All variance components positive for {paradigm}"
- Forbidden patterns: "ERROR", "All models failed to converge", "Negative variance"
- Acceptable warnings: "Model convergence with {alternative optimizer}", "Simplified random structure applied"

**Expected Behavior on Validation Failure:**
- If all 3 models fail to converge -> QUIT with "VALIDATION FAILED: All paradigm models failed to converge (check data quality)"
- If negative variance detected -> QUIT with "VALIDATION FAILED: Negative variance for {paradigm} {component}"
- If NaN in variance components -> QUIT with "VALIDATION FAILED: Estimation failure for {paradigm}"
- Log error to logs/step02_fit_paradigm_lmms.log
- g_debug invoked to diagnose convergence issues

---

### Step 3: Compute Intraclass Correlation Coefficients (ICC) Per Paradigm

**Purpose:** Compute three types of ICC estimates per paradigm to quantify the proportion of variance attributable to between-person differences (trait-like stability) versus within-person fluctuation.

**Dependencies:** Step 2 (variance components)

**Complexity:** Low (<5 minutes - ICC computation from variance components)

**Input:**

**File 1:** data/step02_variance_components.csv
**Source:** Step 2 output
**Format:** CSV with 15 rows (3 paradigms x 5 components)
**Columns:** paradigm, component, estimate
**Purpose:** Variance components for ICC calculation

**Processing:**

1. Load variance components per paradigm
2. For each paradigm (IFR, ICR, IRE), compute 3 ICC estimates:

   **ICC_intercept (baseline ability):**
   - Formula: var_intercept / (var_intercept + var_residual)
   - Interpretation: Proportion of variance in baseline ability (Day 0) due to stable individual differences

   **ICC_slope_simple (forgetting rate, unconditional):**
   - Formula: var_slope / (var_slope + var_residual)
   - Interpretation: Proportion of variance in forgetting rate due to stable individual differences (unconditional on time)
   - NOTE: This is a simplified estimate; more accurate is ICC_slope_conditional

   **ICC_slope_conditional (forgetting rate at specific timepoint, e.g., Day 6):**
   - Formula: [var_intercept + 2*cov_int_slope*Time + var_slope*(Time^2)] / [var_intercept + 2*cov_int_slope*Time + var_slope*(Time^2) + var_residual]
   - Where Time = timepoint of interest (e.g., TSVR_hours corresponding to Day 6 ~= 144 hours)
   - Interpretation: Proportion of variance in forgetting at Day 6 due to stable individual differences (conditional on time)
   - Reference: Hedeker & Gibbons (2006) longitudinal data analysis

3. Interpret ICC magnitudes using thresholds (per 1_concept.md):
   - ICC < 0.20: Low between-person variance
   - 0.20 <= ICC < 0.40: Moderate between-person variance
   - ICC >= 0.40: Substantial between-person variance (trait-like)

4. **NOTE on ICC Scale (per 1_concept.md):**
   - If Time uses log transformation (per RQ 5.3.1 best model): ICC computed on log-time scale
   - Interpretation is for rate of forgetting on the log-transformed time scale
   - This is appropriate because log-time is the scale on which linear forgetting assumptions hold

**Output:**

**File 1:** data/step03_icc_estimates.csv
**Format:** CSV with ICC estimates
**Columns:**
  - paradigm (string: IFR, ICR, IRE)
  - icc_type (string: intercept, slope_simple, slope_conditional)
  - icc_value (float: [0, 1] range)
  - interpretation (string: Low, Moderate, Substantial)
**Expected Rows:** 9 rows (3 paradigms x 3 ICC types)
**Example:**
  - IFR, intercept, 0.42, Substantial
  - IFR, slope_simple, 0.38, Moderate
  - IFR, slope_conditional, 0.45, Substantial
  - [... repeat for ICR and IRE]

**File 2:** data/step03_icc_summary.txt
**Format:** TXT with interpretation
**Contents:**
  - ICC estimates table (all 9 values)
  - Interpretation per paradigm:
    - "IFR: Substantial between-person variance in forgetting rate (ICC_slope_conditional = 0.45)"
    - "ICR: Moderate between-person variance in forgetting rate (ICC_slope_conditional = 0.38)"
    - "IRE: Low between-person variance in forgetting rate (ICC_slope_conditional = 0.18)"
  - Hypothesis evaluation: "ICC > 0.40 threshold met for {N} out of 3 paradigms"
  - Practice effects note: "ICC values interpreted as lower bounds of trait-like stability given potential practice effect confounds"

**Validation Requirement:**

Validation tools MUST be used after ICC computation. Specific validation tools will be determined by rq_tools based on ICC validation requirements (range checks [0, 1], interpretation consistency, computation verification). The rq_analysis agent will embed validation tool calls after the ICC computation tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step03_icc_estimates.csv: 9 rows x 4 columns (paradigm: object, icc_type: object, icc_value: float64, interpretation: object)
- data/step03_icc_summary.txt exists (>500 bytes indicating complete summary)

*Value Ranges:*
- icc_value in [0, 1] (ICC is a proportion, cannot be negative or >1)
- All icc_value values are finite (no NaN, no Inf)

*Data Quality:*
- All 9 rows present (3 paradigms x 3 ICC types)
- All 3 paradigms have estimates for all 3 ICC types
- No NaN values in icc_value column
- Interpretation labels match thresholds:
  - "Low" if icc_value < 0.20
  - "Moderate" if 0.20 <= icc_value < 0.40
  - "Substantial" if icc_value >= 0.40

*Log Validation:*
- Required pattern: "ICC computed for {paradigm}: intercept={value}, slope_simple={value}, slope_conditional={value}"
- Required pattern: "All ICC values in valid range [0, 1]"
- Required pattern: "Interpretation thresholds applied"
- Forbidden patterns: "ERROR", "ICC out of range", "Negative ICC", "ICC > 1.0"
- Acceptable warnings: None expected for ICC computation

**Expected Behavior on Validation Failure:**
- If icc_value < 0 or > 1 -> QUIT with "VALIDATION FAILED: ICC out of range for {paradigm} {icc_type}"
- If NaN in icc_value -> QUIT with "VALIDATION FAILED: ICC computation failed for {paradigm}"
- If row count != 9 -> QUIT with "VALIDATION FAILED: Expected 9 ICC estimates, found {N}"
- Log error to logs/step03_compute_icc.log
- g_debug invoked to diagnose computation errors

---

### Step 4: Extract Individual Random Effects Per Paradigm

**Purpose:** Extract individual random effects (intercepts and slopes) for all 100 participants per paradigm. This produces 300 total rows (100 participants x 3 paradigms), which is REQUIRED for downstream RQ 5.3.8 (Paradigm-Based Clustering).

**Dependencies:** Step 2 (fitted models)

**Complexity:** Low (<5 minutes - random effects extraction from fitted models)

**Input:**

**File 1:** data/step02_lmm_ifr_model.pkl
**Source:** Step 2 output
**Format:** Pickle file containing fitted statsmodels MixedLM object
**Purpose:** Extract random effects for IFR paradigm

**File 2:** data/step02_lmm_icr_model.pkl
**Source:** Step 2 output
**Purpose:** Extract random effects for ICR paradigm

**File 3:** data/step02_lmm_ire_model.pkl
**Source:** Step 2 output
**Purpose:** Extract random effects for IRE paradigm

**Processing:**

1. For each paradigm (IFR, ICR, IRE):
   a. Load fitted model from .pkl file
   b. Extract random effects using model.random_effects attribute (statsmodels API)
   c. Random effects are returned as dict: {UID: [intercept, slope]}
   d. Convert to DataFrame with columns: UID, paradigm, Total_Intercept, Total_Slope
      - UID: Participant identifier (string, e.g., "P001")
      - paradigm: Paradigm identifier (string: IFR, ICR, or IRE)
      - Total_Intercept: Random intercept for this participant in this paradigm (float)
      - Total_Slope: Random slope for this participant in this paradigm (float)
   e. Verify 100 rows per paradigm (all participants present)
2. Concatenate random effects from all 3 paradigms:
   - IFR: 100 rows
   - ICR: 100 rows
   - IRE: 100 rows
   - Total: 300 rows
3. Sort by paradigm, then UID for consistent ordering
4. Save to CSV

**Output:**

**File 1:** data/step04_random_effects.csv
**Format:** CSV with individual random effects
**Columns:**
  - UID (string, participant identifier, e.g., "P001")
  - paradigm (string: IFR, ICR, IRE)
  - Total_Intercept (float, random intercept for participant x paradigm)
  - Total_Slope (float, random slope for participant x paradigm)
**Expected Rows:** 300 rows (100 participants x 3 paradigms)
**Purpose:** REQUIRED for RQ 5.3.8 (Paradigm-Based Clustering analysis)

**File 2:** data/step04_random_effects_descriptives.txt
**Format:** TXT with descriptive statistics
**Contents:**
  - Sample size: 100 participants x 3 paradigms = 300 rows
  - Per-paradigm descriptives:
    - IFR: Mean(Total_Intercept), SD(Total_Intercept), Mean(Total_Slope), SD(Total_Slope)
    - ICR: [same]
    - IRE: [same]
  - Intercept-slope correlation per paradigm (Pearson r, p-value - calculated in Step 5)
  - Cross-paradigm correlations (e.g., IFR slope vs ICR slope - explores consistency)

**Validation Requirement:**

Validation tools MUST be used after random effects extraction. Specific validation tools will be determined by rq_tools based on random effects validation requirements (completeness checks, value range checks, paradigm balance). The rq_analysis agent will embed validation tool calls after the random effects extraction tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step04_random_effects.csv: 300 rows x 4 columns (UID: object, paradigm: object, Total_Intercept: float64, Total_Slope: float64)
- data/step04_random_effects_descriptives.txt exists (>500 bytes)

*Value Ranges:*
- Total_Intercept in [-4, 4] (similar to theta range, random effects should be reasonable)
- Total_Slope unrestricted but typically in [-2, 2] (extreme slopes suggest estimation issues)
- All values finite (no NaN, no Inf)

*Data Quality:*
- Exactly 300 rows (100 participants x 3 paradigms)
- All 3 paradigms present with 100 rows each (balanced)
- All 100 unique UIDs present per paradigm (no missing participants)
- No NaN values in Total_Intercept or Total_Slope columns
- No duplicate UID x paradigm combinations (each participant appears once per paradigm)

*Log Validation:*
- Required pattern: "Random effects extracted for {paradigm}: 100 participants"
- Required pattern: "Total random effects: 300 rows (100 UID x 3 paradigms)"
- Required pattern: "No missing data: all participants present per paradigm"
- Forbidden patterns: "ERROR", "Missing participants", "NaN values detected", "Duplicate UID x paradigm"
- Acceptable warnings: None expected for random effects extraction

**Expected Behavior on Validation Failure:**
- If row count != 300 -> QUIT with "VALIDATION FAILED: Expected 300 rows, found {N}"
- If paradigm counts unbalanced -> QUIT with "VALIDATION FAILED: Paradigm counts not equal (expected 100 each)"
- If missing UIDs -> QUIT with "VALIDATION FAILED: Missing participants in {paradigm}"
- If NaN values -> QUIT with "VALIDATION FAILED: NaN in random effects for {UID} {paradigm}"
- Log error to logs/step04_extract_random_effects.log
- g_debug invoked to diagnose extraction issues

**CRITICAL DEPENDENCY NOTE:**
This output file (data/step04_random_effects.csv) is REQUIRED for downstream RQ 5.3.8 (Paradigm-Based Clustering). RQ 5.3.8 will use the 6 clustering variables (Total_Intercept_IFR, Total_Slope_IFR, Total_Intercept_ICR, Total_Slope_ICR, Total_Intercept_IRE, Total_Slope_IRE) to perform K-means clustering and identify latent profiles of paradigm-specific forgetting patterns.

---

### Step 5: Test Intercept-Slope Correlation Per Paradigm

**Purpose:** Test the correlation between random intercepts and random slopes per paradigm using Pearson correlation with Bonferroni correction (Decision D068: dual p-value reporting). Negative correlation expected (high baseline ability associated with slower forgetting).

**Dependencies:** Step 4 (random effects)

**Complexity:** Low (<5 minutes - correlation tests)

**Input:**

**File 1:** data/step04_random_effects.csv
**Source:** Step 4 output
**Format:** CSV with 300 rows
**Columns:** UID, paradigm, Total_Intercept, Total_Slope
**Purpose:** Random effects for correlation tests

**Processing:**

1. Load random effects per paradigm
2. Stratify by paradigm (IFR, ICR, IRE)
3. For each paradigm:
   a. Extract Total_Intercept and Total_Slope columns (N=100 pairs)
   b. Compute Pearson correlation:
      - r = correlation coefficient
      - p_uncorrected = uncorrected p-value
   c. Apply Bonferroni correction:
      - Number of tests: 15 (per 1_concept.md: 3 paradigms x 5 correlation tests across paradigm types)
      - Bonferroni alpha = 0.05 / 15 = 0.0033
      - p_bonferroni = min(p_uncorrected * 15, 1.0)
   d. Interpretation:
      - If r < 0: Negative correlation (high baseline -> slower forgetting, maintains rank order)
      - If r > 0: Positive correlation (high baseline -> faster forgetting, regression to mean)
      - If p_bonferroni < 0.0033: Significant after correction
4. Report BOTH p_uncorrected and p_bonferroni per Decision D068
5. Compute 95% confidence intervals for r (Fisher z-transformation)

**Output:**

**File 1:** data/step05_intercept_slope_correlation.csv
**Format:** CSV with correlation statistics
**Columns:**
  - paradigm (string: IFR, ICR, IRE)
  - r (float: Pearson correlation coefficient, range [-1, 1])
  - p_uncorrected (float: uncorrected p-value, range [0, 1])
  - p_bonferroni (float: Bonferroni-corrected p-value, range [0, 1])
  - CI_lower (float: lower bound of 95% CI for r)
  - CI_upper (float: upper bound of 95% CI for r)
  - interpretation (string: Negative/Positive/Negligible + Significant/Not Significant)
**Expected Rows:** 3 rows (one per paradigm)
**Example:**
  - IFR, -0.42, 0.0001, 0.0015, -0.58, -0.23, "Negative correlation, significant after Bonferroni"
  - ICR, -0.35, 0.0003, 0.0045, -0.52, -0.15, "Negative correlation, not significant after Bonferroni"
  - IRE, -0.18, 0.08, 1.0, -0.38, 0.04, "Negligible correlation, not significant"

**File 2:** data/step05_correlation_interpretation.txt
**Format:** TXT with interpretation
**Contents:**
  - Correlation table (all 3 paradigms)
  - Interpretation per paradigm:
    - "IFR: r = -0.42 (p_bonferroni = 0.0015): High baseline ability predicts slower forgetting (significant)"
    - [similar for ICR, IRE]
  - Hypothesis evaluation: "Negative intercept-slope correlation found for {N} out of 3 paradigms"
  - Consistency note: "Pattern {consistent/inconsistent} across paradigms"

**Validation Requirement:**

Validation tools MUST be used after correlation tests. Specific validation tools will be determined by rq_tools based on Decision D068 validation requirements (dual p-value presence, Bonferroni correction accuracy, correlation bounds). The rq_analysis agent will embed validation tool calls after the correlation test tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step05_intercept_slope_correlation.csv: 3 rows x 7 columns (paradigm: object, r: float64, p_uncorrected: float64, p_bonferroni: float64, CI_lower: float64, CI_upper: float64, interpretation: object)
- data/step05_correlation_interpretation.txt exists (>300 bytes)

*Value Ranges:*
- r in [-1, 1] (correlation bounds)
- p_uncorrected in [0, 1] (p-value bounds)
- p_bonferroni in [0, 1] (p-value bounds)
- p_bonferroni >= p_uncorrected (correction cannot decrease p-value)
- CI_lower < CI_upper (confidence interval ordering)
- CI_lower >= -1, CI_upper <= 1 (correlation bounds apply to CIs)

*Data Quality:*
- Exactly 3 rows (one per paradigm)
- All 3 paradigms present
- BOTH p_uncorrected AND p_bonferroni present (Decision D068 compliance)
- No NaN values in any column

*Log Validation:*
- Required pattern: "Correlation test for {paradigm}: r={value}, p_uncorrected={value}, p_bonferroni={value}"
- Required pattern: "Decision D068 compliance: dual p-values reported"
- Required pattern: "Bonferroni correction applied: alpha=0.0033 (0.05 / 15 tests)"
- Forbidden patterns: "ERROR", "Missing p-value", "Bonferroni not applied"
- Acceptable warnings: None expected for correlation tests

**Expected Behavior on Validation Failure:**
- If r out of range -> QUIT with "VALIDATION FAILED: Correlation r out of [-1, 1] for {paradigm}"
- If p_bonferroni < p_uncorrected -> QUIT with "VALIDATION FAILED: Bonferroni correction error for {paradigm}"
- If missing p-value -> QUIT with "VALIDATION FAILED: Decision D068 violation - missing {p_uncorrected/p_bonferroni}"
- If row count != 3 -> QUIT with "VALIDATION FAILED: Expected 3 correlation tests, found {N}"
- Log error to logs/step05_correlation_tests.log
- g_debug invoked to diagnose correlation computation issues

---

### Step 6: Compare ICC Across Paradigms and Prepare Barplot Data

**Purpose:** Create paradigm ICC comparison table and prepare plot source CSV for barplot visualization showing ICC_slope_conditional for all three paradigms with 95% confidence intervals (Option B architecture - plot data preparation during analysis, visualization by rq_plots later).

**Dependencies:** Step 3 (ICC estimates), Step 2 (variance components for CI computation)

**Complexity:** Low (<5 minutes - ICC comparison and barplot data preparation)

**Input:**

**File 1:** data/step03_icc_estimates.csv
**Source:** Step 3 output
**Format:** CSV with 9 rows (3 paradigms x 3 ICC types)
**Columns:** paradigm, icc_type, icc_value, interpretation
**Purpose:** ICC estimates for comparison

**File 2:** data/step02_variance_components.csv
**Source:** Step 2 output
**Format:** CSV with 15 rows (3 paradigms x 5 components)
**Purpose:** Variance components for computing ICC confidence intervals (bootstrap or delta method)

**Processing:**

1. Load ICC estimates and variance components
2. Filter to ICC_slope_conditional only (primary ICC for hypothesis testing)
3. For each paradigm:
   a. Extract ICC_slope_conditional value
   b. Compute 95% confidence interval (bootstrap or delta method)
      - Method 1 (Bootstrap): Resample random effects 1000 times, recompute ICC, take 2.5th and 97.5th percentiles
      - Method 2 (Delta method): Approximate SE of ICC using variance-covariance matrix, compute CI = ICC +/- 1.96*SE
   c. Interpretation label from Step 3 (Low, Moderate, Substantial)
4. Create comparison table:
   - Paradigm ordering: IFR, ICR, IRE (consistent with retrieval support gradient)
   - ICC_slope_conditional values with CIs
   - Interpretation labels
5. Prepare barplot data for rq_plots:
   - Columns: paradigm, icc_value, CI_lower, CI_upper, interpretation
   - Format: CSV with 3 rows (ready for plotting)

**Output:**

**File 1:** data/step06_paradigm_icc_comparison.csv
**Format:** CSV with ICC comparison
**Columns:**
  - paradigm (string: IFR, ICR, IRE)
  - icc_slope_conditional (float: [0, 1] range)
  - CI_lower (float: lower 95% CI)
  - CI_upper (float: upper 95% CI)
  - interpretation (string: Low, Moderate, Substantial)
  - rank (int: 1-3, paradigm ordering by ICC magnitude)
**Expected Rows:** 3 rows (one per paradigm)
**Example:**
  - IFR, 0.45, 0.38, 0.52, Substantial, 1
  - ICR, 0.38, 0.30, 0.46, Moderate, 2
  - IRE, 0.28, 0.19, 0.37, Moderate, 3

**File 2:** data/step06_paradigm_icc_barplot_data.csv
**Format:** CSV, plot source data for barplot (Option B: rq_plots reads this)
**Columns:**
  - paradigm (string: IFR, ICR, IRE)
  - icc_value (float: ICC_slope_conditional)
  - CI_lower (float: lower 95% CI)
  - CI_upper (float: upper 95% CI)
  - interpretation (string: Low/Moderate/Substantial for color coding)
**Expected Rows:** 3 rows (one per paradigm)
**Purpose:** Plot source CSV for barplot showing ICC across paradigms with error bars

**File 3:** data/step06_comparison_interpretation.txt
**Format:** TXT with interpretation
**Contents:**
  - ICC comparison table (all 3 paradigms)
  - Paradigm ordering by ICC magnitude: "IFR > ICR > IRE" or "No significant differences"
  - Hypothesis evaluation:
    - "ICC > 0.40 threshold met for {N} out of 3 paradigms"
    - "Substantial between-person variance found in: {paradigm list}"
  - Paradigm-specific interpretations:
    - "IFR: Highest ICC (0.45) - strongest trait-like stability"
    - "ICR: Moderate ICC (0.38) - intermediate stability"
    - "IRE: Lowest ICC (0.28) - ceiling effects may attenuate individual differences"
  - Limitations note: "Ceiling effects in Recognition may compress variance; practice effects may inflate within-person variance"

**Validation Requirement:**

Validation tools MUST be used after ICC comparison and plot data preparation. Specific validation tools will be determined by rq_tools based on plot data validation requirements (completeness checks, value range checks, CI ordering). The rq_analysis agent will embed validation tool calls after the plot data preparation tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step06_paradigm_icc_comparison.csv: 3 rows x 6 columns (paradigm: object, icc_slope_conditional: float64, CI_lower: float64, CI_upper: float64, interpretation: object, rank: int64)
- data/step06_paradigm_icc_barplot_data.csv: 3 rows x 5 columns (paradigm: object, icc_value: float64, CI_lower: float64, CI_upper: float64, interpretation: object)
- data/step06_comparison_interpretation.txt exists (>400 bytes)

*Value Ranges:*
- icc_slope_conditional in [0, 1] (ICC bounds)
- CI_lower in [0, 1], CI_upper in [0, 1] (CI bounds)
- CI_lower < icc_slope_conditional < CI_upper (ordering)
- rank in [1, 2, 3] (ordinal ranking)

*Data Quality:*
- Exactly 3 rows in both CSV files (one per paradigm)
- All 3 paradigms present (IFR, ICR, IRE)
- No NaN values in any column
- Interpretation labels consistent with Step 3
- CI_upper > CI_lower for all paradigms (confidence interval ordering)
- Plot data ready for rq_plots (correct column names, no missing values)

*Log Validation:*
- Required pattern: "ICC comparison table created for 3 paradigms"
- Required pattern: "Plot data prepared: 3 rows (IFR, ICR, IRE)"
- Required pattern: "All confidence intervals valid (CI_lower < ICC < CI_upper)"
- Forbidden patterns: "ERROR", "Missing paradigm", "CI ordering violation", "NaN values"
- Acceptable warnings: None expected for comparison table creation

**Expected Behavior on Validation Failure:**
- If row count != 3 -> QUIT with "VALIDATION FAILED: Expected 3 paradigms, found {N}"
- If CI ordering violated -> QUIT with "VALIDATION FAILED: CI_lower >= ICC or ICC >= CI_upper for {paradigm}"
- If missing paradigm -> QUIT with "VALIDATION FAILED: Paradigm {IFR/ICR/IRE} missing from comparison"
- If NaN values -> QUIT with "VALIDATION FAILED: NaN in comparison table or plot data"
- Log error to logs/step06_compare_icc_prepare_plot.log
- g_debug invoked to diagnose comparison issues

**Plot Specification for rq_plots (Phase 2 - Visualization):**

**Plot Description:** Barplot showing ICC_slope_conditional for Free Recall, Cued Recall, and Recognition paradigms with 95% confidence interval error bars. X-axis = Paradigm (IFR, ICR, IRE), Y-axis = ICC (0 to 1). Horizontal reference line at ICC = 0.40 (threshold for substantial between-person variance). Bars colored by interpretation (Low=red, Moderate=yellow, Substantial=green).

**Source CSV:** data/step06_paradigm_icc_barplot_data.csv
**Required Columns:** paradigm, icc_value, CI_lower, CI_upper, interpretation
**Plotting Function:** Barplot with error bars (rq_plots maps to plotting function from tools/plots.py)

**NOTE:** PNG output (plots/step06_paradigm_icc_barplot.png) will be created by rq_plots agent during visualization phase (Step 17 workflow), NOT during this analysis step.

---

## Expected Data Formats

### Composite_ID Structure (Inherited from RQ 5.3.1)

**Format:** {UID}_{test}
**Example:** P001_T1 (Participant 001, Test session 1)
**Components:**
- UID: Participant unique identifier (P### with leading zeros, e.g., P001, P042)
- test: Test session (T1, T2, T3, T4 for Days 0, 1, 3, 6)

**Usage:** Composite_ID is the primary key in long-format data (step00_theta_scores_validated.csv), linking participants to test sessions.

### Paradigm Categories

**Values:** IFR, ICR, IRE (3 categories)
**Mapping:**
- IFR: Item Free Recall (self-initiated retrieval, minimal support)
- ICR: Item Cued Recall (cued retrieval, moderate support)
- IRE: Item Recognition (recognition memory, maximal support)

**Exclusions:** Passive paradigms (RFR, TCR, RRE) excluded per Chapter 5 focus on interactive VR experience (inherited from RQ 5.3.1).

### Variance Component Structure

**Components (per paradigm):**
- var_intercept: Between-person variance in baseline ability (Day 0)
- var_slope: Between-person variance in forgetting rate (slope over time)
- cov_int_slope: Covariance between intercept and slope (can be negative)
- corr_int_slope: Correlation between intercept and slope (cov / sqrt(var_int * var_slope))
- var_residual: Within-person residual variance (measurement error + state fluctuation)

**Interpretation:**
- High var_intercept: Large individual differences in baseline memory
- High var_slope: Large individual differences in forgetting rate
- Negative cov_int_slope: High baseline predicts slower forgetting (maintains rank order)
- High ICC: Forgetting is trait-like (stable individual differences)

### ICC Types

**Three ICC estimates per paradigm:**

1. **ICC_intercept:** Proportion of baseline ability variance due to person
   - Formula: var_intercept / (var_intercept + var_residual)
   - Interpretation: Trait-like stability of baseline memory at Day 0

2. **ICC_slope_simple:** Proportion of slope variance due to person (unconditional)
   - Formula: var_slope / (var_slope + var_residual)
   - Interpretation: Simplified estimate of forgetting rate stability

3. **ICC_slope_conditional:** Proportion of variance at Day 6 due to person (conditional)
   - Formula: [var_intercept + 2*cov_int_slope*Time + var_slope*(Time^2)] / [... + var_residual]
   - Interpretation: Most accurate estimate of forgetting rate stability at specific timepoint
   - **Primary ICC for hypothesis testing:** ICC > 0.40 threshold

**Scale Note:** If Time uses log transformation (per RQ 5.3.1 best model), ICC computed on log-time scale. Interpretation is for rate of forgetting on log-transformed time scale (appropriate because log-time is where linear assumptions hold).

### Time Variable (Inherited from RQ 5.3.1)

**TSVR_hours:** Time Since VR in hours (actual elapsed time, NOT nominal days)
- Per Decision D070: Use actual time for precision
- T1 (Day 0): TSVR ~= 0 hours (encoding)
- T2 (Day 1): TSVR ~= 24 hours (varies by participant schedule)
- T3 (Day 3): TSVR ~= 72 hours
- T4 (Day 6): TSVR ~= 144 hours

**Functional Form:** Determined by RQ 5.3.1 best model (likely log-transformed per Chapter 5 pattern)

---

## Cross-RQ Dependencies

### Dependency Type: DERIVED Data from Other RQs

**This RQ requires outputs from:**

**RQ 5.3.1 (Paradigm-Specific Trajectories - ROOT RQ for paradigm analysis)**

**Required Files:**
1. results/ch5/5.3.1/data/step04_lmm_input.csv
   - Used in: Step 0 (data loading)
   - Contents: Theta scores in long format (1200 rows: 100 participants x 4 tests x 3 paradigms)
   - Columns: composite_ID, UID, test, TSVR_hours, paradigm, theta, se
   - Rationale: RQ 5.3.1 performs IRT calibration on paradigm-specific items and creates 3-dimensional theta scores. This RQ uses those theta scores to decompose variance per paradigm.

2. results/ch5/5.3.1/data/step05_lmm_fitted_model.pkl
   - Used in: Step 1 (model metadata loading)
   - Contents: Best-fitting LMM model with Paradigm x Time interaction
   - Rationale: Identify functional form (Linear, Log, Quadratic, etc.) to apply same time transformation in paradigm-stratified models

3. results/ch5/5.3.1/data/step05_model_comparison.csv
   - Used in: Step 1 (model metadata loading)
   - Contents: AIC comparison of 5 candidate models
   - Rationale: Verify which model was selected as best (used for consistency in paradigm-stratified models)

**Execution Order Constraint:**
1. RQ 5.3.1 must complete Steps 1-5 (IRT calibration, TSVR merge, LMM fitting, model selection)
2. This RQ (5.3.7) executes after RQ 5.3.1 completes
3. This RQ does NOT re-calibrate IRT models (uses existing theta scores from RQ 5.3.1)

**Data Source Boundaries:**
- **RAW data:** None (this RQ uses only DERIVED data from RQ 5.3.1)
- **DERIVED data:** Theta scores and LMM model from RQ 5.3.1
- **Scope:** This RQ focuses on variance decomposition ONLY (no new IRT calibration, no new trajectory modeling, just stratified LMMs and ICC computation)

**Validation:**
- Step 0: Check results/ch5/5.3.1/data/step04_lmm_input.csv exists (circuit breaker: EXPECTATIONS ERROR if absent)
- Step 1: Check results/ch5/5.3.1/data/step05_lmm_fitted_model.pkl exists (circuit breaker: EXPECTATIONS ERROR if absent)
- If either file missing -> QUIT with error -> user must execute RQ 5.3.1 first

---

## Validation Requirements

### CRITICAL MANDATE

Every analysis step in this plan MUST use validation tools after analysis tool execution.

This is not optional. This is the core architectural principle preventing cascading failures observed in v3.0 (where analysis errors propagated undetected through 5+ downstream steps before discovery).

**Exact Specification Requirement:**

> "Validation tools MUST be used after analysis tool execution"

**Implementation:**
- rq_tools (Step 11 workflow) will read tools_inventory.md validation tools section
- rq_tools will specify BOTH analysis tool + validation tool per step in 3_tools.yaml
- rq_analysis (Step 12 workflow) will embed validation tool call AFTER analysis tool call in 4_analysis.yaml
- g_code (Step 14 workflow) will generate stepN_name.py scripts with validation function calls
- bash execution (Step 14 workflow) will run analysis -> validation -> error on validation failure

**Downstream Agent Requirements:**
- **rq_tools:** MUST specify validation tool for EVERY analysis step (no exceptions)
- **rq_analysis:** MUST embed validation tool call for EVERY analysis step (no exceptions)
- **g_code:** MUST generate code with validation function calls (no exceptions)
- **rq_inspect:** MUST verify validation ran successfully (checks logs/stepN_name.log for validation output)

### Validation Requirements By Step

#### Step 0: Load Theta Scores from RQ 5.3.1

**Analysis Tool:** (determined by rq_tools - likely tools.data.read_csv with validation wrapper)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_data_columns + validate_numeric_range)

**What Validation Checks:**
- Output file exists (data/step00_theta_scores_validated.csv)
- Expected row count (1200 rows: 100 participants x 4 tests x 3 paradigms)
- Expected column count (7 columns: composite_ID, UID, test, TSVR_hours, paradigm, theta, se)
- Required columns present (composite_ID, UID, test, TSVR_hours, paradigm, theta, se)
- No missing data (0 NaN values in theta, TSVR_hours, paradigm columns)
- Paradigm categories correct (IFR, ICR, IRE with 400 rows each)
- Test categories correct (T1, T2, T3, T4 with 300 rows each)
- Theta values in valid range ([-4, 4])
- SE values positive and reasonable ([0.1, 1.5])
- TSVR values reasonable ([0, 200] hours)
- Balanced design (all 100 participants have 12 rows)

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Expected 1200 rows, found 987")
- Log failure to logs/step00_load_theta_scores.log
- Quit script immediately (do NOT proceed to Step 1)
- g_debug invoked by master to diagnose RQ 5.3.1 output issues

---

#### Step 1: Load RQ 5.3.1 Best-Fitting Model Metadata

**Analysis Tool:** (determined by rq_tools - likely custom model metadata extraction)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_model_convergence + custom checks)

**What Validation Checks:**
- Model file exists (results/ch5/5.3.1/data/step05_lmm_fitted_model.pkl)
- Model loaded successfully (pickle deserialization successful)
- Model converged (model.converged = True)
- Paradigm factor present in model formula
- Functional form identified (Linear, Log, Quadratic, etc.)
- Paradigm categories verified (IFR, ICR, IRE)
- Paradigm counts balanced (400 observations each)
- Metadata YAML file created (data/step01_model_metadata.yaml)
- Paradigm categories CSV created (data/step01_paradigm_categories.csv with 3 rows)

**Expected Behavior on Validation Failure:**
- Raise error with specific failure (e.g., "Model file missing from RQ 5.3.1")
- Log failure to logs/step01_load_model_metadata.log
- Quit script immediately
- g_debug invoked to diagnose RQ 5.3.1 model issues

---

#### Step 2: Fit Paradigm-Stratified LMMs with Random Slopes

**Analysis Tool:** (determined by rq_tools - likely tools.analysis_lmm.fit_lmm_trajectory_tsvr applied 3 times)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_lmm_convergence + validate_lmm_assumptions_comprehensive)

**What Validation Checks (TECHNICAL - per paradigm):**
- Model converged (model.converged = True) OR convergence contingency plan applied successfully
- Variance components extracted (5 components per paradigm)
- All variance components positive (var_intercept > 0, var_slope > 0, var_residual > 0)
- Correlation in valid range (corr_int_slope in [-1, 1])
- Model summary file created (.pkl file >1KB)
- Variance components CSV has 15 rows (3 paradigms x 5 components)
- No NaN values in variance components

**What Validation Checks (ASSUMPTION - per 1_concept.md - MANDATORY):**
1. Residual Normality: Q-Q plot + Shapiro-Wilk test (accept if p > 0.01)
2. Homoscedasticity: Residuals vs fitted plot; Levene's test by test session
3. Random Effects Normality: Q-Q plot of random intercept and slope estimates
4. Independence: ACF plot of residuals (no significant autocorrelation)
5. Linearity: Residuals vs Time predictor (no systematic patterns)
6. Outliers: Cook's distance < 4/N threshold (N=100 per paradigm)

**Remedial Actions (if assumptions violated):**
- If normality violated: Report robust standard errors; note impact on variance component estimates
- If heteroscedasticity: Consider variance function by test session
- If outliers detected: Sensitivity analysis excluding influential participants; report ICC with and without outliers
- Document all violations and their potential impact on ICC interpretation

**Expected Behavior on Validation Failure:**
- Raise error with specific failure (e.g., "All 3 paradigm models failed to converge")
- Log failure to logs/step02_fit_paradigm_lmms.log
- Quit script immediately
- g_debug invoked to diagnose convergence or assumption violations

---

#### Step 3: Compute Intraclass Correlation Coefficients (ICC) Per Paradigm

**Analysis Tool:** (determined by rq_tools - likely tools.analysis_lmm.compute_icc_from_variance_components)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_icc_bounds)

**What Validation Checks:**
- Output file exists (data/step03_icc_estimates.csv)
- Expected row count (9 rows: 3 paradigms x 3 ICC types)
- Expected column count (4 columns: paradigm, icc_type, icc_value, interpretation)
- All ICC values in [0, 1] range (no negative ICC, no ICC > 1)
- No NaN values in icc_value column
- All 3 ICC types present per paradigm (intercept, slope_simple, slope_conditional)
- Interpretation labels correct (Low < 0.20, Moderate 0.20-0.40, Substantial >= 0.40)

**Expected Behavior on Validation Failure:**
- Raise error with specific failure (e.g., "ICC out of range for IFR slope_conditional")
- Log failure to logs/step03_compute_icc.log
- Quit script immediately
- g_debug invoked to diagnose ICC computation errors

---

#### Step 4: Extract Individual Random Effects Per Paradigm

**Analysis Tool:** (determined by rq_tools - likely tools.analysis_lmm.extract_random_effects_from_lmm applied 3 times)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_dataframe_structure + custom checks)

**What Validation Checks:**
- Output file exists (data/step04_random_effects.csv)
- Expected row count (300 rows: 100 participants x 3 paradigms)
- Expected column count (4 columns: UID, paradigm, Total_Intercept, Total_Slope)
- All 100 unique UIDs present (no missing participants)
- All 3 paradigms present with 100 rows each (balanced)
- No NaN values in Total_Intercept or Total_Slope columns
- No duplicate UID x paradigm combinations
- Random effects in reasonable range (Total_Intercept in [-4, 4], Total_Slope typically in [-2, 2])

**CRITICAL DEPENDENCY CHECK:**
- Verify output exists for downstream RQ 5.3.8 dependency
- Log confirmation: "Random effects exported for RQ 5.3.8 clustering analysis"

**Expected Behavior on Validation Failure:**
- Raise error with specific failure (e.g., "Expected 300 rows, found 287")
- Log failure to logs/step04_extract_random_effects.log
- Quit script immediately
- g_debug invoked to diagnose extraction issues

---

#### Step 5: Test Intercept-Slope Correlation Per Paradigm

**Analysis Tool:** (determined by rq_tools - likely tools.analysis_lmm.test_intercept_slope_correlation_d068)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_correlation_test_d068)

**What Validation Checks (DECISION D068 - MANDATORY):**
- Output file exists (data/step05_intercept_slope_correlation.csv)
- Expected row count (3 rows: one per paradigm)
- BOTH p_uncorrected AND p_bonferroni present (Decision D068 compliance)
- p_bonferroni >= p_uncorrected (correction cannot decrease p-value)
- Correlation coefficient in [-1, 1] range
- Confidence intervals valid (CI_lower < CI_upper, both in [-1, 1])
- Bonferroni correction accurate (p_bonferroni = min(p_uncorrected * 15, 1.0))
- All 3 paradigms present

**Expected Behavior on Validation Failure:**
- Raise error with specific failure (e.g., "Decision D068 violation: missing p_bonferroni")
- Log failure to logs/step05_correlation_tests.log
- Quit script immediately
- g_debug invoked to diagnose correlation test issues

---

#### Step 6: Compare ICC Across Paradigms and Prepare Barplot Data

**Analysis Tool:** (determined by rq_tools - likely custom ICC comparison + plot data preparation)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_plot_data_completeness + validate_dataframe_structure)

**What Validation Checks:**
- Output files exist (data/step06_paradigm_icc_comparison.csv, data/step06_paradigm_icc_barplot_data.csv)
- Expected row count (3 rows per file: one per paradigm)
- All 3 paradigms present (IFR, ICR, IRE)
- ICC values in [0, 1] range
- Confidence intervals valid (CI_lower < icc_value < CI_upper, all in [0, 1])
- No NaN values in any column
- Interpretation labels consistent with Step 3
- Plot data ready for rq_plots (correct column names, no missing values)

**Expected Behavior on Validation Failure:**
- Raise error with specific failure (e.g., "CI ordering violation for ICR")
- Log failure to logs/step06_compare_icc_prepare_plot.log
- Quit script immediately
- g_debug invoked to diagnose comparison issues

---

## Summary

**Total Steps:** 7 analysis steps (Step 0 through Step 6)

**Estimated Runtime:** Medium (~30-60 minutes total)
- Step 0: Low (<5 min - data loading)
- Step 1: Low (<5 min - metadata loading)
- Step 2: High (30-45 min - 3 LMM fits with random slopes)
- Step 3: Low (<5 min - ICC computation)
- Step 4: Low (<5 min - random effects extraction)
- Step 5: Low (<5 min - correlation tests)
- Step 6: Low (<5 min - ICC comparison and plot data)

**Cross-RQ Dependencies:** RQ 5.3.1 (Paradigm-Specific Trajectories) must complete Steps 1-5 before this RQ can execute

**Primary Outputs:**
- data/step02_variance_components.csv (15 rows: 5 components x 3 paradigms)
- data/step03_icc_estimates.csv (9 rows: 3 ICC types x 3 paradigms)
- data/step04_random_effects.csv (300 rows: 100 participants x 3 paradigms - REQUIRED for RQ 5.3.8)
- data/step05_intercept_slope_correlation.csv (3 rows: correlation tests per paradigm with Decision D068 dual p-values)
- data/step06_paradigm_icc_barplot_data.csv (plot source CSV for barplot)

**Validation Coverage:** 100% (all 7 steps have validation requirements with substance criteria documented)

**Decisions Applied:**
- Decision D068: Dual p-value reporting (uncorrected + Bonferroni) for correlation tests
- Decision D070: TSVR_hours as time variable (inherited from RQ 5.3.1)
- Bates et al. (2015): Convergence contingency plan if random slopes fail

**Limitations Acknowledged:**
- Practice effects may inflate within-person variance (ICC interpreted as lower bound)
- Ceiling effects in Recognition may compress variance
- Dropout bias may bias ICC upward if poor performers missing

---

## Next Steps (Workflow)

1. User reviews and approves this plan (Step 7 user gate)
2. Workflow continues to Step 11: rq_tools reads this plan -> creates 3_tools.yaml
3. Workflow continues to Step 12: rq_analysis reads this plan + 3_tools.yaml -> creates 4_analysis.yaml
4. Workflow continues to Step 14: g_code reads 4_analysis.yaml -> generates stepN_name.py scripts
5. Workflow continues to Step 17: rq_plots reads plot source CSVs -> generates barplot PNG

---

**Version History:**
- v1.0 (2025-12-02): Initial plan created by rq_planner agent for RQ 5.3.7

---

**End of Analysis Plan**
