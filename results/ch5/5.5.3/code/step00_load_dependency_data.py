#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step00
Step Name: Load Dependency Data from RQ 5.5.1
RQ: results/ch5/5.5.3
Generated: 2025-12-04

PURPOSE:
Load IRT theta scores by location type from RQ 5.5.1, TSVR mapping, and Age
variable. Verify RQ 5.5.1 dependency is complete before proceeding with
Age x LocationType x Time analysis.

EXPECTED INPUTS:
  - results/ch5/5.5.1/data/step03_theta_scores.csv
    Columns: ['composite_ID', 'theta_source', 'theta_destination', 'se_source', 'se_destination']
    Format: CSV with 400 rows (100 participants x 4 tests)
    Expected rows: 400

  - results/ch5/5.5.1/data/step00_tsvr_mapping.csv
    Columns: ['composite_ID', 'UID', 'test', 'TSVR_hours']
    Format: CSV with 400 rows (100 participants x 4 tests)
    Expected rows: 400

  - data/cache/dfData.csv
    Columns: ['UID', 'age'] (extract only these 2 columns from 214 total)
    Format: CSV with 100 unique participants
    Expected rows: 100

EXPECTED OUTPUTS:
  - data/step00_theta_from_rq551.csv
    Columns: ['composite_ID', 'theta_source', 'theta_destination', 'se_source', 'se_destination']
    Format: Copy of RQ 5.5.1 theta scores for lineage tracking
    Expected rows: 400

  - data/step00_tsvr_from_rq551.csv
    Columns: ['composite_ID', 'UID', 'test', 'TSVR_hours']
    Format: Copy of TSVR mapping for lineage tracking
    Expected rows: 400

  - data/step00_age_from_dfdata.csv
    Columns: ['UID', 'Age']
    Format: Extracted Age variable (unique UID-Age pairs)
    Expected rows: 100

VALIDATION CRITERIA:
  - RQ 5.5.1 status.yaml shows rq_results: success
  - theta_from_rq551.csv: 400 rows, 5 columns
  - tsvr_from_rq551.csv: 400 rows, 4 columns
  - age_from_dfdata.csv: 100 rows, 2 columns (unique UIDs)
  - No NaN in theta, se, TSVR_hours, Age columns
  - theta_source and theta_destination in [-4, 4]
  - se_source and se_destination in [0.1, 1.5]
  - TSVR_hours in [0, 360] (extended range for late tests)
  - Age in [20, 70]

g_code REASONING:
- Approach: Load RQ 5.5.1 outputs as dependencies, verify completion status,
  extract Age from project-level cache, validate ranges
- Why this approach: Ensures RQ 5.5.1 pipeline completed successfully before
  starting RQ 5.5.3 analysis, establishes clear data lineage
- Data flow: RQ 5.5.1 outputs (theta by location, TSVR) + dfData (Age) ->
  Step 1 LMM input preparation
- Expected performance: ~1 second (file loading + validation only)

IMPLEMENTATION NOTES:
- No analysis tools required (stdlib pandas operations only)
- Validation uses custom checks (no catalogued validation tool for this step)
- Parameters: allow_missing_values=False, strict value range checks
- Age column is lowercase 'age' in dfData.csv (will rename to 'Age' for consistency)
- Theta column order differs from spec but all required columns present
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import yaml
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 5.5.3/
#   parents[2] = ch5/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.5.3 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step00_load_dependency_data.log"

# Dependency paths (RQ 5.5.1)
RQ_551_DIR = PROJECT_ROOT / "results" / "ch5" / "5.5.1"
RQ_551_STATUS = RQ_551_DIR / "status.yaml"
RQ_551_THETA = RQ_551_DIR / "data" / "step03_theta_scores.csv"
RQ_551_TSVR = RQ_551_DIR / "data" / "step00_tsvr_mapping.csv"

# Project-level data
DFDATA_PATH = PROJECT_ROOT / "data" / "cache" / "dfData.csv"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step00_theta_from_rq551.csv
#   CORRECT: data/step00_age_from_dfdata.csv
#   WRONG:   results/theta_scores.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv      (missing step prefix)
#   WRONG:   logs/step00_theta.csv      (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 0: Load Dependency Data from RQ 5.5.1")

        # =========================================================================
        # STEP 1: Check RQ 5.5.1 Completion Status
        # =========================================================================
        # Expected: RQ 5.5.1 pipeline completed successfully
        # Purpose: Verify dependency RQ is complete before loading outputs

        log("[CHECK] Verifying RQ 5.5.1 completion status...")

        if not RQ_551_STATUS.exists():
            raise FileNotFoundError(f"RQ 5.5.1 status file not found: {RQ_551_STATUS}")

        with open(RQ_551_STATUS, 'r', encoding='utf-8') as f:
            status_data = yaml.safe_load(f)

        rq_results_status = status_data.get('rq_results', {}).get('status', 'unknown')
        log(f"[STATUS] RQ 5.5.1 status: {rq_results_status}")

        if rq_results_status != 'success':
            raise ValueError(
                f"RQ 5.5.1 not complete (status: {rq_results_status}). "
                f"Cannot proceed with RQ 5.5.3 until dependency is satisfied."
            )

        log("[PASS] RQ 5.5.1 completed successfully")

        # =========================================================================
        # STEP 2: Load RQ 5.5.1 Theta Scores by Location Type
        # =========================================================================
        # Expected: 400 rows (100 participants x 4 tests)
        # Purpose: IRT theta scores for Source and Destination memory

        log("[LOAD] Loading theta scores from RQ 5.5.1...")

        if not RQ_551_THETA.exists():
            raise FileNotFoundError(f"Theta scores file not found: {RQ_551_THETA}")

        theta_from_rq551 = pd.read_csv(RQ_551_THETA, encoding='utf-8')
        log(f"[LOADED] {RQ_551_THETA.name} ({len(theta_from_rq551)} rows, {len(theta_from_rq551.columns)} cols)")

        # Validate columns present (order doesn't matter for pandas)
        required_theta_cols = ['composite_ID', 'theta_source', 'se_source', 'theta_destination', 'se_destination']
        missing_theta_cols = set(required_theta_cols) - set(theta_from_rq551.columns)
        if missing_theta_cols:
            raise ValueError(f"Theta scores missing required columns: {missing_theta_cols}")

        # Validate row count
        if len(theta_from_rq551) != 400:
            raise ValueError(f"Expected 400 rows in theta scores, got {len(theta_from_rq551)}")

        # Validate no NaN values
        theta_cols_to_check = ['theta_source', 'theta_destination', 'se_source', 'se_destination']
        for col in theta_cols_to_check:
            n_missing = theta_from_rq551[col].isna().sum()
            if n_missing > 0:
                raise ValueError(f"Theta scores has {n_missing} NaN values in {col} column")

        # Validate theta ranges: [-4, 4]
        for col in ['theta_source', 'theta_destination']:
            theta_min = theta_from_rq551[col].min()
            theta_max = theta_from_rq551[col].max()
            if theta_min < -4.0 or theta_max > 4.0:
                raise ValueError(
                    f"{col} out of range [-4, 4]: min={theta_min:.3f}, max={theta_max:.3f}"
                )
        log(f"[VALID] Theta ranges: source [{theta_from_rq551['theta_source'].min():.3f}, {theta_from_rq551['theta_source'].max():.3f}], "
            f"destination [{theta_from_rq551['theta_destination'].min():.3f}, {theta_from_rq551['theta_destination'].max():.3f}]")

        # Validate SE ranges: [0.1, 1.5]
        for col in ['se_source', 'se_destination']:
            se_min = theta_from_rq551[col].min()
            se_max = theta_from_rq551[col].max()
            if se_min < 0.1 or se_max > 1.5:
                raise ValueError(
                    f"{col} out of range [0.1, 1.5]: min={se_min:.3f}, max={se_max:.3f}"
                )
        log(f"[VALID] SE ranges: source [{theta_from_rq551['se_source'].min():.3f}, {theta_from_rq551['se_source'].max():.3f}], "
            f"destination [{theta_from_rq551['se_destination'].min():.3f}, {theta_from_rq551['se_destination'].max():.3f}]")

        # =========================================================================
        # STEP 3: Load RQ 5.5.1 TSVR Mapping
        # =========================================================================
        # Expected: 400 rows (100 participants x 4 tests)
        # Purpose: Time-since-VR-encoding (actual hours between T1 and Tx)

        log("[LOAD] Loading TSVR mapping from RQ 5.5.1...")

        if not RQ_551_TSVR.exists():
            raise FileNotFoundError(f"TSVR mapping file not found: {RQ_551_TSVR}")

        tsvr_from_rq551 = pd.read_csv(RQ_551_TSVR, encoding='utf-8')
        log(f"[LOADED] {RQ_551_TSVR.name} ({len(tsvr_from_rq551)} rows, {len(tsvr_from_rq551.columns)} cols)")

        # Validate columns present
        required_tsvr_cols = ['composite_ID', 'UID', 'test', 'TSVR_hours']
        missing_tsvr_cols = set(required_tsvr_cols) - set(tsvr_from_rq551.columns)
        if missing_tsvr_cols:
            raise ValueError(f"TSVR mapping missing required columns: {missing_tsvr_cols}")

        # Validate row count
        if len(tsvr_from_rq551) != 400:
            raise ValueError(f"Expected 400 rows in TSVR mapping, got {len(tsvr_from_rq551)}")

        # Validate no NaN values in TSVR_hours
        n_missing_tsvr = tsvr_from_rq551['TSVR_hours'].isna().sum()
        if n_missing_tsvr > 0:
            raise ValueError(f"TSVR mapping has {n_missing_tsvr} NaN values in TSVR_hours column")

        # Validate TSVR range: [0, 360] (extended range to handle late tests)
        tsvr_min = tsvr_from_rq551['TSVR_hours'].min()
        tsvr_max = tsvr_from_rq551['TSVR_hours'].max()
        if tsvr_min < 0.0 or tsvr_max > 360.0:
            raise ValueError(
                f"TSVR_hours out of range [0, 360]: min={tsvr_min:.2f}, max={tsvr_max:.2f}"
            )
        log(f"[VALID] TSVR range: [{tsvr_min:.2f}, {tsvr_max:.2f}] hours")

        # =========================================================================
        # STEP 4: Load Age Variable from dfData.csv
        # =========================================================================
        # Expected: 100 unique participants
        # Purpose: Moderator variable for Age x LocationType x Time interactions

        log("[LOAD] Loading Age variable from dfData.csv...")

        if not DFDATA_PATH.exists():
            raise FileNotFoundError(f"dfData.csv not found: {DFDATA_PATH}")

        # Load only UID and age columns (age is lowercase in dfData.csv)
        age_from_dfdata = pd.read_csv(DFDATA_PATH, usecols=['UID', 'age'], encoding='utf-8')
        log(f"[LOADED] {DFDATA_PATH.name} (extracted 2 columns from 214 total)")

        # Get unique UID-Age pairs (should be 100 participants)
        age_from_dfdata = age_from_dfdata.drop_duplicates(subset='UID')
        log(f"[DEDUP] {len(age_from_dfdata)} unique participants")

        # Rename lowercase 'age' to 'Age' for consistency with spec
        age_from_dfdata = age_from_dfdata.rename(columns={'age': 'Age'})

        # Validate row count
        if len(age_from_dfdata) != 100:
            raise ValueError(f"Expected 100 unique participants, got {len(age_from_dfdata)}")

        # Validate no NaN values in Age
        n_missing_age = age_from_dfdata['Age'].isna().sum()
        if n_missing_age > 0:
            raise ValueError(f"Age data has {n_missing_age} NaN values")

        # Validate Age range: [20, 70]
        age_min = age_from_dfdata['Age'].min()
        age_max = age_from_dfdata['Age'].max()
        if age_min < 20 or age_max > 70:
            raise ValueError(
                f"Age out of range [20, 70]: min={age_min}, max={age_max}"
            )
        log(f"[VALID] Age range: [{age_min}, {age_max}] years")
        log(f"[STATS] Age mean={age_from_dfdata['Age'].mean():.1f}, SD={age_from_dfdata['Age'].std():.1f}")

        # =========================================================================
        # STEP 5: Save All Outputs
        # =========================================================================
        # These outputs will be used by: Step 1 (prepare LMM input)

        log("[SAVE] Saving dependency data copies...")

        # Output 1: Theta scores from RQ 5.5.1
        output_theta_path = RQ_DIR / "data" / "step00_theta_from_rq551.csv"
        theta_from_rq551.to_csv(output_theta_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_theta_path.name} ({len(theta_from_rq551)} rows, {len(theta_from_rq551.columns)} cols)")

        # Output 2: TSVR mapping from RQ 5.5.1
        output_tsvr_path = RQ_DIR / "data" / "step00_tsvr_from_rq551.csv"
        tsvr_from_rq551.to_csv(output_tsvr_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_tsvr_path.name} ({len(tsvr_from_rq551)} rows, {len(tsvr_from_rq551.columns)} cols)")

        # Output 3: Age variable from dfData.csv
        output_age_path = RQ_DIR / "data" / "step00_age_from_dfdata.csv"
        age_from_dfdata.to_csv(output_age_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_age_path.name} ({len(age_from_dfdata)} rows, {len(age_from_dfdata.columns)} cols)")

        # =========================================================================
        # STEP 6: Final Validation Summary
        # =========================================================================
        # Validates: All data loaded correctly with expected structure and ranges

        log("[VALIDATION] Final validation summary:")
        validation_checks = [
            ("RQ 5.5.1 completion status", rq_results_status == 'success'),
            ("Theta scores row count", len(theta_from_rq551) == 400),
            ("TSVR mapping row count", len(tsvr_from_rq551) == 400),
            ("Age data row count", len(age_from_dfdata) == 100),
            ("Theta scores columns", set(required_theta_cols).issubset(set(theta_from_rq551.columns))),
            ("TSVR mapping columns", set(required_tsvr_cols).issubset(set(tsvr_from_rq551.columns))),
            ("Age data columns", set(['UID', 'Age']).issubset(set(age_from_dfdata.columns))),
            ("No NaN in theta_source", theta_from_rq551['theta_source'].isna().sum() == 0),
            ("No NaN in theta_destination", theta_from_rq551['theta_destination'].isna().sum() == 0),
            ("No NaN in TSVR_hours", tsvr_from_rq551['TSVR_hours'].isna().sum() == 0),
            ("No NaN in Age", age_from_dfdata['Age'].isna().sum() == 0),
            ("Theta source range [-4, 4]",
             theta_from_rq551['theta_source'].min() >= -4.0 and theta_from_rq551['theta_source'].max() <= 4.0),
            ("Theta destination range [-4, 4]",
             theta_from_rq551['theta_destination'].min() >= -4.0 and theta_from_rq551['theta_destination'].max() <= 4.0),
            ("SE source range [0.1, 1.5]",
             theta_from_rq551['se_source'].min() >= 0.1 and theta_from_rq551['se_source'].max() <= 1.5),
            ("SE destination range [0.1, 1.5]",
             theta_from_rq551['se_destination'].min() >= 0.1 and theta_from_rq551['se_destination'].max() <= 1.5),
            ("TSVR range [0, 360]",
             tsvr_from_rq551['TSVR_hours'].min() >= 0.0 and tsvr_from_rq551['TSVR_hours'].max() <= 360.0),
            ("Age range [20, 70]",
             age_from_dfdata['Age'].min() >= 20 and age_from_dfdata['Age'].max() <= 70),
        ]

        all_passed = True
        for check_name, check_result in validation_checks:
            status_str = "[PASS]" if check_result else "[FAIL]"
            log(f"{status_str} {check_name}")
            if not check_result:
                all_passed = False

        if not all_passed:
            raise ValueError("Validation failed - see log for details")

        log(f"[VALIDATION] All {len(validation_checks)} checks passed")

        log("[SUCCESS] Step 0 complete - All dependency data loaded and validated")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
