#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step00
Step Name: Prepare Paradigm Data
RQ: results/ch5/rq3
Generated: 2025-11-24

PURPOSE:
Filter RQ 5.1 data to IFR/ICR/IRE paradigms only, create paradigm-based Q-matrix.
This step prepares data for paradigm trajectory analysis (Free Recall, Cued Recall,
Recognition) by excluding Room Free Recall (RFR) and Task Cued Recall (TCR) items.

EXPECTED INPUTS:
  - results/ch5/rq1/data/step00_irt_input.csv
    Columns: composite_ID + TQ_* item columns (105 items total)
    Format: Wide format, binary responses (0/1)
    Expected rows: 400 (100 participants x 4 test sessions)

  - results/ch5/rq1/data/step00_tsvr_mapping.csv
    Columns: composite_ID, UID, test, TSVR_hours
    Format: CSV with time-since-VR-encoding mapping
    Expected rows: 400

EXPECTED OUTPUTS:
  - data/step00_irt_input.csv
    Columns: composite_ID + IFR/ICR/IRE item columns only (~72 items)
    Format: Wide format, filtered to paradigm items
    Expected rows: 400

  - data/step00_q_matrix.csv
    Columns: item_name, free_recall, cued_recall, recognition
    Format: Q-matrix with one-hot factor assignment
    Expected rows: ~72 (one per item)

  - data/step00_tsvr_mapping.csv
    Columns: composite_ID, UID, test, TSVR_hours
    Format: Local copy of TSVR mapping
    Expected rows: 400

VALIDATION CRITERIA:
  - Row count preserved (400 rows)
  - Q-matrix factor columns sum to 1 per row (each item in exactly one factor)
  - No RFR/TCR columns in output
  - At least 10 items per paradigm factor
  - All Q-matrix item_names present in filtered IRT input columns

g_code REASONING:
- Approach: Filter columns by pattern matching (IFR-, ICR-, IRE-)
- Why this approach: RQ 5.3 focuses on paradigm-based trajectory, needs subset of RQ 5.1 data
- Data flow: RQ 5.1 data (all items) -> Filter to 3 paradigms -> Create Q-matrix -> Save
- Expected performance: ~seconds (CSV filtering operations)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib (pandas pattern matching)
- Validation tool: inline assertions
- Parameters: Pattern matching for IFR, ICR, IRE; exclude RFR, TCR
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import shutil
import traceback
from datetime import datetime

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (5 levels deep from project root)
# parents[4] = REMEMVR/ (code -> rq3 -> ch5 -> results -> REMEMVR)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/rq3
LOG_FILE = RQ_DIR / "logs" / "step00_prepare_paradigm_data.log"

# Source RQ directory (RQ 5.1)
SOURCE_RQ_DIR = RQ_DIR.parents[1] / "ch5" / "rq1"  # results/ch5/rq1

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    formatted_msg = f"[{timestamp}] {msg}"
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{formatted_msg}\n")
    print(formatted_msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        # Initialize log file
        LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
        with open(LOG_FILE, 'w', encoding='utf-8') as f:
            f.write(f"Step 00: Prepare Paradigm Data - Log\n")
            f.write(f"{'='*60}\n\n")

        log("[START] Step 00: Prepare Paradigm Data")
        log(f"[INFO] RQ Directory: {RQ_DIR}")
        log(f"[INFO] Source RQ Directory: {SOURCE_RQ_DIR}")

        # =========================================================================
        # STEP 1: Load Input Data from RQ 5.1
        # =========================================================================
        # Expected: IRT input with all VR items (105 columns), TSVR mapping
        # Purpose: Filter to IFR/ICR/IRE paradigms for paradigm trajectory analysis

        log("[LOAD] Loading RQ 5.1 IRT input data...")
        irt_input_path = SOURCE_RQ_DIR / "data" / "step00_irt_input.csv"
        df_rq1_input = pd.read_csv(irt_input_path, encoding='utf-8')
        log(f"[LOADED] {irt_input_path} ({len(df_rq1_input)} rows, {len(df_rq1_input.columns)} cols)")

        log("[LOAD] Loading RQ 5.1 TSVR mapping...")
        tsvr_path = SOURCE_RQ_DIR / "data" / "step00_tsvr_mapping.csv"
        df_tsvr = pd.read_csv(tsvr_path, encoding='utf-8')
        log(f"[LOADED] {tsvr_path} ({len(df_tsvr)} rows, {len(df_tsvr.columns)} cols)")

        # =========================================================================
        # STEP 2: Filter Columns to Paradigm Items (IFR, ICR, IRE)
        # =========================================================================
        # What this does: Keep only columns matching IFR, ICR, IRE patterns
        # Why: RQ 5.3 analyzes paradigm-based trajectories (Free/Cued/Recognition)
        # Exclude: RFR (Room Free Recall), TCR (Task Cued Recall)

        log("[FILTER] Filtering columns to IFR, ICR, IRE paradigms...")

        # Get all column names except composite_ID
        all_cols = df_rq1_input.columns.tolist()
        id_col = "composite_ID"
        item_cols = [c for c in all_cols if c != id_col]

        # Count original items
        log(f"[INFO] Original item count: {len(item_cols)}")

        # Define paradigm patterns to INCLUDE
        paradigm_patterns = {
            'free_recall': 'IFR',      # Item Free Recall
            'cued_recall': 'ICR',      # Item Cued Recall
            'recognition': 'IRE'       # Item Recognition
        }

        # Define patterns to EXCLUDE
        exclude_patterns = ['RFR', 'TCR']  # Room Free Recall, Task Cued Recall

        # Filter to paradigm items only
        paradigm_cols = []
        for col in item_cols:
            # Include if matches any paradigm pattern AND doesn't match exclude patterns
            include_col = False
            for paradigm, pattern in paradigm_patterns.items():
                if pattern in col:
                    include_col = True
                    break

            # Exclude if matches exclude patterns
            for exclude_pattern in exclude_patterns:
                if exclude_pattern in col:
                    include_col = False
                    break

            if include_col:
                paradigm_cols.append(col)

        # Create filtered dataframe
        df_filtered = df_rq1_input[[id_col] + paradigm_cols].copy()
        log(f"[FILTERED] Retained {len(paradigm_cols)} paradigm columns")

        # Count by paradigm
        ifr_count = sum(1 for c in paradigm_cols if 'IFR' in c)
        icr_count = sum(1 for c in paradigm_cols if 'ICR' in c)
        ire_count = sum(1 for c in paradigm_cols if 'IRE' in c)
        log(f"[INFO] Free Recall (IFR): {ifr_count} items")
        log(f"[INFO] Cued Recall (ICR): {icr_count} items")
        log(f"[INFO] Recognition (IRE): {ire_count} items")

        # =========================================================================
        # STEP 3: Create Q-Matrix (Factor Assignment)
        # =========================================================================
        # What this does: Create Q-matrix defining which factor each item loads on
        # Why: IRT calibration needs factor -> item mapping
        # Format: item_name, free_recall, cued_recall, recognition (one-hot)

        log("[QMATRIX] Creating paradigm-based Q-matrix...")

        q_matrix_rows = []
        for col in paradigm_cols:
            row = {
                'item_name': col,
                'free_recall': 1 if 'IFR' in col else 0,
                'cued_recall': 1 if 'ICR' in col else 0,
                'recognition': 1 if 'IRE' in col else 0
            }
            q_matrix_rows.append(row)

        df_qmatrix = pd.DataFrame(q_matrix_rows)
        log(f"[CREATED] Q-matrix with {len(df_qmatrix)} items")

        # Verify one-hot encoding (each item in exactly one factor)
        factor_sums = df_qmatrix[['free_recall', 'cued_recall', 'recognition']].sum(axis=1)
        assert (factor_sums == 1).all(), "Q-matrix validation failed: Items not in exactly one factor"
        log("[PASS] Q-matrix validation: All items in exactly one factor")

        # =========================================================================
        # STEP 4: Save Outputs
        # =========================================================================
        # These outputs will be used by: Step 01 IRT calibration

        # Ensure output directories exist
        (RQ_DIR / "data").mkdir(parents=True, exist_ok=True)

        # Save filtered IRT input
        output_irt_path = RQ_DIR / "data" / "step00_irt_input.csv"
        df_filtered.to_csv(output_irt_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_irt_path} ({len(df_filtered)} rows, {len(df_filtered.columns)} cols)")

        # Save Q-matrix
        output_qmatrix_path = RQ_DIR / "data" / "step00_q_matrix.csv"
        df_qmatrix.to_csv(output_qmatrix_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_qmatrix_path} ({len(df_qmatrix)} rows)")

        # Copy TSVR mapping (local copy for this RQ)
        output_tsvr_path = RQ_DIR / "data" / "step00_tsvr_mapping.csv"
        shutil.copy(tsvr_path, output_tsvr_path)
        log(f"[COPIED] TSVR mapping to {output_tsvr_path}")

        # =========================================================================
        # STEP 5: Run Validation
        # =========================================================================
        # Validates: Row count, Q-matrix structure, no excluded columns, minimum items

        log("[VALIDATION] Running step 00 validations...")

        # Validation 1: Row count preserved
        assert len(df_filtered) == 400, f"Row count mismatch: expected 400, got {len(df_filtered)}"
        log("[PASS] Row count preserved: 400 rows")

        # Validation 2: Q-matrix structure valid (already checked above)
        log("[PASS] Q-matrix structure valid: Each item in exactly one factor")

        # Validation 3: No RFR/TCR columns
        filtered_cols = df_filtered.columns.tolist()
        rfr_count = sum(1 for c in filtered_cols if 'RFR' in c)
        tcr_count = sum(1 for c in filtered_cols if 'TCR' in c)
        assert rfr_count == 0, f"Found {rfr_count} RFR columns (should be 0)"
        assert tcr_count == 0, f"Found {tcr_count} TCR columns (should be 0)"
        log("[PASS] No RFR/TCR columns in output")

        # Validation 4: Minimum items per paradigm (at least 10)
        min_items_per_paradigm = 10
        assert ifr_count >= min_items_per_paradigm, f"IFR items ({ifr_count}) below minimum ({min_items_per_paradigm})"
        assert icr_count >= min_items_per_paradigm, f"ICR items ({icr_count}) below minimum ({min_items_per_paradigm})"
        assert ire_count >= min_items_per_paradigm, f"IRE items ({ire_count}) below minimum ({min_items_per_paradigm})"
        log(f"[PASS] Minimum items per paradigm: IFR={ifr_count}, ICR={icr_count}, IRE={ire_count} (>= {min_items_per_paradigm})")

        # Validation 5: Q-matrix item names match IRT columns
        qmatrix_items = set(df_qmatrix['item_name'].tolist())
        irt_items = set(paradigm_cols)
        assert qmatrix_items == irt_items, f"Q-matrix items don't match IRT columns"
        log("[PASS] Q-matrix item names match IRT input columns")

        # =========================================================================
        # STEP 6: Summary
        # =========================================================================

        log("")
        log("=" * 60)
        log("[SUCCESS] Step 00 Complete: Paradigm Data Prepared")
        log("=" * 60)
        log(f"  Input rows: 400 (from RQ 5.1)")
        log(f"  Output rows: {len(df_filtered)}")
        log(f"  Total paradigm items: {len(paradigm_cols)}")
        log(f"    - Free Recall (IFR): {ifr_count}")
        log(f"    - Cued Recall (ICR): {icr_count}")
        log(f"    - Recognition (IRE): {ire_count}")
        log(f"  Excluded patterns: RFR (Room Free Recall), TCR (Task Cued Recall)")
        log(f"  Output files:")
        log(f"    - {output_irt_path}")
        log(f"    - {output_qmatrix_path}")
        log(f"    - {output_tsvr_path}")
        log("")
        log("[NEXT] Step 01: IRT Calibration Pass 1 (All Items)")

        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
