#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step04a
Step Name: validate_irt_assumptions
RQ: results/ch5/rq11 (IRT-CTT Convergent Validity Comparison)
Generated: 2025-11-29

PURPOSE:
Perform comprehensive assumption checks for IRT LMM to validate statistical
rigor. Tests residual normality, homoscedasticity, random effects normality,
independence (autocorrelation), linearity, outliers, and convergence. Generates
6 diagnostic plots and comprehensive text report.

EXPECTED INPUTS:
  - data/step03_irt_lmm_model.pkl
    Format: Pickle file containing statsmodels MixedLMResults object
    Expected: Fitted IRT LMM from step03 (random slopes or intercepts-only)

  - data/step03_irt_lmm_input.csv
    Columns: ['composite_ID', 'UID', 'test', 'domain', 'TSVR_hours', 'IRT_score']
    Format: Long-format LMM input (400 UID x test x 3 domains = 1200 rows)
    Expected rows: 1200

EXPECTED OUTPUTS:
  - results/step04a_irt_assumptions_report.txt
    Format: Comprehensive text report of all 7 diagnostics
    Expected size: >500 characters (detailed diagnostics)

  - plots/step04a_irt_diagnostics/
    Contains: 6 diagnostic plots (PNG format)
      - qq_residuals.png (residual normality)
      - residuals_vs_fitted.png (homoscedasticity)
      - qq_random_intercepts.png (random intercept normality)
      - qq_random_slopes.png (random slope normality, if present)
      - acf.png (autocorrelation function)
      - cooks_distance.png (outlier detection)

VALIDATION CRITERIA:
  - Assumption report file exists
  - Text report > 500 characters (comprehensive diagnostics)
  - Diagnostic plot directory exists with 6 plots

g_code REASONING:
- Approach: Use validate_lmm_assumptions_comprehensive tool (RQ 5.8 comprehensive
  diagnostics implementation with 7 checks + 6 plots + partial residual CSVs)
- Why this approach: Schielzeth et al. 2020 comprehensive LMM validation protocol
  ensures statistical validity before interpreting coefficients
- Data flow: Load fitted model + input data -> Run 7 diagnostics -> Generate plots
  and text report -> Validation confirms outputs exist
- Expected performance: ~30 seconds (plot generation + statistical tests)

IMPLEMENTATION NOTES:
- Analysis tool: validate_lmm_assumptions_comprehensive from tools.validation
- Validation tool: check_file_exists from tools.validation
- Parameters: acf_lag1_threshold=0.1 (autocorrelation), alpha=0.05 (significance)
- Critical: Creates plots/step04a_irt_diagnostics/ directory if doesn't exist
- Statsmodels model loading: Use MixedLMResults.load() method (NOT pickle.load())
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tool
from tools.validation import validate_lmm_assumptions_comprehensive

# Import validation tool
from tools.validation import check_file_exists

# Import statsmodels for model loading
from statsmodels.regression.mixed_linear_model import MixedLMResults

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/rq11 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step04a_validate_irt_assumptions.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step03_irt_lmm_input.csv
#   CORRECT: results/step04a_irt_assumptions_report.txt
#   WRONG:   logs/step04a_assumptions_report.txt  (txt in logs folder)
#   WRONG:   data/irt_assumptions.txt             (missing step prefix)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 4a: Validate IRT LMM Assumptions")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Fitted IRT model from step03 (MixedLMResults object)
        #           and original LMM input data (1200 rows)
        # Purpose: Run comprehensive assumption diagnostics on fitted model

        log("[LOAD] Loading fitted IRT model from step03...")
        # CRITICAL: Use MixedLMResults.load() method (NOT pickle.load())
        # Reason: pickle.load() causes patsy/eval errors with statsmodels models
        irt_model_path = RQ_DIR / "data" / "step03_irt_lmm_model.pkl"
        irt_model = MixedLMResults.load(str(irt_model_path))
        log(f"[LOADED] IRT model from {irt_model_path}")
        log(f"[INFO] Model converged: {irt_model.converged}")
        log(f"[INFO] Number of observations: {irt_model.nobs}")
        log(f"[INFO] Number of groups (UIDs): {len(irt_model.model.group_labels)}")

        log("[LOAD] Loading IRT LMM input data...")
        # Expected: composite_ID, UID, test, domain, TSVR_hours, IRT_score
        # Expected rows: 1200 (400 UID x test x 3 domains)
        irt_lmm_input = pd.read_csv(RQ_DIR / "data" / "step03_irt_lmm_input.csv")
        log(f"[LOADED] step03_irt_lmm_input.csv ({len(irt_lmm_input)} rows, {len(irt_lmm_input.columns)} cols)")

        # =========================================================================
        # STEP 2: Create Output Directory for Diagnostic Plots
        # =========================================================================
        # Tool: validate_lmm_assumptions_comprehensive requires output_dir to exist
        # Purpose: Ensure plots directory created before running diagnostics

        output_dir = RQ_DIR / "plots" / "step04a_irt_diagnostics"
        output_dir.mkdir(parents=True, exist_ok=True)
        log(f"[CREATED] Diagnostic plots directory: {output_dir}")

        # =========================================================================
        # STEP 3: Run Comprehensive Assumption Validation
        # =========================================================================
        # Tool: validate_lmm_assumptions_comprehensive
        # What it does: 7 diagnostic checks + 6 plots + partial residual CSVs
        #   1. Residual normality (Shapiro-Wilk + Q-Q plot)
        #   2. Homoscedasticity (Breusch-Pagan + residuals vs fitted)
        #   3. Random effects normality (Shapiro-Wilk + Q-Q plots)
        #   4. Autocorrelation (ACF plot + Lag-1 test)
        #   5. Linearity (partial residual CSVs)
        #   6. Outliers (Cook's distance)
        #   7. Convergence diagnostics
        # Expected output: Dict with diagnostics, plot paths, and message

        log("[ANALYSIS] Running validate_lmm_assumptions_comprehensive...")
        log(f"[CONFIG] ACF Lag-1 threshold: 0.1")
        log(f"[CONFIG] Significance level (alpha): 0.05")

        assumptions_result = validate_lmm_assumptions_comprehensive(
            lmm_result=irt_model,  # Fitted IRT model from step03
            data=irt_lmm_input,    # Original input data (1200 rows)
            output_dir=output_dir, # plots/step04a_irt_diagnostics/
            acf_lag1_threshold=0.1,  # Autocorrelation threshold (max 10% correlation at lag 1)
            alpha=0.05                # Significance level for statistical tests
        )
        log("[DONE] Assumption validation complete")

        # =========================================================================
        # STEP 4: Save Comprehensive Diagnostics Report
        # =========================================================================
        # These outputs will be used by: rq_inspect (validation) and researcher review
        # Purpose: Document assumption satisfaction for statistical validity

        log("[SAVE] Saving assumption diagnostics report...")
        # Output: results/step04a_irt_assumptions_report.txt
        # Contains: Text summary of all 7 diagnostics + pass/fail status
        report_path = RQ_DIR / "results" / "step04a_irt_assumptions_report.txt"

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("IRT LMM ASSUMPTION VALIDATION REPORT\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"Overall Status: {'[PASS]' if assumptions_result['valid'] else '[FAIL]'}\n\n")
            f.write(assumptions_result['message'] + "\n\n")

            f.write("DIAGNOSTICS SUMMARY:\n")
            f.write("-" * 80 + "\n")
            for key, value in assumptions_result['diagnostics'].items():
                f.write(f"{key}: {value}\n")

            f.write("\n" + "-" * 80 + "\n")
            f.write(f"DIAGNOSTIC PLOTS GENERATED ({len(assumptions_result['plot_paths'])} files):\n")
            f.write("-" * 80 + "\n")
            for plot_path in assumptions_result['plot_paths']:
                f.write(f"  - {plot_path.name}\n")

        log(f"[SAVED] {report_path} ({report_path.stat().st_size} bytes)")

        # =========================================================================
        # STEP 5: Validate Outputs
        # =========================================================================
        # Tool: check_file_exists
        # Validates: Assumption report exists (>500 bytes) and plots directory exists
        # Threshold: Text report must be >500 characters for comprehensive diagnostics

        log("[VALIDATION] Validating assumption report file...")
        report_validation = check_file_exists(
            file_path=report_path,
            min_size_bytes=500  # Comprehensive report expected to be >500 characters
        )

        if not report_validation['valid']:
            raise FileNotFoundError(f"Assumption report validation failed: {report_validation['message']}")

        log(f"[VALIDATION] Assumption report exists: {report_validation['size_bytes']} bytes (>500 required)")

        log("[VALIDATION] Validating diagnostic plots directory...")
        # Check directory exists and contains plots
        if not output_dir.exists():
            raise FileNotFoundError(f"Diagnostic plots directory missing: {output_dir}")

        plot_files = list(output_dir.glob("*.png"))
        log(f"[VALIDATION] Found {len(plot_files)} diagnostic plots")

        if len(plot_files) < 6:
            raise FileNotFoundError(f"Expected 6 diagnostic plots, found {len(plot_files)}")

        log("[VALIDATION] All outputs validated successfully")

        # Report validation results summary
        log("\n[ASSUMPTION VALIDATION SUMMARY]")
        log(f"  Overall Status: {'[PASS]' if assumptions_result['valid'] else '[FAIL]'}")
        log(f"  Diagnostic plots: {len(plot_files)} generated")
        log(f"  Report size: {report_validation['size_bytes']} bytes")

        if assumptions_result['valid']:
            log("[SUCCESS] IRT LMM assumptions satisfied")
        else:
            log("[WARNING] Some assumptions violated - review diagnostics report")

        log("[SUCCESS] Step 4a complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
