#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step01
Step Name: step01_compute_ctt
RQ: results/ch5/rq11
Generated: 2025-11-29

PURPOSE:
Calculate CTT (Classical Test Theory) mean scores per UID × test × domain using
the same purified item set as RQ 5.1 IRT Pass 2. This enables convergent
validity comparison between IRT theta estimates and CTT proportion-correct scores.

EXPECTED INPUTS:
  - data/step00_raw_data_filtered.csv
    Columns: UID, TEST, [69 item columns for purified items]
    Format: Wide-format dichotomized item responses (0/1 only, no partial scores)
    Expected rows: 400 (100 participants × 4 test sessions)
    Source: Step 00 - filtered to purified items from RQ 5.1, dichotomized

  - data/step00_purified_items.csv
    Columns: item_name, factor, a, b
    Format: IRT item parameters from RQ 5.1 Pass 2 purification
    Expected rows: 40-60 (purified item subset, ~40-50% retention)
    Source: Step 00 - copied from RQ 5.1 Step 2 purification

EXPECTED OUTPUTS:
  - data/step01_ctt_scores.csv
    Columns: composite_ID, UID, test, domain, CTT_score, n_items
    Format: Long format (one row per UID × test × domain)
    Expected rows: 1200 (400 UID × test × 3 domains)
    Description: CTT mean scores (proportion correct) per domain

VALIDATION CRITERIA:
  - Exactly 1200 rows (400 UID × test × 3 domains)
  - All required columns present (6 columns)
  - All 3 domains present (What, Where, When)
  - CTT_score in [0, 1] (proportion correct)
  - n_items > 0 (at least 1 item per domain)

g_code REASONING:
- Approach: Parse item tags to identify domain membership (What=-N-, Where=-L-/-U-/-D-,
  When=-O-), compute mean score per domain using pandas groupby, reshape to long format
- Why this approach: CTT mean scores are simplest psychometric metric (proportion correct),
  enabling direct comparison with IRT theta scores for convergent validity
- Data flow: Wide dichotomized data → parse domain from item tags → compute domain means
  → reshape to long format with composite_ID for merging with IRT theta
- Expected performance: ~2-5 seconds (simple pandas aggregation, N=400 observations)

IMPLEMENTATION NOTES:
- Analysis tool: Standard library (pandas DataFrame operations)
- Validation tool: tools.validation.validate_data_format
- Parameters: Domain tag patterns (What=-N-, Where=-L-/-U-/-D-, When=-O-)
- NOTE: 4_analysis.yaml specifies column name 'dimension' but actual data uses 'factor'
  (verified from RQ 5.1 source). Code uses 'factor' to match actual data.
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_data_format

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/rq11 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step01_compute_ctt_mean_scores.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step01_ctt_scores.csv
#   CORRECT: logs/step01_compute_ctt_mean_scores.log
#   WRONG:   results/ctt_scores.csv             (wrong folder + no prefix)
#   WRONG:   data/ctt_scores.csv                (missing step prefix)
#   WRONG:   logs/step01_ctt_scores.csv         (CSV in logs folder)

# =============================================================================
# Domain Tag Patterns (from RQ 5.1 IRT analysis)
# =============================================================================

DOMAIN_PATTERNS = {
    'what': '-N-',      # What items (narrative/object content)
    'where': ['-L-', '-U-', '-D-'],  # Where items (left/up/down spatial)
    'when': '-O-'       # When items (temporal order)
}

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Helper Functions
# =============================================================================

def parse_domain_from_item_tag(item_name: str) -> str:
    """
    Parse domain from item tag pattern.

    Rules:
      - What items: contain '-N-'
      - Where items: contain '-L-', '-U-', or '-D-'
      - When items: contain '-O-'

    Args:
        item_name: Item tag (e.g., 'TQ_ICR-N-i1')

    Returns:
        Domain name ('what', 'where', 'when')

    Raises:
        ValueError: If item doesn't match any known pattern
    """
    item_lower = item_name.lower()

    if DOMAIN_PATTERNS['what'].lower() in item_lower:
        return 'what'
    elif any(pattern.lower() in item_lower for pattern in DOMAIN_PATTERNS['where']):
        return 'where'
    elif DOMAIN_PATTERNS['when'].lower() in item_lower:
        return 'when'
    else:
        raise ValueError(f"Item '{item_name}' doesn't match any known domain pattern")

def compute_ctt_mean_scores(df_wide: pd.DataFrame, item_list: List[str],
                            domain_map: Dict[str, str]) -> pd.DataFrame:
    """
    Compute CTT mean scores per UID × TEST × domain.

    CTT score = mean(item responses) for items in domain
    This is simply proportion correct (since items are dichotomized 0/1).

    Args:
        df_wide: Wide-format data with UID, TEST, and item columns
        item_list: List of item names to include
        domain_map: Dict mapping item_name -> domain

    Returns:
        Long-format DataFrame with columns:
          - composite_ID: {UID}_{test} format
          - UID: Participant ID
          - test: Test session (T1, T2, T3, T4)
          - domain: Domain name (what, where, when)
          - CTT_score: Mean score (proportion correct)
          - n_items: Number of items in domain
    """
    log("[CTT] Computing domain-wise CTT mean scores...")

    # Group items by domain
    items_by_domain = {'what': [], 'where': [], 'when': []}
    for item in item_list:
        if item not in domain_map:
            log(f"[WARNING] Item '{item}' not in domain map, skipping")
            continue
        domain = domain_map[item]
        items_by_domain[domain].append(item)

    log(f"[CTT] Domain item counts:")
    for domain, items in items_by_domain.items():
        log(f"  - {domain}: {len(items)} items")

    # Compute mean scores per domain
    ctt_rows = []

    for idx, row in df_wide.iterrows():
        uid = row['UID']
        test = row['TEST']
        composite_id = f"{uid}_{test}"

        for domain, items in items_by_domain.items():
            # Extract scores for items in this domain
            domain_scores = [row[item] for item in items if item in df_wide.columns]

            # Compute mean (handling NaN)
            ctt_score = np.nanmean(domain_scores) if domain_scores else np.nan
            n_items = len(domain_scores)

            ctt_rows.append({
                'composite_ID': composite_id,
                'UID': uid,
                'test': test,
                'domain': domain,
                'CTT_score': ctt_score,
                'n_items': n_items
            })

    df_ctt = pd.DataFrame(ctt_rows)

    log(f"[CTT] Computed CTT scores: {len(df_ctt)} rows (UID x test x domain)")
    log(f"[CTT] Score range: [{df_ctt['CTT_score'].min():.3f}, {df_ctt['CTT_score'].max():.3f}]")

    return df_ctt

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 01: Compute CTT Mean Scores")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Wide-format dichotomized item responses (0/1) filtered to
        #           purified items from RQ 5.1 Pass 2
        # Purpose: Compute CTT (proportion correct) scores per domain for
        #          convergent validity comparison with IRT theta

        log("[LOAD] Loading input data...")

        # Load raw data (wide format, dichotomized)
        raw_data_path = RQ_DIR / "data/step00_raw_data_filtered.csv"
        df_raw = pd.read_csv(raw_data_path)
        log(f"[LOADED] {raw_data_path.name} ({len(df_raw)} rows, {len(df_raw.columns)} cols)")

        # Load purified item list (to know which items to use and their domains)
        purified_items_path = RQ_DIR / "data/step00_purified_items.csv"
        df_items = pd.read_csv(purified_items_path)
        log(f"[LOADED] {purified_items_path.name} ({len(df_items)} items)")

        # NOTE: 4_analysis.yaml says column name is 'dimension' but actual data uses 'factor'
        # This is verified from RQ 5.1 source data. Using 'factor' to match reality.
        if 'factor' not in df_items.columns:
            raise ValueError(f"Expected column 'factor' in purified items, found: {df_items.columns.tolist()}")

        # =========================================================================
        # STEP 2: Parse Domain Assignments
        # =========================================================================
        # Map each item to its domain using tag patterns
        # Domain assignment logic matches RQ 5.1 IRT analysis for consistency

        log("[PARSE] Parsing domain assignments from item tags...")

        item_domain_map = {}
        for _, row in df_items.iterrows():
            item_name = row['item_name']
            try:
                domain = parse_domain_from_item_tag(item_name)
                item_domain_map[item_name] = domain
            except ValueError as e:
                log(f"[WARNING] {e}")

        # Verify domain assignments match 'factor' column from RQ 5.1
        mismatches = []
        for _, row in df_items.iterrows():
            item_name = row['item_name']
            expected_domain = row['factor'].lower()  # RQ 5.1 uses lowercase
            parsed_domain = item_domain_map.get(item_name)

            if parsed_domain != expected_domain:
                mismatches.append(f"{item_name}: parsed={parsed_domain}, expected={expected_domain}")

        if mismatches:
            log(f"[WARNING] Domain assignment mismatches ({len(mismatches)} items):")
            for mismatch in mismatches[:5]:  # Show first 5
                log(f"  - {mismatch}")
            log("[INFO] Using 'factor' column from RQ 5.1 as authoritative source")
            # Override parsed domains with RQ 5.1 'factor' column
            item_domain_map = dict(zip(df_items['item_name'], df_items['factor'].str.lower()))
        else:
            log(f"[PASS] Domain assignments match RQ 5.1 'factor' column ({len(item_domain_map)} items)")

        # =========================================================================
        # STEP 3: Compute CTT Mean Scores
        # =========================================================================
        # For each UID × test × domain combination:
        #   CTT_score = mean(item_responses) where items belong to domain
        # This is proportion correct since items are dichotomized (0/1)

        item_list = df_items['item_name'].tolist()
        df_ctt = compute_ctt_mean_scores(df_raw, item_list, item_domain_map)

        # =========================================================================
        # STEP 4: Save CTT Scores
        # =========================================================================
        # Output: Long-format CTT scores for merging with IRT theta in step 2

        output_path = RQ_DIR / "data/step01_ctt_scores.csv"
        df_ctt.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(df_ctt)} rows, {len(df_ctt.columns)} cols)")

        # =========================================================================
        # STEP 5: Validate Output
        # =========================================================================
        # Validation criteria:
        #   - Exactly 1200 rows (400 UID × test × 3 domains)
        #   - All required columns present
        #   - All 3 domains present
        #   - CTT_score in [0, 1]
        #   - n_items > 0

        log("[VALIDATION] Running validate_data_format...")

        # Required columns check
        required_columns = ['composite_ID', 'UID', 'test', 'domain', 'CTT_score', 'n_items']
        validation_result = validate_data_format(df_ctt, required_columns)

        if not validation_result['valid']:
            raise ValueError(f"Column validation failed: {validation_result['message']}")

        log(f"[VALIDATION] Column check: {validation_result['message']}")

        # Row count check
        expected_rows = 1200  # 400 UID × test × 3 domains
        if len(df_ctt) != expected_rows:
            raise ValueError(f"Expected {expected_rows} rows, got {len(df_ctt)}")
        log(f"[VALIDATION] Row count check: {len(df_ctt)} rows (expected {expected_rows})")

        # Domain completeness check
        unique_domains = df_ctt['domain'].unique()
        expected_domains = {'what', 'where', 'when'}
        if set(unique_domains) != expected_domains:
            raise ValueError(f"Expected domains {expected_domains}, got {set(unique_domains)}")
        log(f"[VALIDATION] Domain check: {sorted(unique_domains)} (expected {sorted(expected_domains)})")

        # CTT score range check
        ctt_min = df_ctt['CTT_score'].min()
        ctt_max = df_ctt['CTT_score'].max()
        if ctt_min < 0 or ctt_max > 1:
            raise ValueError(f"CTT_score out of [0,1] range: [{ctt_min:.3f}, {ctt_max:.3f}]")
        log(f"[VALIDATION] CTT_score range: [{ctt_min:.3f}, {ctt_max:.3f}] (expected [0, 1])")

        # Item count check
        n_items_min = df_ctt['n_items'].min()
        if n_items_min <= 0:
            raise ValueError(f"Some domains have 0 items (min={n_items_min})")
        log(f"[VALIDATION] n_items range: [{n_items_min}, {df_ctt['n_items'].max()}] (all > 0)")

        log("[SUCCESS] Step 01 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
