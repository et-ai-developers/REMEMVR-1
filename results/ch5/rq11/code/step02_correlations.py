#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step02
Step Name: Correlation Analysis (IRT vs CTT per Domain)
RQ: results/ch5/rq11
Generated: 2025-11-29

PURPOSE:
Compute Pearson correlations between IRT theta scores and CTT mean scores for
each domain (What, Where, When), test significance with Holm-Bonferroni
correction per Decision D068 dual p-value reporting philosophy.

EXPECTED INPUTS:
  - data/step00_irt_theta_loaded.csv
    Columns: ['composite_ID', 'theta_what', 'theta_where', 'theta_when']
    Format: IRT theta scores from RQ 5.1 (local copy)
    Expected rows: ~400 (100 UIDs x 4 tests)

  - data/step01_ctt_scores.csv
    Columns: ['composite_ID', 'UID', 'test', 'domain', 'CTT_score', 'n_items']
    Format: CTT mean scores per UID x test x domain
    Expected rows: ~1200 (400 observations x 3 domains)

EXPECTED OUTPUTS:
  - results/step02_correlations.csv
    Columns: ['domain', 'r', 'CI_lower', 'CI_upper', 'p_uncorrected', 'p_holm', 'n', 'threshold_0.70', 'threshold_0.90']
    Format: Pearson correlations with dual p-values (uncorrected + Holm-Bonferroni)
    Expected rows: 4 (What, Where, When, Overall)

VALIDATION CRITERIA:
  - BOTH p_uncorrected AND p_holm columns present (Decision D068)
  - r values in [-1, 1] (correlation coefficient bounds)
  - CI_lower < r < CI_upper (confidence interval brackets point estimate)
  - p_holm >= p_uncorrected (correction cannot make p-value smaller)
  - Exactly 4 rows (What, Where, When, Overall)

g_code REASONING:
- Approach: Reshape IRT theta to long format, merge with CTT on composite_ID + domain,
  compute Pearson r per domain using scipy.stats.pearsonr, apply Holm-Bonferroni
  correction to p-values, test against r > 0.70 (strong) and r > 0.90 (exceptional)
  thresholds per psychometric standards.
- Why this approach: IRT has wide format (3 domain columns), CTT has long format
  (1 score column with domain indicator). Reshaping IRT to long allows direct merge
  and correlation per domain. Fisher z-transformation for CI computation handles
  non-normal sampling distribution of r. Holm-Bonferroni is less conservative than
  standard Bonferroni (sequentially rejective procedure).
- Data flow: IRT wide -> IRT long -> merge with CTT -> correlations per domain ->
  overall correlation (all domains pooled) -> Holm correction -> threshold tests
- Expected performance: <5 seconds (pure pandas/numpy operations, no model fitting)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib (pandas, scipy, numpy)
- Validation tool: tools.validation.validate_correlation_test_d068
- Parameters: Pearson correlation, Fisher z-transform for CI, Holm-Bonferroni correction
  for m=4 tests (3 domains + overall), alpha=0.05, thresholds=[0.70, 0.90]
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback
from scipy import stats

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_correlation_test_d068

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/chX/rqY (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step02_correlations.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html) AND statistical summaries (.csv)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Note: Correlation statistics are final RESULTS (not intermediate data),
# so results/step02_correlations.csv is appropriate per architectural intent.

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Helper Functions
# =============================================================================

def fisher_z_transform(r: float) -> float:
    """
    Fisher z-transformation for correlation coefficient.

    z = 0.5 * ln((1+r)/(1-r))
    """
    return 0.5 * np.log((1 + r) / (1 - r))

def inverse_fisher_z(z: float) -> float:
    """
    Inverse Fisher z-transformation.

    r = (e^(2z) - 1) / (e^(2z) + 1)
    """
    e2z = np.exp(2 * z)
    return (e2z - 1) / (e2z + 1)

def compute_correlation_ci(r: float, n: int, ci_level: float = 0.95) -> Tuple[float, float]:
    """
    Compute confidence interval for Pearson correlation using Fisher z-transform.

    Parameters
    ----------
    r : float
        Correlation coefficient
    n : int
        Sample size
    ci_level : float
        Confidence level (default 0.95)

    Returns
    -------
    Tuple[float, float]
        (CI_lower, CI_upper)
    """
    # Fisher z-transform
    z = fisher_z_transform(r)

    # Standard error in z-space
    se_z = 1 / np.sqrt(n - 3)

    # Z-score for CI level
    alpha = 1 - ci_level
    z_critical = stats.norm.ppf(1 - alpha / 2)

    # CI in z-space
    z_lower = z - z_critical * se_z
    z_upper = z + z_critical * se_z

    # Transform back to r-space
    r_lower = inverse_fisher_z(z_lower)
    r_upper = inverse_fisher_z(z_upper)

    return r_lower, r_upper

def holm_bonferroni_correction(p_values: List[float], alpha: float = 0.05) -> List[float]:
    """
    Apply Holm-Bonferroni correction to p-values.

    Sequentially rejective procedure (less conservative than standard Bonferroni).

    Parameters
    ----------
    p_values : List[float]
        Uncorrected p-values
    alpha : float
        Family-wise error rate (default 0.05)

    Returns
    -------
    List[float]
        Corrected p-values (same length and order as input)
    """
    m = len(p_values)

    # Create list of (index, p_value) tuples
    indexed_pvals = [(i, p) for i, p in enumerate(p_values)]

    # Sort by p-value (ascending)
    indexed_pvals.sort(key=lambda x: x[1])

    # Compute corrected p-values
    corrected = [0.0] * m
    for k, (orig_idx, p) in enumerate(indexed_pvals):
        # Holm correction: min(1, p * (m - k))
        # where k is rank (0-indexed, so k=0 is smallest p-value)
        corrected[orig_idx] = min(1.0, p * (m - k))

    return corrected

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 02: Correlation Analysis (IRT vs CTT per Domain)")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: IRT theta scores (wide format) and CTT scores (long format)
        # Purpose: Prepare data for correlation analysis per domain

        log("[LOAD] Loading input data...")

        # Load IRT theta scores (wide format: 3 domain columns)
        irt_theta = pd.read_csv(RQ_DIR / "data/step00_irt_theta_loaded.csv")
        log(f"[LOADED] step00_irt_theta_loaded.csv ({len(irt_theta)} rows, {len(irt_theta.columns)} cols)")

        # Load CTT scores (long format: 1 score column with domain indicator)
        ctt_scores = pd.read_csv(RQ_DIR / "data/step01_ctt_scores.csv")
        log(f"[LOADED] step01_ctt_scores.csv ({len(ctt_scores)} rows, {len(ctt_scores.columns)} cols)")

        # =========================================================================
        # STEP 2: Reshape IRT Theta to Long Format
        # =========================================================================
        # Tool: pandas.melt
        # What it does: Convert wide format (theta_what, theta_where, theta_when columns)
        #               to long format (domain column + IRT_score column)
        # Expected output: 1200 rows (400 observations x 3 domains)

        log("[RESHAPE] Reshaping IRT theta to long format...")

        # Reshape: composite_ID stays as identifier, melt domain columns
        irt_long = pd.melt(
            irt_theta,
            id_vars=['composite_ID'],
            value_vars=['theta_what', 'theta_where', 'theta_when'],
            var_name='theta_column',
            value_name='IRT_score'
        )

        # Map theta column names to domain labels (lowercase to match CTT)
        domain_mapping = {
            'theta_what': 'what',
            'theta_where': 'where',
            'theta_when': 'when'
        }
        irt_long['domain'] = irt_long['theta_column'].map(domain_mapping)
        irt_long = irt_long.drop(columns=['theta_column'])

        log(f"[RESHAPE] IRT long format: {len(irt_long)} rows")

        # =========================================================================
        # STEP 3: Merge IRT and CTT on composite_ID + domain
        # =========================================================================
        # Tool: pandas.merge
        # What it does: Join IRT and CTT scores for same observation and domain
        # Expected output: 1200 rows with both IRT_score and CTT_score columns

        log("[MERGE] Merging IRT and CTT scores...")

        # Merge on composite_ID and domain
        merged = pd.merge(
            irt_long,
            ctt_scores[['composite_ID', 'domain', 'CTT_score']],
            on=['composite_ID', 'domain'],
            how='inner'
        )

        log(f"[MERGED] {len(merged)} rows with both IRT and CTT scores")

        # Sanity check: Should have 1200 rows (400 observations x 3 domains)
        if len(merged) != 1200:
            log(f"[WARNING] Expected 1200 rows, got {len(merged)}")

        # =========================================================================
        # STEP 4: Compute Pearson Correlations per Domain
        # =========================================================================
        # Tool: scipy.stats.pearsonr
        # What it does: Compute Pearson r and uncorrected p-value per domain
        # Expected output: 3 domain-specific correlations

        log("[ANALYSIS] Computing Pearson correlations per domain...")

        correlations = []

        for domain in ['what', 'where', 'when']:
            # Filter to domain
            domain_data = merged[merged['domain'] == domain].copy()
            n = len(domain_data)

            # Compute Pearson correlation
            r, p_uncorrected = stats.pearsonr(
                domain_data['IRT_score'],
                domain_data['CTT_score']
            )

            # Compute 95% CI using Fisher z-transform
            ci_lower, ci_upper = compute_correlation_ci(r, n, ci_level=0.95)

            # Test thresholds
            threshold_0_70 = r > 0.70
            threshold_0_90 = r > 0.90

            correlations.append({
                'domain': domain,
                'r': r,
                'CI_lower': ci_lower,
                'CI_upper': ci_upper,
                'p_uncorrected': p_uncorrected,
                'n': n,
                'threshold_0.70': threshold_0_70,
                'threshold_0.90': threshold_0_90
            })

            log(f"[CORRELATION] {domain}: r={r:.3f}, p={p_uncorrected:.4f}, n={n}")

        # =========================================================================
        # STEP 5: Compute Overall Correlation (All Domains Pooled)
        # =========================================================================
        # Tool: scipy.stats.pearsonr
        # What it does: Compute overall correlation across all domains combined
        # Expected output: 1 overall correlation (n=1200)

        log("[ANALYSIS] Computing overall correlation (all domains pooled)...")

        r_overall, p_overall = stats.pearsonr(
            merged['IRT_score'],
            merged['CTT_score']
        )

        n_overall = len(merged)
        ci_lower_overall, ci_upper_overall = compute_correlation_ci(
            r_overall, n_overall, ci_level=0.95
        )

        correlations.append({
            'domain': 'Overall',
            'r': r_overall,
            'CI_lower': ci_lower_overall,
            'CI_upper': ci_upper_overall,
            'p_uncorrected': p_overall,
            'n': n_overall,
            'threshold_0.70': r_overall > 0.70,
            'threshold_0.90': r_overall > 0.90
        })

        log(f"[CORRELATION] Overall: r={r_overall:.3f}, p={p_overall:.4f}, n={n_overall}")

        # =========================================================================
        # STEP 6: Apply Holm-Bonferroni Correction
        # =========================================================================
        # Tool: Custom holm_bonferroni_correction function
        # What it does: Sequentially rejective correction for m=4 tests
        # Expected output: Corrected p-values (p_holm column)

        log("[CORRECTION] Applying Holm-Bonferroni correction (m=4 tests)...")

        # Convert to DataFrame for easier manipulation
        corr_df = pd.DataFrame(correlations)

        # Extract uncorrected p-values
        p_uncorrected_list = corr_df['p_uncorrected'].tolist()

        # Apply Holm-Bonferroni correction
        p_holm_list = holm_bonferroni_correction(p_uncorrected_list, alpha=0.05)

        # Add corrected p-values to DataFrame
        corr_df['p_holm'] = p_holm_list

        log("[CORRECTION] Holm-Bonferroni correction applied")

        # Report dual p-values (Decision D068)
        for _, row in corr_df.iterrows():
            log(f"[D068] {row['domain']}: p_uncorrected={row['p_uncorrected']:.4f}, p_holm={row['p_holm']:.4f}")

        # =========================================================================
        # STEP 7: Save Correlation Results
        # =========================================================================
        # These outputs will be used by: Step 7 (scatterplot data preparation)

        log("[SAVE] Saving correlation results...")

        # Reorder columns for clarity
        corr_df = corr_df[[
            'domain', 'r', 'CI_lower', 'CI_upper',
            'p_uncorrected', 'p_holm', 'n',
            'threshold_0.70', 'threshold_0.90'
        ]]

        # Save to data/ folder (per folder conventions, statistical outputs go to data/)
        output_path = RQ_DIR / "data/step02_correlations.csv"
        corr_df.to_csv(output_path, index=False, encoding='utf-8')

        log(f"[SAVED] step02_correlations.csv ({len(corr_df)} rows, {len(corr_df.columns)} cols)")

        # =========================================================================
        # STEP 8: Run Validation Tool
        # =========================================================================
        # Tool: tools.validation.validate_correlation_test_d068
        # Validates: Decision D068 dual p-value reporting (uncorrected + corrected)
        # Threshold: D068 compliance required (both p_uncorrected and p_holm present)

        log("[VALIDATION] Running validate_correlation_test_d068...")

        validation_result = validate_correlation_test_d068(
            correlation_df=corr_df,
            required_cols=['p_uncorrected', 'p_holm']
        )

        # Report validation results
        log(f"[VALIDATION] valid: {validation_result['valid']}")
        log(f"[VALIDATION] d068_compliant: {validation_result['d068_compliant']}")
        log(f"[VALIDATION] message: {validation_result['message']}")

        # Check if validation passed
        if not validation_result['valid']:
            missing = validation_result.get('missing_cols', [])
            raise ValueError(f"Validation failed: {validation_result['message']} (missing: {missing})")

        # Additional validation criteria from 4_analysis.yaml
        log("[VALIDATION] Checking additional criteria...")

        # r values in [-1, 1]
        if not all(corr_df['r'].between(-1, 1)):
            raise ValueError("Correlation coefficients outside [-1, 1] bounds")
        log("[VALIDATION] r values in [-1, 1]: PASS")

        # CI_lower < r < CI_upper
        ci_check = all((corr_df['CI_lower'] < corr_df['r']) & (corr_df['r'] < corr_df['CI_upper']))
        if not ci_check:
            raise ValueError("Confidence intervals do not bracket point estimates")
        log("[VALIDATION] CI_lower < r < CI_upper: PASS")

        # p_holm >= p_uncorrected
        p_check = all(corr_df['p_holm'] >= corr_df['p_uncorrected'])
        if not p_check:
            raise ValueError("Corrected p-values smaller than uncorrected (impossible)")
        log("[VALIDATION] p_holm >= p_uncorrected: PASS")

        # Exactly 4 rows
        if len(corr_df) != 4:
            raise ValueError(f"Expected 4 rows (What, Where, When, Overall), got {len(corr_df)}")
        log("[VALIDATION] Exactly 4 rows: PASS")

        log("[SUCCESS] Step 02 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
