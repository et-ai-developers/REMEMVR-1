# Analysis Plan: RQ 5.3.8 - Paradigm-Based Clustering

**Research Question:** 5.3.8
**Created:** 2025-12-02
**Status:** Planning complete, ready for tool specification (rq_tools)

---

## Overview

This RQ examines whether participants can be grouped into latent classes based on paradigm-specific forgetting trajectories. Using random effects (intercepts and slopes) from RQ 5.3.7's paradigm-stratified LMMs, we apply K-means clustering to identify discrete memory phenotypes.

**Pipeline:** K-means clustering (unsupervised machine learning)
**Steps:** 8 analysis steps (load -> standardize -> model selection -> fit -> validate -> characterize -> plot)
**Estimated Runtime:** Low-Medium (clustering is computationally efficient, <10 minutes total)

**Key Methodological Decisions:**
- K-means selected over LPA (exploratory nature, N=100 at lower bound for stable LPA)
- BIC-based model selection (K=2-6 tested, parsimony rule if BIC difference <2)
- Comprehensive validation: silhouette >=0.40, Davies-Bouldin <1.5, bootstrap stability Jaccard >=0.75
- Sphericity assumption check via PCA variance and visual inspection

**No Project-Wide Decisions Apply:**
- Decision D039 (2-pass IRT): Not applicable (no IRT analysis)
- Decision D068 (dual p-values): Not applicable (no hypothesis tests)
- Decision D069 (dual-scale plots): Not applicable (no trajectory plots)
- Decision D070 (TSVR time): Not applicable (no LMM analysis)

---

## Analysis Plan

This RQ requires 8 steps:

### Step 0: Load and Reshape Random Effects from RQ 5.3.7

**Dependencies:** None (first step, but requires RQ 5.3.7 completion)
**Complexity:** Low (data loading and reshaping only)

**Purpose:** Load paradigm-specific random effects from RQ 5.3.7 and reshape from long format (100 participants x 3 paradigms) to wide format (100 participants x 6 features).

**Input:**

**File:** results/ch5/5.3.7/data/step04_random_effects.csv
**Source:** Generated by RQ 5.3.7 Step 4 (extract random effects from paradigm-stratified LMMs)
**Format:** CSV, long format (one row per participant-paradigm combination)
**Columns:**
  - `UID` (string, participant identifier, e.g., "P001")
  - `paradigm` (string, values: {Free, Cued, Recognition})
  - `Total_Intercept` (float, random intercept for participant within paradigm)
  - `Total_Slope` (float, random slope for participant within paradigm)
**Expected Rows:** 300 (100 participants x 3 paradigms)
**Expected Values:**
  - Total_Intercept: typically in [-2, 2] (theta scale)
  - Total_Slope: typically in [-0.5, 0.5] (theta change per TSVR_hours unit)

**Processing:**

1. Load results/ch5/5.3.7/data/step04_random_effects.csv
2. Verify 300 rows (100 participants x 3 paradigms)
3. Verify no missing values (all participants must have all 3 paradigms)
4. Reshape from long to wide:
   - Pivot on paradigm, creating 6 columns:
     - Total_Intercept_Free (intercept for Free Recall)
     - Total_Slope_Free (slope for Free Recall)
     - Total_Intercept_Cued (intercept for Cued Recall)
     - Total_Slope_Cued (slope for Cued Recall)
     - Total_Intercept_Recognition (intercept for Recognition)
     - Total_Slope_Recognition (slope for Recognition)
5. Verify wide format: 100 rows x 7 columns (UID + 6 features)

**Output:**

**File 1:** data/step00_random_effects_wide.csv
**Format:** CSV, wide format (one row per participant)
**Columns:**
  - `UID` (string, participant identifier)
  - `Total_Intercept_Free` (float, Free Recall intercept)
  - `Total_Slope_Free` (float, Free Recall slope)
  - `Total_Intercept_Cued` (float, Cued Recall intercept)
  - `Total_Slope_Cued` (float, Cued Recall slope)
  - `Total_Intercept_Recognition` (float, Recognition intercept)
  - `Total_Slope_Recognition` (float, Recognition slope)
**Expected Rows:** 100 participants
**Expected Columns:** 7 (UID + 6 features)

**Validation Requirement:**

Validation tools MUST be used after data loading and reshaping. Specific validation tools will be determined by rq_tools based on data format requirements. The rq_analysis agent will embed validation tool calls after the data loading tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step00_random_effects_wide.csv: 100 rows x 7 columns
- Column names exact match: UID, Total_Intercept_Free, Total_Slope_Free, Total_Intercept_Cued, Total_Slope_Cued, Total_Intercept_Recognition, Total_Slope_Recognition
- Data types: UID (string/object), all 6 features (float64)

*Value Ranges:*
- Total_Intercept_* in [-3, 3] (theta scale, outside suggests model error)
- Total_Slope_* in [-1, 1] (change per hour, outside suggests unrealistic forgetting rate)

*Data Quality:*
- No NaN values tolerated (all 100 participants must have all 6 features)
- Expected N: Exactly 100 rows (all participants from RQ 5.3.7)
- No duplicate UIDs (each participant appears once)

*Log Validation:*
- Required pattern: "Loaded 300 rows from RQ 5.3.7"
- Required pattern: "Reshaped to 100 participants x 6 features"
- Required pattern: "No missing values detected"
- Forbidden patterns: "ERROR", "NaN values detected", "Missing paradigm data"

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Expected 100 rows, found 87")
- Log failure to logs/step00_load_reshape_random_effects.log
- Quit script immediately (do NOT proceed to Step 1)
- g_debug invoked to diagnose root cause (likely RQ 5.3.7 incomplete or data corruption)

---

### Step 1: Standardize Features to Z-Scores

**Dependencies:** Step 0 (requires wide-format random effects)
**Complexity:** Low (simple standardization)

**Purpose:** Standardize all 6 features to z-scores (mean=0, SD=1) to ensure equal weighting in K-means clustering. Prevents features with larger scales from dominating distance calculations.

**Input:**

**File:** data/step00_random_effects_wide.csv
**Source:** Generated by Step 0 (reshaped random effects)
**Format:** CSV, wide format (100 rows x 7 columns)
**Columns:** UID + 6 random effect features (see Step 0 output)

**Processing:**

1. Load data/step00_random_effects_wide.csv
2. Separate UID column (identifier) from 6 feature columns (clustering variables)
3. Standardize each of 6 features independently:
   - Compute mean and SD for each feature across 100 participants
   - Transform: z = (x - mean) / SD
   - Result: mean=0, SD=1 for each feature
4. Verify standardization successful (tolerance: |mean| < 0.01, |SD - 1| < 0.01)
5. Combine UID with standardized features

**Output:**

**File 1:** data/step01_standardized_features.csv
**Format:** CSV, wide format (one row per participant)
**Columns:**
  - `UID` (string, participant identifier)
  - `Total_Intercept_Free_z` (float, z-score)
  - `Total_Slope_Free_z` (float, z-score)
  - `Total_Intercept_Cued_z` (float, z-score)
  - `Total_Slope_Cued_z` (float, z-score)
  - `Total_Intercept_Recognition_z` (float, z-score)
  - `Total_Slope_Recognition_z` (float, z-score)
**Expected Rows:** 100 participants
**Expected Columns:** 7 (UID + 6 z-scores)

**File 2:** data/step01_standardization_summary.txt
**Format:** Plain text report
**Content:** Mean and SD for each feature before standardization, verification that z-scores have mean~0 and SD~1

**Validation Requirement:**

Validation tools MUST be used after standardization. Specific validation tools will be determined by rq_tools (likely tools.validation.validate_standardization). The rq_analysis agent will embed validation tool calls after the standardization tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step01_standardized_features.csv: 100 rows x 7 columns
- data/step01_standardization_summary.txt: text report with pre/post statistics
- Column names: UID + 6 features with "_z" suffix
- Data types: UID (string/object), all 6 z-scores (float64)

*Value Ranges:*
- Z-scores typically in [-3, 3] (values beyond suggest outliers, acceptable for clustering)
- Mean of each z-score column: [-0.05, 0.05] (tolerance for sampling variation)
- SD of each z-score column: [0.95, 1.05] (tolerance for sampling variation)

*Data Quality:*
- No NaN values tolerated
- Expected N: Exactly 100 rows (no data loss)
- No duplicate UIDs

*Log Validation:*
- Required pattern: "Standardization successful: all features mean~0, SD~1"
- Required pattern: "VALIDATION - PASS: standardization"
- Forbidden patterns: "ERROR", "VALIDATION - FAIL", "Mean or SD out of tolerance"

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Feature Total_Intercept_Free_z has mean=0.15, expected ~0")
- Log failure to logs/step01_standardize_features.log
- Quit script immediately
- g_debug invoked to diagnose (likely implementation error in standardization formula)

---

### Step 2: Test K=1 to K=6 and Select Optimal K via BIC

**Dependencies:** Step 1 (requires standardized features)
**Complexity:** Medium (multiple model fits, ~5 minutes)

**Purpose:** Fit K-means models for K=1 to K=6, compute BIC for each, identify optimal K as BIC minimum. Use parsimony rule: if BIC difference between K and K+1 is <2, prefer simpler K.

**Input:**

**File:** data/step01_standardized_features.csv
**Source:** Generated by Step 1 (standardized z-scores)
**Format:** CSV, 100 rows x 7 columns (UID + 6 z-scores)

**Processing:**

1. Load standardized features (exclude UID column, use only 6 z-score columns)
2. For each K in {1, 2, 3, 4, 5, 6}:
   - Fit K-means with K clusters
   - Use random_state=42 for reproducibility
   - Use n_init=50 for stability (50 random initializations, keep best)
   - Extract inertia (within-cluster sum of squares)
   - Compute BIC = N * log(inertia / N) + K * log(N) where N=100
   - Store K, inertia, BIC
3. Identify K_min = argmin(BIC)
4. Apply parsimony rule:
   - If BIC[K_min+1] - BIC[K_min] < 2, prefer K_min (simpler model)
   - Report final selected K with rationale
5. Create elbow plot: K vs inertia, K vs BIC

**Output:**

**File 1:** data/step02_cluster_selection.csv
**Format:** CSV, one row per K tested
**Columns:**
  - `K` (int, number of clusters: 1, 2, 3, 4, 5, 6)
  - `inertia` (float, within-cluster sum of squares)
  - `BIC` (float, Bayesian Information Criterion)
**Expected Rows:** 6 (one per K value)

**File 2:** data/step02_optimal_k.txt
**Format:** Plain text report
**Content:** Selected K value, BIC values for all K, rationale for selection (BIC minimum + parsimony rule if applicable)

**File 3:** data/step02_elbow_plot_data.csv
**Format:** CSV for plotting (K, inertia, BIC)
**Purpose:** Source data for elbow plot (created later by rq_plots)

**Validation Requirement:**

Validation tools MUST be used after K-means model selection. Specific validation tools will be determined by rq_tools based on clustering validation requirements. The rq_analysis agent will embed validation tool calls after the model selection tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step02_cluster_selection.csv: 6 rows x 3 columns (K, inertia, BIC)
- data/step02_optimal_k.txt: text report with selected K
- data/step02_elbow_plot_data.csv: 6 rows (elbow plot source)
- Data types: K (int), inertia (float64), BIC (float64)

*Value Ranges:*
- K in {1, 2, 3, 4, 5, 6} (exact values)
- Inertia: decreases monotonically as K increases (inertia[K] > inertia[K+1])
- BIC: should show minimum in range K=2 to K=6 (K=1 is baseline, not meaningful cluster solution)
- Selected K in [2, 6] (K=1 not a clustering solution)

*Data Quality:*
- Exactly 6 rows (one per K value)
- No NaN values
- No duplicate K values
- Inertia strictly decreasing sequence

*Log Validation:*
- Required pattern: "Optimal K selected: {K} (BIC = {value})"
- Required pattern: "BIC minimum at K={K_min}"
- Required pattern: "Parsimony rule applied: {yes/no}"
- Forbidden patterns: "ERROR", "Selected K=1", "BIC all equal"

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "BIC minimum at K=1, no meaningful clustering")
- Log failure to logs/step02_cluster_selection.log
- Quit script immediately
- g_debug invoked to diagnose (likely insufficient variance in features or convergence failure)

---

### Step 3: Fit Final K-Means Model with Optimal K

**Dependencies:** Step 2 (requires optimal K selection)
**Complexity:** Low (single model fit, <1 minute)

**Purpose:** Fit final K-means model using optimal K selected in Step 2. Extract cluster assignments for all 100 participants and cluster centers (K x 6 feature means).

**Input:**

**File 1:** data/step01_standardized_features.csv
**Source:** Generated by Step 1 (standardized z-scores)

**File 2:** data/step02_optimal_k.txt
**Source:** Generated by Step 2 (selected K value)
**Parse:** Extract optimal K value from text report

**Processing:**

1. Load standardized features (UID + 6 z-scores)
2. Read optimal K from step02_optimal_k.txt
3. Fit K-means with K clusters:
   - Use random_state=42 (reproducibility)
   - Use n_init=50 (stability via multiple initializations)
   - Fit on 6 z-score columns (exclude UID)
4. Extract cluster assignments (100 x 1 vector: cluster label per participant)
5. Extract cluster centers (K x 6 matrix: mean z-score per feature per cluster)
6. Assign cluster labels to participants

**Output:**

**File 1:** data/step03_cluster_assignments.csv
**Format:** CSV, one row per participant
**Columns:**
  - `UID` (string, participant identifier)
  - `cluster` (int, cluster assignment: 0 to K-1)
**Expected Rows:** 100 participants
**Expected Columns:** 2

**File 2:** data/step03_cluster_centers.csv
**Format:** CSV, one row per cluster
**Columns:**
  - `cluster` (int, cluster ID: 0 to K-1)
  - `Total_Intercept_Free_z` (float, cluster center on Free Recall intercept)
  - `Total_Slope_Free_z` (float, cluster center on Free Recall slope)
  - `Total_Intercept_Cued_z` (float, cluster center on Cued Recall intercept)
  - `Total_Slope_Cued_z` (float, cluster center on Cued Recall slope)
  - `Total_Intercept_Recognition_z` (float, cluster center on Recognition intercept)
  - `Total_Slope_Recognition_z` (float, cluster center on Recognition slope)
**Expected Rows:** K (one per cluster)
**Expected Columns:** 7 (cluster ID + 6 feature centers)

**File 3:** data/step03_cluster_sizes.txt
**Format:** Plain text report
**Content:** Cluster sizes (N per cluster), verify no cluster <10% of sample (min N=10)

**Validation Requirement:**

Validation tools MUST be used after final K-means fitting. Specific validation tools will be determined by rq_tools (likely tools.validation.validate_cluster_assignment). The rq_analysis agent will embed validation tool calls after the clustering tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step03_cluster_assignments.csv: 100 rows x 2 columns (UID, cluster)
- data/step03_cluster_centers.csv: K rows x 7 columns (cluster + 6 centers)
- data/step03_cluster_sizes.txt: text report with cluster sizes
- Data types: UID (string/object), cluster (int), centers (float64)

*Value Ranges:*
- cluster in {0, 1, ..., K-1} (consecutive integers, no gaps)
- Cluster centers (z-scores): typically in [-2, 2] (cluster means should be within reasonable range)
- Each cluster size >= 10 (>= 10% of N=100 per concept.md constraint)

*Data Quality:*
- Exactly 100 rows in assignments (all participants assigned)
- Exactly K rows in centers (one per cluster)
- No NaN values
- No duplicate UIDs in assignments
- No duplicate cluster IDs in centers
- Cluster labels are consecutive 0 to K-1 (no missing clusters)

*Log Validation:*
- Required pattern: "K-means fitted with K={K} clusters"
- Required pattern: "VALIDATION - PASS: cluster assignment"
- Required pattern: "All cluster sizes >= 10"
- Forbidden patterns: "ERROR", "VALIDATION - FAIL", "Cluster size < 10"

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Cluster 2 has N=8, below minimum 10")
- Log failure to logs/step03_fit_final_kmeans.log
- Quit script immediately
- g_debug invoked to diagnose (may need to reduce K if cluster sizes unbalanced)

---

### Step 4: Validate Cluster Quality (Silhouette, Davies-Bouldin, Dunn)

**Dependencies:** Step 3 (requires cluster assignments)
**Complexity:** Low (validation metrics computation, <1 minute)

**Purpose:** Compute cluster quality metrics to assess clustering solution. Metrics: silhouette score (cohesion), Davies-Bouldin index (separation), Dunn index (compactness). Thresholds from concept.md: silhouette >=0.40, Davies-Bouldin <1.5.

**Input:**

**File 1:** data/step01_standardized_features.csv
**Source:** Generated by Step 1 (standardized features used for clustering)

**File 2:** data/step03_cluster_assignments.csv
**Source:** Generated by Step 3 (cluster labels)

**Processing:**

1. Load standardized features (6 z-score columns)
2. Load cluster assignments (100 participants -> cluster labels)
3. Compute silhouette score:
   - Silhouette coefficient per participant (cohesion within cluster vs separation to nearest cluster)
   - Mean silhouette across all participants
   - Target: >= 0.40 (acceptable structure per concept.md)
   - Interpretation: <0.25 = no structure, 0.25-0.50 = weak, 0.51-0.70 = reasonable, >0.70 = strong
4. Compute Davies-Bouldin index:
   - Ratio of within-cluster to between-cluster distances
   - Lower is better
   - Target: < 1.5 (acceptable separation per concept.md)
5. Compute Dunn index:
   - Ratio of minimum inter-cluster distance to maximum intra-cluster distance
   - Higher is better (no threshold specified, report for completeness)
6. Report all three metrics with interpretations

**Output:**

**File 1:** data/step04_cluster_quality_metrics.csv
**Format:** CSV, one row per metric
**Columns:**
  - `metric` (string, metric name: silhouette, davies_bouldin, dunn)
  - `value` (float, computed metric value)
  - `threshold` (float, target threshold if applicable, else NaN)
  - `pass` (bool, True if meets threshold, else False)
**Expected Rows:** 3 (one per metric)

**File 2:** data/step04_quality_interpretation.txt
**Format:** Plain text report
**Content:** Interpretation of each metric (e.g., "Silhouette = 0.52: reasonable cluster structure"), overall assessment (acceptable/marginal/poor)

**Validation Requirement:**

Validation tools MUST be used after cluster quality computation. Specific validation tools will be determined by rq_tools based on clustering validation requirements. The rq_analysis agent will embed validation tool calls after the quality metric computation tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step04_cluster_quality_metrics.csv: 3 rows x 4 columns (metric, value, threshold, pass)
- data/step04_quality_interpretation.txt: text report
- Data types: metric (string/object), value (float64), threshold (float64 or NaN), pass (bool)

*Value Ranges:*
- Silhouette in [-1, 1] (typically [0, 1] for reasonable clustering)
- Davies-Bouldin > 0 (no upper bound, but >5 suggests poor clustering)
- Dunn > 0 (no upper bound, but >2 is rare)

*Data Quality:*
- Exactly 3 rows (silhouette, davies_bouldin, dunn)
- No NaN in value column
- threshold column: NaN for dunn (no threshold), numeric for silhouette/davies_bouldin
- pass column: True/False based on threshold comparison

*Log Validation:*
- Required pattern: "Silhouette score: {value}"
- Required pattern: "Davies-Bouldin index: {value}"
- Required pattern: "Dunn index: {value}"
- If silhouette < 0.40 OR davies_bouldin >= 1.5: Log WARNING (not error - tentative clustering still reported)
- Forbidden patterns: "ERROR", "NaN in metric value"

**Expected Behavior on Validation Failure:**
- If metrics fail to compute: Raise error, log to logs/step04_validate_cluster_quality.log, quit
- If metrics computed but thresholds NOT met: Log warning, continue (clustering reported as tentative per concept.md)
- g_debug invoked only if computation fails (not threshold failures)

---

### Step 5: Assess Cluster Stability via Bootstrap Resampling

**Dependencies:** Step 3 (requires cluster assignments from full sample)
**Complexity:** Medium (bootstrap iterations, ~5 minutes for 100 iterations)

**Purpose:** Assess cluster stability via bootstrap resampling. Resample 80% of participants 100 times, refit K-means, compute Jaccard index (overlap with original clustering). Target: mean Jaccard >= 0.75 for stable clusters (per concept.md).

**Input:**

**File 1:** data/step01_standardized_features.csv
**Source:** Generated by Step 1 (standardized features)

**File 2:** data/step03_cluster_assignments.csv
**Source:** Generated by Step 3 (original cluster assignments)

**File 3:** data/step02_optimal_k.txt
**Source:** Generated by Step 2 (optimal K value)

**Processing:**

1. Load standardized features (UID + 6 z-scores)
2. Load original cluster assignments (UID + cluster label)
3. Read optimal K
4. Bootstrap resampling:
   - For iteration i in 1:100:
     - Sample 80% of participants (N=80) without replacement
     - Fit K-means on subsample (K clusters, random_state=42+i for iteration-specific seed)
     - Assign cluster labels to subsampled participants
     - Compute Jaccard index: overlap between original and bootstrap clustering for subsampled participants
     - Store Jaccard[i]
5. Compute summary statistics:
   - Mean Jaccard across 100 iterations
   - 95% CI for Jaccard (2.5th percentile, 97.5th percentile)
   - Check: mean Jaccard >= 0.75 (stable) vs < 0.75 (tentative per concept.md)
6. Report stability assessment

**Output:**

**File 1:** data/step05_bootstrap_stability.csv
**Format:** CSV, one row per bootstrap iteration
**Columns:**
  - `iteration` (int, 1 to 100)
  - `jaccard` (float, Jaccard coefficient for this iteration)
**Expected Rows:** 100

**File 2:** data/step05_stability_summary.txt
**Format:** Plain text report
**Content:** Mean Jaccard, 95% CI, stability interpretation (stable if >= 0.75, tentative if < 0.75)

**Validation Requirement:**

Validation tools MUST be used after bootstrap stability assessment. Specific validation tools will be determined by rq_tools (likely tools.validation.validate_bootstrap_stability). The rq_analysis agent will embed validation tool calls after the stability computation tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step05_bootstrap_stability.csv: 100 rows x 2 columns (iteration, jaccard)
- data/step05_stability_summary.txt: text report with mean, CI, interpretation
- Data types: iteration (int), jaccard (float64)

*Value Ranges:*
- iteration in {1, 2, ..., 100} (consecutive integers)
- Jaccard in [0, 1] (overlap coefficient, 1=perfect agreement, 0=no overlap)
- Mean Jaccard typically > 0.50 (below suggests unstable clustering)

*Data Quality:*
- Exactly 100 rows (one per bootstrap iteration)
- No NaN values in jaccard column
- No duplicate iteration numbers

*Log Validation:*
- Required pattern: "Bootstrap stability: mean Jaccard = {value}"
- Required pattern: "95% CI: [{lower}, {upper}]"
- If mean Jaccard < 0.75: Log WARNING (not error - tentative clustering reported)
- Forbidden patterns: "ERROR", "NaN in Jaccard"

**Expected Behavior on Validation Failure:**
- If stability computation fails: Raise error, log to logs/step05_bootstrap_stability.log, quit
- If mean Jaccard < 0.75: Log warning, continue (clustering reported as tentative per concept.md)
- g_debug invoked only if computation fails (not threshold failures)

---

### Step 6: Characterize Clusters (Profile Descriptions)

**Dependencies:** Step 3 (requires cluster assignments and centers)
**Complexity:** Low (descriptive statistics, <1 minute)

**Purpose:** Characterize clusters by computing mean intercepts and slopes per paradigm for each cluster (in original scale, not z-scores). Assign interpretive labels based on patterns (e.g., "High performers", "Paradigm-selective impairment").

**Input:**

**File 1:** data/step00_random_effects_wide.csv
**Source:** Generated by Step 0 (original random effects, NOT standardized)

**File 2:** data/step03_cluster_assignments.csv
**Source:** Generated by Step 3 (cluster labels per participant)

**File 3:** data/step03_cluster_centers.csv
**Source:** Generated by Step 3 (cluster centers in z-score scale)

**Processing:**

1. Load original random effects (UID + 6 features in original scale)
2. Load cluster assignments (UID + cluster label)
3. Merge on UID to create single DataFrame with random effects + cluster labels
4. For each cluster k in 0 to K-1:
   - Compute mean, SD, min, max for each of 6 features within cluster k
   - Identify cluster size (N participants)
   - Assess pattern:
     - High/Low intercepts across paradigms (generalized performance)
     - Paradigm-selective patterns (e.g., low Free Recall intercept but high Recognition intercept)
     - Positive/Negative slopes (forgetting vs improvement over time)
5. Assign interpretive labels based on patterns (e.g., "Cluster 0: High performers with minimal forgetting", "Cluster 1: Paradigm-selective impairment in Free Recall")
6. Create summary table with cluster labels, sizes, mean profiles

**Output:**

**File 1:** data/step06_cluster_characterization.csv
**Format:** CSV, one row per cluster per feature (long format for detailed statistics)
**Columns:**
  - `cluster` (int, cluster ID: 0 to K-1)
  - `feature` (string, feature name: Total_Intercept_Free, Total_Slope_Free, etc.)
  - `mean` (float, mean value within cluster)
  - `SD` (float, standard deviation within cluster)
  - `min` (float, minimum value within cluster)
  - `max` (float, maximum value within cluster)
  - `N` (int, cluster size)
**Expected Rows:** K x 6 (K clusters x 6 features)

**File 2:** data/step06_cluster_profiles.txt
**Format:** Plain text report
**Content:** Interpretive labels per cluster, narrative descriptions of profiles (e.g., "Cluster 0 shows high Free Recall intercepts (mean=1.2) with stable slopes (mean=-0.05), suggesting strong self-initiated retrieval with minimal forgetting")

**Validation Requirement:**

Validation tools MUST be used after cluster characterization. Specific validation tools will be determined by rq_tools (likely tools.validation.validate_cluster_summary_stats). The rq_analysis agent will embed validation tool calls after the characterization tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step06_cluster_characterization.csv: K x 6 rows (clusters x features)
- data/step06_cluster_profiles.txt: text report with interpretive labels
- Data types: cluster (int), feature (string/object), mean/SD/min/max (float64), N (int)

*Value Ranges:*
- cluster in {0, 1, ..., K-1}
- mean typically in [-2, 2] for intercepts, [-0.5, 0.5] for slopes (original scale)
- SD >= 0 (variance within cluster, cannot be negative)
- min <= mean <= max (logical consistency)
- N >= 10 (cluster size constraint from concept.md)

*Data Quality:*
- Exactly K x 6 rows (complete factorial: all clusters x all features)
- No NaN values
- min <= mean <= max for all rows
- SD >= 0 for all rows
- N > 0 for all rows

*Log Validation:*
- Required pattern: "Cluster characterization complete: K={K} clusters"
- Required pattern: "All clusters characterized with mean, SD, min, max"
- Forbidden patterns: "ERROR", "NaN in cluster statistics", "min > max"

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Cluster 1 feature Total_Intercept_Free has min=1.5 > max=1.2")
- Log failure to logs/step06_characterize_clusters.log
- Quit script immediately
- g_debug invoked to diagnose (likely data corruption or computation error)

---

### Step 7: Prepare Scatter Plot Matrix Data

**Dependencies:** Step 1 (standardized features), Step 3 (cluster assignments)
**Complexity:** Low (data aggregation for plotting, <1 minute)

**Purpose:** Prepare plot source CSV for scatter plot matrix (6x6 grid showing all pairwise feature combinations, colored by cluster). This is a plot data preparation step (Option B architecture) - creates CSV in data/ folder, rq_plots generates PNG later.

**CRITICAL NOTE:** Plot data preparation IS an analysis step. It:
- Gets executed in Step 14 CODE EXECUTION LOOP (g_code -> bash -> rq_inspect)
- MUST have validation requirements (same as any analysis step)
- Outputs to data/*.csv (not plots/*.csv) and validated by rq_inspect
- Created by g_code during analysis (NOT by rq_plots during visualization)

**Input:**

**File 1:** data/step01_standardized_features.csv
**Source:** Generated by Step 1 (standardized z-scores)

**File 2:** data/step03_cluster_assignments.csv
**Source:** Generated by Step 3 (cluster labels)

**File 3:** data/step03_cluster_centers.csv
**Source:** Generated by Step 3 (cluster centers for marking on plot)

**Plot Description:** Scatter plot matrix (6x6 grid) showing all pairwise combinations of 6 features, points colored by cluster assignment, cluster centers marked with reference symbols.

**Required Data Sources:**
- data/step01_standardized_features.csv (UID + 6 z-scores)
- data/step03_cluster_assignments.csv (UID + cluster label)
- data/step03_cluster_centers.csv (K x 6 cluster center coordinates)

**Aggregation Logic:**
1. Merge standardized features with cluster assignments on UID
2. Result: 100 rows x 8 columns (UID + 6 z-scores + cluster)
3. No aggregation needed (scatter plot shows individual points, not means)
4. Cluster centers stored separately for plotting layer (reference markers)

**Output (Plot Source CSV):** data/step07_scatter_matrix_data.csv

**Required Columns:**
- `UID` (string, participant identifier)
- `Total_Intercept_Free_z` (float, z-score)
- `Total_Slope_Free_z` (float, z-score)
- `Total_Intercept_Cued_z` (float, z-score)
- `Total_Slope_Cued_z` (float, z-score)
- `Total_Intercept_Recognition_z` (float, z-score)
- `Total_Slope_Recognition_z` (float, z-score)
- `cluster` (int, cluster label: 0 to K-1)

**Expected Rows:** 100 (one per participant)

**Validation Requirement:**

Validation tools MUST be used after plot data preparation tool execution. Specific validation tools determined by rq_tools based on plot data format requirements.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step07_scatter_matrix_data.csv exists (exact path)
- Expected rows: 100 (all participants)
- Expected columns: 8 (UID + 6 features + cluster)
- Data types: float (z-scores), int (cluster), string (UID)

*Value Ranges:*
- Z-scores typically in [-3, 3] (outliers acceptable for clustering)
- cluster in {0, 1, ..., K-1} (consecutive integers from Step 3)

*Data Quality:*
- No NaN values tolerated (all cells must have valid values)
- Expected N: Exactly 100 rows (no data loss)
- No duplicate UIDs
- All participants have cluster assignment (100 participants matched between features and assignments)

*Log Validation:*
- Required pattern: "Plot data preparation complete: 100 rows created"
- Required pattern: "All participants assigned to clusters"
- Forbidden patterns: "ERROR", "NaN values detected", "Unassigned participants"

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Expected 100 rows, found 98")
- Log failure to logs/step07_prepare_scatter_matrix_data.log
- Quit script immediately (do NOT proceed to rq_plots)
- g_debug invoked to diagnose root cause

**Plotting Function (rq_plots will call):** Scatter plot matrix with cluster colors
- rq_plots agent maps this description to specific tools/plotting.py function
- Plot reads data/step07_scatter_matrix_data.csv (created by this step)
- Plot reads data/step03_cluster_centers.csv (for marking cluster centers)
- No data aggregation in rq_plots (visualization-only per Option B)
- PNG output saved to plots/ folder by rq_plots

---

## Expected Outputs

### Data Files (ALL analysis inputs and outputs - intermediate and final)
- data/step00_random_effects_wide.csv (from Step 0: reshaped random effects)
- data/step01_standardized_features.csv (from Step 1: z-scores)
- data/step01_standardization_summary.txt (from Step 1: pre/post statistics)
- data/step02_cluster_selection.csv (from Step 2: K, inertia, BIC)
- data/step02_optimal_k.txt (from Step 2: selected K with rationale)
- data/step02_elbow_plot_data.csv (from Step 2: elbow plot source)
- data/step03_cluster_assignments.csv (from Step 3: UID, cluster)
- data/step03_cluster_centers.csv (from Step 3: K x 6 cluster centers)
- data/step03_cluster_sizes.txt (from Step 3: cluster sizes)
- data/step04_cluster_quality_metrics.csv (from Step 4: silhouette, DB, Dunn)
- data/step04_quality_interpretation.txt (from Step 4: quality assessment)
- data/step05_bootstrap_stability.csv (from Step 5: 100 Jaccard values)
- data/step05_stability_summary.txt (from Step 5: mean Jaccard, CI)
- data/step06_cluster_characterization.csv (from Step 6: cluster profiles)
- data/step06_cluster_profiles.txt (from Step 6: interpretive labels)
- data/step07_scatter_matrix_data.csv (from Step 7: plot source CSV)

### Logs (ONLY execution logs - .log files capturing stdout/stderr)
- logs/step00_load_reshape_random_effects.log
- logs/step01_standardize_features.log
- logs/step02_cluster_selection.log
- logs/step03_fit_final_kmeans.log
- logs/step04_validate_cluster_quality.log
- logs/step05_bootstrap_stability.log
- logs/step06_characterize_clusters.log
- logs/step07_prepare_scatter_matrix_data.log

### Plots (EMPTY until rq_plots runs)
- plots/elbow_plot.png (created by rq_plots, NOT analysis steps)
- plots/scatter_matrix.png (created by rq_plots)

### Results (EMPTY until rq_results runs)
- results/summary.md (created by rq_results, NOT analysis steps)

---

## Expected Data Formats

### Step 0 -> Step 1 Transformation (Long to Wide Reshape)

**Input Format (from RQ 5.3.7):**
- File: results/ch5/5.3.7/data/step04_random_effects.csv
- Format: Long (one row per participant-paradigm combination)
- Columns: UID, paradigm, Total_Intercept, Total_Slope
- Example rows:
  - P001, Free, 1.2, -0.05
  - P001, Cued, 0.8, -0.03
  - P001, Recognition, 0.5, -0.02

**Reshape Logic:**
- Pivot on paradigm column (Free, Cued, Recognition)
- Create 6 columns: Total_Intercept_Free, Total_Slope_Free, Total_Intercept_Cued, Total_Slope_Cued, Total_Intercept_Recognition, Total_Slope_Recognition
- Each participant becomes single row

**Output Format:**
- File: data/step00_random_effects_wide.csv
- Format: Wide (one row per participant)
- Columns: UID, Total_Intercept_Free, Total_Slope_Free, Total_Intercept_Cued, Total_Slope_Cued, Total_Intercept_Recognition, Total_Slope_Recognition
- Example row:
  - P001, 1.2, -0.05, 0.8, -0.03, 0.5, -0.02

### Step 1 -> Step 2 Transformation (Standardization)

**Input Format:**
- Original scale random effects (means and SDs vary by feature)

**Standardization Logic:**
- For each feature: z = (x - mean) / SD
- Result: mean=0, SD=1 for all features (equal weighting in clustering)

**Output Format:**
- Columns renamed with "_z" suffix
- All features on same scale (z-scores)

### Step 3 -> Step 6 Transformation (Cluster Assignment)

**Step 3 Output:**
- Cluster assignments: UID -> cluster label (0 to K-1)

**Step 6 Processing:**
- Merge original random effects with cluster assignments
- Group by cluster, compute summary statistics per feature
- Characterize cluster profiles based on mean intercepts/slopes

**Step 6 Output:**
- Long format: K x 6 rows (cluster x feature combinations)
- Summary statistics: mean, SD, min, max, N per cluster per feature

### Column Naming Conventions

**From names.md (if applicable):**
- UID: participant identifier (no underscore)
- cluster: cluster assignment (lowercase)
- Total_Intercept_*: random intercept from RQ 5.3.7 LMM
- Total_Slope_*: random slope from RQ 5.3.7 LMM
- *_z suffix: standardized z-score version

**Paradigm suffixes:**
- _Free: Free Recall paradigm (IFR)
- _Cued: Cued Recall paradigm (ICR)
- _Recognition: Recognition paradigm (IRE)

---

## Cross-RQ Dependencies

### Dependency Type: DERIVED Data from Other RQs (Dependencies Exist)

**This RQ requires outputs from:**
- **RQ 5.3.7** (Paradigm-Specific Variance Decomposition)
  - File: results/ch5/5.3.7/data/step04_random_effects.csv
  - Used in: Step 0 (load random effects for clustering)
  - Rationale: RQ 5.3.7 fits paradigm-stratified LMMs and extracts random intercepts and slopes for each participant in each paradigm. This RQ clusters participants based on those 6 random effects (intercept + slope per paradigm).

**Execution Order Constraint:**
1. RQ 5.3.7 must complete first (provides step04_random_effects.csv)
2. This RQ executes second (uses RQ 5.3.7 random effects as clustering features)

**Data Source Boundaries:**
- **RAW data:** None (no direct extraction from master.xlsx)
- **DERIVED data:** results/ch5/5.3.7/data/step04_random_effects.csv (random effects from paradigm LMMs)
- **Scope:** This RQ does NOT fit LMMs (uses random effects from RQ 5.3.7 as input features)

**Validation:**
- Step 0: Check results/ch5/5.3.7/data/step04_random_effects.csv exists (circuit breaker: FILE_MISSING if absent)
- Step 0: Verify 300 rows (100 participants x 3 paradigms)
- If file missing -> quit with error -> user must execute RQ 5.3.7 first

---

## Validation Requirements

**CRITICAL MANDATE:**

Every analysis step in this plan MUST use validation tools after analysis tool execution.

This is not optional. This is the core architectural principle preventing cascading failures observed in v3.0 (where analysis errors propagated undetected through 5+ downstream steps before discovery).

**Exact Specification Requirement:**

> "Validation tools MUST be used after analysis tool execution"

**Implementation:**
- rq_tools (Step 11 workflow) will read tool_inventory.md validation tools section
- rq_tools will specify BOTH analysis tool + validation tool per step in 3_tools.yaml
- rq_analysis (Step 12 workflow) will embed validation tool call AFTER analysis tool call in 4_analysis.yaml
- g_code (Step 14 workflow) will generate stepN_name.py scripts with validation function calls
- bash execution (Step 14 workflow) will run analysis -> validation -> error on validation failure

**Downstream Agent Requirements:**
- **rq_tools:** MUST specify validation tool for EVERY analysis step (no exceptions)
- **rq_analysis:** MUST embed validation tool call for EVERY analysis step (no exceptions)
- **g_code:** MUST generate code with validation function calls (no exceptions)
- **rq_inspect:** MUST verify validation ran successfully (checks logs/stepN_name.log for validation output)

### Validation Requirements By Step

#### Step 0: Load and Reshape Random Effects
**Analysis Tool:** (determined by rq_tools - likely pandas.read_csv + pivot)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_dataframe_structure)

**What Validation Checks:**
- Output file exists (data/step00_random_effects_wide.csv)
- Expected column count (7 columns: UID + 6 features)
- Expected row count (100 rows: 100 participants)
- No unexpected NaN patterns (all participants must have all 6 features)
- Value ranges (intercepts in [-3, 3], slopes in [-1, 1])

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message
- Log failure to logs/step00_load_reshape_random_effects.log
- Quit script immediately (do NOT proceed to Step 1)
- g_debug invoked to diagnose root cause

#### Step 1: Standardize Features
**Analysis Tool:** (determined by rq_tools - likely sklearn.preprocessing.StandardScaler or manual z-score)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_standardization)

**What Validation Checks:**
- Output file exists (data/step01_standardized_features.csv)
- Expected column count (7 columns: UID + 6 z-scores)
- Expected row count (100 rows)
- Standardization correct (mean ~0, SD ~1 for all features within tolerance)
- No NaN values

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message
- Log failure to logs/step01_standardize_features.log
- Quit script immediately
- g_debug invoked to diagnose

#### Step 2: Cluster Selection via BIC
**Analysis Tool:** (determined by rq_tools - likely sklearn.cluster.KMeans with BIC computation)
**Validation Tool:** (determined by rq_tools - custom BIC validation)

**What Validation Checks:**
- Output files exist (cluster_selection.csv, optimal_k.txt, elbow_plot_data.csv)
- Expected rows in cluster_selection.csv (6 rows for K=1-6)
- Inertia decreases monotonically
- BIC minimum at K in [2, 6] (K=1 not meaningful clustering)
- Selected K reported in optimal_k.txt

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message
- Log failure to logs/step02_cluster_selection.log
- Quit script immediately
- g_debug invoked to diagnose

#### Step 3: Fit Final K-Means
**Analysis Tool:** (determined by rq_tools - likely sklearn.cluster.KMeans)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_cluster_assignment)

**What Validation Checks:**
- Output files exist (cluster_assignments.csv, cluster_centers.csv, cluster_sizes.txt)
- Expected rows in assignments (100 participants)
- Expected rows in centers (K clusters)
- Cluster labels consecutive 0 to K-1 (no gaps)
- All cluster sizes >= 10 (10% minimum per concept.md)
- No NaN values

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message
- Log failure to logs/step03_fit_final_kmeans.log
- Quit script immediately
- g_debug invoked to diagnose

#### Step 4: Validate Cluster Quality
**Analysis Tool:** (determined by rq_tools - likely sklearn.metrics silhouette_score, davies_bouldin_score, custom Dunn)
**Validation Tool:** (determined by rq_tools - custom quality threshold validation)

**What Validation Checks:**
- Output files exist (cluster_quality_metrics.csv, quality_interpretation.txt)
- Expected rows in metrics (3 metrics: silhouette, davies_bouldin, dunn)
- Metric values in valid ranges (silhouette in [-1,1], DB > 0, Dunn > 0)
- Threshold checks: silhouette >= 0.40 (acceptable), davies_bouldin < 1.5 (acceptable)
- If thresholds not met: Log warning (not error - tentative clustering reported)

**Expected Behavior on Validation Failure:**
- If metrics fail to compute: Raise error, quit, g_debug invoked
- If thresholds not met: Log warning, continue (clustering reported as tentative)

#### Step 5: Bootstrap Stability
**Analysis Tool:** (determined by rq_tools - custom bootstrap resampling with KMeans)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_bootstrap_stability)

**What Validation Checks:**
- Output files exist (bootstrap_stability.csv, stability_summary.txt)
- Expected rows in stability (100 bootstrap iterations)
- Jaccard values in [0, 1]
- Mean Jaccard computed, 95% CI computed
- Threshold check: mean Jaccard >= 0.75 (stable)
- If threshold not met: Log warning (not error - tentative clustering reported)

**Expected Behavior on Validation Failure:**
- If stability computation fails: Raise error, quit, g_debug invoked
- If threshold not met: Log warning, continue (clustering reported as tentative)

#### Step 6: Characterize Clusters
**Analysis Tool:** (determined by rq_tools - pandas groupby with summary statistics)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_cluster_summary_stats)

**What Validation Checks:**
- Output files exist (cluster_characterization.csv, cluster_profiles.txt)
- Expected rows in characterization (K x 6: all clusters x all features)
- Summary statistics logical (min <= mean <= max, SD >= 0, N > 0)
- No NaN values
- All clusters have N >= 10

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message
- Log failure to logs/step06_characterize_clusters.log
- Quit script immediately
- g_debug invoked to diagnose

#### Step 7: Prepare Scatter Matrix Plot Data
**Analysis Tool:** (determined by rq_tools - pandas merge)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_plot_data_completeness)

**What Validation Checks:**
- Output file exists (step07_scatter_matrix_data.csv)
- Expected rows (100 participants)
- Expected columns (8: UID + 6 z-scores + cluster)
- All participants have cluster assignments (no NaN in cluster column)
- Z-score values reasonable (typically in [-3, 3])
- Cluster values in {0, 1, ..., K-1}

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message
- Log failure to logs/step07_prepare_scatter_matrix_data.log
- Quit script immediately
- g_debug invoked to diagnose

---

## Summary

**Total Steps:** 8 (Step 0-7)
**Estimated Runtime:** Low-Medium (~15-20 minutes total: 5 min Step 2, 5 min Step 5, <10 min all others)
**Cross-RQ Dependencies:** RQ 5.3.7 (must complete first)
**Primary Outputs:** Cluster assignments (100 participants), cluster quality metrics (silhouette, Davies-Bouldin, Dunn, bootstrap stability), cluster characterization (profile descriptions), scatter plot matrix data
**Validation Coverage:** 100% (all 8 steps have validation requirements)

**Methodological Notes:**
- K-means selected over LPA (exploratory nature, computational efficiency, interpretability)
- BIC model selection with parsimony rule (prefer simpler K if BIC difference <2)
- Comprehensive validation: quality metrics + stability assessment
- Tentative clustering reported if thresholds not met (not failure - transparent reporting)
- Sphericity assumption checked visually (scatter matrix should show circular clusters)

**Expected Cluster Profiles (Exploratory):**
- 2-4 clusters expected per concept.md
- Possible profiles: generalized high/low performers, paradigm-selective patterns (poor Free Recall but intact Recognition suggesting recollection deficit)
- Cluster sizes balanced (no cluster <10% of sample)

---

**Next Steps (Workflow):**
1. User reviews and approves this plan (Step 7 user gate)
2. Workflow continues to Step 11: rq_tools reads this plan -> creates 3_tools.yaml
3. Workflow continues to Step 12: rq_analysis reads this plan + 3_tools.yaml -> creates 4_analysis.yaml
4. Workflow continues to Step 14: g_code reads 4_analysis.yaml -> generates stepN_name.py scripts

---

**Version History:**
- v1.0 (2025-12-02): Initial plan created by rq_planner agent for RQ 5.3.8
