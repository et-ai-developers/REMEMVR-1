#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step00
Step Name: Load and Reshape Random Effects
RQ: results/ch5/5.3.8
Generated: 2025-12-04

PURPOSE:
Load paradigm-specific random effects from RQ 5.3.7 and reshape from long to
wide format (100 participants x 6 features) for K-means clustering.

EXPECTED INPUTS:
- results/ch5/5.3.7/data/step04_random_effects.csv
  Columns: ['UID', 'paradigm', 'Total_Intercept', 'Total_Slope']
  Format: Long format (300 rows: 100 UIDs × 3 paradigms)
  Paradigms: IFR (Free), ICR (Cued), IRE (Recognition)
  Expected rows: 300

EXPECTED OUTPUTS:
- data/step00_random_effects_wide.csv
  Columns: ['UID', 'Total_Intercept_Free', 'Total_Slope_Free',
           'Total_Intercept_Cued', 'Total_Slope_Cued',
           'Total_Intercept_Recognition', 'Total_Slope_Recognition']
  Format: Wide format (100 rows: 1 per participant)
  Expected rows: 100

VALIDATION CRITERIA:
- Row count = 100 (all participants)
- All 7 columns present (UID + 6 features)
- No missing values (all participants have all paradigms)

g_code REASONING:
- Approach: Pivot long to wide format using paradigm as columns
- Why this approach: K-means needs wide format (participants × features matrix)
- Data flow: 300 rows long → 100 rows wide (pivot on paradigm)
- Expected performance: <1 second (simple pandas pivot)

IMPLEMENTATION NOTES:
- Analysis: pandas pivot_table operation (stdlib)
- Validation: tools.validation.validate_dataframe_structure
- Paradigm mapping: IFR→Free, ICR→Cued, IRE→Recognition (for column names)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import traceback

# Add project root to path for imports
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

from tools.validation import validate_dataframe_structure

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.3.8
LOG_FILE = RQ_DIR / "logs" / "step00_load_reshape_random_effects.log"

# Input from RQ 5.3.7
INPUT_FILE = PROJECT_ROOT / "results" / "ch5" / "5.3.7" / "data" / "step04_random_effects.csv"

# Output
OUTPUT_FILE = RQ_DIR / "data" / "step00_random_effects_wide.csv"

# Paradigm mapping (codes in data → column names)
PARADIGM_MAP = {
    'IFR': 'Free',
    'ICR': 'Cued',
    'IRE': 'Recognition'
}

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 00: Load and Reshape Random Effects")

        # =========================================================================
        # STEP 1: Load Long-Format Random Effects from RQ 5.3.7
        # =========================================================================
        # Expected: 300 rows (100 participants × 3 paradigms)
        # Purpose: Source data for K-means feature matrix

        log(f"[LOAD] Loading random effects from RQ 5.3.7...")
        log(f"[LOAD] File: {INPUT_FILE}")

        df_long = pd.read_csv(INPUT_FILE, encoding='utf-8')
        log(f"[LOADED] {len(df_long)} rows, {len(df_long.columns)} columns")

        # Verify expected structure
        expected_cols = ['UID', 'paradigm', 'Total_Intercept', 'Total_Slope']
        if list(df_long.columns) != expected_cols:
            raise ValueError(f"Unexpected columns. Expected {expected_cols}, got {list(df_long.columns)}")

        # Verify row count
        if len(df_long) != 300:
            raise ValueError(f"Expected 300 rows (100 UIDs × 3 paradigms), got {len(df_long)}")

        # Check paradigm values
        paradigms = sorted(df_long['paradigm'].unique())
        log(f"[INFO] Paradigms in data: {paradigms}")
        if paradigms != ['ICR', 'IFR', 'IRE']:
            raise ValueError(f"Expected paradigms [ICR, IFR, IRE], got {paradigms}")

        # Check for missing values
        if df_long.isnull().any().any():
            raise ValueError("Found missing values in input data")

        log(f"[INFO] Verified: 300 rows, 3 paradigms, no missing values")

        # =========================================================================
        # STEP 2: Pivot to Wide Format
        # =========================================================================
        # Tool: pandas pivot operation
        # What it does: Reshape long (300×4) to wide (100×7)
        # Expected output: UID + 6 features (intercept + slope per paradigm)

        log("[ANALYSIS] Pivoting from long to wide format...")

        # Map paradigm codes to column names
        df_long['paradigm_name'] = df_long['paradigm'].map(PARADIGM_MAP)

        # Pivot: create separate columns for each paradigm × feature combination
        df_wide = df_long.pivot(
            index='UID',
            columns='paradigm_name',
            values=['Total_Intercept', 'Total_Slope']
        )

        # Flatten multi-level columns: (feature, paradigm) → feature_paradigm
        df_wide.columns = [f'{feature}_{paradigm}' for feature, paradigm in df_wide.columns]

        # Reset index to make UID a column
        df_wide = df_wide.reset_index()

        log(f"[DONE] Pivot complete: {len(df_wide)} rows, {len(df_wide.columns)} columns")
        log(f"[INFO] Columns: {list(df_wide.columns)}")

        # Verify column names
        expected_wide_cols = [
            'UID',
            'Total_Intercept_Cued', 'Total_Intercept_Free', 'Total_Intercept_Recognition',
            'Total_Slope_Cued', 'Total_Slope_Free', 'Total_Slope_Recognition'
        ]
        # Sort both lists for comparison (pivot column order may vary)
        if sorted(df_wide.columns) != sorted(expected_wide_cols):
            raise ValueError(f"Column name mismatch after pivot. Expected {expected_wide_cols}, got {list(df_wide.columns)}")

        # =========================================================================
        # STEP 3: Save Wide-Format Output
        # =========================================================================
        # Output: 100 rows × 7 columns (UID + 6 features)
        # Downstream: step01 will standardize these features

        log(f"[SAVE] Saving wide-format random effects...")
        df_wide.to_csv(OUTPUT_FILE, index=False, encoding='utf-8')
        log(f"[SAVED] {OUTPUT_FILE} ({len(df_wide)} rows, {len(df_wide.columns)} cols)")

        # =========================================================================
        # STEP 4: Run Validation Tool
        # =========================================================================
        # Tool: validate_dataframe_structure
        # Validates: Row count = 100, all 7 columns present, no missing values

        log("[VALIDATION] Running validate_dataframe_structure...")

        validation_result = validate_dataframe_structure(
            df=df_wide,
            expected_rows=100,
            expected_columns=expected_wide_cols,
            column_types=None  # No type checking needed
        )

        # Report validation results
        if isinstance(validation_result, dict):
            for key, value in validation_result.items():
                log(f"[VALIDATION] {key}: {value}")

        # Check validation passed
        if not validation_result.get('valid', False):
            raise ValueError(f"Validation failed: {validation_result.get('message', 'Unknown error')}")

        log("[SUCCESS] Step 00 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
