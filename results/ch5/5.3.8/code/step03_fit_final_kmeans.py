#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step03
Step Name: Fit Final K-Means Model
RQ: results/ch5/5.3.8
Generated: 2025-12-04

PURPOSE:
Fit final K-means model using optimal K from step02, extract cluster assignments
and centers for downstream analysis and interpretation.

EXPECTED INPUTS:
- data/step01_standardized_features.csv
  Columns: ['UID', 'Total_Intercept_Cued_z', 'Total_Intercept_Free_z',
           'Total_Intercept_Recognition_z', 'Total_Slope_Cued_z',
           'Total_Slope_Free_z', 'Total_Slope_Recognition_z']
  Format: Z-scores (mean=0, SD=1)
  Expected rows: 100

- data/step02_optimal_k.txt
  Selected K value (parse from text)

EXPECTED OUTPUTS:
- data/step03_cluster_assignments.csv
  Columns: ['UID', 'cluster']
  Format: Cluster assignment per participant (cluster IDs: 0 to K-1)
  Expected rows: 100

- data/step03_cluster_centers.csv
  Columns: ['cluster', 'Total_Intercept_Cued_z', 'Total_Intercept_Free_z',
           'Total_Intercept_Recognition_z', 'Total_Slope_Cued_z',
           'Total_Slope_Free_z', 'Total_Slope_Recognition_z']
  Format: Cluster centers (K rows × 6 feature means)
  Expected rows: K

- data/step03_cluster_sizes.txt
  Cluster sizes report (N per cluster)

VALIDATION CRITERIA:
- All 100 participants assigned (length = 100)
- Cluster IDs consecutive 0 to K-1 (no gaps)
- Each cluster has >= 10 members (10% minimum per concept.md)

g_code REASONING:
- Approach: Fit K-means with optimal K, extract assignments + centers
- Why this approach: Optimal K from BIC selection (step02)
- Data flow: Standardized features → K-means → assignments + centers
- Expected performance: ~2-3 seconds (single K-means fit, n_init=50)

IMPLEMENTATION NOTES:
- Analysis: sklearn KMeans (stdlib)
- Validation: tools.validation.validate_cluster_assignment
- Parameters: random_state=42 (reproducibility), n_init=50 (stability)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
import traceback

# Add project root to path for imports
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

from tools.validation import validate_cluster_assignment

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.3.8
LOG_FILE = RQ_DIR / "logs" / "step03_fit_final_kmeans.log"

# Inputs
INPUT_FILE = RQ_DIR / "data" / "step01_standardized_features.csv"
OPTIMAL_K_FILE = RQ_DIR / "data" / "step02_optimal_k.txt"

# Outputs
ASSIGNMENTS_FILE = RQ_DIR / "data" / "step03_cluster_assignments.csv"
CENTERS_FILE = RQ_DIR / "data" / "step03_cluster_centers.csv"
SIZES_FILE = RQ_DIR / "data" / "step03_cluster_sizes.txt"

# K-means parameters
RANDOM_STATE = 42
N_INIT = 50

# Feature columns (exclude UID)
FEATURE_COLS = [
    'Total_Intercept_Cued_z',
    'Total_Intercept_Free_z',
    'Total_Intercept_Recognition_z',
    'Total_Slope_Cued_z',
    'Total_Slope_Free_z',
    'Total_Slope_Recognition_z'
]

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 03: Fit Final K-Means Model")

        # =========================================================================
        # STEP 1: Load Standardized Features
        # =========================================================================
        # Expected: 100 rows × 7 columns (UID + 6 z-score features)
        # Purpose: Feature matrix for final K-means fit

        log(f"[LOAD] Loading standardized features...")
        df = pd.read_csv(INPUT_FILE, encoding='utf-8')
        log(f"[LOADED] {len(df)} rows, {len(df.columns)} columns")

        # Verify feature columns exist
        missing_cols = [col for col in FEATURE_COLS if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Missing feature columns: {missing_cols}")

        # Extract feature matrix (exclude UID)
        X = df[FEATURE_COLS].values
        UIDs = df['UID'].values

        log(f"[INFO] Feature matrix: {X.shape[0]} participants, {X.shape[1]} features")

        # =========================================================================
        # STEP 2: Read Optimal K from Step 02
        # =========================================================================
        # Expected: Single integer value (K=2-6)
        # Purpose: Number of clusters for final K-means fit

        log(f"[LOAD] Reading optimal K from {OPTIMAL_K_FILE}...")

        with open(OPTIMAL_K_FILE, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # Parse optimal K (last line should be "OPTIMAL K: X")
        optimal_k_line = [line for line in lines if line.startswith("OPTIMAL K:")]
        if not optimal_k_line:
            raise ValueError(f"Could not find 'OPTIMAL K:' line in {OPTIMAL_K_FILE}")

        K_optimal = int(optimal_k_line[0].split(':')[1].strip())
        log(f"[INFO] Optimal K={K_optimal}")

        # =========================================================================
        # STEP 3: Fit Final K-Means Model
        # =========================================================================
        # Tool: sklearn KMeans
        # What it does: Assign participants to K clusters, compute cluster centers
        # Expected output: 100 cluster assignments + K cluster centers

        log(f"[ANALYSIS] Fitting final K-means with K={K_optimal}...")
        log(f"[INFO] Parameters: n_init={N_INIT}, random_state={RANDOM_STATE}")

        kmeans = KMeans(
            n_clusters=K_optimal,
            random_state=RANDOM_STATE,
            n_init=N_INIT,
            max_iter=300
        )
        kmeans.fit(X)

        # Extract cluster assignments (0 to K-1)
        cluster_labels = kmeans.labels_

        # Extract cluster centers (K × 6 matrix)
        cluster_centers = kmeans.cluster_centers_

        log(f"[DONE] K-means fitting complete")
        log(f"[INFO] Cluster assignments: {len(cluster_labels)} participants")
        log(f"[INFO] Cluster centers: {cluster_centers.shape[0]} clusters × {cluster_centers.shape[1]} features")

        # =========================================================================
        # STEP 4: Save Cluster Assignments
        # =========================================================================
        # Output: 100 rows × 2 columns (UID, cluster)
        # Downstream: step04 (quality validation), step05 (bootstrap stability)

        log(f"[SAVE] Saving cluster assignments...")

        df_assignments = pd.DataFrame({
            'UID': UIDs,
            'cluster': cluster_labels
        })

        df_assignments.to_csv(ASSIGNMENTS_FILE, index=False, encoding='utf-8')
        log(f"[SAVED] {ASSIGNMENTS_FILE} ({len(df_assignments)} rows)")

        # =========================================================================
        # STEP 5: Save Cluster Centers
        # =========================================================================
        # Output: K rows × 7 columns (cluster ID + 6 feature means)
        # Downstream: step06 (characterization), rq_plots (scatter matrix markers)

        log(f"[SAVE] Saving cluster centers...")

        df_centers = pd.DataFrame(
            cluster_centers,
            columns=FEATURE_COLS
        )
        df_centers.insert(0, 'cluster', range(K_optimal))

        df_centers.to_csv(CENTERS_FILE, index=False, encoding='utf-8')
        log(f"[SAVED] {CENTERS_FILE} ({len(df_centers)} rows)")

        # =========================================================================
        # STEP 6: Compute and Save Cluster Sizes
        # =========================================================================
        # Output: Text report (N per cluster)
        # Downstream: Verify minimum size >= 10 (10% threshold)

        log(f"[SAVE] Computing cluster sizes...")

        cluster_sizes = pd.Series(cluster_labels).value_counts().sort_index()

        log(f"[INFO] Cluster sizes:")
        for cluster_id, size in cluster_sizes.items():
            pct = (size / len(cluster_labels)) * 100
            log(f"[INFO]   Cluster {cluster_id}: N={size} ({pct:.1f}%)")

        # Write sizes report
        with open(SIZES_FILE, 'w', encoding='utf-8') as f:
            f.write("CLUSTER SIZES REPORT\n")
            f.write("=" * 80 + "\n\n")

            f.write(f"Total participants: {len(cluster_labels)}\n")
            f.write(f"Number of clusters: {K_optimal}\n\n")

            f.write("CLUSTER SIZES:\n")
            f.write("-" * 80 + "\n")

            for cluster_id, size in cluster_sizes.items():
                pct = (size / len(cluster_labels)) * 100
                f.write(f"Cluster {cluster_id}:  N={size:3d}  ({pct:5.1f}%)\n")

            f.write("\n")
            f.write("MINIMUM SIZE THRESHOLD:\n")
            f.write("-" * 80 + "\n")
            min_size = cluster_sizes.min()
            min_threshold = 10
            f.write(f"Minimum cluster size: {min_size}\n")
            f.write(f"Threshold (10%): {min_threshold}\n")

            if min_size >= min_threshold:
                f.write(f"STATUS: PASS (all clusters >= {min_threshold})\n")
            else:
                f.write(f"STATUS: FAIL (cluster {cluster_sizes.idxmin()} has only {min_size} members)\n")

        log(f"[SAVED] {SIZES_FILE}")

        # =========================================================================
        # STEP 7: Run Validation Tool
        # =========================================================================
        # Tool: validate_cluster_assignment
        # Validates: 100 participants assigned, cluster IDs 0 to K-1, sizes >= 10

        log("[VALIDATION] Running validate_cluster_assignment...")

        validation_result = validate_cluster_assignment(
            assignments_df=df_assignments,
            n_participants=100,
            min_cluster_size=10,
            cluster_col='cluster'
        )

        # Report validation results
        if isinstance(validation_result, dict):
            for key, value in validation_result.items():
                log(f"[VALIDATION] {key}: {value}")

        # Check validation passed
        if not validation_result.get('valid', False):
            raise ValueError(f"Validation failed: {validation_result.get('message', 'Unknown error')}")

        log("[SUCCESS] Step 03 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
