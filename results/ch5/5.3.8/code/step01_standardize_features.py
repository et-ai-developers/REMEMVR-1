#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step01
Step Name: Standardize Features
RQ: results/ch5/5.3.8
Generated: 2025-12-04

PURPOSE:
Standardize all 6 features to z-scores (mean=0, SD=1) for equal weighting
in K-means clustering. Prevents features with large variances from dominating
cluster assignments.

EXPECTED INPUTS:
- data/step00_random_effects_wide.csv
  Columns: ['UID', 'Total_Intercept_Cued', 'Total_Intercept_Free',
           'Total_Intercept_Recognition', 'Total_Slope_Cued',
           'Total_Slope_Free', 'Total_Slope_Recognition']
  Format: Wide format (100 rows, original scale)
  Expected rows: 100

EXPECTED OUTPUTS:
- data/step01_standardized_features.csv
  Columns: ['UID', 'Total_Intercept_Cued_z', 'Total_Intercept_Free_z',
           'Total_Intercept_Recognition_z', 'Total_Slope_Cued_z',
           'Total_Slope_Free_z', 'Total_Slope_Recognition_z']
  Format: Z-scores (mean=0, SD=1)
  Expected rows: 100

- data/step01_standardization_summary.txt
  Pre/post standardization statistics (mean, SD per feature)

VALIDATION CRITERIA:
- Mean of each z-score column within [-0.01, 0.01]
- SD of each z-score column within [0.99, 1.01]
- No NaN values in standardized columns

g_code REASONING:
- Approach: Manual z-score calculation (x - mean) / SD
- Why this approach: K-means uses Euclidean distance, requires equal weighting
- Data flow: Original scale → z-scores (preserves relationships)
- Expected performance: <1 second (simple arithmetic)

IMPLEMENTATION NOTES:
- Analysis: Manual z-score calculation (stdlib pandas operations)
- Validation: tools.validation.validate_standardization
- Column naming: Append _z suffix to original names
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import traceback

# Add project root to path for imports
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

from tools.validation import validate_standardization

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/5.3.8
LOG_FILE = RQ_DIR / "logs" / "step01_standardize_features.log"

# Input from step00
INPUT_FILE = RQ_DIR / "data" / "step00_random_effects_wide.csv"

# Outputs
OUTPUT_FILE = RQ_DIR / "data" / "step01_standardized_features.csv"
SUMMARY_FILE = RQ_DIR / "data" / "step01_standardization_summary.txt"

# Feature columns to standardize (exclude UID)
FEATURE_COLS = [
    'Total_Intercept_Cued',
    'Total_Intercept_Free',
    'Total_Intercept_Recognition',
    'Total_Slope_Cued',
    'Total_Slope_Free',
    'Total_Slope_Recognition'
]

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 01: Standardize Features")

        # =========================================================================
        # STEP 1: Load Wide-Format Random Effects
        # =========================================================================
        # Expected: 100 rows × 7 columns (UID + 6 features, original scale)
        # Purpose: Source data for z-score standardization

        log(f"[LOAD] Loading wide-format random effects...")
        df_wide = pd.read_csv(INPUT_FILE, encoding='utf-8')
        log(f"[LOADED] {len(df_wide)} rows, {len(df_wide.columns)} columns")

        # Verify feature columns exist
        missing_cols = [col for col in FEATURE_COLS if col not in df_wide.columns]
        if missing_cols:
            raise ValueError(f"Missing feature columns: {missing_cols}")

        log(f"[INFO] Feature columns: {FEATURE_COLS}")

        # =========================================================================
        # STEP 2: Compute Pre-Standardization Statistics
        # =========================================================================
        # Tool: pandas describe()
        # What it does: Compute mean, SD for each feature (original scale)
        # Purpose: Document pre-standardization statistics for reproducibility

        log("[ANALYSIS] Computing pre-standardization statistics...")

        pre_stats = df_wide[FEATURE_COLS].describe().loc[['mean', 'std']]
        log(f"[INFO] Pre-standardization statistics:")
        for col in FEATURE_COLS:
            mean_val = pre_stats.loc['mean', col]
            std_val = pre_stats.loc['std', col]
            log(f"[INFO]   {col}: mean={mean_val:.6f}, SD={std_val:.6f}")

        # =========================================================================
        # STEP 3: Standardize Features to Z-Scores
        # =========================================================================
        # Tool: Manual z-score calculation
        # Formula: z = (x - mean) / SD
        # Expected output: mean~0, SD~1 for each feature

        log("[ANALYSIS] Standardizing features to z-scores...")

        df_standardized = df_wide[['UID']].copy()

        for col in FEATURE_COLS:
            mean_val = df_wide[col].mean()
            std_val = df_wide[col].std()

            # Z-score: (x - mean) / SD
            z_col = f"{col}_z"
            df_standardized[z_col] = (df_wide[col] - mean_val) / std_val

            log(f"[INFO] Standardized {col} -> {z_col}")

        log(f"[DONE] Standardization complete: {len(df_standardized.columns)} columns")

        # =========================================================================
        # STEP 4: Compute Post-Standardization Statistics
        # =========================================================================
        # Verify: mean~0, SD~1 for all z-score columns

        log("[ANALYSIS] Computing post-standardization statistics...")

        z_cols = [f"{col}_z" for col in FEATURE_COLS]
        post_stats = df_standardized[z_cols].describe().loc[['mean', 'std']]

        log(f"[INFO] Post-standardization statistics:")
        for z_col in z_cols:
            mean_val = post_stats.loc['mean', z_col]
            std_val = post_stats.loc['std', z_col]
            log(f"[INFO]   {z_col}: mean={mean_val:.6f}, SD={std_val:.6f}")

        # =========================================================================
        # STEP 5: Save Outputs
        # =========================================================================
        # Output 1: Standardized features (100 rows × 7 columns)
        # Output 2: Standardization summary (pre/post statistics)

        log(f"[SAVE] Saving standardized features...")
        df_standardized.to_csv(OUTPUT_FILE, index=False, encoding='utf-8')
        log(f"[SAVED] {OUTPUT_FILE} ({len(df_standardized)} rows, {len(df_standardized.columns)} cols)")

        # Write summary report
        log(f"[SAVE] Saving standardization summary...")
        with open(SUMMARY_FILE, 'w', encoding='utf-8') as f:
            f.write("STANDARDIZATION SUMMARY\n")
            f.write("=" * 80 + "\n\n")

            f.write("PRE-STANDARDIZATION (Original Scale):\n")
            f.write("-" * 80 + "\n")
            for col in FEATURE_COLS:
                mean_val = pre_stats.loc['mean', col]
                std_val = pre_stats.loc['std', col]
                f.write(f"{col:40s}  mean={mean_val:10.6f}  SD={std_val:10.6f}\n")

            f.write("\n")
            f.write("POST-STANDARDIZATION (Z-Scores):\n")
            f.write("-" * 80 + "\n")
            for z_col in z_cols:
                mean_val = post_stats.loc['mean', z_col]
                std_val = post_stats.loc['std', z_col]
                f.write(f"{z_col:40s}  mean={mean_val:10.6f}  SD={std_val:10.6f}\n")

            f.write("\n")
            f.write("VERIFICATION:\n")
            f.write("-" * 80 + "\n")
            max_mean_dev = abs(post_stats.loc['mean']).max()
            max_sd_dev = abs(post_stats.loc['std'] - 1.0).max()
            f.write(f"Maximum mean deviation from 0: {max_mean_dev:.6f} (threshold: 0.01)\n")
            f.write(f"Maximum SD deviation from 1: {max_sd_dev:.6f} (threshold: 0.01)\n")

            if max_mean_dev < 0.01 and max_sd_dev < 0.01:
                f.write("\nSTATUS: PASS - Standardization successful\n")
            else:
                f.write("\nSTATUS: FAIL - Standardization outside tolerance\n")

        log(f"[SAVED] {SUMMARY_FILE}")

        # =========================================================================
        # STEP 6: Run Validation Tool
        # =========================================================================
        # Tool: validate_standardization
        # Validates: mean~0, SD~1 for all z-score columns (tolerance = 0.01)

        log("[VALIDATION] Running validate_standardization...")

        validation_result = validate_standardization(
            df=df_standardized,
            column_names=z_cols,
            tolerance=0.01
        )

        # Report validation results
        if isinstance(validation_result, dict):
            for key, value in validation_result.items():
                log(f"[VALIDATION] {key}: {value}")

        # Check validation passed
        if not validation_result.get('valid', False):
            raise ValueError(f"Validation failed: {validation_result.get('message', 'Unknown error')}")

        log("[SUCCESS] Step 01 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
