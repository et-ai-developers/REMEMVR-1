# 3_tools.yaml - Tool Catalog (Analysis + Validation Tools)
# Created by: rq_tools agent (Step 11)
# Consumed by: rq_analysis agent (Step 12)
# Architecture: Tool Catalog (Option A) - Each tool listed once, deduplication
# RQ: 5.12 - CTT-IRT Convergence via Item Purification

# ============================================================================
# ANALYSIS TOOLS
# ============================================================================

analysis_tools:

  # -------------------------------------------------------------------------
  # CTT Analysis Tools
  # -------------------------------------------------------------------------

  compute_cronbachs_alpha:
    module: "tools.analysis_ctt"
    function: "compute_cronbachs_alpha"
    signature: "compute_cronbachs_alpha(data: DataFrame, n_bootstrap: int = 1000) -> Dict[str, float]"
    validation_tool: "validate_numeric_range"

    input_files:
      - path: "data/step00_raw_scores.csv"
        required_columns: ["composite_ID", "UID", "TEST", "TQ_*"]
        expected_rows: "~400 (100 participants � 4 tests)"
        data_types:
          composite_ID: "string (format: {UID}_{test})"
          UID: "string (participant identifier)"
          TEST: "int (values: 1, 2, 3, 4)"
          TQ_items: "int (dichotomized 0/1 item responses)"

    output_files:
      - path: "data/step04_reliability_assessment.csv"
        columns: ["domain", "alpha_full", "CI_lower_full", "CI_upper_full", "alpha_purified", "CI_lower_purified", "CI_upper_purified", "delta_alpha"]
        description: "Cronbach's alpha for full vs purified CTT item sets with bootstrap 95% CIs"

    parameters:
      data: "pd.DataFrame (items as columns, participants as rows)"
      n_bootstrap: "int (default 1000, bootstrap iterations for 95% CI)"

    description: "Compute Cronbach's alpha internal consistency reliability with bootstrap confidence intervals. For dichotomous items (0/1), equals KR-20. Used to assess whether IRT purification maintains CTT reliability."
    source_reference: "tools_inventory.md lines 468-476"

  compare_correlations_dependent:
    module: "tools.analysis_ctt"
    function: "compare_correlations_dependent"
    signature: "compare_correlations_dependent(r12: float, r13: float, r23: float, n: int) -> Dict[str, float]"
    validation_tool: "validate_correlation_test_d068"

    input_files:
      - path: "data/step00_theta_scores.csv"
        required_columns: ["composite_ID", "theta_what", "theta_where", "theta_when"]
        source: "RQ 5.1 Step 3 output"
      - path: "data/step02_ctt_full_scores.csv"
        required_columns: ["composite_ID", "CTT_full_what", "CTT_full_where", "CTT_full_when"]
        source: "Step 2 output (full CTT)"
      - path: "data/step03_ctt_purified_scores.csv"
        required_columns: ["composite_ID", "CTT_purified_what", "CTT_purified_where", "CTT_purified_when"]
        source: "Step 3 output (purified CTT)"

    output_files:
      - path: "data/step05_correlation_analysis.csv"
        columns: ["domain", "r_full_irt", "r_purified_irt", "r_full_purified", "delta_r", "steiger_z", "p_uncorrected", "p_bonferroni", "interpretation"]
        description: "Steiger's z-test comparing dependent correlations (Full CTT-IRT vs Purified CTT-IRT) per Decision D068 dual p-value reporting"

    parameters:
      r12: "float (correlation 1-2, e.g., r(Full CTT, IRT))"
      r13: "float (correlation 1-3, e.g., r(Full CTT, Purified CTT))"
      r23: "float (correlation 2-3, e.g., r(Purified CTT, IRT))"
      n: "int (sample size, N=400 composite observations)"

    description: "Test if two dependent correlations differ significantly using Steiger's z-test. Appropriate when both correlations share a common variable (same N=100 participants). Implements Decision D068 dual p-value reporting (uncorrected + Bonferroni)."
    source_reference: "tools_inventory.md lines 481-488"

  # -------------------------------------------------------------------------
  # LMM Analysis Tools
  # -------------------------------------------------------------------------

  fit_lmm_trajectory_tsvr:
    module: "tools.analysis_lmm"
    function: "fit_lmm_trajectory_tsvr"
    signature: "fit_lmm_trajectory_tsvr(theta_scores: DataFrame, tsvr_data: DataFrame, formula: str, groups: str = 'UID', re_formula: str = '~Days', reml: bool = False) -> MixedLMResults"
    validation_tool: "validate_lmm_convergence"

    input_files:
      - path: "data/step06_standardized_outcomes.csv"
        required_columns: ["composite_ID", "UID", "TSVR_hours", "domain", "z_full_ctt", "z_purified_ctt", "z_irt_theta"]
        expected_rows: "~1200 (400 composite_IDs � 3 domains)"
        data_types:
          composite_ID: "string"
          UID: "string"
          TSVR_hours: "float (actual hours since encoding)"
          domain: "string (what, where, when)"
          z_scores: "float (standardized outcomes, mean=0, SD=1)"

    output_files:
      - path: "data/step07_lmm_model_comparison.csv"
        columns: ["measurement", "AIC", "BIC", "logLik", "delta_AIC", "interpretation"]
        description: "AIC comparison of 3 parallel LMMs (Full CTT, Purified CTT, IRT theta) with identical formulas"
      - path: "data/step07_lmm_full_ctt_summary.txt"
        description: "Statsmodels LMM summary for Full CTT (z-standardized)"
      - path: "data/step07_lmm_purified_ctt_summary.txt"
        description: "Statsmodels LMM summary for Purified CTT (z-standardized)"
      - path: "data/step07_lmm_irt_theta_summary.txt"
        description: "Statsmodels LMM summary for IRT theta (z-standardized)"
      - path: "data/step07_lmm_full_ctt_fixed_effects.csv"
        columns: ["term", "coef", "se", "z", "p"]
        description: "Fixed effects table for Full CTT model"
      - path: "data/step07_lmm_purified_ctt_fixed_effects.csv"
        columns: ["term", "coef", "se", "z", "p"]
        description: "Fixed effects table for Purified CTT model"
      - path: "data/step07_lmm_irt_theta_fixed_effects.csv"
        columns: ["term", "coef", "se", "z", "p"]
        description: "Fixed effects table for IRT theta model"

    parameters:
      theta_scores: "pd.DataFrame (standardized outcomes in long format)"
      tsvr_data: "pd.DataFrame (TSVR time variable from RQ 5.1)"
      formula: "str (fixed effects: z_outcome ~ (TSVR_hours + log(TSVR_hours+1)) � domain)"
      groups: "str (default 'UID', grouping variable for random effects)"
      re_formula: "str (default '~TSVR_hours', random intercepts + slopes per participant)"
      reml: "bool (default False, ML estimation for AIC comparison per Burnham & Anderson)"

    description: "Fit LMM using TSVR (actual hours since encoding) as time variable per Decision D070. Parallel design: fit identical formula to 3 standardized measurements (Full CTT, Purified CTT, IRT theta) and compare AIC to isolate measurement method effects."
    source_reference: "tools_inventory.md lines 98-103"

  extract_fixed_effects_from_lmm:
    module: "tools.analysis_lmm"
    function: "extract_fixed_effects_from_lmm"
    signature: "extract_fixed_effects_from_lmm(result: MixedLMResults) -> DataFrame"
    validation_tool: "validate_data_format"

    input_files:
      - path: "data/step07_lmm_*_summary.txt"
        source: "Fitted MixedLMResults objects from step 7"

    output_files:
      - path: "data/step07_lmm_full_ctt_fixed_effects.csv"
        columns: ["effect", "coefficient", "std_error", "z_value", "p_value"]
        description: "Fixed effects table for Full CTT model"
      - path: "data/step07_lmm_purified_ctt_fixed_effects.csv"
        columns: ["effect", "coefficient", "std_error", "z_value", "p_value"]
        description: "Fixed effects table for Purified CTT model"
      - path: "data/step07_lmm_irt_theta_fixed_effects.csv"
        columns: ["effect", "coefficient", "std_error", "z_value", "p_value"]
        description: "Fixed effects table for IRT theta model"

    parameters:
      result: "MixedLMResults (fitted LMM object from statsmodels)"

    description: "Extract fixed effects table from fitted LMM. Used to save coefficient tables for 3 parallel models (Full CTT, Purified CTT, IRT theta) for comparison."
    source_reference: "tools_inventory.md lines 113-119"

# ============================================================================
# VALIDATION TOOLS
# ============================================================================

validation_tools:

  # -------------------------------------------------------------------------
  # File & Data Structure Validation
  # -------------------------------------------------------------------------

  check_file_exists:
    module: "tools.validation"
    function: "check_file_exists"
    signature: "check_file_exists(file_path: Union[str, Path], min_size_bytes: int = 0) -> Dict[str, Any]"

    input_files: []  # Validation checks file existence, not content

    parameters:
      file_path: "Union[str, Path] (path to file to check)"
      min_size_bytes: "int (default 0, minimum file size threshold)"

    criteria:
      - "File exists at specified path"
      - "Path points to file, not directory"
      - "File size >= min_size_bytes (if specified)"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool (True if file exists and meets criteria)"
        file_path: "str (file path checked)"
        size_bytes: "int (file size, 0 if file doesn't exist)"
        message: "str (validation result message)"

    behavior_on_failure:
      action: "raise FileNotFoundError"
      log_to: "logs/step00_load_data.log"
      invoke: "g_debug (master invokes after error)"

    description: "Validate file exists and optionally meets minimum size requirement. Used in Step 0 to verify RQ 5.1 cross-RQ dependencies exist before proceeding."
    source_reference: "tools_inventory.md lines 343-350"

  validate_data_columns:
    module: "tools.validation"
    function: "validate_data_columns"
    signature: "validate_data_columns(df: DataFrame, required_columns: List[str]) -> Dict[str, Any]"

    input_files:
      - path: "data/step00_*.csv"
        source: "Loaded DataFrames from Step 0"

    parameters:
      df: "pd.DataFrame (data to validate)"
      required_columns: "List[str] (required column names, case-sensitive)"

    criteria:
      - "All required columns present in DataFrame"
      - "Case-sensitive column name matching"
      - "No missing required columns"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool (True if all required columns present)"
        missing_columns: "List[str] (columns not found)"
        existing_columns: "List[str] (columns found)"
        n_required: "int (number of required columns)"
        n_missing: "int (number of missing columns)"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step00_load_data.log"
      invoke: "g_debug (master invokes after error)"

    description: "Validate DataFrame contains all required columns. Used in Step 0 to verify RQ 5.1 files and dfData.csv have expected structure before CTT computation."
    source_reference: "tools_inventory.md lines 402-410"

  validate_dataframe_structure:
    module: "tools.validation"
    function: "validate_dataframe_structure"
    signature: "validate_dataframe_structure(df: DataFrame, expected_rows: Union[int, Tuple[int, int]], expected_columns: List[str], column_types: Optional[Dict[str, type]] = None) -> Dict[str, Any]"

    input_files:
      - path: "data/step01_item_mapping.csv"
        source: "Item mapping output from Step 1"

    parameters:
      df: "pd.DataFrame (data to validate)"
      expected_rows: "Union[int, Tuple[int, int]] (exact count or (min, max) range)"
      expected_columns: "List[str] (required columns)"
      column_types: "Optional[Dict[str, type]] (expected dtypes)"

    criteria:
      - "Row count in expected range"
      - "All required columns present"
      - "Column types match (if specified)"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool (True if all checks pass)"
        message: "str (validation result)"
        checks: "Dict[str, bool] (individual check results)"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step01_map_items.log"
      invoke: "g_debug (master invokes after error)"

    description: "Validate DataFrame structure (rows, columns, types). Used in Step 1 to verify item mapping completeness (expected ~50 items, retained ~38)."
    source_reference: "tools_inventory.md lines 580-588"

  # -------------------------------------------------------------------------
  # Numeric Range Validation
  # -------------------------------------------------------------------------

  validate_numeric_range:
    module: "tools.validation"
    function: "validate_numeric_range"
    signature: "validate_numeric_range(data: Union[np.ndarray, pd.Series], min_val: float, max_val: float, column_name: str) -> Dict[str, Any]"

    input_files:
      - path: "data/step02_ctt_full_scores.csv"
        source: "Full CTT scores from Step 2"
      - path: "data/step03_ctt_purified_scores.csv"
        source: "Purified CTT scores from Step 3"
      - path: "data/step04_reliability_assessment.csv"
        source: "Cronbach's alpha from Step 4"

    parameters:
      data: "Union[np.ndarray, pd.Series] (numeric data to validate)"
      min_val: "float (minimum allowed value, inclusive)"
      max_val: "float (maximum allowed value, inclusive)"
      column_name: "str (for error messages)"

    criteria:
      - "All values >= min_val"
      - "All values <= max_val"
      - "No NaN values (unless explicitly allowed)"
      - "No infinite values"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool (True if all values in range)"
        message: "str (validation result)"
        out_of_range_count: "int (number of violations)"
        violations: "list (first 10 out-of-range values)"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/stepNN_*.log (varies by step)"
      invoke: "g_debug (master invokes after error)"

    description: "Validate numeric values fall within specified range. Used in Steps 2-4 to ensure CTT scores in [0, 1] and Cronbach's alpha in [0, 1]."
    source_reference: "tools_inventory.md lines 492-500"

  # -------------------------------------------------------------------------
  # Statistical Test Validation (Decision D068)
  # -------------------------------------------------------------------------

  validate_correlation_test_d068:
    module: "tools.validation"
    function: "validate_correlation_test_d068"
    signature: "validate_correlation_test_d068(correlation_df: DataFrame, required_cols: List[str] = None) -> Dict[str, Any]"

    input_files:
      - path: "data/step05_correlation_analysis.csv"
        source: "Correlation test results from Step 5"

    parameters:
      correlation_df: "pd.DataFrame (correlation test results with p-value columns)"
      required_cols: "List[str] (optional custom required columns, defaults to D068 spec)"

    criteria:
      - "BOTH uncorrected and corrected p-values present (Decision D068)"
      - "p_uncorrected column exists"
      - "At least one correction method: p_bonferroni, p_holm, or p_fdr"
      - "All p-values in [0, 1] range"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool (True if D068 compliant)"
        d068_compliant: "bool (dual p-values present)"
        missing_cols: "List[str] (missing columns if invalid)"
        message: "str (validation result)"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step05_correlation_analysis.log"
      invoke: "g_debug (master invokes after error)"

    description: "Validate correlation test results include Decision D068 dual p-value reporting. Ensures Steiger's z-test output contains BOTH uncorrected and Bonferroni-corrected p-values for transparent exploratory analysis."
    source_reference: "tools_inventory.md lines 454-462"

  # -------------------------------------------------------------------------
  # Standardization & LMM Validation
  # -------------------------------------------------------------------------

  validate_standardization:
    module: "tools.validation"
    function: "validate_standardization"
    signature: "validate_standardization(df: DataFrame, column_names: List[str], tolerance: float = 0.01) -> Dict[str, Any]"

    input_files:
      - path: "data/step06_standardized_outcomes.csv"
        source: "Z-scored outcomes from Step 6"

    parameters:
      df: "pd.DataFrame (data with standardized columns)"
      column_names: "List[str] (columns to validate: z_full_ctt, z_purified_ctt, z_irt_theta)"
      tolerance: "float (default 0.01, deviation tolerance from ideal mean=0, SD=1)"

    criteria:
      - "mean(z) within tolerance of 0 (default �0.01)"
      - "sd(z) within tolerance of 1 (default �0.01)"
      - "Checks all specified columns"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool (True if all columns standardized correctly)"
        message: "str (validation result)"
        mean_values: "Dict[str, float] (actual means per column)"
        sd_values: "Dict[str, float] (actual SDs per column)"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step06_standardize_outcomes.log"
      invoke: "g_debug (master invokes after error)"

    description: "Validate z-score standardization (mean H 0, SD H 1). Critical for valid AIC comparison per Burnham & Anderson - different outcome scales (CTT [0,1] vs IRT theta [logit]) must be standardized before parallel LMM fitting."
    source_reference: "tools_inventory.md lines 550-558"

  validate_lmm_convergence:
    module: "tools.validation"
    function: "validate_lmm_convergence"
    signature: "validate_lmm_convergence(lmm_result: MixedLMResults) -> Dict[str, Any]"

    input_files:
      - path: "data/step07_lmm_*_summary.txt"
        source: "Fitted LMM results from Step 7"

    parameters:
      lmm_result: "MixedLMResults (fitted LMM object from statsmodels)"

    criteria:
      - "Model converged (no convergence warnings)"
      - "No singular fit (random effects variance > 0)"
      - "All fixed effects have finite estimates (no NaN/Inf)"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        converged: "bool (True if model converged)"
        message: "str (convergence status message)"
        warnings: "list (convergence warnings if any)"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step07_fit_parallel_lmms.log"
      invoke: "g_debug (master invokes after error)"

    description: "Check LMM model convergence status and warnings. Used in Step 7 to verify all 3 parallel models (Full CTT, Purified CTT, IRT theta) converged successfully before AIC comparison."
    source_reference: "tools_inventory.md lines 327-333"

  validate_model_convergence:
    module: "tools.validation"
    function: "validate_model_convergence"
    signature: "validate_model_convergence(lmm_result: MixedLMResults) -> Dict[str, Any]"

    input_files:
      - path: "data/step07_lmm_*_summary.txt"
        source: "Fitted LMM results from Step 7"

    parameters:
      lmm_result: "MixedLMResults (fitted LMM object from statsmodels)"

    criteria:
      - "model.converged attribute = True"
      - "Optimization algorithm reached solution"
      - "No collinearity issues"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool (True if converged)"
        message: "str (convergence validation message)"
        converged: "bool (model.converged attribute value)"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step07_fit_parallel_lmms.log"
      invoke: "g_debug (master invokes after error)"

    description: "Validate statsmodels LMM model converged successfully by checking model.converged attribute. Complementary to validate_lmm_convergence - checks basic convergence flag."
    source_reference: "tools_inventory.md lines 540-548"

  # -------------------------------------------------------------------------
  # Plot Data Validation
  # -------------------------------------------------------------------------

  validate_plot_data_completeness:
    module: "tools.validation"
    function: "validate_plot_data_completeness"
    signature: "validate_plot_data_completeness(plot_data: DataFrame, required_domains: List[str], required_groups: List[str], domain_col: str = 'domain', group_col: str = 'group') -> Dict[str, Any]"

    input_files:
      - path: "plots/step08_correlation_comparison_data.csv"
        source: "Correlation comparison plot data from Step 8"
      - path: "plots/step08_aic_comparison_data.csv"
        source: "AIC comparison plot data from Step 8"

    parameters:
      plot_data: "pd.DataFrame (plot source data)"
      required_domains: "List[str] (expected domains: what, where, when)"
      required_groups: "List[str] (expected groups/measurements)"
      domain_col: "str (default 'domain', domain column name)"
      group_col: "str (default 'measurement_type' or 'measurement', group column name)"

    criteria:
      - "All required domains present (what, where, when)"
      - "All required measurements present (Full CTT, Purified CTT, IRT theta)"
      - "No missing categories in factorial design"
      - "No NaN values"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool (True if all categories present)"
        message: "str (validation result)"
        missing_domains: "List[str] (domains not found)"
        missing_groups: "List[str] (groups not found)"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step08_prepare_plot_data.log"
      invoke: "g_debug (master invokes after error)"

    description: "Verify all domains/groups present in plot data. Ensures complete factorial design for methodological comparison visualizations (correlation comparison + AIC comparison)."
    source_reference: "tools_inventory.md lines 590-598"

  # -------------------------------------------------------------------------
  # Data Format Validation (for LMM fixed effects tables)
  # -------------------------------------------------------------------------

  validate_data_format:
    module: "tools.validation"
    function: "validate_data_format"
    signature: "validate_data_format(df: DataFrame, required_cols: List[str]) -> Dict[str, Any]"

    input_files:
      - path: "data/step07_lmm_*_fixed_effects.csv"
        source: "Fixed effects tables from Step 7"

    parameters:
      df: "pd.DataFrame (DataFrame to validate)"
      required_cols: "List[str] (required column names: effect, coefficient, std_error, z_value, p_value)"

    criteria:
      - "All required columns present"
      - "Case-sensitive column name matching"
      - "Column order irrelevant"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool (True if all required columns present)"
        message: "str (validation result)"
        missing_cols: "List[str] (columns not found)"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step07_fit_parallel_lmms.log"
      invoke: "g_debug (master invokes after error)"

    description: "Validate DataFrame has all required columns present. Used to verify LMM fixed effects tables have expected format after extraction from MixedLMResults objects."
    source_reference: "tools_inventory.md lines 504-512"

# ============================================================================
# SUMMARY
# ============================================================================

summary:
  analysis_tools_count: 4
  validation_tools_count: 10
  total_unique_tools: 14
  mandatory_decisions_embedded: ["D068", "D070"]

  notes:
    - "Decision D068: Dual p-value reporting implemented in compare_correlations_dependent (uncorrected + Bonferroni)"
    - "Decision D070: TSVR time variable used in fit_lmm_trajectory_tsvr (actual hours, not nominal days)"
    - "Decision D069: NOT applicable (methodological comparison, not trajectory analysis)"
    - "Z-score standardization: Critical for valid AIC comparison per Burnham & Anderson (different outcome scales)"
    - "Parallel LMM design: Identical formula across 3 measurements isolates measurement method effects"
    - "Steiger's z-test: Appropriate for dependent correlations (same participants contribute to all correlations)"
    - "Cronbach's alpha: For dichotomous items (0/1), equals KR-20 reliability coefficient"
    - "Cross-RQ dependency: Requires RQ 5.1 completion (purified items, theta scores, TSVR mapping)"
    - "Stdlib functions exempted: pandas groupby/mean, numpy operations (not cataloged per agent rules)"
    - "Each tool listed ONCE: Deduplication across steps, rq_analysis creates step sequencing"
