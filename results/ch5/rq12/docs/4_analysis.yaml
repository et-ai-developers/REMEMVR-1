# ============================================================================
# ANALYSIS RECIPE - COMPLETE SPECIFICATION
# ============================================================================
# Generated: 2025-11-28
# RQ: ch5/rq12
# Agent: rq_analysis v4.0.0
# Purpose: Self-contained recipe for g_code agent (reads ONLY this file)
# ============================================================================

metadata:
  rq_id: "ch5/rq12"
  total_steps: 9
  analysis_type: "Hybrid CTT-IRT methodological comparison"
  generated_by: "rq_analysis v4.0.0"
  timestamp: "2025-11-28T00:00:00Z"

# ============================================================================
# ANALYSIS STEPS - COMPLETE SPECIFICATIONS
# ============================================================================

steps:
  # --------------------------------------------------------------------------
  # STEP 0: Load Data from Multiple Sources
  # --------------------------------------------------------------------------
  - name: "step00_load_data"
    step_number: "00"
    description: "Load IRT outputs from RQ 5.1 and raw data for CTT computation"

    analysis_call:
      type: "stdlib"  # pandas/numpy operations - NOT a catalogued tool
      operations:
        - "pd.read_csv('results/ch5/rq1/data/step02_purified_items.csv')"
        - "pd.read_csv('results/ch5/rq1/data/step03_theta_scores.csv')"
        - "pd.read_csv('results/ch5/rq1/data/step00_tsvr_mapping.csv')"
        - "pd.read_csv('data/cache/dfData.csv')"
        - "Filter dfData to TQ_ columns only (~50 items)"
        - "Merge theta scores with TSVR on composite_ID (left join)"
        - "Validate all composite_IDs match across files"
        - "Save outputs to data/step00_*.csv"

    input_files:
      - path: "results/ch5/rq1/data/step02_purified_items.csv"
        required_columns: ["item_name", "dimension", "a", "b"]
        expected_rows: "~38 items"
        description: "IRT item parameters post-purification from RQ 5.1"
      - path: "results/ch5/rq1/data/step03_theta_scores.csv"
        required_columns: ["composite_ID", "theta_common", "se_common", "theta_congruent", "se_congruent", "theta_incongruent", "se_incongruent"]
        expected_rows: "~400 (100 participants x 4 tests)"
        description: "IRT theta scores from RQ 5.1 Pass 2"
      - path: "results/ch5/rq1/data/step00_tsvr_mapping.csv"
        required_columns: ["composite_ID", "TSVR_hours", "test", "UID"]
        expected_rows: "~400"
        description: "Time Since VR mapping from RQ 5.1"
      - path: "data/cache/dfData.csv"
        required_columns: ["UID", "test", "TQ_ item columns"]
        expected_rows: "~400"
        description: "Raw data cache with dichotomized item responses"

    output_files:
      - path: "data/step00_retained_items.csv"
        columns: ["item_name", "dimension", "a", "b"]
        expected_rows: "~38 items"
        description: "IRT-retained items list for purified CTT computation"
      - path: "data/step00_raw_ctt_data.csv"
        columns: ["composite_ID", "UID", "test", "TQ_ item columns (~50)"]
        expected_rows: "~400"
        description: "Raw item responses for CTT scoring (wide format)"
      - path: "data/step00_theta_with_tsvr.csv"
        columns: ["composite_ID", "UID", "test", "TSVR_hours", "theta_common", "se_common", "theta_congruent", "se_congruent", "theta_incongruent", "se_incongruent"]
        expected_rows: "~400"
        description: "IRT theta scores merged with TSVR time variable"

    validation_call:
      type: "inline"  # No catalogued validation tool - inline checks
      criteria:
        - "All RQ 5.1 dependency files exist (purified_items, theta_scores, tsvr_mapping)"
        - "Retained items count in [35, 42] (expected ~38, ±10% tolerance)"
        - "All 400 composite_IDs present in raw_ctt_data (no data loss)"
        - "All 400 rows matched in theta-TSVR merge (no missing TSVR values)"
        - "No duplicate composite_IDs in any output file"
        - "TQ_ item columns: Allow up to 30% NaN per item (missing data acceptable)"
        - "a in [0.5, 4.0] for retained items (post-purification range)"
        - "theta_* in [-5, 5] (allow outliers)"
        - "TSVR_hours in [0, 300] (0=encoding, ~168=1 week)"
      on_failure:
        action: "raise ValueError"
        message: "Step 0 validation failed - see logs/step00_load_data.log for details"
        log_to: "logs/step00_load_data.log"

    log_file: "logs/step00_load_data.log"

  # --------------------------------------------------------------------------
  # STEP 1: Map IRT Items to CTT Items
  # --------------------------------------------------------------------------
  - name: "step01_map_items"
    step_number: "01"
    description: "Create mapping between IRT item names (VR tags) and CTT item names (TQ_ prefix)"

    analysis_call:
      type: "stdlib"  # pandas string parsing + filtering - NOT a catalogued tool
      operations:
        - "Load data/step00_retained_items.csv"
        - "Load data/step00_raw_ctt_data.csv"
        - "Parse IRT item_name format: VR-{paradigm}-{test}-{domain}-ANS"
        - "Construct CTT item name: TQ_{domain}_{variant}_{number}"
        - "Domain mapping: N->N (What), U->U (Where-up), D->D (Where-down), O->O (When)"
        - "Create full_ctt_items list (~50 items from raw_ctt_data columns)"
        - "Create purified_ctt_items list (~38 items matching IRT retained items)"
        - "Verify all purified_ctt_items exist in raw_ctt_data (circuit breaker if missing)"
        - "Save outputs to data/step01_*.csv"

    input_files:
      - path: "data/step00_retained_items.csv"
        required_columns: ["item_name", "dimension", "a", "b"]
        expected_rows: "~38 items"
        description: "IRT-retained items from Step 0"
      - path: "data/step00_raw_ctt_data.csv"
        required_columns: ["composite_ID", "UID", "test", "TQ_ item columns"]
        expected_rows: "~400"
        description: "Raw CTT data from Step 0"

    output_files:
      - path: "data/step01_full_ctt_items.csv"
        columns: ["ctt_item_name", "domain"]
        expected_rows: "~50 items"
        description: "Full CTT item list for Step 2 scoring"
      - path: "data/step01_purified_ctt_items.csv"
        columns: ["ctt_item_name", "irt_item_name", "domain", "a", "b"]
        expected_rows: "~38 items"
        description: "Purified CTT item list matching IRT retained items"

    validation_call:
      type: "inline"
      criteria:
        - "All purified_ctt_items found in raw_ctt_data columns (no missing TQ_ items)"
        - "Domain distribution reasonable: What ~14, Where ~12, When ~12 (±3 items per domain)"
        - "No duplicate ctt_item_names in either file"
        - "All purified items have corresponding IRT parameters (a, b non-null)"
        - "Full CTT item count ~50, purified CTT item count ~38"
      on_failure:
        action: "raise ValueError"
        message: "Step 1 validation failed - IRT item has no matching TQ_ item in raw data"
        log_to: "logs/step01_map_items.log"

    log_file: "logs/step01_map_items.log"

  # --------------------------------------------------------------------------
  # STEP 2: Compute Full CTT Scores
  # --------------------------------------------------------------------------
  - name: "step02_compute_full_ctt"
    step_number: "02"
    description: "Calculate mean accuracy scores per participant x test x domain using ALL available items"

    analysis_call:
      type: "stdlib"  # pandas groupby + mean - NOT a catalogued tool
      operations:
        - "Load data/step01_full_ctt_items.csv"
        - "Load data/step00_raw_ctt_data.csv"
        - "For each domain (What, Where, When):"
        - "  Filter full_ctt_items to domain-specific items"
        - "  Extract corresponding TQ_ columns from raw_ctt_data"
        - "  Compute mean accuracy per composite_ID (row-wise mean, ignore NaN)"
        - "Reshape from wide to long format (composite_ID-domain rows)"
        - "Add metadata: UID (from composite_ID split), test (from composite_ID split)"
        - "Save output to data/step02_full_ctt_scores.csv"

    input_files:
      - path: "data/step01_full_ctt_items.csv"
        required_columns: ["ctt_item_name", "domain"]
        expected_rows: "~50 items"
        description: "Full CTT item list from Step 1"
      - path: "data/step00_raw_ctt_data.csv"
        required_columns: ["composite_ID", "UID", "test", "TQ_ item columns"]
        expected_rows: "~400"
        description: "Raw CTT data from Step 0"

    output_files:
      - path: "data/step02_full_ctt_scores.csv"
        columns: ["composite_ID", "UID", "test", "domain", "ctt_score_full", "n_items_full"]
        expected_rows: "~1200 (400 composite_IDs x 3 domains)"
        description: "Full CTT mean accuracy scores in long format"

    validation_call:
      type: "inline"
      criteria:
        - "Output row count = 1200 (400 composite_IDs x 3 domains, no data loss)"
        - "ctt_score_full in [0, 1] (mean accuracy bounds)"
        - "n_items_full reasonable per domain: What ~18, Where ~16, When ~16 (±5 items tolerance)"
        - "No NaN in ctt_score_full"
        - "Domain balance: Each composite_ID appears exactly 3 times (What, Where, When)"
        - "Mean CTT scores in [0.3, 0.9] (episodic memory tasks moderately difficult)"
      on_failure:
        action: "raise ValueError"
        message: "Step 2 validation failed - expected 1200 rows, some domains missing"
        log_to: "logs/step02_compute_full_ctt.log"

    log_file: "logs/step02_compute_full_ctt.log"

  # --------------------------------------------------------------------------
  # STEP 3: Compute Purified CTT Scores
  # --------------------------------------------------------------------------
  - name: "step03_compute_purified_ctt"
    step_number: "03"
    description: "Calculate mean accuracy scores per participant x test x domain using ONLY IRT-retained items"

    analysis_call:
      type: "stdlib"  # pandas groupby + mean - NOT a catalogued tool
      operations:
        - "Load data/step01_purified_ctt_items.csv"
        - "Load data/step00_raw_ctt_data.csv"
        - "For each domain (What, Where, When):"
        - "  Filter purified_ctt_items to domain-specific items"
        - "  Extract corresponding TQ_ columns from raw_ctt_data"
        - "  Compute mean accuracy per composite_ID (row-wise mean, ignore NaN)"
        - "Reshape from wide to long format"
        - "Add metadata: UID, test (from composite_ID)"
        - "Expected item counts after purification: What ~14, Where ~12, When ~12"
        - "Save output to data/step03_purified_ctt_scores.csv"

    input_files:
      - path: "data/step01_purified_ctt_items.csv"
        required_columns: ["ctt_item_name", "irt_item_name", "domain", "a", "b"]
        expected_rows: "~38 items"
        description: "Purified CTT item list from Step 1"
      - path: "data/step00_raw_ctt_data.csv"
        required_columns: ["composite_ID", "UID", "test", "TQ_ item columns"]
        expected_rows: "~400"
        description: "Raw CTT data from Step 0"

    output_files:
      - path: "data/step03_purified_ctt_scores.csv"
        columns: ["composite_ID", "UID", "test", "domain", "ctt_score_purified", "n_items_purified"]
        expected_rows: "~1200 (400 composite_IDs x 3 domains)"
        description: "Purified CTT mean accuracy scores in long format"

    validation_call:
      type: "inline"
      criteria:
        - "Output row count = 1200 (400 composite_IDs x 3 domains)"
        - "ctt_score_purified in [0, 1] (mean accuracy bounds)"
        - "n_items_purified < n_items_full (purification removes items, never adds)"
        - "n_items_purified reasonable per domain: What ~14, Where ~12, When ~12 (±3 items)"
        - "Item reduction 15-35% per domain vs full CTT"
        - "No NaN in ctt_score_purified"
        - "Expected mean scores: ~0.52-0.72 (slightly higher than full CTT if purification removes noise)"
      on_failure:
        action: "raise ValueError"
        message: "Step 3 validation failed - purified item count exceeds full item count for domain"
        log_to: "logs/step03_compute_purified_ctt.log"

    log_file: "logs/step03_compute_purified_ctt.log"

  # --------------------------------------------------------------------------
  # STEP 4: Assess CTT Reliability (Cronbach's Alpha)
  # --------------------------------------------------------------------------
  - name: "step04_assess_reliability"
    step_number: "04"
    description: "Compute Cronbach's alpha for both full and purified CTT item sets per domain"

    analysis_call:
      type: "catalogued"
      module: "tools.analysis_ctt"
      function: "compute_cronbachs_alpha"
      signature: "compute_cronbachs_alpha(data: DataFrame, n_bootstrap: int = 1000) -> Dict[str, Any]"

      input_files:
        - path: "data/step01_full_ctt_items.csv"
          required_columns: ["ctt_item_name", "domain"]
          expected_rows: "~50 items"
          variable_name: "full_items"
        - path: "data/step01_purified_ctt_items.csv"
          required_columns: ["ctt_item_name", "irt_item_name", "domain", "a", "b"]
          expected_rows: "~38 items"
          variable_name: "purified_items"
        - path: "data/step00_raw_ctt_data.csv"
          required_columns: ["composite_ID", "UID", "test", "TQ_ item columns"]
          expected_rows: "~400"
          variable_name: "raw_data"

      output_files:
        - path: "data/step04_ctt_reliability.csv"
          variable_name: "reliability_results"
          description: "Cronbach's alpha with bootstrap 95% CIs for full and purified item sets"

      parameters:
        data: "raw_data"
        n_bootstrap: 1000

      returns:
        type: "Dict[str, Any]"
        variable_name: "reliability_results"

      description: "Compute Cronbach's alpha internal consistency reliability with bootstrap confidence intervals for full and purified CTT item sets"

    validation_call:
      type: "catalogued"
      module: "tools.validation"
      function: "validate_numeric_range"
      signature: "validate_numeric_range(data: np.ndarray or pd.Series, min_val: float, max_val: float, column_name: str) -> Dict[str, Any]"

      input_files:
        - path: "data/step04_ctt_reliability.csv"
          variable_name: "reliability_results"
          source: "analysis call output (compute_cronbachs_alpha)"

      parameters:
        data: "reliability_results['alpha']"
        min_val: 0.5
        max_val: 0.95
        column_name: "Cronbach's alpha"

      returns:
        type: "Dict[str, Any]"
        variable_name: "validation_result"

      criteria:
        - "All alpha values in range [0.5, 0.95] (episodic memory scales typically 0.7-0.9)"
        - "All CI bounds in valid range [0, 1]"
        - "ci_lower < alpha < ci_upper (CIs must bracket point estimate)"
        - "No NaN or infinite values"
        - "Output row count = 6 (3 domains x 2 item_sets)"

      on_failure:
        action: "raise ValueError(validation_result['message'])"
        log_to: "logs/step04_assess_reliability.log"

      description: "Validate Cronbach's alpha values and confidence intervals are in acceptable ranges"

    log_file: "logs/step04_assess_reliability.log"

  # --------------------------------------------------------------------------
  # STEP 5: Correlation Analysis with Steiger's Z-Test
  # --------------------------------------------------------------------------
  - name: "step05_correlation_analysis"
    step_number: "05"
    description: "Test whether purified CTT correlates more strongly with IRT theta than full CTT using Steiger's z-test"

    analysis_call:
      type: "catalogued"
      module: "tools.analysis_ctt"
      function: "compare_correlations_dependent"
      signature: "compare_correlations_dependent(r12: float, r13: float, r23: float, n: int) -> Dict[str, Any]"

      input_files:
        - path: "data/step02_full_ctt_scores.csv"
          required_columns: ["composite_ID", "domain", "ctt_score_full"]
          expected_rows: "~1200"
          variable_name: "full_ctt"
        - path: "data/step03_purified_ctt_scores.csv"
          required_columns: ["composite_ID", "domain", "ctt_score_purified"]
          expected_rows: "~1200"
          variable_name: "purified_ctt"
        - path: "data/step00_theta_with_tsvr.csv"
          required_columns: ["composite_ID", "theta_common", "theta_congruent", "theta_incongruent"]
          expected_rows: "~400"
          variable_name: "theta_scores"

      output_files:
        - path: "data/step05_correlations.csv"
          variable_name: "correlations"
          description: "Pairwise correlations between Full CTT, Purified CTT, and IRT theta"
        - path: "data/step05_steiger_tests.csv"
          variable_name: "steiger_tests"
          description: "Steiger's z-test results comparing dependent correlations"

      parameters:
        test_type: "two_tailed"

      returns:
        type: "Dict[str, Any]"
        variable_name: "steiger_tests"

      description: "Test whether purified CTT correlates more strongly with IRT theta than full CTT using Steiger's z-test for dependent correlations"

    validation_call:
      type: "catalogued"
      module: "tools.validation"
      function: "validate_correlation_test_d068"
      signature: "validate_correlation_test_d068(correlation_df: DataFrame, required_cols: List[str] = None) -> Dict[str, Any]"

      input_files:
        - path: "data/step05_steiger_tests.csv"
          variable_name: "steiger_tests"
          source: "analysis call output (compare_correlations_dependent)"

      parameters:
        correlation_df: "steiger_tests"
        required_cols: ["r_full_irt", "r_purified_irt", "z_statistic", "p_value"]

      returns:
        type: "Dict[str, Any]"
        variable_name: "validation_result"

      criteria:
        - "Decision D068: Dual p-value reporting (uncorrected + correction method)"
        - "All correlation values r in [-1, 1]"
        - "All p-values in [0, 1]"
        - "No NaN values in correlation or p-value columns"
        - "Expected r(Full CTT, IRT) in [0.90, 0.98] (high convergence expected)"
        - "Expected r(Purified CTT, IRT) in [0.92, 0.99] (higher than full CTT)"

      on_failure:
        action: "raise ValueError(validation_result['message'])"
        log_to: "logs/step05_correlation_analysis.log"

      description: "Validate correlation test results include Decision D068 dual p-value reporting"

    log_file: "logs/step05_correlation_analysis.log"

  # --------------------------------------------------------------------------
  # STEP 6: Standardize Outcomes for AIC Comparison
  # --------------------------------------------------------------------------
  - name: "step06_standardize_outcomes"
    step_number: "06"
    description: "Standardize all three measurement approaches to z-scores for valid AIC comparison"

    analysis_call:
      type: "stdlib"  # pandas z-score transformation - NOT a catalogued tool
      operations:
        - "Load data/step02_full_ctt_scores.csv"
        - "Load data/step03_purified_ctt_scores.csv"
        - "Load data/step00_theta_with_tsvr.csv"
        - "Reshape IRT theta to long format (composite_ID-domain rows) matching CTT scores"
        - "Domain mapping: What->theta_common, Where->theta_congruent, When->theta_incongruent"
        - "Merge Full CTT, Purified CTT, IRT theta on composite_ID-domain"
        - "For each measurement approach (Full CTT, Purified CTT, IRT):"
        - "  Compute z-score: z = (score - mean) / SD"
        - "  Compute within entire sample (not within domain, to preserve domain differences)"
        - "Verify z-score properties: mean ~= 0, SD ~= 1"
        - "Merge with TSVR_hours for LMM time variable"
        - "Save output to data/step06_standardized_scores.csv"

    input_files:
      - path: "data/step02_full_ctt_scores.csv"
        required_columns: ["composite_ID", "domain", "ctt_score_full"]
        expected_rows: "~1200"
        description: "Full CTT scores from Step 2"
      - path: "data/step03_purified_ctt_scores.csv"
        required_columns: ["composite_ID", "domain", "ctt_score_purified"]
        expected_rows: "~1200"
        description: "Purified CTT scores from Step 3"
      - path: "data/step00_theta_with_tsvr.csv"
        required_columns: ["composite_ID", "UID", "test", "TSVR_hours", "theta_common", "theta_congruent", "theta_incongruent"]
        expected_rows: "~400"
        description: "IRT theta scores with TSVR from Step 0"

    output_files:
      - path: "data/step06_standardized_scores.csv"
        columns: ["composite_ID", "UID", "test", "domain", "TSVR_hours", "z_full_ctt", "z_purified_ctt", "z_irt_theta"]
        expected_rows: "~1200 (400 composite_IDs x 3 domains)"
        description: "Standardized z-scores for all three measurement approaches"

    validation_call:
      type: "catalogued"
      module: "tools.validation"
      function: "validate_standardization"
      signature: "validate_standardization(df: pd.DataFrame, column_names: List[str], tolerance: float = 0.01) -> Dict[str, Any]"

      input_files:
        - path: "data/step06_standardized_scores.csv"
          variable_name: "standardized_scores"
          source: "analysis call output (z-score transformation)"

      parameters:
        df: "standardized_scores"
        column_names: ["z_full_ctt", "z_purified_ctt", "z_irt_theta"]
        tolerance: 0.01

      returns:
        type: "Dict[str, Any]"
        variable_name: "validation_result"

      criteria:
        - "All z-score means ≈ 0 (within ±0.01 tolerance)"
        - "All z-score SDs ≈ 1 (within [0.99, 1.01] tolerance)"
        - "All z-scores in range [-4, 4] (allow up to 4 SD for outliers)"
        - "No NaN values"
        - "All 1200 rows retained (no missing values in z-scores)"

      on_failure:
        action: "raise ValueError(validation_result['message'])"
        log_to: "logs/step06_standardize_outcomes.log"

      description: "Validate z-score standardization (mean ≈ 0, SD ≈ 1) for valid AIC comparison across different measurement scales"

    log_file: "logs/step06_standardize_outcomes.log"

  # --------------------------------------------------------------------------
  # STEP 7: Fit Parallel LMMs to Standardized Outcomes
  # --------------------------------------------------------------------------
  - name: "step07_fit_parallel_lmms"
    step_number: "07"
    description: "Fit identical LMM models to all three standardized measurement approaches"

    analysis_call:
      type: "catalogued"
      module: "tools.analysis_lmm"
      function: "fit_lmm_trajectory_tsvr"
      signature: "fit_lmm_trajectory_tsvr(theta_scores: DataFrame, tsvr_data: DataFrame, formula: str, groups: str = 'UID', re_formula: str = '~Days', reml: bool = False) -> MixedLMResults"

      input_files:
        - path: "data/step06_standardized_scores.csv"
          required_columns: ["composite_ID", "UID", "test", "domain", "TSVR_hours", "z_full_ctt", "z_purified_ctt", "z_irt_theta"]
          expected_rows: "~1200"
          variable_name: "standardized_scores"

      output_files:
        - path: "data/step07_lmm_full_ctt_summary.txt"
          variable_name: "lmm_full"
          description: "Full CTT LMM model summary (fixed effects, random effects, AIC, BIC)"
        - path: "data/step07_lmm_purified_ctt_summary.txt"
          variable_name: "lmm_purified"
          description: "Purified CTT LMM model summary"
        - path: "data/step07_lmm_irt_theta_summary.txt"
          variable_name: "lmm_irt"
          description: "IRT theta LMM model summary (gold standard)"
        - path: "data/step07_lmm_comparison.csv"
          variable_name: "lmm_comparison"
          description: "AIC/BIC comparison across three measurement approaches"
        - path: "data/step07_interaction_coefficients.csv"
          variable_name: "interaction_coeffs"
          description: "Domain × Time interaction coefficients for all three approaches"

      parameters:
        theta_scores: "standardized_scores"
        tsvr_data: "standardized_scores"
        formula: "z_Ability ~ (TSVR_hours + log(TSVR_hours+1)) * domain"
        groups: "UID"
        re_formula: "~TSVR_hours"
        reml: false

      returns:
        type: "MixedLMResults"
        unpacking: "lmm_full, lmm_purified, lmm_irt"

      description: "Fit parallel LMMs to all three standardized measurement approaches (Full CTT, Purified CTT, IRT theta) using TSVR as time variable per Decision D070"

    validation_call:
      type: "catalogued"
      module: "tools.validation"
      function: "validate_lmm_convergence"
      signature: "validate_lmm_convergence(lmm_result: MixedLMResults) -> Dict[str, Any]"

      input_files:
        - path: "data/step07_lmm_full_ctt_summary.txt"
          variable_name: "lmm_full"
          source: "analysis call output (fit_lmm_trajectory_tsvr)"
        - path: "data/step07_lmm_purified_ctt_summary.txt"
          variable_name: "lmm_purified"
          source: "analysis call output (fit_lmm_trajectory_tsvr)"
        - path: "data/step07_lmm_irt_theta_summary.txt"
          variable_name: "lmm_irt"
          source: "analysis call output (fit_lmm_trajectory_tsvr)"

      parameters:
        lmm_result: "lmm_full"  # Validate each model separately
        check_singularity: true
        min_observations: 100

      returns:
        type: "Dict[str, Any]"
        variable_name: "validation_result"

      criteria:
        - "Model converged (no convergence warnings)"
        - "No singular fit (random effects variance > 0)"
        - "Minimum 100 observations used"
        - "All fixed effects have finite estimates (no NaN/Inf)"
        - "AIC, BIC positive (typically 1000-5000 for N=1200)"
        - "delta_AIC_vs_IRT for IRT model = 0 (by definition)"
        - "Expected: |delta_AIC_purified| < |delta_AIC_full|"

      on_failure:
        action: "raise ValueError(validation_result['message'])"
        log_to: "logs/step07_fit_parallel_lmms.log"

      description: "Validate LMM converged successfully, no singular fit, all estimates finite"

    log_file: "logs/step07_fit_parallel_lmms.log"

  # --------------------------------------------------------------------------
  # STEP 8: Prepare Comparison Plot Data
  # --------------------------------------------------------------------------
  - name: "step08_prepare_plot_data"
    step_number: "08"
    description: "Aggregate observed means and confidence intervals for all three measurement approaches"

    analysis_call:
      type: "stdlib"  # pandas groupby + agg - NOT a catalogued tool
      operations:
        - "Load data/step02_full_ctt_scores.csv"
        - "Load data/step03_purified_ctt_scores.csv"
        - "Load data/step06_standardized_scores.csv"
        - "For each domain (What, Where, When) and measurement approach (Full CTT, Purified CTT, IRT):"
        - "  Group by test (T1, T2, T3, T4)"
        - "  Compute mean score, 95% CI (± 1.96 * SE)"
        - "  Map test to TSVR_hours: T1->~0, T2->~24, T3->~72, T4->~144 (approximate means)"
        - "Combine all three approaches into single plot source CSV"
        - "Add approach identifier column for plotting (color/linetype grouping)"
        - "Save output to plots/step08_comparison_plot_data.csv"

    input_files:
      - path: "data/step02_full_ctt_scores.csv"
        required_columns: ["composite_ID", "domain", "ctt_score_full", "test"]
        expected_rows: "~1200"
        description: "Full CTT scores from Step 2"
      - path: "data/step03_purified_ctt_scores.csv"
        required_columns: ["composite_ID", "domain", "ctt_score_purified", "test"]
        expected_rows: "~1200"
        description: "Purified CTT scores from Step 3"
      - path: "data/step06_standardized_scores.csv"
        required_columns: ["composite_ID", "domain", "z_irt_theta", "test", "TSVR_hours"]
        expected_rows: "~1200"
        description: "IRT theta scores from Step 6"

    output_files:
      - path: "plots/step08_comparison_plot_data.csv"
        columns: ["domain", "measurement_approach", "test", "time", "mean_score", "CI_lower", "CI_upper", "n"]
        expected_rows: "36 (3 domains x 3 approaches x 4 tests)"
        description: "Plot source CSV for 3-way trajectory comparison visualization"

    validation_call:
      type: "inline"
      criteria:
        - "Output row count = 36 (3 domains x 3 approaches x 4 tests)"
        - "time in [0, 168] hours (0=encoding, ~168=1 week)"
        - "CI_lower < mean_score < CI_upper (CIs must bracket mean)"
        - "n in [90, 100] per test (allowing some missing data)"
        - "All domain-approach-test combinations present"
        - "No NaN in mean_score or CIs"
        - "No duplicate domain-approach-test combinations"
        - "Expected pattern: mean_score decreases across time (forgetting trajectory)"
      on_failure:
        action: "raise ValueError"
        message: "Step 8 validation failed - expected 36 rows, some domain-approach combinations missing"
        log_to: "logs/step08_prepare_plot_data.log"

    log_file: "logs/step08_prepare_plot_data.log"

# ============================================================================
# END OF ANALYSIS RECIPE
# ============================================================================
