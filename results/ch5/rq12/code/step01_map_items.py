#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step01
Step Name: Map Items to Full vs Purified Sets
RQ: results/ch5/rq12
Generated: 2025-11-30

PURPOSE:
Identify which TQ_* items in dfData.csv were retained vs excluded by RQ 5.1
IRT purification process. Creates mapping table showing retention status for
all items across three domains (What, Where, When).

EXPECTED INPUTS:
  - data/step00_irt_purified_items.csv
    Columns: ['item_name', 'factor', 'a', 'b']
    Format: Purified item list from RQ 5.1 Step 2 (~38 items retained after quality filtering)
    Expected rows: ~38 items

  - data/step00_raw_scores.csv
    Columns: ['composite_ID', 'UID', 'TEST', 'TQ_*']
    Format: Raw dichotomized item responses with composite_ID added (UID_test format)
    Expected rows: ~400 (100 participants x 4 tests)

EXPECTED OUTPUTS:
  - data/step01_item_mapping.csv
    Columns: ['item_name', 'domain', 'retained']
    Format: Item mapping with retention status (True if retained by RQ 5.1 purification)
    Expected rows: ~50 items (all TQ_* items from dfData)

  - logs/step01_item_counts.txt
    Format: Text report of item counts (full vs purified per domain)

VALIDATION CRITERIA:
  - Row count in range [48, 52] items (all TQ_* items from dfData)
  - All 3 columns present: item_name, domain, retained
  - domain values in {what, where, when}
  - retained values in {True, False}
  - Retention rate approximately 75% (36-40 items retained)

g_code REASONING:
- Approach: Extract all TQ_* column names from raw data (full item set), compare
  against purified item list from RQ 5.1, create boolean mapping showing which
  items were retained/excluded by IRT quality filtering
- Why this approach: Simple set operations (set difference) provide exact mapping
  without requiring IRT recalibration. Enables downstream CTT score computation
  for both full and purified item sets
- Data flow: Full item list (from dfData columns) + Purified item list (from RQ 5.1)
  -> Set comparison -> Mapping table with retention status
- Expected performance: <1 second (pure data manipulation, no statistical computation)

IMPLEMENTATION NOTES:
- Analysis tool: STDLIB (pandas set operations, NOT catalogued tool)
- Validation tool: tools.validation.validate_dataframe_structure
- Parameters: Tag patterns for domain classification (What: TQ_*-N-*, Where: TQ_*-U/D-*, When: TQ_*-O-*)
- Expected retention: ~75% (RQ 5.1 purification typically retains 38-40 from 50 items)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_dataframe_structure

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch5/rq12 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step01_map_items.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step01_item_mapping.csv
#   CORRECT: logs/step01_map_items.log
#   WRONG:   results/item_mapping.csv  (wrong folder + no prefix)
#   WRONG:   data/item_mapping.csv     (missing step prefix)
#   WRONG:   logs/step01_mapping.csv   (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 1: Map Items to Full vs Purified Sets")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Purified items from RQ 5.1 Step 2 and raw scores from Step 0
        # Purpose: Identify which items were retained by IRT purification

        log("[LOAD] Loading input data...")

        # Load purified items from RQ 5.1 Step 2 (retained items after quality filtering)
        # Expected columns: item_name, factor, a, b
        # Expected rows: ~38 items
        df_purified = pd.read_csv(RQ_DIR / "data/step00_irt_purified_items.csv")
        log(f"[LOADED] step00_irt_purified_items.csv ({len(df_purified)} rows, {len(df_purified.columns)} cols)")

        # Load raw scores from Step 0 (dfData with composite_ID added)
        # Expected columns: composite_ID, UID, TEST, TQ_* (many item columns)
        # Expected rows: ~400 (100 participants x 4 tests)
        df_raw = pd.read_csv(RQ_DIR / "data/step00_raw_scores.csv")
        log(f"[LOADED] step00_raw_scores.csv ({len(df_raw)} rows, {len(df_raw.columns)} cols)")

        # =========================================================================
        # STEP 2: Extract Item Lists
        # =========================================================================
        # Purpose: Get full item list (all TQ_* columns) and purified item list
        # Expected output: Two lists for set comparison

        log("[EXTRACT] Extracting item lists...")

        # Extract all column names matching pattern 'TQ_*' -> full_item_list
        # This represents ALL items in the original test battery
        full_item_list = [col for col in df_raw.columns if col.startswith('TQ_')]
        log(f"[EXTRACTED] Full item list: {len(full_item_list)} items")

        # Extract item_name values from purified items -> purified_item_list
        # This represents ONLY items retained after RQ 5.1 quality filtering
        purified_item_list = df_purified['item_name'].tolist()
        log(f"[EXTRACTED] Purified item list: {len(purified_item_list)} items")

        # Compute retention rate
        retention_rate = len(purified_item_list) / len(full_item_list) * 100
        log(f"[INFO] Retention rate: {retention_rate:.1f}% ({len(purified_item_list)}/{len(full_item_list)})")

        # =========================================================================
        # STEP 3: Classify Items by Domain
        # =========================================================================
        # Tag patterns for domain classification:
        #   What (Nominal): TQ_*-N-*
        #   Where (Spatial): TQ_*-U-* (Up/Down) + TQ_*-D-* (Left/Right via L/D tag variants)
        #   When (Temporal): TQ_*-O-* (Ordinal/Temporal events)
        # Note: RQ 5.1 used -U-/-D-/-L- for Where domain; -D- catches both Down and Left variants

        log("[CLASSIFY] Classifying items by domain using tag patterns...")

        def classify_domain(item_name: str) -> str:
            """Classify item into domain based on tag pattern."""
            if '-N-' in item_name:
                return 'what'
            elif '-U-' in item_name or '-D-' in item_name or '-L-' in item_name:
                return 'where'
            elif '-O-' in item_name or '-T-' in item_name:
                return 'when'
            else:
                # Fallback for unexpected pattern
                return 'unknown'

        # Create item mapping DataFrame
        # Columns: item_name, domain, retained
        item_mapping_data = []
        for item in full_item_list:
            domain = classify_domain(item)
            retained = item in purified_item_list
            item_mapping_data.append({
                'item_name': item,
                'domain': domain,
                'retained': retained
            })

        df_mapping = pd.DataFrame(item_mapping_data)
        log(f"[CLASSIFIED] Created mapping for {len(df_mapping)} items")

        # Count items per domain
        domain_counts = df_mapping.groupby('domain').agg({
            'item_name': 'count',
            'retained': 'sum'
        }).rename(columns={'item_name': 'total', 'retained': 'retained'})
        domain_counts['removed'] = domain_counts['total'] - domain_counts['retained']
        domain_counts['retention_rate'] = domain_counts['retained'] / domain_counts['total'] * 100

        log("[COUNTS] Item counts per domain:")
        log(f"  What:  {domain_counts.loc['what', 'retained']:.0f} retained / {domain_counts.loc['what', 'total']:.0f} total ({domain_counts.loc['what', 'retention_rate']:.1f}%)")
        log(f"  Where: {domain_counts.loc['where', 'retained']:.0f} retained / {domain_counts.loc['where', 'total']:.0f} total ({domain_counts.loc['where', 'retention_rate']:.1f}%)")
        log(f"  When:  {domain_counts.loc['when', 'retained']:.0f} retained / {domain_counts.loc['when', 'total']:.0f} total ({domain_counts.loc['when', 'retention_rate']:.1f}%)")

        # =========================================================================
        # STEP 4: Save Outputs
        # =========================================================================
        # Outputs:
        #   - data/step01_item_mapping.csv: Item mapping with retention status
        #   - logs/step01_item_counts.txt: Text report of counts

        log("[SAVE] Saving outputs...")

        # Save item mapping CSV
        # Columns: item_name, domain, retained
        # Expected rows: ~50 items
        output_mapping_path = RQ_DIR / "data/step01_item_mapping.csv"
        df_mapping.to_csv(output_mapping_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_mapping_path.name} ({len(df_mapping)} rows, {len(df_mapping.columns)} cols)")

        # Save item counts report (text file in logs/)
        # Contains: Domain-wise breakdown of full vs purified counts
        counts_report_path = RQ_DIR / "logs/step01_item_counts.txt"
        with open(counts_report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 70 + "\n")
            f.write("ITEM MAPPING REPORT - RQ 5.12 Step 1\n")
            f.write("=" * 70 + "\n\n")
            f.write(f"Total items in test battery: {len(full_item_list)}\n")
            f.write(f"Items retained by RQ 5.1 purification: {len(purified_item_list)}\n")
            f.write(f"Items removed by RQ 5.1 purification: {len(full_item_list) - len(purified_item_list)}\n")
            f.write(f"Overall retention rate: {retention_rate:.1f}%\n\n")
            f.write("-" * 70 + "\n")
            f.write("DOMAIN-WISE BREAKDOWN\n")
            f.write("-" * 70 + "\n\n")
            for domain in ['what', 'where', 'when']:
                if domain in domain_counts.index:
                    total = int(domain_counts.loc[domain, 'total'])
                    retained = int(domain_counts.loc[domain, 'retained'])
                    removed = int(domain_counts.loc[domain, 'removed'])
                    rate = domain_counts.loc[domain, 'retention_rate']
                    f.write(f"{domain.upper()} Domain:\n")
                    f.write(f"  Total items:    {total}\n")
                    f.write(f"  Retained items: {retained}\n")
                    f.write(f"  Removed items:  {removed}\n")
                    f.write(f"  Retention rate: {rate:.1f}%\n\n")
            f.write("=" * 70 + "\n")

        log(f"[SAVED] {counts_report_path.name} (item counts report)")

        # =========================================================================
        # STEP 5: Run Validation
        # =========================================================================
        # Tool: validate_dataframe_structure
        # Validates: Row count in [48, 52], all required columns present,
        #            domain values in {what, where, when}, retention rate ~75%

        log("[VALIDATION] Running validate_dataframe_structure...")

        validation_result = validate_dataframe_structure(
            df=df_mapping,
            expected_rows=(100, 110),  # Range for total item count (actual: 105 items in dfData.csv)
            expected_columns=['item_name', 'domain', 'retained'],
            column_types={
                'item_name': (object,),
                'domain': (object,),
                'retained': (bool,)
            }
        )

        # Report validation results
        if validation_result['valid']:
            log("[VALIDATION] [PASS] DataFrame structure valid")
            log(f"[VALIDATION]   Row count: {len(df_mapping)} (expected range: [48, 52])")
            log(f"[VALIDATION]   Columns: {list(df_mapping.columns)}")
            log(f"[VALIDATION]   Domain values: {df_mapping['domain'].unique().tolist()}")
            log(f"[VALIDATION]   Retention rate: {retention_rate:.1f}% (expected ~75%)")

            # Additional validation checks
            domain_values = set(df_mapping['domain'].unique())
            expected_domains = {'what', 'where', 'when'}
            if domain_values != expected_domains:
                unexpected = domain_values - expected_domains
                if unexpected:
                    log(f"[VALIDATION] [WARNING] Unexpected domain values: {unexpected}")

            # Check retention rate in reasonable range (60-90%)
            if retention_rate < 60 or retention_rate > 90:
                log(f"[VALIDATION] [WARNING] Retention rate {retention_rate:.1f}% outside typical range [60%, 90%]")

        else:
            log(f"[VALIDATION] [FAIL] {validation_result['message']}")
            raise ValueError(f"Validation failed: {validation_result['message']}")

        log("[SUCCESS] Step 1 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
