#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step00
Step Name: Extract Item-Level Confidence-Accuracy Data
RQ: results/ch6/6.6.1
Generated: 2025-12-07

PURPOSE:
Extract paired TC_* (confidence) and TQ_* (accuracy) items from dfData.csv
for interactive paradigms only (IFR, ICR, IRE). Reshape to long format with
one row per item-response, matching confidence-accuracy pairs by item code.

EXPECTED INPUTS:
  - data/cache/dfData.csv
    Columns: UID, TEST, TSVR, TC_* (confidence items), TQ_* (accuracy items)
    Format: Wide format with ~100 participants × 4 tests = 400 rows
    Source: Project-level data cache

EXPECTED OUTPUTS:
  - data/step00_item_level.csv
    Columns: UID, TEST, TSVR, item_code, confidence, accuracy
    Format: Long format, one row per item-response
    Expected rows: ~27,200 (100 participants × 4 tests × ~68 items)

VALIDATION CRITERIA:
  - All required columns present: UID, TEST, TSVR, item_code, confidence, accuracy
  - confidence values in {0.2, 0.4, 0.6, 0.8, 1.0} (5-point Likert scale)
  - accuracy values in {0, 0.25, 0.5, 1.0} (partial credit scoring)
  - TSVR in [0, 300] hours range (realistic retention intervals)
  - Expected rows: 26,000-28,000 (some variation acceptable)
  - <5% NaN tolerated in confidence/accuracy columns

g_code REASONING:
- Approach: Extract TC_* and TQ_* columns, filter to interactive paradigms,
  reshape to long format, match pairs by item code suffix
- Why this approach: Need item-level data to compute high-confidence error rates
  (confidence >= 0.75 AND accuracy = 0) in downstream Step 1
- Data flow: Wide dfData.csv (400 rows) → Long item-level CSV (~27,200 rows)
- Expected performance: ~10-30 seconds (fast data extraction and reshaping)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib operations (pd.read_csv, pd.melt, filtering, merging)
- Validation tool: tools.validation.validate_data_format
- Parameters: Paradigms include [IFR, ICR, IRE], exclude [RFR, TCR, RRE]
- Critical: confidence MUST be 5-point Likert scale (0.2, 0.4, 0.6, 0.8, 1.0)
- Column format: TC_PARADIGM-DOMAIN-ITEM (hyphen-separated, e.g., TC_IFR-N-i1)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/ch6/6.6.1/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 6.6.1/ (RQ directory)
#   parents[2] = ch6/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_data_format

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch6/6.6.1 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step00_extract_item_level.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step00_item_level.csv
#   CORRECT: logs/step00_extract_item_level.log
#   WRONG:   data/item_level.csv             (missing step prefix)
#   WRONG:   results/item_level.csv          (wrong folder)

# Paradigm filtering configuration
PARADIGMS_INCLUDE = ["IFR", "ICR", "IRE"]  # Interactive paradigms with confidence data
PARADIGMS_EXCLUDE = ["RFR", "TCR", "RRE"]  # Exclude: RFR (no confidence), TCR/RRE (text-based)

# Column prefixes
CONFIDENCE_PREFIX = "TC_"
ACCURACY_PREFIX = "TQ_"

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console with flush."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg, flush=True)


# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 00: Extract Item-Level Confidence-Accuracy Data")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: dfData.csv with UID, TEST, TSVR, TC_* (confidence), TQ_* (accuracy) columns
        # Purpose: Extract item-level confidence-accuracy pairs for HCE analysis

        log("[LOAD] Loading dfData.csv from data/cache/...")
        input_path = PROJECT_ROOT / "data" / "cache" / "dfData.csv"

        if not input_path.exists():
            raise FileNotFoundError(f"Input file not found: {input_path}")

        df_data = pd.read_csv(input_path)
        log(f"[LOADED] dfData.csv ({len(df_data)} rows, {len(df_data.columns)} cols)")

        # Check metadata columns present
        required_meta = ["UID", "TEST", "TSVR"]
        missing_meta = [col for col in required_meta if col not in df_data.columns]
        if missing_meta:
            raise ValueError(f"Missing required metadata columns: {missing_meta}")
        log(f"[CHECK] Metadata columns present: {required_meta}")

        # =========================================================================
        # STEP 2: Extract Confidence and Accuracy Columns
        # =========================================================================
        # Tool: stdlib filtering with column name pattern matching
        # What it does: Identify TC_* and TQ_* columns, filter to interactive paradigms
        # Expected output: List of confidence and accuracy column names

        log("[EXTRACT] Identifying confidence (TC_*) and accuracy (TQ_*) columns...")

        # Find all TC_* and TQ_* columns
        conf_cols = [col for col in df_data.columns if col.startswith(CONFIDENCE_PREFIX)]
        acc_cols = [col for col in df_data.columns if col.startswith(ACCURACY_PREFIX)]

        log(f"[FOUND] {len(conf_cols)} confidence columns (TC_*)")
        log(f"[FOUND] {len(acc_cols)} accuracy columns (TQ_*)")

        # Extract paradigm from column names
        # Column format: TC_PARADIGM-DOMAIN-ITEM (e.g., TC_IFR-N-i1)
        # Need to extract PARADIGM part (between prefix and first hyphen)

        def extract_paradigm(col_name):
            """Extract paradigm from column name (e.g., TC_IFR-N-i1 -> IFR)

            Column format: PREFIX_PARADIGM-DOMAIN-ITEM
            - Remove prefix (TC_ or TQ_)
            - Split by hyphen
            - First part is paradigm code
            """
            # Remove prefix (TC_ or TQ_)
            if col_name.startswith(CONFIDENCE_PREFIX):
                suffix = col_name[len(CONFIDENCE_PREFIX):]
            elif col_name.startswith(ACCURACY_PREFIX):
                suffix = col_name[len(ACCURACY_PREFIX):]
            else:
                return None

            # Split by hyphen and take first part (paradigm code)
            parts = suffix.split("-")
            if len(parts) >= 1:
                return parts[0]  # First part after prefix is paradigm (IFR, ICR, IRE, RFR, etc.)
            return None

        # Filter confidence columns to interactive paradigms
        conf_cols_filtered = []
        for col in conf_cols:
            paradigm = extract_paradigm(col)
            if paradigm in PARADIGMS_INCLUDE:
                conf_cols_filtered.append(col)

        # Filter accuracy columns to interactive paradigms
        acc_cols_filtered = []
        for col in acc_cols:
            paradigm = extract_paradigm(col)
            if paradigm in PARADIGMS_INCLUDE:
                acc_cols_filtered.append(col)

        log(f"[FILTER] {len(conf_cols_filtered)} confidence columns after paradigm filter (IFR, ICR, IRE)")
        log(f"[FILTER] {len(acc_cols_filtered)} accuracy columns after paradigm filter (IFR, ICR, IRE)")
        log(f"[PARADIGMS] Included: {PARADIGMS_INCLUDE}")
        log(f"[PARADIGMS] Excluded: {PARADIGMS_EXCLUDE}")

        # =========================================================================
        # STEP 3: Reshape to Long Format
        # =========================================================================
        # Tool: pd.melt for reshaping wide to long
        # What it does: Create one row per item-response with item_code column
        # Expected output: Two long DataFrames (confidence and accuracy)

        log("[RESHAPE] Converting to long format (one row per item-response)...")

        # Melt confidence columns
        df_conf_long = df_data[required_meta + conf_cols_filtered].melt(
            id_vars=required_meta,
            value_vars=conf_cols_filtered,
            var_name='item_code_conf',
            value_name='confidence'
        )

        # Extract item code suffix (e.g., TC_IFR-N-i1 -> IFR-N-i1)
        df_conf_long['item_code'] = df_conf_long['item_code_conf'].str[len(CONFIDENCE_PREFIX):]
        df_conf_long = df_conf_long.drop(columns=['item_code_conf'])

        log(f"[MELTED] Confidence data: {len(df_conf_long)} rows")

        # Melt accuracy columns
        df_acc_long = df_data[required_meta + acc_cols_filtered].melt(
            id_vars=required_meta,
            value_vars=acc_cols_filtered,
            var_name='item_code_acc',
            value_name='accuracy'
        )

        # Extract item code suffix (e.g., TQ_IFR-N-i1 -> IFR-N-i1)
        df_acc_long['item_code'] = df_acc_long['item_code_acc'].str[len(ACCURACY_PREFIX):]
        df_acc_long = df_acc_long.drop(columns=['item_code_acc'])

        log(f"[MELTED] Accuracy data: {len(df_acc_long)} rows")

        # =========================================================================
        # STEP 4: Match Confidence-Accuracy Pairs
        # =========================================================================
        # Tool: pd.merge on UID, TEST, item_code
        # What it does: Pair confidence and accuracy by matching item codes
        # Expected output: Single DataFrame with both confidence and accuracy columns

        log("[MERGE] Matching confidence-accuracy pairs by item_code...")

        df_item_level = pd.merge(
            df_conf_long,
            df_acc_long[['UID', 'TEST', 'item_code', 'accuracy']],
            on=['UID', 'TEST', 'item_code'],
            how='inner'  # Only keep items with BOTH confidence and accuracy
        )

        log(f"[MERGED] {len(df_item_level)} item-response pairs created")

        # Reorder columns for clarity
        df_item_level = df_item_level[['UID', 'TEST', 'TSVR', 'item_code', 'confidence', 'accuracy']]

        # Report basic statistics
        n_participants = df_item_level['UID'].nunique()
        n_tests = df_item_level['TEST'].nunique()
        n_items = df_item_level['item_code'].nunique()

        log(f"[STATS] Extracted {len(df_item_level)} item-responses from {n_participants} participants")
        log(f"[STATS] {n_tests} test sessions, {n_items} unique items")
        log(f"[STATS] Paradigms included: {', '.join(PARADIGMS_INCLUDE)}")

        # =========================================================================
        # STEP 5: Save Analysis Output
        # =========================================================================
        # Output: data/step00_item_level.csv
        # Contains: Item-level confidence-accuracy pairs for HCE computation

        output_path = RQ_DIR / "data" / "step00_item_level.csv"
        log(f"[SAVE] Saving {output_path}...")

        df_item_level.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path} ({len(df_item_level)} rows, {len(df_item_level.columns)} cols)")

        # =========================================================================
        # STEP 6: Run Validation Tool
        # =========================================================================
        # Tool: tools.validation.validate_data_format
        # Validates: Column presence (first pass - basic structure check)
        # Criteria: All required columns present

        log("[VALIDATION] Running validate_data_format...")

        required_cols = ['UID', 'TEST', 'TSVR', 'item_code', 'confidence', 'accuracy']
        validation_result = validate_data_format(df=df_item_level, required_cols=required_cols)

        if not validation_result['valid']:
            raise ValueError(f"VALIDATION FAILED: {validation_result['message']}")

        log(f"[VALIDATION] PASS - Column check: {validation_result['message']}")

        # =========================================================================
        # STEP 7: Additional Validation Checks (Per 4_analysis.yaml Criteria)
        # =========================================================================
        # Check value ranges and data quality as specified in validation criteria

        log("[VALIDATION] Checking value ranges and data quality...")

        # Check confidence values (must be 5-point Likert scale: 0.2, 0.4, 0.6, 0.8, 1.0)
        valid_confidence = {0.2, 0.4, 0.6, 0.8, 1.0}
        conf_non_null = df_item_level['confidence'].dropna()
        invalid_conf = conf_non_null[~conf_non_null.isin(valid_confidence)]

        if len(invalid_conf) > 0:
            unique_invalid = invalid_conf.unique()
            raise ValueError(
                f"VALIDATION FAILED: Found {len(invalid_conf)} confidence values not in "
                f"{{0.2, 0.4, 0.6, 0.8, 1.0}}. Invalid values: {unique_invalid}"
            )
        log("[VALIDATION] PASS - confidence values in {0.2, 0.4, 0.6, 0.8, 1.0}")

        # Check accuracy values (partial credit scoring: 0, 0.25, 0.5, 1.0)
        valid_accuracy = {0.0, 0.25, 0.5, 1.0}
        acc_non_null = df_item_level['accuracy'].dropna()
        invalid_acc = acc_non_null[~acc_non_null.isin(valid_accuracy)]

        if len(invalid_acc) > 0:
            unique_invalid = invalid_acc.unique()
            raise ValueError(
                f"VALIDATION FAILED: Found {len(invalid_acc)} accuracy values not in {{0, 0.25, 0.5, 1.0}}. "
                f"Invalid values: {unique_invalid}"
            )
        log("[VALIDATION] PASS - accuracy values in {0, 0.25, 0.5, 1.0}")

        # Check TSVR range (allow up to 300 hours for realistic retention intervals)
        tsvr_min = df_item_level['TSVR'].min()
        tsvr_max = df_item_level['TSVR'].max()

        if tsvr_min < 0 or tsvr_max > 300:
            raise ValueError(
                f"VALIDATION FAILED: TSVR out of range [0, 300]. "
                f"Found: min={tsvr_min:.2f}, max={tsvr_max:.2f}"
            )
        log(f"[VALIDATION] PASS - TSVR in reasonable range (min={tsvr_min:.2f}, max={tsvr_max:.2f} hours)")

        # Check expected row count
        expected_min = 26000
        expected_max = 28000
        actual_rows = len(df_item_level)

        if actual_rows < expected_min or actual_rows > expected_max:
            log(f"[VALIDATION] WARNING - Expected rows: {expected_min}-{expected_max}, found: {actual_rows}")
            log(f"[VALIDATION] This may be acceptable if some items have missing data")
        else:
            log(f"[VALIDATION] PASS - Row count within expected range: {actual_rows}")

        # Check missing data tolerance (<5% NaN in confidence/accuracy)
        conf_missing_pct = (df_item_level['confidence'].isna().sum() / len(df_item_level)) * 100
        acc_missing_pct = (df_item_level['accuracy'].isna().sum() / len(df_item_level)) * 100

        log(f"[VALIDATION] Missing data: confidence={conf_missing_pct:.2f}%, accuracy={acc_missing_pct:.2f}%")

        if conf_missing_pct > 5.0:
            raise ValueError(
                f"VALIDATION FAILED: Confidence missing data exceeds 5% tolerance ({conf_missing_pct:.2f}%)"
            )
        if acc_missing_pct > 5.0:
            raise ValueError(
                f"VALIDATION FAILED: Accuracy missing data exceeds 5% tolerance ({acc_missing_pct:.2f}%)"
            )
        log("[VALIDATION] PASS - Missing data < 5% tolerance")

        log("[SUCCESS] Step 00 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        import traceback
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
