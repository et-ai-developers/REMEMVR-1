#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step01
Step Name: Compute HCE Rate Per Participant Per Timepoint
RQ: results/ch6/6.6.1
Generated: 2025-12-07T17:00:00Z

PURPOSE:
Aggregate item-level confidence-accuracy data to compute HCE rate (high-confidence
errors: confidence >= 0.75 AND accuracy = 0) per participant per test session.
HCE rate quantifies metacognitive monitoring failures - instances where participants
are highly confident in incorrect responses.

EXPECTED INPUTS:
  - data/step00_item_level.csv
    Columns: ['UID', 'TEST', 'TSVR', 'item_code', 'confidence', 'accuracy']
    Format: Long format (one row per item-response)
    Expected rows: ~28,800 (100 participants × 4 tests × ~72 items)

EXPECTED OUTPUTS:
  - data/step01_hce_rates.csv
    Columns: ['UID', 'TEST', 'TSVR', 'HCE_rate', 'n_HCE', 'n_total']
    Format: CSV, long format (one row per participant-test combination)
    Expected rows: 400 (100 participants × 4 tests)

VALIDATION CRITERIA:
  - HCE_rate in [0, 1] range (proportion validation)
  - No NaN values in HCE_rate column
  - n_HCE <= n_total (logical constraint)
  - Expected rows: 400 (100 participants × 4 tests)
  - All 100 participants present
  - All 4 tests present per participant

g_code REASONING:
- Approach: Group by UID and TEST, compute HCE count and rate per group
- Why this approach: Metacognitive monitoring analysis requires per-participant
  tracking of high-confidence errors over time. Aggregation to participant-test
  level enables LMM trajectory analysis in Step 2.
- Data flow: Item-level (28,800 rows) → Participant-test level (400 rows)
- Expected performance: <10 seconds (simple aggregation)

IMPLEMENTATION NOTES:
- Confidence scale: {0.2, 0.4, 0.6, 0.8, 1.0} (5-level from Step 00 actual output)
- HCE threshold: confidence >= 0.75 means confidence in {0.8, 1.0}
- Validation tool: tools.validation.validate_probability_range
- Parameters: probability_df=df_hce_rates, prob_columns=['HCE_rate']
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/ (6.6.1)
#   parents[2] = chX/ (ch6)
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_probability_range

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch6/6.6.1 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step01_compute_hce_rates.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step01_hce_rates.csv
#   CORRECT: logs/step01_compute_hce_rates.log
#   WRONG:   results/hce_rates.csv  (wrong folder + no prefix)
#   WRONG:   data/hce_rates.csv     (missing step prefix)
#   WRONG:   logs/step01_hce_rates.csv (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg, flush=True)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 01: Compute HCE Rate Per Participant Per Timepoint")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Item-level confidence-accuracy pairs from Step 0
        # Purpose: Aggregate to participant-test level to compute HCE rates

        log("[LOAD] Loading input data...")
        input_path = RQ_DIR / "data" / "step00_item_level.csv"
        df_item = pd.read_csv(input_path, encoding='utf-8')
        log(f"[LOADED] step00_item_level.csv ({len(df_item)} rows, {len(df_item.columns)} cols)")
        log(f"[INFO] Input columns: {df_item.columns.tolist()}")
        log(f"[INFO] Confidence scale: {sorted(df_item['confidence'].dropna().unique())}")
        log(f"[INFO] Accuracy values: {sorted(df_item['accuracy'].dropna().unique())}")

        # =========================================================================
        # STEP 2: Compute HCE Rates Per Participant Per Test
        # =========================================================================
        # Tool: stdlib (pandas groupby aggregation)
        # What it does: Identify high-confidence errors (confidence >= 0.75 AND accuracy = 0)
        #               and compute proportion per participant per test
        # Expected output: 400 rows (100 participants × 4 tests)

        log("[ANALYSIS] Computing HCE rates...")

        # Filter to valid rows (non-NaN confidence AND accuracy)
        df_valid = df_item.dropna(subset=['confidence', 'accuracy']).copy()
        log(f"[INFO] Valid rows after NaN filter: {len(df_valid)} (dropped {len(df_item) - len(df_valid)} rows)")

        # Define HCE condition: confidence >= 0.75 AND accuracy = 0
        # Note: Confidence scale is {0.2, 0.4, 0.6, 0.8, 1.0}, so >= 0.75 means {0.8, 1.0}
        df_valid['is_HCE'] = (df_valid['confidence'] >= 0.75) & (df_valid['accuracy'] == 0)

        # Group by UID and TEST
        agg_dict = {
            'is_HCE': 'sum',       # Count HCE instances
            'item_code': 'count',  # Count total items (valid rows)
            'TSVR': 'first'        # Get TSVR (should be constant per UID-TEST)
        }

        df_grouped = df_valid.groupby(['UID', 'TEST'], as_index=False).agg(agg_dict)

        # Rename columns
        df_grouped.rename(columns={
            'is_HCE': 'n_HCE',
            'item_code': 'n_total'
        }, inplace=True)

        # Compute HCE_rate = n_HCE / n_total
        df_grouped['HCE_rate'] = df_grouped['n_HCE'] / df_grouped['n_total']

        # Reorder columns to match specification
        df_hce_rates = df_grouped[['UID', 'TEST', 'TSVR', 'HCE_rate', 'n_HCE', 'n_total']]

        log(f"[DONE] Computed HCE rates for {len(df_hce_rates)} observations")
        log(f"[INFO] Unique participants: {df_hce_rates['UID'].nunique()}")
        log(f"[INFO] Unique tests: {df_hce_rates['TEST'].nunique()} ({sorted(df_hce_rates['TEST'].unique())})")
        log(f"[INFO] HCE_rate range: [{df_hce_rates['HCE_rate'].min():.4f}, {df_hce_rates['HCE_rate'].max():.4f}]")
        log(f"[INFO] Mean HCE_rate: {df_hce_rates['HCE_rate'].mean():.4f}")
        log(f"[INFO] n_HCE range: [{df_hce_rates['n_HCE'].min()}, {df_hce_rates['n_HCE'].max()}]")
        log(f"[INFO] n_total range: [{df_hce_rates['n_total'].min()}, {df_hce_rates['n_total'].max()}]")

        # =========================================================================
        # STEP 3: Save Analysis Output
        # =========================================================================
        # Output: data/step01_hce_rates.csv
        # Contains: HCE rates per participant-test for LMM trajectory analysis

        log("[SAVE] Saving output data...")
        output_path = RQ_DIR / "data" / "step01_hce_rates.csv"
        df_hce_rates.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] step01_hce_rates.csv ({len(df_hce_rates)} rows, {len(df_hce_rates.columns)} cols)")

        # =========================================================================
        # STEP 4: Run Validation Tool
        # =========================================================================
        # Tool: tools.validation.validate_probability_range
        # Validates: HCE_rate in [0, 1] range (proportion validation)
        # Threshold: No NaN, no out-of-range values

        log("[VALIDATION] Running validate_probability_range...")
        validation_result = validate_probability_range(
            probability_df=df_hce_rates,
            prob_columns=['HCE_rate']
        )

        # Report validation results
        # Expected: valid=True, no violations
        if validation_result['valid']:
            log(f"[PASS] Probability range validation: {validation_result['message']}")
        else:
            log(f"[FAIL] Probability range validation: {validation_result['message']}")
            if 'violations' in validation_result and validation_result['violations']:
                for violation in validation_result['violations']:
                    log(f"[FAIL]   - {violation}")
            raise ValueError(f"Validation failed: {validation_result['message']}")

        # Additional logical checks not covered by validate_probability_range
        log("[VALIDATION] Running additional logical checks...")

        # Check: n_HCE <= n_total
        invalid_counts = df_hce_rates[df_hce_rates['n_HCE'] > df_hce_rates['n_total']]
        if len(invalid_counts) > 0:
            log(f"[FAIL] Found {len(invalid_counts)} rows where n_HCE > n_total")
            log(f"[FAIL] Examples: {invalid_counts.head()}")
            raise ValueError("Logical constraint violated: n_HCE > n_total")
        else:
            log("[PASS] All rows satisfy n_HCE <= n_total")

        # Check: Expected 400 rows (100 participants × 4 tests)
        expected_rows = 400
        if len(df_hce_rates) != expected_rows:
            log(f"[FAIL] Expected {expected_rows} rows, found {len(df_hce_rates)}")
            raise ValueError(f"Expected {expected_rows} rows, found {len(df_hce_rates)}")
        else:
            log(f"[PASS] Row count matches expected: {expected_rows}")

        # Check: All 100 participants present
        n_participants = df_hce_rates['UID'].nunique()
        if n_participants != 100:
            log(f"[FAIL] Expected 100 participants, found {n_participants}")
            raise ValueError(f"Expected 100 participants, found {n_participants}")
        else:
            log(f"[PASS] All 100 participants present")

        # Check: All 4 tests per participant
        tests_per_participant = df_hce_rates.groupby('UID')['TEST'].nunique()
        if not (tests_per_participant == 4).all():
            missing_tests = tests_per_participant[tests_per_participant < 4]
            log(f"[FAIL] {len(missing_tests)} participants missing test sessions")
            log(f"[FAIL] Examples: {missing_tests.head()}")
            raise ValueError(f"{len(missing_tests)} participants missing test sessions")
        else:
            log(f"[PASS] All participants have 4 test sessions")

        # Check: No NaN in HCE_rate
        nan_count = df_hce_rates['HCE_rate'].isna().sum()
        if nan_count > 0:
            log(f"[FAIL] Found {nan_count} NaN values in HCE_rate")
            raise ValueError(f"Found {nan_count} NaN values in HCE_rate")
        else:
            log("[PASS] No NaN values in HCE_rate")

        log("[SUCCESS] Step 01 complete - All validations passed")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
