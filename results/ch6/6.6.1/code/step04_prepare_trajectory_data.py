#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step04
Step Name: Prepare trajectory plot data
RQ: results/ch6/6.6.1
Generated: 2025-12-07

PURPOSE:
Aggregate HCE rates by timepoint for trajectory plot with 95% confidence intervals.
This step transforms individual-level HCE rates (Step 1 output) into plot-ready summary
statistics showing mean trajectory over time with uncertainty quantification.

EXPECTED INPUTS:
- data/step01_hce_rates.csv
  Columns: ['UID', 'TEST', 'TSVR', 'HCE_rate', 'n_HCE', 'n_total']
  Format: Long format (one row per participant-test combination)
  Expected rows: ~400 (100 participants × 4 tests)

EXPECTED OUTPUTS:
- data/step04_hce_trajectory_data.csv
  Columns: ['time', 'HCE_rate_mean', 'CI_lower', 'CI_upper', 'test']
  Format: CSV, plot source data for trajectory visualization
  Expected rows: 4 (one per timepoint: T1, T2, T3, T4)

VALIDATION CRITERIA:
- All 4 tests present (T1, T2, T3, T4)
- Exactly 4 rows (one per timepoint)
- No NaN values in any column
- CI_upper > CI_lower for all rows (valid confidence intervals)
- time values monotonically increasing (T1 < T2 < T3 < T4)
- HCE_rate_mean in [0, 1] (proportion range)

g_code REASONING:
- Approach: Standard pandas groupby aggregation with confidence interval computation
- Why this approach: Simple, efficient, and transparent statistical summary
- Data flow: Individual rates → grouped by TEST → mean/SE/CI computed → plot-ready format
- Expected performance: <1 second (lightweight aggregation on ~400 rows)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib (pandas operations, no catalogued tool)
- Validation tool: tools.validation.validate_plot_data_completeness
- Parameters: Group by TEST, 95% confidence level (Z=1.96)
- SE computation: SD / sqrt(N) where N is number of participants per timepoint
- CI computation: mean ± 1.96*SE (assumes normality via CLT)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/ (6.6.1/)
#   parents[2] = chX/ (ch6/)
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_plot_data_completeness

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch6/6.6.1 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step04_prepare_trajectory_data.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step04_hce_trajectory_data.csv
#   CORRECT: logs/step04_prepare_trajectory_data.log
#   WRONG:   results/trajectory_data.csv  (wrong folder + no prefix)
#   WRONG:   data/trajectory_data.csv     (missing step prefix)
#   WRONG:   logs/step04_trajectory.csv   (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console with UTF-8 encoding."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg, flush=True)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 04: Prepare trajectory plot data")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Individual-level HCE rates from Step 1 (400 rows: 100 participants × 4 tests)
        # Purpose: Aggregate to timepoint-level means for trajectory visualization

        log("[LOAD] Loading HCE rates from Step 1...")
        input_path = RQ_DIR / "data" / "step01_hce_rates.csv"
        df_hce_rates = pd.read_csv(input_path, encoding='utf-8')
        log(f"[LOADED] {input_path.name} ({len(df_hce_rates)} rows, {len(df_hce_rates.columns)} cols)")
        log(f"[INFO] Columns: {list(df_hce_rates.columns)}")
        log(f"[INFO] Tests present: {sorted(df_hce_rates['TEST'].unique())}")

        # =========================================================================
        # STEP 2: Aggregate by Timepoint
        # =========================================================================
        # What it does: Group by TEST, compute mean HCE rate with confidence intervals
        # Expected output: 4 rows (T1, T2, T3, T4) with mean, SE, CI

        log("[ANALYSIS] Aggregating HCE rates by timepoint...")

        # Group by TEST and compute statistics
        grouped = df_hce_rates.groupby('TEST', as_index=False).agg({
            'TSVR': 'mean',           # Mean time per timepoint (for x-axis)
            'HCE_rate': ['mean', 'std', 'count']  # Mean, SD, N for confidence intervals
        })

        # Flatten column names from multi-level index
        grouped.columns = ['TEST', 'time', 'HCE_rate_mean', 'HCE_rate_std', 'n_participants']

        log(f"[INFO] Aggregated to {len(grouped)} timepoints")
        log(f"[INFO] Timepoints: {list(grouped['TEST'].values)}")
        log(f"[INFO] Sample sizes per timepoint: {list(grouped['n_participants'].values)}")

        # =========================================================================
        # STEP 3: Compute Confidence Intervals
        # =========================================================================
        # Standard error: SE = SD / sqrt(N)
        # 95% CI: mean ± 1.96*SE (assumes normality via Central Limit Theorem)

        log("[ANALYSIS] Computing 95% confidence intervals...")

        # Compute standard error
        grouped['SE'] = grouped['HCE_rate_std'] / np.sqrt(grouped['n_participants'])

        # Compute 95% confidence intervals
        z_critical = 1.96  # Z-score for 95% confidence level
        grouped['CI_lower'] = grouped['HCE_rate_mean'] - z_critical * grouped['SE']
        grouped['CI_upper'] = grouped['HCE_rate_mean'] + z_critical * grouped['SE']

        # Clip CI bounds to [0, 1] (proportions cannot be negative or >1)
        grouped['CI_lower'] = grouped['CI_lower'].clip(lower=0.0)
        grouped['CI_upper'] = grouped['CI_upper'].clip(upper=1.0)

        log("[INFO] Confidence intervals computed")
        for idx, row in grouped.iterrows():
            log(f"[INFO]   {row['TEST']}: mean={row['HCE_rate_mean']:.4f}, "
                f"CI=[{row['CI_lower']:.4f}, {row['CI_upper']:.4f}], "
                f"SE={row['SE']:.4f}")

        # =========================================================================
        # STEP 4: Prepare Plot-Ready Output
        # =========================================================================
        # Format: time, HCE_rate_mean, CI_lower, CI_upper, test
        # Rename TEST to 'test' for consistency with plot specification

        log("[SAVE] Preparing plot-ready output...")

        # Select and rename columns
        df_plot_data = grouped[['time', 'HCE_rate_mean', 'CI_lower', 'CI_upper', 'TEST']].copy()
        df_plot_data.rename(columns={'TEST': 'test'}, inplace=True)

        # Convert test column to string (required for validation)
        df_plot_data['test'] = df_plot_data['test'].astype(int).astype(str)

        # Sort by time (chronological order)
        df_plot_data = df_plot_data.sort_values('time').reset_index(drop=True)

        log(f"[INFO] Plot data prepared: {len(df_plot_data)} rows, {len(df_plot_data.columns)} cols")

        # =========================================================================
        # STEP 5: Save Output
        # =========================================================================

        output_path = RQ_DIR / "data" / "step04_hce_trajectory_data.csv"
        df_plot_data.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(df_plot_data)} rows, {len(df_plot_data.columns)} cols)")

        # =========================================================================
        # STEP 6: Validation
        # =========================================================================
        # Tool: validate_plot_data_completeness
        # Validates: All 4 tests present (T1, T2, T3, T4)

        log("[VALIDATION] Running validate_plot_data_completeness...")

        validation_result = validate_plot_data_completeness(
            plot_data=df_plot_data,
            required_domains=[],                    # No domain grouping
            required_groups=["1", "2", "3", "4"],   # Test sessions as strings
            domain_col="test",
            group_col="test"
        )

        log(f"[VALIDATION] {validation_result['message']}")

        if not validation_result['valid']:
            error_msg = f"Validation failed: {validation_result['message']}"
            log(f"[ERROR] {error_msg}")
            raise ValueError(error_msg)

        # Additional validation checks (beyond validate_plot_data_completeness)
        log("[VALIDATION] Running additional checks...")

        # Check exactly 4 rows
        if len(df_plot_data) != 4:
            error_msg = f"Expected exactly 4 rows, found {len(df_plot_data)}"
            log(f"[ERROR] {error_msg}")
            raise ValueError(error_msg)

        # Check no NaN values
        if df_plot_data.isnull().any().any():
            nan_cols = df_plot_data.columns[df_plot_data.isnull().any()].tolist()
            error_msg = f"NaN values detected in columns: {nan_cols}"
            log(f"[ERROR] {error_msg}")
            raise ValueError(error_msg)

        # Check CI_upper > CI_lower
        invalid_ci = df_plot_data[df_plot_data['CI_upper'] <= df_plot_data['CI_lower']]
        if len(invalid_ci) > 0:
            error_msg = f"Invalid confidence intervals found (CI_upper <= CI_lower) for tests: {invalid_ci['test'].tolist()}"
            log(f"[ERROR] {error_msg}")
            raise ValueError(error_msg)

        # Check time values monotonically increasing
        if not df_plot_data['time'].is_monotonic_increasing:
            error_msg = "Time values not monotonically increasing"
            log(f"[ERROR] {error_msg}")
            raise ValueError(error_msg)

        # Check HCE_rate_mean in [0, 1]
        out_of_range = df_plot_data[(df_plot_data['HCE_rate_mean'] < 0) | (df_plot_data['HCE_rate_mean'] > 1)]
        if len(out_of_range) > 0:
            error_msg = f"HCE_rate_mean out of [0, 1] range for tests: {out_of_range['test'].tolist()}"
            log(f"[ERROR] {error_msg}")
            raise ValueError(error_msg)

        log("[VALIDATION] All checks passed")
        log(f"[VALIDATION] Plot data preparation complete: {len(df_plot_data)} timepoints created")
        log(f"[VALIDATION] All tests represented: {', '.join(map(str, sorted(df_plot_data['test'].values)))}")

        log("[SUCCESS] Step 04 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
