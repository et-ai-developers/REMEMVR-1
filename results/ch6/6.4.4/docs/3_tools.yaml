# 3_tools.yaml - Tool Catalog for RQ 6.4.4
# Created by: rq_tools agent
# Architecture: Tool Catalog (each tool listed once, deduplication)
# RQ: Paradigm-Specific Trait Variance in Confidence Decline

analysis_tools:
  fit_lmm_trajectory_tsvr:
    module: "tools.analysis_lmm"
    function: "fit_lmm_trajectory_tsvr"
    signature: "fit_lmm_trajectory_tsvr(theta_scores: DataFrame, tsvr_data: DataFrame, formula: str, groups: str = 'UID', re_formula: str = '~Days', reml: bool = False) -> MixedLMResults"
    validation_tool: "validate_lmm_convergence"

    input_files:
      - path: "data/step00_lmm_input.csv"
        required_columns: ["UID", "test", "TSVR_hours", "paradigm", "theta_confidence", "se_confidence"]
        expected_rows: "400 per paradigm (100 participants x 4 tests, stratified by paradigm)"
        data_types:
          UID: "string (participant identifier)"
          test: "string (T1, T2, T3, T4)"
          TSVR_hours: "float (actual hours since encoding, Decision D070)"
          paradigm: "string (IFR, ICR, IRE)"
          theta_confidence: "float (IRT ability estimate)"
          se_confidence: "float (standard error)"

    output_files:
      - path: "data/step01_lmm_ifr_summary.txt"
        description: "LMM summary for Free Recall paradigm (IFR)"
      - path: "data/step01_lmm_icr_summary.txt"
        description: "LMM summary for Cued Recall paradigm (ICR)"
      - path: "data/step01_lmm_ire_summary.txt"
        description: "LMM summary for Recognition paradigm (IRE)"

    parameters:
      formula: "theta_confidence ~ TSVR_hours + (TSVR_hours | UID)"
      groups: "UID"
      re_formula: "~TSVR_hours"
      reml: false

    description: "Fit paradigm-stratified LMMs with random slopes to estimate variance in confidence decline rates. Decision D070 uses TSVR_hours (actual hours) not nominal days."
    source_reference: "tools_inventory.md lines 97-104"

  extract_random_effects_from_lmm:
    module: "tools.analysis_lmm"
    function: "extract_random_effects_from_lmm"
    signature: "extract_random_effects_from_lmm(result: MixedLMResults) -> Dict[str, Any]"
    validation_tool: "validate_variance_positivity"

    input_files:
      - path: "fitted LMM models (in-memory Python objects)"
        source: "Step 1 LMM fitting output"

    output_files:
      - path: "data/step02_variance_components.csv"
        columns: ["paradigm", "var_intercept", "var_slope", "cov_int_slope", "var_residual", "var_total"]
        description: "Variance components per paradigm (3 rows: IFR, ICR, IRE)"

    parameters:
      extract_variance_components: true
      extract_icc: false

    description: "Extract variance components (intercept, slope, covariance, residual) from fitted LMMs for ICC computation."
    source_reference: "tools_inventory.md lines 121-128"

  compute_icc_from_variance_components:
    module: "tools.analysis_lmm"
    function: "compute_icc_from_variance_components"
    signature: "compute_icc_from_variance_components(variance_components_df: DataFrame, slope_name: str = 'TSVR_hours', timepoint: float = 6.0) -> DataFrame"
    validation_tool: "validate_icc_bounds"

    input_files:
      - path: "data/step02_variance_components.csv"
        required_columns: ["Component", "Variance"]
        source: "Step 2 variance extraction output"

    output_files:
      - path: "data/step03_icc_estimates.csv"
        columns: ["paradigm", "ICC_intercept", "ICC_slope_simple", "ICC_slope_conditional", "interpretation_intercept", "interpretation_slope"]
        description: "ICC estimates per paradigm with interpretations"

    parameters:
      slope_name: "TSVR_hours"
      timepoint: 6.0

    description: "Compute 3 ICC estimates per paradigm: baseline variance (ICC_intercept), unconditional slope variance (ICC_slope_simple), and conditional slope variance at Day 6 (ICC_slope_conditional)."
    source_reference: "tools_inventory.md lines 165-173"

validation_tools:
  validate_lmm_convergence:
    module: "tools.validation"
    function: "validate_lmm_convergence"
    signature: "validate_lmm_convergence(lmm_result: MixedLMResults) -> Dict[str, Any]"

    input_files:
      - path: "fitted LMM models (in-memory)"
        source: "Step 1 LMM fitting output"

    parameters:
      check_convergence_status: true

    criteria:
      - "Model converged successfully (converged attribute = True)"
      - "No singular fit warnings (random effects variance > 0)"
      - "All variance components finite (no NaN/Inf)"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        converged: "bool (True if all criteria passed)"
        message: "str (human-readable explanation)"
        warnings: "List[str] (convergence warnings if any)"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step01_fit_lmm.log"
      invoke: "g_debug (master invokes after error)"

    description: "Validate LMM converged successfully with no singular fit issues."
    source_reference: "tools_inventory.md lines 330-334"

  validate_variance_positivity:
    module: "tools.validation"
    function: "validate_variance_positivity"
    signature: "validate_variance_positivity(variance_df: DataFrame, component_col: str = 'component', value_col: str = 'variance') -> Dict[str, Any]"

    input_files:
      - path: "data/step02_variance_components.csv"
        required_columns: ["paradigm", "var_intercept", "var_slope", "var_residual"]
        source: "Step 2 variance extraction output"

    parameters:
      component_col: "component"
      value_col: "variance"

    criteria:
      - "All variance components > 0 (negative variance impossible)"
      - "No NaN values in variance columns"
      - "var_total = var_intercept + var_slope + var_residual (sanity check)"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool (True if all variances positive)"
        message: "str (error details if invalid)"
        negative_components: "List[str] (components with invalid values)"
        variance_range: "Tuple[float, float] (min, max variance)"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step02_extract_variance.log"
      invoke: "g_debug (master invokes after error)"

    description: "Validate all variance components are positive (negative variance indicates estimation error)."
    source_reference: "tools_inventory.md lines 608-617"

  validate_icc_bounds:
    module: "tools.validation"
    function: "validate_icc_bounds"
    signature: "validate_icc_bounds(icc_df: DataFrame, icc_col: str = 'icc_value') -> Dict[str, Any]"

    input_files:
      - path: "data/step03_icc_estimates.csv"
        required_columns: ["paradigm", "ICC_intercept", "ICC_slope_simple", "ICC_slope_conditional"]
        source: "Step 3 ICC computation output"

    parameters:
      icc_col: "icc_value"

    criteria:
      - "All ICC values in [0, 1] range (outside indicates computation error)"
      - "No NaN values in ICC columns"
      - "Interpretation strings match ICC value ranges"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool (True if all ICCs in [0,1])"
        message: "str (error details if invalid)"
        out_of_bounds: "List[Dict] (ICC values outside range)"
        icc_range: "Tuple[float, float] (min, max ICC)"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step03_compute_icc.log"
      invoke: "g_debug (master invokes after error)"

    description: "Validate ICC values are in valid [0, 1] range (proportion of variance)."
    source_reference: "tools_inventory.md lines 619-627"

  validate_data_format:
    module: "tools.validation"
    function: "validate_data_format"
    signature: "validate_data_format(df: DataFrame, required_cols: List[str]) -> Dict[str, Any]"

    input_files:
      - path: "varies per step (data import, comparisons)"
        source: "Step 0, 4, 5 outputs"

    parameters:
      required_cols: "varies per validation call"

    criteria:
      - "All required columns present (case-sensitive)"
      - "Expected row count matches plan specification"
      - "No unexpected NaN patterns"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool (True if all columns present)"
        message: "str (missing columns if invalid)"
        missing_cols: "List[str] (columns not found)"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/stepNN_<step_name>.log"
      invoke: "g_debug (master invokes after error)"

    description: "Validate DataFrame has required columns and basic structure."
    source_reference: "tools_inventory.md lines 552-561"

summary:
  analysis_tools_count: 3
  validation_tools_count: 4
  total_unique_tools: 7
  mandatory_decisions_embedded: ["D070"]
  notes:
    - "This RQ uses DERIVED data from RQ 6.4.1 (theta_confidence scores)"
    - "No IRT calibration or purification steps required"
    - "Variance decomposition approach tests paradigm-specific trait variance in confidence decline"
    - "ICC_slope comparisons determine if retrieval support moderates trait-like individual differences"
    - "Step 5 depends on Ch5 5.3.7 completion (accuracy ICC comparison)"
