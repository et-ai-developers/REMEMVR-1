# 3_tools.yaml - Tool Catalog for RQ 6.8.4
# Created by: rq_tools agent
# RQ: 6.8.4 Source-Destination Confidence Clustering
# Analysis: K-means clustering on location-stratified LMM random effects
# Architecture: Tool Catalog (each tool listed once, validation paired)

analysis_tools:
  pandas_pivot:
    module: "pandas"
    function: "pivot"
    signature: "pivot(data: DataFrame, index: str, columns: str, values: List[str]) -> DataFrame"
    validation_tool: "validate_dataframe_structure"

    description: "Reshape random effects from long format (200 rows, 2 locations) to wide format (100 rows, 4 features)"

  sklearn_standard_scaler:
    module: "sklearn.preprocessing"
    function: "StandardScaler"
    signature: "StandardScaler().fit_transform(X: np.ndarray) -> np.ndarray"
    validation_tool: "validate_standardization"

    description: "Z-score standardization of 4 features (mean=0, SD=1) for equal weighting in K-means"

  sklearn_kmeans_bic:
    module: "sklearn.cluster"
    function: "KMeans"
    signature: "KMeans(n_clusters: int, random_state: int).fit(X: np.ndarray) -> KMeans"
    validation_tool: "validate_numeric_range"

    description: "K-means clustering for K=1 to 6 with BIC computation for model selection"

  sklearn_kmeans_final:
    module: "sklearn.cluster"
    function: "KMeans"
    signature: "KMeans(n_clusters: int, random_state: int).fit(X: np.ndarray) -> KMeans"
    validation_tool: "validate_cluster_assignment"

    description: "Final K-means clustering using optimal K from BIC selection"

  sklearn_silhouette_score:
    module: "sklearn.metrics"
    function: "silhouette_score"
    signature: "silhouette_score(X: np.ndarray, labels: np.ndarray) -> float"
    validation_tool: "validate_numeric_range"

    description: "Compute Silhouette coefficient (cluster cohesion vs separation, range [-1,1])"

  sklearn_davies_bouldin_score:
    module: "sklearn.metrics"
    function: "davies_bouldin_score"
    signature: "davies_bouldin_score(X: np.ndarray, labels: np.ndarray) -> float"
    validation_tool: "validate_numeric_range"

    description: "Compute Davies-Bouldin index (cluster similarity, lower is better)"

  custom_jaccard_bootstrap:
    module: "custom"
    function: "compute_jaccard_bootstrap"
    signature: "compute_jaccard_bootstrap(X: np.ndarray, labels: np.ndarray, n_bootstrap: int) -> np.ndarray"
    validation_tool: "validate_bootstrap_stability"

    description: "Bootstrap stability analysis via Jaccard coefficient (100 iterations)"

  pandas_groupby_describe:
    module: "pandas"
    function: "groupby"
    signature: "groupby(by: str).describe() -> DataFrame"
    validation_tool: "validate_cluster_summary_stats"

    description: "Compute cluster summary statistics (mean, SD, min, max per feature)"

  scipy_chi2_contingency:
    module: "scipy.stats"
    function: "chi2_contingency"
    signature: "chi2_contingency(observed: np.ndarray) -> Tuple[float, float, int, np.ndarray]"
    validation_tool: "validate_hypothesis_test_dual_pvalues"

    description: "Chi-square test of association between confidence and accuracy clusters (Decision D068 dual p-values)"

  pandas_concat:
    module: "pandas"
    function: "concat"
    signature: "concat(objs: List[DataFrame], axis: int) -> DataFrame"
    validation_tool: "validate_dataframe_structure"

    description: "Concatenate clustering quality metrics from 6 RQs (5 Ch5 + this RQ)"

  sklearn_pca:
    module: "sklearn.decomposition"
    function: "PCA"
    signature: "PCA(n_components: int).fit_transform(X: np.ndarray) -> np.ndarray"
    validation_tool: "validate_dataframe_structure"

    description: "PCA projection from 4D feature space to 2D for visualization"

validation_tools:
  validate_dataframe_structure:
    module: "tools.validation"
    function: "validate_dataframe_structure"
    signature: "validate_dataframe_structure(df: DataFrame, expected_rows: Union[int, Tuple[int, int]], expected_columns: List[str], column_types: Optional[Dict[str, type]]) -> Dict[str, Any]"

    criteria:
      - "Row count matches expected (exact or range)"
      - "All required columns present"
      - "Column types match specification (if provided)"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        message: "str"
        checks: "Dict[str, bool]"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/stepNN_*.log"
      invoke: "g_debug (master invokes)"

    description: "Validate DataFrame structure (rows, columns, types)"

  validate_standardization:
    module: "tools.validation"
    function: "validate_standardization"
    signature: "validate_standardization(df: DataFrame, column_names: List[str], tolerance: float) -> Dict[str, Any]"

    criteria:
      - "Mean of each z-score column in [-tolerance, tolerance]"
      - "SD of each z-score column in [1-tolerance, 1+tolerance]"
      - "No NaN values in standardized features"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        message: "str"
        mean_values: "Dict[str, float]"
        sd_values: "Dict[str, float]"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step01_standardize_features.log"
      invoke: "g_debug (master invokes)"

    description: "Validate z-score standardization (mean H 0, SD H 1)"

  validate_numeric_range:
    module: "tools.validation"
    function: "validate_numeric_range"
    signature: "validate_numeric_range(data: Union[np.ndarray, pd.Series], min_val: float, max_val: float, column_name: str) -> Dict[str, Any]"

    criteria:
      - "All values >= min_val"
      - "All values <= max_val"
      - "No NaN values"
      - "No infinite values"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        message: "str"
        out_of_range_count: "int"
        violations: "list"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/stepNN_*.log"
      invoke: "g_debug (master invokes)"

    description: "Validate numeric values within specified range"

  validate_cluster_assignment:
    module: "tools.validation"
    function: "validate_cluster_assignment"
    signature: "validate_cluster_assignment(cluster_labels: Union[np.ndarray, pd.Series], n_expected: int, min_cluster_size: int) -> Dict[str, Any]"

    criteria:
      - "All participants assigned (length = n_expected)"
      - "Cluster IDs consecutive starting from 0"
      - "Each cluster >= min_cluster_size members"
      - "No empty clusters"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        message: "str"
        cluster_sizes: "Dict[int, int]"
        n_clusters: "int"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step03_fit_final_kmeans.log"
      invoke: "g_debug (master invokes)"

    description: "Validate K-means cluster assignments (consecutive IDs, minimum size)"

  validate_bootstrap_stability:
    module: "tools.validation"
    function: "validate_bootstrap_stability"
    signature: "validate_bootstrap_stability(jaccard_values: Union[np.ndarray, List[float]], min_jaccard_threshold: float) -> Dict[str, Any]"

    criteria:
      - "All Jaccard values in [0, 1]"
      - "Mean Jaccard >= min_jaccard_threshold (typically 0.75)"
      - "95% CI computed via percentile method"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        message: "str"
        mean_jaccard: "float"
        ci_lower: "float"
        ci_upper: "float"
        above_threshold: "bool"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step04_validate_clustering_quality.log"
      invoke: "g_debug (master invokes)"

    description: "Validate clustering stability via Jaccard bootstrap (threshold typically 0.75)"

  validate_cluster_summary_stats:
    module: "tools.validation"
    function: "validate_cluster_summary_stats"
    signature: "validate_cluster_summary_stats(summary_df: DataFrame, min_col: str, mean_col: str, max_col: str, sd_col: str, n_col: str) -> Dict[str, Any]"

    criteria:
      - "min <= mean <= max for all features"
      - "SD >= 0 for all features"
      - "N > 0 for all clusters"
      - "No NaN in summary statistics"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        message: "str"
        failed_checks: "List[str]"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step05_characterize_clusters.log"
      invoke: "g_debug (master invokes)"

    description: "Validate cluster summary statistics consistency (min<=mean<=max, SD>=0, N>0)"

  validate_hypothesis_test_dual_pvalues:
    module: "tools.validation"
    function: "validate_hypothesis_test_dual_pvalues"
    signature: "validate_hypothesis_test_dual_pvalues(interaction_df: DataFrame, required_terms: List[str], alpha_bonferroni: float) -> Dict[str, Any]"

    criteria:
      - "All required statistical terms present (e.g., chi-square test statistic)"
      - "BOTH p_uncorrected AND correction method columns present (Decision D068)"
      - "Accepts p_bonferroni, p_holm, or p_fdr"

    expected_output:
      format: "Dict[str, Any]"
      fields:
        valid: "bool"
        d068_compliant: "bool"
        missing_terms: "List[str]"
        missing_cols: "List[str]"
        message: "str"

    behavior_on_failure:
      action: "raise ValueError"
      log_to: "logs/step06_crosstab_ch5.log"
      invoke: "g_debug (master invokes)"

    description: "Validate hypothesis test includes Decision D068 dual p-value reporting"

summary:
  analysis_tools_count: 11
  validation_tools_count: 7
  total_unique_tools: 18
  mandatory_decisions_embedded: ["D068"]
  notes:
    - "Clustering pipeline uses stdlib (pandas, sklearn, scipy) - no custom analysis tools required"
    - "All validation tools from tools.validation module (existing inventory)"
    - "Custom Jaccard bootstrap function needs TDD implementation if not in inventory"
    - "Decision D068 enforced via validate_hypothesis_test_dual_pvalues for chi-square test"

---
# End of 3_tools.yaml
