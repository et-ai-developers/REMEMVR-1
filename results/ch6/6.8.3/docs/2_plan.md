# Analysis Plan: RQ 6.8.3 - Source-Destination Confidence ICC

**Research Question:** 6.8.3
**Created:** 2025-12-06
**Status:** Planning complete, ready for tool specification (rq_tools)

---

## Overview

This RQ examines whether the opposite intercept-slope correlation pattern discovered in Ch5 5.5.6 for accuracy (Source r=+0.99, Destination r=-0.90) replicates in confidence data. The analysis uses DERIVED data from RQ 6.8.1 (location-stratified confidence theta scores) to fit separate LMMs for source (-U-) and destination (-D-) confidence, extract variance components, and compute intercept-slope correlations per location type.

**Pipeline:** LMM (Linear Mixed Models) with random slopes for variance decomposition + correlation analysis

**Steps:** 6 total analysis steps

**Estimated Runtime:** Medium (~30-45 minutes total - primarily LMM fitting with random slopes)

**Key Decisions Applied:**
- Decision D070: TSVR as time variable (actual hours since encoding, not nominal days)
- Decision D068: Dual p-value reporting for intercept-slope correlation tests
- Cross-RQ Dependency: Requires RQ 6.8.1 outputs (theta confidence scores) + Ch5 5.5.6 accuracy results for comparison

---

## Analysis Plan

This RQ requires 6 steps:

### Step 0: Extract Confidence Theta Data from RQ 6.8.1

**Dependencies:** None (first step, but requires RQ 6.8.1 completion)

**Complexity:** Low (data extraction from existing RQ outputs)

**Purpose:** Load location-stratified confidence theta scores from RQ 6.8.1, merge with TSVR time variable, prepare long-format data for LMM fitting.

**Input:**

**File 1:** results/ch6/6.8.1/data/step03_theta_confidence_location.csv
**Source:** Generated by RQ 6.8.1 Step 3 (IRT Pass 2 calibration with 2-factor GRM for source and destination confidence)
**Format:** CSV with columns:
  - `composite_ID` (string, format: {UID}_{test}, e.g., P001_T1)
  - `theta_source` (float, confidence ability for -U- pick-up locations)
  - `se_source` (float, standard error for source confidence)
  - `theta_destination` (float, confidence ability for -D- put-down locations)
  - `se_destination` (float, standard error for destination confidence)
**Expected Rows:** 400 (100 participants x 4 test sessions)
**Expected Columns:** 5 (composite_ID + 4 theta/SE columns)

**File 2:** data/master.xlsx (Sheet: TSVR_lookup)
**Source:** Project-level timing data
**Format:** Excel sheet with columns:
  - `composite_ID` (string, matches theta file format)
  - `TSVR_hours` (float, actual time since VR session in hours)
**Expected Rows:** 400+ (all participant-test combinations)

**Processing:**

**Step 0a: Reshape to Long Format**
1. Read theta_confidence_location.csv
2. Parse composite_ID -> extract UID and test
3. Reshape wide -> long:
   - Original: 1 row = 1 participant-test with theta_source + theta_destination columns
   - Result: 2 rows = 1 participant-test-location combination each
4. Create location_type column: "Source" or "Destination"
5. Create theta column: theta_source values for Source rows, theta_destination for Destination rows
6. Create se column: se_source for Source rows, se_destination for Destination rows

**Step 0b: Merge TSVR Time Variable**
1. Read TSVR_lookup from master.xlsx
2. Left join reshaped theta data with TSVR on composite_ID
3. Validate: All composite_IDs matched (no missing TSVR values)

**Output:**

**File 1:** data/step00_lmm_input_confidence_location.csv
**Format:** CSV, long format (one row per participant-test-location combination)
**Columns:**
  - `UID` (string, participant identifier, e.g., P001)
  - `test` (string, test session: T1, T2, T3, T4)
  - `location_type` (string, categorical: "Source" or "Destination")
  - `theta` (float, confidence ability estimate)
  - `se` (float, standard error of theta)
  - `TSVR_hours` (float, actual time since encoding per Decision D070)
**Expected Rows:** 800 (100 participants x 4 tests x 2 location types)
**Expected Columns:** 6

**Validation Requirement:**

Validation tools MUST be used after data extraction tool execution. Specific validation tools determined by rq_tools based on data format requirements.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step00_lmm_input_confidence_location.csv exists (exact path)
- Expected rows: 800 (100 participants x 4 tests x 2 location types)
- Expected columns: 6 (UID, test, location_type, theta, se, TSVR_hours)
- Data types: UID (object), test (object), location_type (object), theta (float64), se (float64), TSVR_hours (float64)

*Value Ranges:*
- theta in [-3, 3] (typical IRT ability range)
- se in [0.1, 1.0] (above 1.0 = unreliable estimates)
- TSVR_hours in [0, 200] hours (encoding to ~8 days post)
- location_type in {Source, Destination} (exactly 2 categories)
- test in {T1, T2, T3, T4} (exactly 4 test sessions)

*Data Quality:*
- No NaN values allowed (all cells must have valid values)
- Expected N: Exactly 800 rows (no missing data tolerance)
- Each UID has 8 rows (4 tests x 2 location types)
- Balanced design: 400 Source rows + 400 Destination rows
- No duplicate (UID x test x location_type) combinations

*Log Validation:*
- Required pattern: "Data extraction complete: 800 rows created"
- Required pattern: "All 100 participants have 8 observations (4 tests x 2 locations)"
- Required pattern: "TSVR merge successful: 0 missing values"
- Forbidden patterns: "ERROR", "Missing TSVR", "NaN values detected"
- Acceptable warnings: None expected for extraction

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Expected 800 rows, found 785")
- Log failure to logs/step00_extract_data.log
- Quit script immediately (do NOT proceed to Step 1)
- g_debug invoked to diagnose root cause

---

### Step 1: Fit Source Confidence LMM with Random Slopes

**Dependencies:** Step 0 (requires long-format LMM input)

**Complexity:** Medium (15-20 minutes for random slope model)

**Purpose:** Fit LMM for Source confidence trajectories with random intercepts and random slopes to estimate variance components.

**Input:**

**File 1:** data/step00_lmm_input_confidence_location.csv
**Source:** Generated by Step 0
**Format:** CSV, long format
**Columns:** UID, test, location_type, theta, se, TSVR_hours
**Filter:** location_type == "Source" (400 rows: 100 participants x 4 tests)

**Processing:**

**Model Specification:**
```
theta ~ TSVR_hours + (TSVR_hours | UID)
```

**Interpretation:**
- Fixed effect: TSVR_hours (average confidence trajectory across all participants)
- Random intercept per UID (individual baseline confidence differences)
- Random slope per UID (individual forgetting rate differences)
- Outcome: theta (confidence ability on IRT scale)
- Time variable: TSVR_hours (Decision D070 - actual elapsed time)

**Fitting Method:**
- Use statsmodels MixedLM with REML estimation
- Allow random intercepts and random slopes to correlate (full variance-covariance matrix)
- Convergence criteria: Default statsmodels tolerances

**Output:**

**File 1:** data/step01_source_lmm_model_summary.txt
**Format:** Text file with model summary
**Contents:**
- Fixed effects table (coefficient, SE, z, p-value for TSVR_hours)
- Random effects variance-covariance matrix (var_intercept, var_slope, cov_int_slope, var_residual)
- Model fit indices (log-likelihood, AIC, BIC)
- Convergence status

**File 2:** data/step01_source_variance_components.csv
**Format:** CSV with variance decomposition
**Columns:**
  - `component` (string: var_intercept, var_slope, cov_int_slope, var_residual, corr_int_slope)
  - `estimate` (float: variance/covariance/correlation value)
**Expected Rows:** 5 (4 variance parameters + 1 correlation)

**Validation Requirement:**

Validation tools MUST be used after LMM fitting tool execution. Specific validation tools determined by rq_tools based on LMM convergence and variance component requirements.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step01_source_lmm_model_summary.txt exists
- data/step01_source_variance_components.csv exists
- Expected rows in variance CSV: 5
- Expected columns in variance CSV: 2 (component, estimate)
- Data types: component (object), estimate (float64)

*Value Ranges:*
- var_intercept > 0 (variance must be positive)
- var_slope > 0 (variance must be positive)
- var_residual > 0 (variance must be positive)
- cov_int_slope: unrestricted (can be positive, negative, or zero)
- corr_int_slope in [-1, 1] (correlation bounds)

*Data Quality:*
- No NaN values in variance components (model must converge)
- All variance components strictly positive (no boundary issues)
- Correlation computed correctly: corr = cov / sqrt(var_intercept * var_slope)

*Log Validation:*
- Required pattern: "Source LMM converged: True"
- Required pattern: "VALIDATION - PASS: variance components extracted"
- Required pattern: "Random effects covariance matrix is positive definite"
- Forbidden patterns: "ERROR", "CONVERGENCE FAILED", "Singular matrix"
- Acceptable warnings: "REML iterations" (informational, not concerning if convergence achieved)

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Source LMM did not converge")
- Log failure to logs/step01_fit_source_lmm.log
- Quit script immediately (do NOT proceed to Step 2)
- g_debug invoked to diagnose convergence issues

---

### Step 2: Fit Destination Confidence LMM with Random Slopes

**Dependencies:** Step 0 (requires long-format LMM input)

**Complexity:** Medium (15-20 minutes for random slope model)

**Purpose:** Fit LMM for Destination confidence trajectories with random intercepts and random slopes to estimate variance components.

**Input:**

**File 1:** data/step00_lmm_input_confidence_location.csv
**Source:** Generated by Step 0
**Format:** CSV, long format
**Columns:** UID, test, location_type, theta, se, TSVR_hours
**Filter:** location_type == "Destination" (400 rows: 100 participants x 4 tests)

**Processing:**

**Model Specification:**
```
theta ~ TSVR_hours + (TSVR_hours | UID)
```

**Interpretation:** Same as Step 1 but for Destination confidence trajectories

**Fitting Method:** Same as Step 1 (REML with full variance-covariance matrix)

**Output:**

**File 1:** data/step02_destination_lmm_model_summary.txt
**Format:** Text file with model summary
**Contents:** Same structure as Step 1 output

**File 2:** data/step02_destination_variance_components.csv
**Format:** CSV with variance decomposition
**Columns:** component, estimate
**Expected Rows:** 5 (4 variance parameters + 1 correlation)

**Validation Requirement:**

Validation tools MUST be used after LMM fitting tool execution. Specific validation tools determined by rq_tools based on LMM convergence and variance component requirements.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step02_destination_lmm_model_summary.txt exists
- data/step02_destination_variance_components.csv exists
- Expected rows in variance CSV: 5
- Expected columns in variance CSV: 2 (component, estimate)
- Data types: component (object), estimate (float64)

*Value Ranges:*
- var_intercept > 0
- var_slope > 0
- var_residual > 0
- cov_int_slope: unrestricted
- corr_int_slope in [-1, 1]

*Data Quality:*
- No NaN values in variance components
- All variance components strictly positive
- Correlation computed correctly

*Log Validation:*
- Required pattern: "Destination LMM converged: True"
- Required pattern: "VALIDATION - PASS: variance components extracted"
- Required pattern: "Random effects covariance matrix is positive definite"
- Forbidden patterns: "ERROR", "CONVERGENCE FAILED", "Singular matrix"
- Acceptable warnings: "REML iterations"

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message
- Log failure to logs/step02_fit_destination_lmm.log
- Quit script immediately (do NOT proceed to Step 3)
- g_debug invoked to diagnose convergence issues

---

### Step 3: Extract Random Effects for Both Location Types

**Dependencies:** Steps 1, 2 (requires fitted LMMs for both location types)

**Complexity:** Low (extraction from fitted models)

**Purpose:** Extract participant-level random intercepts and random slopes from both Source and Destination LMMs for downstream clustering analysis (RQ 6.8.4 dependency).

**Input:**

**File 1:** data/step01_source_lmm_model_summary.txt (Source LMM fitted model object)
**File 2:** data/step02_destination_lmm_model_summary.txt (Destination LMM fitted model object)

**Processing:**

**Step 3a: Extract Source Random Effects**
1. Load Source LMM fitted model
2. Extract random intercepts per UID
3. Extract random slopes per UID
4. Create DataFrame with columns: UID, location_type="Source", random_intercept, random_slope

**Step 3b: Extract Destination Random Effects**
1. Load Destination LMM fitted model
2. Extract random intercepts per UID
3. Extract random slopes per UID
4. Create DataFrame with columns: UID, location_type="Destination", random_intercept, random_slope

**Step 3c: Combine Both Location Types**
1. Stack Source and Destination DataFrames (vertical concatenation)
2. Save combined file for RQ 6.8.4 (clustering analysis requires both location types)

**Output:**

**File 1:** data/step03_random_effects.csv
**Format:** CSV, long format (one row per UID-location combination)
**Columns:**
  - `UID` (string, participant identifier)
  - `location_type` (string, categorical: "Source" or "Destination")
  - `random_intercept` (float, individual deviation from population mean baseline confidence)
  - `random_slope` (float, individual deviation from population mean forgetting rate)
**Expected Rows:** 200 (100 participants x 2 location types)
**Expected Columns:** 4

**CRITICAL DEPENDENCY NOTE:**
This file is REQUIRED by RQ 6.8.4 (Source-Destination Confidence Clustering). RQ 6.8.4 uses random intercepts and slopes to identify participant subgroups with distinct forgetting patterns. The file must contain exactly 200 rows (100 participants x 2 location types) for clustering analysis.

**Validation Requirement:**

Validation tools MUST be used after random effects extraction tool execution. Specific validation tools determined by rq_tools based on data format requirements.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step03_random_effects.csv exists (exact path)
- Expected rows: 200 (100 participants x 2 location types)
- Expected columns: 4 (UID, location_type, random_intercept, random_slope)
- Data types: UID (object), location_type (object), random_intercept (float64), random_slope (float64)

*Value Ranges:*
- random_intercept in [-3, 3] (typical range for individual deviations on theta scale)
- random_slope in [-1, 1] (typical range for individual forgetting rate deviations)
- location_type in {Source, Destination} (exactly 2 categories)

*Data Quality:*
- No NaN values allowed (all participants must have random effects)
- Expected N: Exactly 200 rows (100 participants x 2 location types)
- Each UID appears exactly 2 times (once for Source, once for Destination)
- Balanced design: 100 Source rows + 100 Destination rows

*Log Validation:*
- Required pattern: "Random effects extraction complete: 200 rows created"
- Required pattern: "All 100 participants have 2 location types"
- Required pattern: "VALIDATION - PASS: random effects range checks"
- Forbidden patterns: "ERROR", "NaN values detected", "Missing participants"
- Acceptable warnings: None expected for extraction

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message
- Log failure to logs/step03_extract_random_effects.log
- Quit script immediately (do NOT proceed to Step 4)
- g_debug invoked to diagnose extraction issues

---

### Step 4: Compute Intercept-Slope Correlations Per Location Type

**Dependencies:** Steps 1, 2 (requires variance components from both LMMs)

**Complexity:** Low (correlation computation from variance components)

**Purpose:** Compute intercept-slope correlations for Source and Destination confidence separately, with confidence intervals and dual p-values per Decision D068.

**Input:**

**File 1:** data/step01_source_variance_components.csv
**File 2:** data/step02_destination_variance_components.csv

**Processing:**

**Step 4a: Extract Correlation Estimates**
1. Read source_variance_components.csv -> extract corr_int_slope for Source
2. Read destination_variance_components.csv -> extract corr_int_slope for Destination

**Step 4b: Compute Confidence Intervals**
1. For each location type, use Fisher's z-transformation to compute 95% CI:
   - z = 0.5 * ln((1+r) / (1-r))
   - SE_z = 1 / sqrt(N - 3) where N = 100 participants
   - CI_z = z +/- 1.96 * SE_z
   - Back-transform to r scale: r = (exp(2*z) - 1) / (exp(2*z) + 1)

**Step 4c: Test Correlation Significance (Decision D068 Dual P-Values)**
1. For each location type, test H0: rho = 0
2. Compute test statistic: t = r * sqrt(N-2) / sqrt(1-r^2)
3. Compute uncorrected p-value (two-tailed t-test with df=N-2)
4. Compute Bonferroni-corrected p-value: p_bonf = p_uncorrected * 2 (2 comparisons: Source and Destination)
5. Report BOTH p-values per Decision D068

**Output:**

**File 1:** data/step04_intercept_slope_correlations.csv
**Format:** CSV with correlation results per location type
**Columns:**
  - `location_type` (string: "Source" or "Destination")
  - `correlation` (float: intercept-slope correlation r)
  - `CI_lower` (float: lower bound of 95% CI)
  - `CI_upper` (float: upper bound of 95% CI)
  - `p_uncorrected` (float: uncorrected p-value for H0: rho=0)
  - `p_bonferroni` (float: Bonferroni-corrected p-value)
  - `N` (int: sample size = 100 participants)
**Expected Rows:** 2 (Source and Destination)
**Expected Columns:** 7

**Validation Requirement:**

Validation tools MUST be used after correlation computation tool execution. Specific validation tools determined by rq_tools based on Decision D068 dual p-value requirements.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step04_intercept_slope_correlations.csv exists (exact path)
- Expected rows: 2 (Source and Destination)
- Expected columns: 7 (location_type, correlation, CI_lower, CI_upper, p_uncorrected, p_bonferroni, N)
- Data types: location_type (object), correlation (float64), CI bounds (float64), p-values (float64), N (int64)

*Value Ranges:*
- correlation in [-1, 1] (correlation bounds)
- CI_lower in [-1, 1], CI_upper in [-1, 1]
- CI_lower <= correlation <= CI_upper (CI must contain point estimate)
- p_uncorrected in [0, 1] (p-value bounds)
- p_bonferroni in [0, 1] (can exceed 1.0 mathematically but capped at 1.0)
- N == 100 (all participants included)

*Data Quality:*
- No NaN values allowed
- BOTH location types present (Source and Destination)
- Decision D068 compliance: BOTH p_uncorrected AND p_bonferroni columns present
- p_bonferroni >= p_uncorrected (Bonferroni correction should increase or maintain p-value)

*Log Validation:*
- Required pattern: "Intercept-slope correlations computed for 2 location types"
- Required pattern: "VALIDATION - PASS: Decision D068 dual p-values present"
- Required pattern: "Source correlation: r = [value], p_uncorrected = [value], p_bonferroni = [value]"
- Required pattern: "Destination correlation: r = [value], p_uncorrected = [value], p_bonferroni = [value]"
- Forbidden patterns: "ERROR", "NaN correlation", "Missing p-value column"
- Acceptable warnings: "Bonferroni p-value capped at 1.0" (if uncorrected * 2 > 1.0)

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message
- Log failure to logs/step04_compute_correlations.log
- Quit script immediately (do NOT proceed to Step 5)
- g_debug invoked to diagnose computation issues

---

### Step 5: Compare Confidence Correlations to Ch5 5.5.6 Accuracy Correlations

**Dependencies:** Step 4 (requires confidence correlations), Ch5 5.5.6 results (requires accuracy correlations)

**Complexity:** Low (data comparison and documentation)

**Purpose:** Document whether the opposite correlation pattern from Ch5 5.5.6 accuracy (Source r=+0.99, Destination r=-0.90) replicates in confidence data.

**Input:**

**File 1:** data/step04_intercept_slope_correlations.csv
**Source:** Generated by Step 4 (confidence correlations)

**File 2:** results/ch5/5.5.6/data/intercept_slope_correlations.csv (EXPECTED PATH - verify during execution)
**Source:** Ch5 5.5.6 final results (accuracy correlations)
**Format:** CSV with columns: location_type, correlation, CI_lower, CI_upper, p_uncorrected, p_bonferroni, N
**Expected Rows:** 2 (Source and Destination accuracy correlations)

**Processing:**

**Step 5a: Load Both Datasets**
1. Read confidence correlations (this RQ Step 4 output)
2. Read accuracy correlations (Ch5 5.5.6 output)
3. Validate both files have same structure (location_type column with Source and Destination)

**Step 5b: Merge for Side-by-Side Comparison**
1. Merge on location_type
2. Rename columns for clarity:
   - correlation_confidence, correlation_accuracy
   - CI_lower_confidence, CI_lower_accuracy
   - CI_upper_confidence, CI_upper_accuracy
   - p_uncorrected_confidence, p_uncorrected_accuracy
   - p_bonferroni_confidence, p_bonferroni_accuracy

**Step 5c: Compute Comparison Metrics**
1. Direction consistency: Do Source and Destination have opposite signs in BOTH datasets?
   - Accuracy: Source r > 0, Destination r < 0 (opposite pattern from Ch5 5.5.6)
   - Confidence: Check if same opposite pattern holds
2. Magnitude comparison: Absolute difference in correlation magnitudes
   - |r_confidence - r_accuracy| per location type
3. CI overlap: Do confidence CIs overlap with accuracy CIs?
   - Source: Does [CI_lower_conf, CI_upper_conf] overlap with [CI_lower_acc, CI_upper_acc]?
   - Destination: Same check

**Output:**

**File 1:** data/step05_ch5_comparison.csv
**Format:** CSV with side-by-side comparison
**Columns:**
  - `location_type` (string: Source or Destination)
  - `correlation_confidence` (float: this RQ confidence correlation)
  - `CI_lower_confidence` (float: 95% CI lower bound for confidence)
  - `CI_upper_confidence` (float: 95% CI upper bound for confidence)
  - `p_uncorrected_confidence` (float: uncorrected p-value for confidence)
  - `p_bonferroni_confidence` (float: Bonferroni p-value for confidence)
  - `correlation_accuracy` (float: Ch5 5.5.6 accuracy correlation)
  - `CI_lower_accuracy` (float: 95% CI lower bound for accuracy)
  - `CI_upper_accuracy` (float: 95% CI upper bound for accuracy)
  - `p_uncorrected_accuracy` (float: uncorrected p-value for accuracy)
  - `p_bonferroni_accuracy` (float: Bonferroni p-value for accuracy)
  - `direction_match` (bool: True if both have same sign, False if opposite)
  - `magnitude_difference` (float: |r_conf - r_acc|)
  - `CI_overlap` (bool: True if CIs overlap, False if non-overlapping)
**Expected Rows:** 2 (Source and Destination)
**Expected Columns:** 14

**File 2:** data/step05_pattern_replication_summary.txt
**Format:** Text summary with interpretation
**Contents:**
- Statement: "Ch5 5.5.6 Accuracy Pattern: Source r=+0.99 (high baseline -> slower decay), Destination r=-0.90 (high baseline -> faster decay)"
- Statement: "RQ 6.8.3 Confidence Pattern: Source r=[value] ([interpretation]), Destination r=[value] ([interpretation])"
- Test 1: Direction consistency - Do both datasets show opposite signs?
- Test 2: Magnitude similarity - How much do correlations differ?
- Test 3: Statistical significance - Are confidence correlations significant at p<0.05 (Bonferroni)?
- Overall assessment: Does confidence replicate accuracy pattern?

**Validation Requirement:**

Validation tools MUST be used after comparison tool execution. Specific validation tools determined by rq_tools based on cross-RQ data format requirements.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step05_ch5_comparison.csv exists (exact path)
- data/step05_pattern_replication_summary.txt exists
- Expected rows in CSV: 2 (Source and Destination)
- Expected columns in CSV: 14
- Data types: location_type (object), correlations (float64), CIs (float64), p-values (float64), direction_match (bool), magnitude_difference (float64), CI_overlap (bool)

*Value Ranges:*
- All correlation values in [-1, 1]
- All CI bounds in [-1, 1]
- All p-values in [0, 1]
- magnitude_difference >= 0 (absolute difference cannot be negative)

*Data Quality:*
- No NaN values allowed (both datasets must have valid correlations)
- BOTH location types present in comparison
- direction_match computed correctly (Source and Destination have opposite signs in BOTH datasets = True)
- CI_overlap computed correctly (confidence CI overlaps accuracy CI)

*Log Validation:*
- Required pattern: "Ch5 5.5.6 comparison complete: 2 location types compared"
- Required pattern: "VALIDATION - PASS: cross-RQ data merge successful"
- Required pattern: "Source accuracy r = +0.99, Source confidence r = [value]"
- Required pattern: "Destination accuracy r = -0.90, Destination confidence r = [value]"
- Required pattern: "Direction consistency: [True/False]"
- Forbidden patterns: "ERROR", "Ch5 5.5.6 file not found", "Merge failed"
- Acceptable warnings: None expected for comparison

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Ch5 5.5.6 file not found at expected path")
- Log failure to logs/step05_compare_to_ch5.log
- Quit script immediately (do NOT proceed to Step 6)
- g_debug invoked to diagnose cross-RQ dependency issues

---

### Step 6: Prepare ICC Summary Plot Data

**Dependencies:** Steps 1, 2, 4 (requires variance components and correlations from both location types)

**Complexity:** Low (data aggregation for visualization)

**Purpose:** Create plot source CSV showing variance decomposition (ICC components) and intercept-slope correlations per location type for downstream visualization by rq_plots.

**Input:**

**File 1:** data/step01_source_variance_components.csv (Source LMM variance components)
**File 2:** data/step02_destination_variance_components.csv (Destination LMM variance components)
**File 3:** data/step04_intercept_slope_correlations.csv (Correlations with CIs)

**Processing:**

**Step 6a: Compute ICC Components Per Location Type**
1. For Source:
   - ICC_intercept = var_intercept / (var_intercept + var_residual)
   - ICC_slope_simple = var_slope / (var_slope + var_residual)
   - ICC_total = (var_intercept + var_slope) / (var_intercept + var_slope + var_residual)
2. For Destination: Same computation

**Step 6b: Aggregate Plot Data**
1. Combine variance components, ICC estimates, and correlations
2. Create long-format CSV with one row per location type
3. Include all ICC components for bar plot + correlation for annotation

**Output:**

**File 1:** data/step06_icc_summary_plot_data.csv
**Format:** CSV, plot source data for ICC summary visualization
**Columns:**
  - `location_type` (string: Source or Destination)
  - `var_intercept` (float: intercept variance)
  - `var_slope` (float: slope variance)
  - `var_residual` (float: residual variance)
  - `ICC_intercept` (float: intercept ICC)
  - `ICC_slope_simple` (float: slope ICC simple)
  - `ICC_total` (float: total ICC)
  - `correlation` (float: intercept-slope correlation)
  - `CI_lower` (float: correlation CI lower bound)
  - `CI_upper` (float: correlation CI upper bound)
**Expected Rows:** 2 (Source and Destination)
**Expected Columns:** 10

**Plot Description:**
Two-panel figure showing (1) stacked bar chart of variance components (intercept, slope, residual) per location type, and (2) scatter plot or bar plot of intercept-slope correlations with error bars (CIs). Highlights opposite correlation pattern: Source positive vs Destination negative.

**Plotting Function (rq_plots will call):**
Custom ICC summary visualization (bar chart + scatter/bar with error bars)
- rq_plots reads data/step06_icc_summary_plot_data.csv
- Generates two-panel figure with variance decomposition + correlations
- PNG output saved to plots/ folder by rq_plots

**Validation Requirement:**

Validation tools MUST be used after plot data preparation tool execution. Specific validation tools determined by rq_tools based on plot data format requirements.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step06_icc_summary_plot_data.csv exists (exact path)
- Expected rows: 2 (Source and Destination)
- Expected columns: 10
- Data types: location_type (object), all numeric columns (float64)

*Value Ranges:*
- All variance components > 0 (variances must be positive)
- All ICC values in [0, 1] (proportion of variance)
- correlation in [-1, 1]
- CI_lower in [-1, 1], CI_upper in [-1, 1]
- CI_lower <= correlation <= CI_upper

*Data Quality:*
- No NaN values allowed
- BOTH location types present (Source and Destination)
- ICC components sum logically: ICC_total should approximately equal (var_intercept + var_slope) / (var_intercept + var_slope + var_residual)
- CIs computed correctly (contain point estimate)

*Log Validation:*
- Required pattern: "ICC summary plot data prepared: 2 location types"
- Required pattern: "VALIDATION - PASS: ICC bounds [0,1]"
- Required pattern: "Variance components positive for all location types"
- Forbidden patterns: "ERROR", "NaN ICC", "Negative variance"
- Acceptable warnings: None expected for plot data preparation

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message
- Log failure to logs/step06_prepare_icc_plot_data.log
- Quit script immediately
- g_debug invoked to diagnose plot data issues

---

## Expected Outputs

### Data Files (ALL analysis inputs and outputs - intermediate and final)

**From Step 0 (Data Extraction):**
- data/step00_lmm_input_confidence_location.csv (800 rows: 100 participants x 4 tests x 2 location types, long format with TSVR)

**From Step 1 (Source LMM):**
- data/step01_source_lmm_model_summary.txt (Source LMM fitted model summary)
- data/step01_source_variance_components.csv (5 rows: Source variance decomposition)

**From Step 2 (Destination LMM):**
- data/step02_destination_lmm_model_summary.txt (Destination LMM fitted model summary)
- data/step02_destination_variance_components.csv (5 rows: Destination variance decomposition)

**From Step 3 (Random Effects Extraction):**
- data/step03_random_effects.csv (200 rows: 100 participants x 2 location types, REQUIRED for RQ 6.8.4)

**From Step 4 (Correlation Computation):**
- data/step04_intercept_slope_correlations.csv (2 rows: Source and Destination correlations with CIs and dual p-values)

**From Step 5 (Ch5 Comparison):**
- data/step05_ch5_comparison.csv (2 rows: side-by-side comparison of confidence vs accuracy correlations)
- data/step05_pattern_replication_summary.txt (Text interpretation of pattern replication)

**From Step 6 (ICC Plot Data):**
- data/step06_icc_summary_plot_data.csv (2 rows: variance components, ICC estimates, correlations for visualization)

### Logs (ONLY execution logs - .log files capturing stdout/stderr)

- logs/step00_extract_data.log
- logs/step01_fit_source_lmm.log
- logs/step02_fit_destination_lmm.log
- logs/step03_extract_random_effects.log
- logs/step04_compute_correlations.log
- logs/step05_compare_to_ch5.log
- logs/step06_prepare_icc_plot_data.log

### Plots (EMPTY until rq_plots runs)

- plots/icc_summary.png (created by rq_plots, NOT analysis steps)
  - Two-panel figure: variance decomposition + intercept-slope correlations
  - Highlights opposite pattern: Source r > 0 vs Destination r < 0

### Results (EMPTY until rq_results runs)

- results/summary.md (created by rq_results, NOT analysis steps)

---

## Expected Data Formats

### Step 0 Output: LMM Input (Long Format with TSVR)

**File:** data/step00_lmm_input_confidence_location.csv

**Format:** Long format (one row per participant-test-location combination)

**Key Transformation:** Wide -> Long reshape
- **Input:** 1 row per participant-test with theta_source and theta_destination columns (400 rows)
- **Output:** 2 rows per participant-test, one for each location type (800 rows)

**Columns:**
- `UID`: Participant identifier (P001, P002, ..., P100)
- `test`: Test session (T1, T2, T3, T4)
- `location_type`: Location type (Source or Destination)
- `theta`: Confidence ability estimate (from theta_source or theta_destination)
- `se`: Standard error (from se_source or se_destination)
- `TSVR_hours`: Time since VR session (actual hours per Decision D070)

**Expected Values:**
- 100 unique UIDs
- 4 test sessions per UID
- 2 location types per UID-test combination
- 800 total rows (100 x 4 x 2)
- Balanced design: 400 Source + 400 Destination rows

---

### Step 3 Output: Random Effects for Clustering

**File:** data/step03_random_effects.csv

**Format:** Long format (one row per participant-location combination)

**Critical Use:** REQUIRED INPUT for RQ 6.8.4 (clustering analysis)

**Columns:**
- `UID`: Participant identifier
- `location_type`: Source or Destination
- `random_intercept`: Individual baseline confidence deviation
- `random_slope`: Individual forgetting rate deviation

**Expected Values:**
- 100 unique UIDs
- 2 location types per UID
- 200 total rows (100 x 2)
- Balanced design: 100 Source + 100 Destination rows

**RQ 6.8.4 Dependency:**
- RQ 6.8.4 uses this file to perform K-means clustering on (random_intercept, random_slope) space
- Identifies participant subgroups with distinct forgetting patterns
- Separate clustering for Source and Destination location types

---

### Step 5 Output: Cross-RQ Comparison

**File:** data/step05_ch5_comparison.csv

**Format:** Wide format (one row per location type with confidence and accuracy columns)

**Key Comparison:** Confidence correlations (this RQ) vs Accuracy correlations (Ch5 5.5.6)

**Columns:**
- `location_type`: Source or Destination
- `correlation_confidence`: This RQ intercept-slope correlation
- `CI_lower_confidence`, `CI_upper_confidence`: 95% CIs for confidence
- `p_uncorrected_confidence`, `p_bonferroni_confidence`: Dual p-values (Decision D068)
- `correlation_accuracy`: Ch5 5.5.6 intercept-slope correlation
- `CI_lower_accuracy`, `CI_upper_accuracy`: 95% CIs for accuracy
- `p_uncorrected_accuracy`, `p_bonferroni_accuracy`: Dual p-values (Decision D068)
- `direction_match`: Do both datasets show opposite signs for Source vs Destination?
- `magnitude_difference`: |r_confidence - r_accuracy|
- `CI_overlap`: Do confidence and accuracy CIs overlap?

**Expected Pattern (Hypothesis):**
- **Source:** Both accuracy and confidence show POSITIVE correlations (r > 0)
  - High baseline -> slower decay (regression to mean pattern)
- **Destination:** Both accuracy and confidence show NEGATIVE correlations (r < 0)
  - High baseline -> faster decay (fan effect pattern)
- **Replication:** direction_match = TRUE (opposite pattern replicates across confidence)

---

## Cross-RQ Dependencies

### Dependency Type: DERIVED Data from Other RQs

**This RQ requires outputs from:**

**1. RQ 6.8.1** (Source-Destination Confidence Trajectories)
  - File: results/ch6/6.8.1/data/step03_theta_confidence_location.csv
  - Used in: Step 0 (extract confidence theta scores with location stratification)
  - Rationale: RQ 6.8.1 performs IRT calibration with 2-factor GRM (source and destination confidence). This RQ uses those theta scores to fit LMMs and compute variance components.

**2. Ch5 5.5.6** (Source-Destination Accuracy ICC - Opposite Correlation Pattern)
  - File: results/ch5/5.5.6/data/intercept_slope_correlations.csv (EXPECTED PATH - verify during execution)
  - Used in: Step 5 (compare confidence correlations to accuracy correlations)
  - Rationale: Ch5 5.5.6 discovered opposite intercept-slope correlations for accuracy (Source r=+0.99, Destination r=-0.90). This RQ tests whether the pattern replicates in confidence data.

**Execution Order Constraint:**
1. RQ 6.8.1 must complete Steps 1-3 (IRT calibration for confidence)
2. Ch5 5.5.6 must complete (provides accuracy correlation benchmarks)
3. This RQ executes (uses both outputs for ICC analysis and comparison)

**Data Source Boundaries (Per Specification 5.1.6):**
- **DERIVED data from RQ 6.8.1:** theta_confidence_location.csv (IRT-derived confidence scores)
- **DERIVED data from Ch5 5.5.6:** intercept_slope_correlations.csv (accuracy ICC benchmarks)
- **RAW data:** master.xlsx TSVR_lookup sheet (timing data)
- **Scope:** This RQ does NOT re-calibrate IRT models (uses RQ 6.8.1 theta scores as fixed)

**Validation:**
- Step 0: Check results/ch6/6.8.1/data/step03_theta_confidence_location.csv exists (circuit breaker: FILE_MISSING if absent)
- Step 5: Check results/ch5/5.5.6/data/intercept_slope_correlations.csv exists (circuit breaker: FILE_MISSING if absent)
- If either file missing -> quit with error -> user must execute dependency RQs first

**Reference:** Specification section 5.1.6 (Data Source Boundaries)

---

## Validation Requirements

**CRITICAL MANDATE:**

Every analysis step in this plan MUST use validation tools after analysis tool execution.

This is not optional. This is the core architectural principle preventing cascading failures observed in v3.0 (where analysis errors propagated undetected through 5+ downstream steps before discovery).

**Exact Specification Requirement:**

> "Validation tools MUST be used after analysis tool execution"

**Implementation:**
- rq_tools (Step 11 workflow) will read tool_inventory.md validation tools section
- rq_tools will specify BOTH analysis tool + validation tool per step in 3_tools.yaml
- rq_analysis (Step 12 workflow) will embed validation tool call AFTER analysis tool call in 4_analysis.yaml
- g_code (Step 14 workflow) will generate stepN_name.py scripts with validation function calls
- bash execution (Step 14 workflow) will run analysis -> validation -> error on validation failure

**Downstream Agent Requirements:**
- **rq_tools:** MUST specify validation tool for EVERY analysis step (no exceptions)
- **rq_analysis:** MUST embed validation tool call for EVERY analysis step (no exceptions)
- **g_code:** MUST generate code with validation function calls (no exceptions)
- **rq_inspect:** MUST verify validation ran successfully (checks logs/stepN_name.log for validation output)

### Validation Requirements By Step

#### Step 0: Extract Confidence Theta Data from RQ 6.8.1

**Analysis Tool:** (determined by rq_tools - likely tools.data extraction + pandas reshape/merge)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_dataframe_structure)

**What Validation Checks (SUBSTANCE - rq_inspect scope):**
- Output file exists (data/step00_lmm_input_confidence_location.csv)
- Expected row count (800 rows: 100 participants x 4 tests x 2 location types)
- Expected column count (6 columns: UID, test, location_type, theta, se, TSVR_hours)
- No NaN values in any column
- Balanced design (400 Source rows + 400 Destination rows)
- Each UID has exactly 8 rows (4 tests x 2 location types)
- TSVR merge successful (no missing values)

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Expected 800 rows, found 785")
- Log failure to logs/step00_extract_data.log
- Quit script immediately (do NOT proceed to Step 1)
- g_debug invoked by master to diagnose root cause

---

#### Step 1: Fit Source Confidence LMM with Random Slopes

**Analysis Tool:** (determined by rq_tools - likely tools.lmm.fit_lmm_trajectory_tsvr)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_lmm_convergence + validate_variance_positivity)

**What Validation Checks:**
- Output files exist (model summary TXT + variance components CSV)
- LMM converged successfully (no convergence warnings)
- All variance components strictly positive (var_intercept, var_slope, var_residual > 0)
- Correlation in valid range (corr_int_slope in [-1, 1])
- Random effects covariance matrix is positive definite

**Expected Behavior on Validation Failure:**
- Raise error with specific failure (e.g., "Source LMM did not converge")
- Log failure to logs/step01_fit_source_lmm.log
- Quit script immediately
- g_debug invoked to diagnose convergence issues

---

#### Step 2: Fit Destination Confidence LMM with Random Slopes

**Analysis Tool:** (determined by rq_tools - likely tools.lmm.fit_lmm_trajectory_tsvr)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_lmm_convergence + validate_variance_positivity)

**What Validation Checks:** Same as Step 1 but for Destination LMM

**Expected Behavior on Validation Failure:**
- Raise error with specific failure
- Log failure to logs/step02_fit_destination_lmm.log
- Quit script immediately
- g_debug invoked to diagnose convergence issues

---

#### Step 3: Extract Random Effects for Both Location Types

**Analysis Tool:** (determined by rq_tools - likely tools.lmm.extract_random_effects_from_lmm)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_dataframe_structure)

**What Validation Checks:**
- Output file exists (data/step03_random_effects.csv)
- Expected row count (200 rows: 100 participants x 2 location types)
- Expected column count (4 columns: UID, location_type, random_intercept, random_slope)
- No NaN values
- Each UID appears exactly 2 times (once for Source, once for Destination)
- Balanced design (100 Source + 100 Destination rows)

**Expected Behavior on Validation Failure:**
- Raise error with specific failure
- Log failure to logs/step03_extract_random_effects.log
- Quit script immediately
- g_debug invoked to diagnose extraction issues

---

#### Step 4: Compute Intercept-Slope Correlations Per Location Type

**Analysis Tool:** (determined by rq_tools - likely custom correlation computation with Fisher's z CI)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_correlation_test_d068)

**What Validation Checks:**
- Output file exists (data/step04_intercept_slope_correlations.csv)
- Expected row count (2 rows: Source and Destination)
- Expected column count (7 columns including location_type, correlation, CIs, dual p-values, N)
- Decision D068 compliance: BOTH p_uncorrected AND p_bonferroni columns present
- Correlation in [-1, 1] range
- CIs contain point estimate (CI_lower <= correlation <= CI_upper)
- p-values in [0, 1] range

**Expected Behavior on Validation Failure:**
- Raise error with specific failure
- Log failure to logs/step04_compute_correlations.log
- Quit script immediately
- g_debug invoked to diagnose computation issues

---

#### Step 5: Compare Confidence Correlations to Ch5 5.5.6 Accuracy Correlations

**Analysis Tool:** (determined by rq_tools - likely pandas merge + custom comparison metrics)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_dataframe_structure)

**What Validation Checks:**
- Output files exist (comparison CSV + summary TXT)
- Ch5 5.5.6 file found at expected path (cross-RQ dependency)
- Expected row count (2 rows: Source and Destination)
- Expected column count (14 columns with confidence and accuracy comparisons)
- No NaN values (both datasets must have valid correlations)
- direction_match computed correctly
- CI_overlap computed correctly

**Expected Behavior on Validation Failure:**
- Raise error with specific failure (e.g., "Ch5 5.5.6 file not found")
- Log failure to logs/step05_compare_to_ch5.log
- Quit script immediately
- g_debug invoked to diagnose cross-RQ dependency issues

---

#### Step 6: Prepare ICC Summary Plot Data

**Analysis Tool:** (determined by rq_tools - likely custom ICC computation + pandas aggregation)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_icc_bounds + validate_plot_data_completeness)

**What Validation Checks:**
- Output file exists (data/step06_icc_summary_plot_data.csv)
- Expected row count (2 rows: Source and Destination)
- Expected column count (10 columns: variance components, ICCs, correlations, CIs)
- All ICC values in [0, 1] range
- All variance components positive
- Correlation in [-1, 1] range
- CIs contain point estimate

**Expected Behavior on Validation Failure:**
- Raise error with specific failure
- Log failure to logs/step06_prepare_icc_plot_data.log
- Quit script immediately
- g_debug invoked to diagnose plot data issues

---

## Summary

**Total Steps:** 6 (Step 0: extraction + Steps 1-6: LMM fitting, variance decomposition, correlation computation, cross-RQ comparison, plot data preparation)

**Estimated Runtime:** Medium (~30-45 minutes total - primarily LMM fitting with random slopes for 2 location types)

**Cross-RQ Dependencies:**
- RQ 6.8.1 (confidence theta scores with location stratification)
- Ch5 5.5.6 (accuracy intercept-slope correlations for comparison)

**Primary Outputs:**
- Variance components per location type (var_intercept, var_slope, cov_int_slope, var_residual)
- Intercept-slope correlations per location type with CIs and dual p-values (Decision D068)
- Random effects file for RQ 6.8.4 clustering (200 rows: 100 participants x 2 location types)
- Cross-RQ comparison documenting pattern replication (confidence vs accuracy)
- ICC summary plot data for visualization

**Validation Coverage:** 100% (all 6 steps have validation requirements with 4-layer substance criteria)

**Critical Test:** Does the opposite correlation pattern from Ch5 5.5.6 accuracy (Source r=+0.99 positive, Destination r=-0.90 negative) replicate in confidence data? This tests whether source and destination memory systems have fundamentally different forgetting dynamics at both memory and metacognitive levels.

---

**Next Steps (Workflow):**
1. User reviews and approves this plan (Step 7 user gate)
2. Workflow continues to Step 11: rq_tools reads this plan -> creates 3_tools.yaml
3. Workflow continues to Step 12: rq_analysis reads this plan + 3_tools.yaml -> creates 4_analysis.yaml
4. Workflow continues to Step 14: g_code reads 4_analysis.yaml -> generates stepN_name.py scripts

---

**Version History:**
- v1.0 (2025-12-06): Initial plan created by rq_planner agent for RQ 6.8.3
