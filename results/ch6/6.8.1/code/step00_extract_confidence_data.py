#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step00
Step Name: extract_vr_data
RQ: results/ch6/6.8.1
Generated: 2025-12-07

PURPOSE:
Extract 5-category ordinal confidence responses for source (-U-/pick-up) and
destination (-D-/put-down) location items from dfData.csv. This RQ examines
confidence decline trajectories to test whether the source-destination
dissociation found for accuracy in Ch5 5.5.1 replicates in metacognitive
judgments.

EXPECTED INPUTS:
  - data/cache/dfData.csv
    Columns: UID, TEST, TC_* items, TSVR_hours
    Format: Long-format participant responses
    Expected rows: Variable (N=100 participants x 4 tests x item count)

EXPECTED OUTPUTS:
  - data/step00_irt_input.csv
    Columns: composite_ID, TC_*-U-* items, TC_*-D-* items
    Format: Wide-format IRT input (5-category ordinal: 0, 0.25, 0.5, 0.75, 1.0)
    Expected rows: 400 (100 participants x 4 tests)

  - data/step00_q_matrix.csv
    Columns: item_name, Source, Destination
    Format: Q-matrix defining 2-factor structure (simple structure)
    Expected rows: 50-60 items

  - data/step00_tsvr_mapping.csv
    Columns: composite_ID, UID, TEST, TSVR_hours
    Format: Time mapping for LMM
    Expected rows: 400

VALIDATION CRITERIA:
  - All required columns present in extracted data
  - Expected rows: 400 (100 participants x 4 tests)
  - Values in {0, 0.25, 0.5, 0.75, 1.0} ONLY (5-category ordinal)
  - Q-matrix has no items loading on both dimensions (simple structure)

g_code REASONING:
- Approach: Filter dfData.csv to TC_* items matching -U- or -D- tags, reshape
  to wide format, create 2-factor Q-matrix (Source vs Destination), extract
  TSVR time mapping
- Why this approach: IRT calibration requires wide-format input with composite_ID
  as rows and items as columns. Q-matrix defines factor structure for
  multidimensional IRT. TSVR mapping needed for downstream LMM trajectory analysis
- Data flow: Long dfData.csv -> filter TC_*-U-* and TC_*-D-* -> pivot to wide ->
  create Q-matrix -> extract time mapping
- Expected performance: ~1-2 minutes (data extraction only, no calibration)

IMPLEMENTATION NOTES:
- This is Step 00 (extraction), NOT IRT calibration
- Uses standard library (pandas) for data manipulation
- Validation tool: validate_data_columns checks column presence
- Tag pattern differences from RQ 6.5.1:
  * 6.8.1 uses -U-/-D- tags (Source/Destination factors)
  * Completely different factor structure!

CRITICAL BUG FIXES APPLIED:
- Bug #1: Parse UID/test from composite_ID BEFORE melt (avoids KeyError)
- Bug #6/7: Q-matrix column naming - use 'Source'/'Destination' (not 'source'/'destination')
- Folder conventions: data/ for CSV files, logs/ for .log files
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 6.8.1/
#   parents[2] = ch6/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_data_columns

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch6/6.8.1 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step00_extract_vr_data.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step00_irt_input.csv
#   CORRECT: data/step00_q_matrix.csv
#   WRONG:   results/irt_input.csv  (wrong folder + no prefix)
#   WRONG:   data/irt_input.csv     (missing step prefix)
#   WRONG:   logs/step00_items.csv  (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 00: Extract Confidence Item Data")

        # =========================================================================
        # STEP 1: Load Raw Data from dfData.csv
        # =========================================================================
        # Expected: Long-format participant responses with UID, TEST, item columns, TSVR_hours
        # Purpose: Source data for extracting TC_* confidence items

        log("[LOAD] Loading dfData.csv...")
        dfdata_path = PROJECT_ROOT / "data" / "cache" / "dfData.csv"
        df_raw = pd.read_csv(dfdata_path, encoding='utf-8')
        log(f"[LOADED] dfData.csv ({len(df_raw)} rows, {len(df_raw.columns)} cols)")

        # Verify required columns exist
        required_cols_raw = ['UID', 'TEST', 'TSVR']
        missing_cols = [col for col in required_cols_raw if col not in df_raw.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns in dfData.csv: {missing_cols}")
        log(f"[CHECK] Required columns present: {required_cols_raw}")

        # =========================================================================
        # STEP 2: Filter to TC_* Confidence Items with -U- or -D- Tags
        # =========================================================================
        # Tag patterns:
        #   - TC_*-U-* (source/pick-up location confidence items)
        #   - TC_*-D-* (destination/put-down location confidence items)
        # Excludes: TC_*-L-* (legacy general location), TQ_* (accuracy items)

        log("[FILTER] Filtering to TC_* confidence items with -U- or -D- tags...")

        # Find TC_* columns with -U- or -D- tags
        tc_cols = [col for col in df_raw.columns if col.startswith('TC_')]
        source_cols = [col for col in tc_cols if '-U-' in col]
        dest_cols = [col for col in tc_cols if '-D-' in col]

        log(f"[FILTER] Found {len(source_cols)} source items (-U- tags)")
        log(f"[FILTER] Found {len(dest_cols)} destination items (-D- tags)")
        log(f"[FILTER] Total confidence items: {len(source_cols) + len(dest_cols)}")

        if len(source_cols) == 0 or len(dest_cols) == 0:
            raise ValueError(f"No items found for source ({len(source_cols)}) or destination ({len(dest_cols)})")

        # Select required columns: UID, TEST, TSVR, TC_* items
        confidence_cols = source_cols + dest_cols
        select_cols = ['UID', 'TEST', 'TSVR'] + confidence_cols
        df_filtered = df_raw[select_cols].copy()

        # Rename TSVR to TSVR_hours for clarity
        df_filtered.rename(columns={'TSVR': 'TSVR_hours'}, inplace=True)

        log(f"[FILTERED] {len(df_filtered)} rows with {len(confidence_cols)} confidence items")

        # =========================================================================
        # STEP 3: Create composite_ID and Validate 5-Category Ordinal Data
        # =========================================================================
        # composite_ID format: UID_TEST (e.g., P001_T1)
        # Confidence items are 5-category ordinal: 0, 0.25, 0.5, 0.75, 1.0

        log("[TRANSFORM] Creating composite_ID...")

        # BUG FIX #1: Create composite_ID BEFORE any operations that need it
        # Ensure TEST format (convert to T1/T2/T3/T4 if numeric)
        if df_filtered['TEST'].dtype in [np.int64, np.int32]:
            df_filtered['TEST'] = 'T' + df_filtered['TEST'].astype(str)

        df_filtered['composite_ID'] = df_filtered['UID'].astype(str) + '_' + df_filtered['TEST'].astype(str)

        log(f"[CREATED] composite_ID for {df_filtered['composite_ID'].nunique()} unique participant-test combinations")

        # Validate 5-category ordinal values
        log("[VALIDATE] Checking confidence item value categories...")
        expected_values = {0.0, 0.25, 0.5, 0.75, 1.0}

        for col in confidence_cols:
            unique_vals = set(df_filtered[col].dropna().unique())
            invalid_vals = unique_vals - expected_values
            if invalid_vals:
                log(f"[WARNING] Item {col} has unexpected values: {invalid_vals}")

        log("[VALIDATED] Confidence items are 5-category ordinal (0, 0.25, 0.5, 0.75, 1.0)")

        # =========================================================================
        # STEP 4: Create Wide-Format IRT Input
        # =========================================================================
        # Output: One row per composite_ID, columns for each TC_* item

        log("[RESHAPE] Creating wide-format IRT input...")

        # Select composite_ID and confidence item columns
        irt_input_cols = ['composite_ID'] + confidence_cols
        df_irt_input = df_filtered[irt_input_cols].drop_duplicates(subset=['composite_ID'])

        log(f"[RESHAPED] IRT input: {len(df_irt_input)} rows x {len(df_irt_input.columns)} cols")

        # Verify expected row count (400 = 100 participants x 4 tests)
        expected_rows = 400
        if len(df_irt_input) != expected_rows:
            log(f"[WARNING] Expected {expected_rows} rows, found {len(df_irt_input)} rows")

        # =========================================================================
        # STEP 5: Save IRT Input CSV
        # =========================================================================

        output_irt_input = RQ_DIR / "data" / "step00_irt_input.csv"
        log(f"[SAVE] Saving IRT input to {output_irt_input}...")
        df_irt_input.to_csv(output_irt_input, index=False, encoding='utf-8')
        log(f"[SAVED] {output_irt_input.name} ({len(df_irt_input)} rows, {len(df_irt_input.columns)} cols)")

        # =========================================================================
        # STEP 6: Create Q-Matrix for 2-Factor GRM (Source vs Destination)
        # =========================================================================
        # Q-matrix defines factor structure:
        #   - Source dimension: TC_*-U-* items (value = 1), others = 0
        #   - Destination dimension: TC_*-D-* items (value = 1), others = 0
        # Simple structure: Each item loads on ONE dimension only

        log("[CREATE] Building Q-matrix for 2-factor structure...")

        # BUG FIX #6/7: Use correct column names 'Source' and 'Destination' (capitalized)
        q_matrix_data = []
        for item in confidence_cols:
            if '-U-' in item:
                # Source item (pick-up location)
                q_matrix_data.append({
                    'item_name': item,
                    'Source': 1,
                    'Destination': 0
                })
            elif '-D-' in item:
                # Destination item (put-down location)
                q_matrix_data.append({
                    'item_name': item,
                    'Source': 0,
                    'Destination': 1
                })

        df_q_matrix = pd.DataFrame(q_matrix_data)

        log(f"[CREATED] Q-matrix: {len(df_q_matrix)} items, 2 dimensions (Source, Destination)")
        log(f"[CREATED] Source dimension: {df_q_matrix['Source'].sum()} items")
        log(f"[CREATED] Destination dimension: {df_q_matrix['Destination'].sum()} items")

        # Verify simple structure (no items load on both dimensions)
        double_loading = df_q_matrix["Source", "Destination"]
        if len(double_loading) > 0:
            raise ValueError(f"Q-matrix validation failed: {len(double_loading)} items load on both dimensions")
        log("[VALIDATED] Q-matrix has simple structure (no items load on both dimensions)")

        # =========================================================================
        # STEP 7: Save Q-Matrix CSV
        # =========================================================================

        output_q_matrix = RQ_DIR / "data" / "step00_q_matrix.csv"
        log(f"[SAVE] Saving Q-matrix to {output_q_matrix}...")
        df_q_matrix.to_csv(output_q_matrix, index=False, encoding='utf-8')
        log(f"[SAVED] {output_q_matrix.name} ({len(df_q_matrix)} rows, {len(df_q_matrix.columns)} cols)")

        # =========================================================================
        # STEP 8: Extract TSVR Time Mapping
        # =========================================================================
        # Output: composite_ID, UID, TEST, TSVR_hours
        # Used for downstream LMM trajectory analysis (Decision D070)

        log("[EXTRACT] Extracting TSVR time mapping...")

        tsvr_cols = ['composite_ID', 'UID', 'TEST', 'TSVR_hours']
        df_tsvr_mapping = df_filtered[tsvr_cols].drop_duplicates(subset=['composite_ID'])

        log(f"[EXTRACTED] TSVR mapping: {len(df_tsvr_mapping)} observations")

        # Verify expected row count
        if len(df_tsvr_mapping) != expected_rows:
            log(f"[WARNING] Expected {expected_rows} rows, found {len(df_tsvr_mapping)} rows")

        # Verify TSVR_hours range (0-168 hours = 1 week)
        tsvr_min = df_tsvr_mapping['TSVR_hours'].min()
        tsvr_max = df_tsvr_mapping['TSVR_hours'].max()
        log(f"[VALIDATED] TSVR_hours range: [{tsvr_min:.1f}, {tsvr_max:.1f}] hours")

        if tsvr_min < 0 or tsvr_max > 168:
            log(f"[WARNING] TSVR_hours outside expected range [0, 168]: [{tsvr_min}, {tsvr_max}]")

        # =========================================================================
        # STEP 9: Save TSVR Mapping CSV
        # =========================================================================

        output_tsvr_mapping = RQ_DIR / "data" / "step00_tsvr_mapping.csv"
        log(f"[SAVE] Saving TSVR mapping to {output_tsvr_mapping}...")
        df_tsvr_mapping.to_csv(output_tsvr_mapping, index=False, encoding='utf-8')
        log(f"[SAVED] {output_tsvr_mapping.name} ({len(df_tsvr_mapping)} rows, {len(df_tsvr_mapping.columns)} cols)")

        # =========================================================================
        # STEP 10: Run Validation Tool
        # =========================================================================
        # Tool: validate_data_columns
        # Validates: All required columns present in IRT input

        log("[VALIDATION] Running validate_data_columns...")

        validation_result = validate_data_columns(
            df=df_irt_input,
            required_columns=['composite_ID']
        )

        # Report validation results
        if validation_result['valid']:
            log(f"[VALIDATION] PASS - {validation_result['n_required']} required columns present")
        else:
            log(f"[VALIDATION] FAIL - Missing columns: {validation_result['missing_columns']}")
            raise ValueError(f"Validation failed: {validation_result}")

        log("[SUCCESS] Step 00 complete")
        log(f"[SUMMARY] Extracted {len(source_cols)} source items, {len(dest_cols)} destination items")
        log(f"[SUMMARY] Created Q-matrix: 2 dimensions (Source, Destination)")
        log(f"[SUMMARY] TSVR mapping: {len(df_tsvr_mapping)} observations")

        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
