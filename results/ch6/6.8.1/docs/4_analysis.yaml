# ============================================================================
# ANALYSIS RECIPE - COMPLETE SPECIFICATION
# ============================================================================
# Generated: 2025-12-06T17:05:00
# RQ: ch6/6.8.1
# Agent: rq_analysis v4.1.0
# Purpose: Self-contained recipe for g_code agent (reads ONLY this file)
# ============================================================================

metadata:
  rq_id: "ch6/6.8.1"
  total_steps: 8
  analysis_type: "IRT->LMM source-destination confidence trajectory analysis"
  generated_by: "rq_analysis v4.1.0"
  timestamp: "2025-12-06T17:05:00"

# ============================================================================
# ANALYSIS STEPS - COMPLETE SPECIFICATIONS
# ============================================================================

steps:
  # --------------------------------------------------------------------------
  # STEP 0: Extract Confidence Item Data
  # --------------------------------------------------------------------------
  - name: "step00_extract_vr_data"
    step_number: "00"
    description: "Extract 5-category ordinal confidence responses for source (-U-) and destination (-D-) location items from dfData.csv"

    analysis_call:
      type: "stdlib"
      operations:
        - "pd.read_csv('data/cache/dfData.csv')"
        - "Filter to TC_* items matching -U- or -D- tags"
        - "Create composite_ID = UID + '_' + TEST"
        - "Reshape to wide format (composite_ID x item columns)"
        - "Create Q-matrix for 2-factor GRM (Source dimension, Destination dimension)"
        - "Extract TSVR mapping (composite_ID, UID, TEST, TSVR_hours)"
        - "Save outputs to data/"

      input_files:
        - path: "data/cache/dfData.csv"
          required_columns: ["UID", "TEST", "TC_*", "TSVR_hours"]
          description: "Project-level data source with confidence item responses"

      output_files:
        - path: "data/step00_irt_input.csv"
          columns: ["composite_ID", "TC_*-U-*", "TC_*-D-*"]
          description: "Wide-format IRT input (one row per composite_ID)"
          expected_rows: 400
        - path: "data/step00_q_matrix.csv"
          columns: ["item_name", "Source", "Destination"]
          description: "Q-matrix defining 2-factor structure (simple structure - each item loads on ONE dimension)"
          expected_rows: "50-60"
        - path: "data/step00_tsvr_mapping.csv"
          columns: ["composite_ID", "UID", "TEST", "TSVR_hours"]
          description: "Time mapping for LMM"
          expected_rows: 400

      parameters:
        tag_patterns:
          source: ["TC_*-U-*"]
          destination: ["TC_*-D-*"]
        value_categories: [0, 0.25, 0.5, 0.75, 1.0]

    validation_call:
      module: "tools.validation"
      function: "validate_data_columns"
      signature: "validate_data_columns(df: DataFrame, required_columns: List[str]) -> Dict[str, Any]"

      input_files:
        - path: "data/step00_irt_input.csv"
          source: "analysis call output (extraction step)"

      parameters:
        df: "irt_input"
        required_columns: ["composite_ID"]

      criteria:
        - "All required columns present in extracted data"
        - "Expected rows: 400 (100 participants x 4 tests)"
        - "Values in {0, 0.25, 0.5, 0.75, 1.0} ONLY (5-category ordinal)"
        - "Q-matrix has no items loading on both dimensions (simple structure)"

      on_failure:
        action: "raise ValueError"
        log_to: "logs/step00_extract_vr_data.log"

    log_file: "logs/step00_extract_vr_data.log"

  # --------------------------------------------------------------------------
  # STEP 1: IRT Calibration Pass 1
  # --------------------------------------------------------------------------
  - name: "step01_irt_calibration_pass1"
    step_number: "01"
    description: "Calibrate 2-factor GRM on all confidence items (before purification) - Pass 1 of Decision D039 2-pass purification"

    analysis_call:
      module: "tools.analysis_irt"
      function: "fit_irt_grm"
      signature: "fit_irt_grm(model: IWAVE, response_matrix: Tensor, missing_mask: Tensor, batch_size: int, iw_samples: int, mc_samples: int) -> IWAVE"

      input_files:
        - path: "data/step00_irt_input.csv"
          required_columns: ["composite_ID", "TC_*"]
          variable_name: "irt_input"
        - path: "data/step00_q_matrix.csv"
          required_columns: ["item_name", "Source", "Destination"]
          variable_name: "q_matrix"

      output_files:
        - path: "data/step01_pass1_item_params.csv"
          variable_name: "item_params"
          columns: ["item_name", "dimension", "a", "b1", "b2", "b3", "b4"]
          description: "GRM item parameters from Pass 1 (discrimination a, thresholds b1-b4)"
          expected_rows: "50-60"
        - path: "data/step01_pass1_theta.csv"
          variable_name: "theta_scores"
          columns: ["composite_ID", "theta_Source", "theta_Destination", "se_Source", "se_Destination"]
          description: "Participant confidence ability estimates from Pass 1"
          expected_rows: 400

      parameters:
        n_factors: 2
        n_cats: 5
        correlated_factors: true
        device: "cpu"
        seed: 42
        batch_size: 2048
        iw_samples: 100
        mc_samples: 1
        scoring_batch_size: 2048
        scoring_mc_samples: 100
        scoring_iw_samples: 100

      returns:
        type: "IWAVE"
        unpacking: "fitted_model"

    validation_call:
      module: "tools.validation"
      function: "validate_irt_convergence"
      signature: "validate_irt_convergence(results: Dict[str, Any]) -> Dict[str, Any]"

      input_files: []

      parameters:
        results: "convergence_info"

      criteria:
        - "Model converged (loss stabilized)"
        - "Parameter estimates finite (no NaN/Inf)"
        - "Threshold ordering preserved (b1 < b2 < b3 < b4 for all items)"

      on_failure:
        action: "raise ValueError"
        log_to: "logs/step01_irt_calibration_pass1.log"

    log_file: "logs/step01_irt_calibration_pass1.log"

  # --------------------------------------------------------------------------
  # STEP 2: Purify Items (Decision D039)
  # --------------------------------------------------------------------------
  - name: "step02_purify_items"
    step_number: "02"
    description: "Filter items by quality thresholds: a >= 0.4 AND mean(|b1|,|b2|,|b3|,|b4|) <= 3.0 per Decision D039"

    analysis_call:
      module: "tools.analysis_irt"
      function: "filter_items_by_quality"
      signature: "filter_items_by_quality(df_items: DataFrame, a_threshold: float, b_threshold: float) -> Tuple[DataFrame, DataFrame]"

      input_files:
        - path: "data/step01_pass1_item_params.csv"
          required_columns: ["item_name", "dimension", "a", "b1", "b2", "b3", "b4"]
          variable_name: "pass1_params"

      output_files:
        - path: "data/step02_purified_items.csv"
          variable_name: "purified_items"
          columns: ["item_name", "dimension", "a", "mean_abs_b"]
          description: "Items retained after purification"
          expected_rows: "15-40"
        - path: "data/step02_purification_report.txt"
          variable_name: "purification_report"
          description: "Text report of excluded items with reasons"

      parameters:
        df_items: "pass1_params"
        a_threshold: 0.4
        b_threshold: 3.0

      returns:
        type: "Tuple[DataFrame, DataFrame]"
        unpacking: "purified_items, purification_report"

    validation_call:
      module: "tools.validation"
      function: "validate_irt_parameters"
      signature: "validate_irt_parameters(df_items: DataFrame, a_min: float, b_max: float, a_col: str, b_col: str) -> Dict[str, Any]"

      input_files:
        - path: "data/step02_purified_items.csv"
          source: "analysis call output (purified items)"

      parameters:
        df_items: "purified_items"
        a_min: 0.4
        b_max: 3.0
        a_col: "a"
        b_col: "mean_abs_b"

      criteria:
        - "a >= 0.4 for ALL retained items"
        - "mean_abs_b <= 3.0 for ALL retained items"
        - "At least 5 items per dimension (minimum for stable calibration)"

      on_failure:
        action: "raise ValueError"
        log_to: "logs/step02_purify_items.log"

    log_file: "logs/step02_purify_items.log"

  # --------------------------------------------------------------------------
  # STEP 3: IRT Calibration Pass 2
  # --------------------------------------------------------------------------
  - name: "step03_irt_calibration_pass2"
    step_number: "03"
    description: "Recalibrate 2-factor GRM on purified items only - Pass 2 of Decision D039 2-pass purification (final theta estimates for LMM)"

    analysis_call:
      module: "tools.analysis_irt"
      function: "fit_irt_grm"
      signature: "fit_irt_grm(model: IWAVE, response_matrix: Tensor, missing_mask: Tensor, batch_size: int, iw_samples: int, mc_samples: int) -> IWAVE"

      input_files:
        - path: "data/step00_irt_input.csv"
          required_columns: ["composite_ID", "TC_*"]
          variable_name: "irt_input"
        - path: "data/step02_purified_items.csv"
          required_columns: ["item_name", "dimension"]
          variable_name: "purified_items"

      output_files:
        - path: "data/step03_item_parameters.csv"
          variable_name: "item_params_final"
          columns: ["item_name", "dimension", "a", "b1", "b2", "b3", "b4"]
          description: "Final GRM item parameters from Pass 2 calibration"
          expected_rows: "15-40"
        - path: "data/step03_theta_scores.csv"
          variable_name: "theta_scores_final"
          columns: ["composite_ID", "theta_Source", "theta_Destination", "se_Source", "se_Destination"]
          description: "Final confidence ability estimates (USED FOR LMM)"
          expected_rows: 400

      parameters:
        n_factors: 2
        n_cats: 5
        correlated_factors: true
        device: "cpu"
        seed: 42
        batch_size: 2048
        iw_samples: 100
        mc_samples: 1
        scoring_batch_size: 2048
        scoring_mc_samples: 100
        scoring_iw_samples: 100

      returns:
        type: "IWAVE"
        unpacking: "fitted_model_pass2"

    validation_call:
      module: "tools.validation"
      function: "validate_numeric_range"
      signature: "validate_numeric_range(data: Union[np.ndarray, pd.Series], min_val: float, max_val: float, column_name: str) -> Dict[str, Any]"

      input_files:
        - path: "data/step03_theta_scores.csv"
          source: "analysis call output (final theta estimates)"

      parameters:
        data: "theta_scores_final['theta_Source']"
        min_val: -4.0
        max_val: 4.0
        column_name: "theta_Source"

      criteria:
        - "All theta values in [-4, 4] (typical IRT ability range)"
        - "SE values lower on average than Pass 1 (improved precision)"
        - "All 400 observations have theta estimates (no NaN)"

      on_failure:
        action: "raise ValueError"
        log_to: "logs/step03_irt_calibration_pass2.log"

    log_file: "logs/step03_irt_calibration_pass2.log"

  # --------------------------------------------------------------------------
  # STEP 4: Merge Theta with TSVR (Decision D070)
  # --------------------------------------------------------------------------
  - name: "step04_merge_theta_tsvr"
    step_number: "04"
    description: "Merge theta confidence estimates with TSVR time variable and create long-format LMM input per Decision D070"

    analysis_call:
      type: "stdlib"
      operations:
        - "pd.read_csv('data/step03_theta_scores.csv')"
        - "pd.read_csv('data/step00_tsvr_mapping.csv')"
        - "Merge on composite_ID (left join - keep all theta scores, add TSVR)"
        - "Reshape wide -> long (theta_Source and theta_Destination become rows)"
        - "Create LocationType factor (Source vs Destination)"
        - "Create TSVR_hours_z (z-scored TSVR for LMM numerical stability)"
        - "Save long-format LMM input to data/step04_lmm_input.csv"

      input_files:
        - path: "data/step03_theta_scores.csv"
          required_columns: ["composite_ID", "theta_Source", "theta_Destination", "se_Source", "se_Destination"]
          variable_name: "theta_scores"
        - path: "data/step00_tsvr_mapping.csv"
          required_columns: ["composite_ID", "UID", "TEST", "TSVR_hours"]
          variable_name: "tsvr_mapping"

      output_files:
        - path: "data/step04_lmm_input.csv"
          columns: ["UID", "TEST", "TSVR_hours", "TSVR_hours_z", "LocationType", "theta_confidence", "se"]
          description: "Long-format LMM input (800 rows: 400 composite_IDs x 2 location types)"
          expected_rows: 800

      parameters:
        merge_key: "composite_ID"
        location_types: ["Source", "Destination"]

    validation_call:
      module: "tools.validation"
      function: "validate_data_format"
      signature: "validate_data_format(df: DataFrame, required_cols: List[str]) -> Dict[str, Any]"

      input_files:
        - path: "data/step04_lmm_input.csv"
          source: "analysis call output (merge step)"

      parameters:
        df: "lmm_input"
        required_cols: ["UID", "TEST", "TSVR_hours", "TSVR_hours_z", "LocationType", "theta_confidence", "se"]

      criteria:
        - "All 800 rows have non-null values (no NaN from merge)"
        - "Exactly 2 rows per composite_ID (Source + Destination)"
        - "TSVR_hours_z approximately N(0, 1) (z-scored)"
        - "LocationType in {Source, Destination} ONLY"

      on_failure:
        action: "raise ValueError"
        log_to: "logs/step04_merge_theta_tsvr.log"

    log_file: "logs/step04_merge_theta_tsvr.log"

  # --------------------------------------------------------------------------
  # STEP 5: Fit LMM with LocationType x Time Interaction
  # --------------------------------------------------------------------------
  - name: "step05_fit_lmm"
    step_number: "05"
    description: "Fit Linear Mixed Model testing LocationType x Time interaction to determine if source and destination confidence decline at different rates"

    analysis_call:
      module: "tools.analysis_lmm"
      function: "fit_lmm_trajectory_tsvr"
      signature: "fit_lmm_trajectory_tsvr(theta_scores: DataFrame, tsvr_data: DataFrame, formula: str, groups: str, re_formula: str, reml: bool) -> MixedLMResults"

      input_files:
        - path: "data/step04_lmm_input.csv"
          required_columns: ["UID", "TEST", "TSVR_hours", "TSVR_hours_z", "LocationType", "theta_confidence", "se"]
          variable_name: "lmm_input"

      output_files:
        - path: "data/step05_lmm_model_summary.txt"
          variable_name: "lmm_summary"
          description: "LMM model summary (fixed effects, random effects, fit statistics)"
        - path: "data/step05_fixed_effects.csv"
          variable_name: "fixed_effects"
          columns: ["term", "coefficient", "se", "t_value", "p_value"]
          description: "Fixed effects table from LMM"
          expected_rows: 4
        - path: "data/step05_random_effects_variance.csv"
          variable_name: "random_variance"
          columns: ["component", "variance", "sd"]
          description: "Random effects variance components"
          expected_rows: 3

      parameters:
        theta_scores: "lmm_input"
        tsvr_data: "lmm_input"
        formula: "theta_confidence ~ LocationType * TSVR_hours_z"
        groups: "UID"
        re_formula: "~TSVR_hours_z"
        reml: false

      returns:
        type: "MixedLMResults"
        unpacking: "lmm_model"

    validation_call:
      module: "tools.validation"
      function: "validate_lmm_convergence"
      signature: "validate_lmm_convergence(lmm_result: MixedLMResults) -> Dict[str, Any]"

      input_files: []

      parameters:
        lmm_result: "lmm_model"

      criteria:
        - "Model converged successfully"
        - "No singular fit warnings"
        - "All fixed effects finite (no NaN/Inf)"
        - "Variance components > 0 (no boundary issues)"

      on_failure:
        action: "raise ValueError"
        log_to: "logs/step05_fit_lmm.log"

    log_file: "logs/step05_fit_lmm.log"

  # --------------------------------------------------------------------------
  # STEP 6: Compute Location Contrasts (Decision D068)
  # --------------------------------------------------------------------------
  - name: "step06_compute_post_hoc_contrasts"
    step_number: "06"
    description: "Extract LocationType x Time interaction, compute source vs destination slope contrasts with dual p-values per Decision D068"

    analysis_call:
      module: "tools.analysis_lmm"
      function: "compute_contrasts_pairwise"
      signature: "compute_contrasts_pairwise(lmm_result: MixedLMResults, comparisons: List[str], family_alpha: float) -> DataFrame"

      input_files:
        - path: "data/step05_fixed_effects.csv"
          required_columns: ["term", "coefficient", "se", "t_value", "p_value"]
          variable_name: "fixed_effects"

      output_files:
        - path: "data/step06_location_contrasts.csv"
          variable_name: "location_contrasts"
          columns: ["contrast", "estimate", "se", "t_value", "p_uncorrected", "p_bonferroni"]
          description: "Location contrasts with dual p-values (Decision D068)"
          expected_rows: 1
        - path: "data/step06_marginal_slopes.csv"
          variable_name: "marginal_slopes"
          columns: ["LocationType", "slope", "se"]
          description: "Marginal slopes by location type"
          expected_rows: 2
        - path: "data/step06_effect_sizes.csv"
          variable_name: "effect_sizes"
          columns: ["term", "cohens_f2", "interpretation"]
          description: "Effect sizes for interaction term"
          expected_rows: 1

      parameters:
        lmm_result: "lmm_model"
        comparisons: ["Destination-Source"]
        family_alpha: 0.05

      returns:
        type: "DataFrame"
        unpacking: "location_contrasts"

    validation_call:
      module: "tools.validation"
      function: "validate_contrasts_d068"
      signature: "validate_contrasts_d068(contrasts_df: DataFrame) -> Dict[str, Any]"

      input_files:
        - path: "data/step06_location_contrasts.csv"
          source: "analysis call output (contrast computation)"

      parameters:
        contrasts_df: "location_contrasts"

      criteria:
        - "Both p_uncorrected AND p_bonferroni columns present (Decision D068)"
        - "p_bonferroni >= p_uncorrected for all rows (correction logic)"
        - "All p-values in [0, 1] range"

      on_failure:
        action: "raise ValueError"
        log_to: "logs/step06_compute_post_hoc_contrasts.log"

    log_file: "logs/step06_compute_post_hoc_contrasts.log"

  # --------------------------------------------------------------------------
  # STEP 7: Prepare Trajectory Plot Data (Option B Architecture)
  # --------------------------------------------------------------------------
  - name: "step07_prepare_trajectory_plot_data"
    step_number: "07"
    description: "Aggregate analysis outputs to create plot source CSVs for theta-scale and probability-scale trajectory plots (Decision D069 dual-scale requirement)"

    analysis_call:
      type: "stdlib"
      operations:
        - "pd.read_csv('data/step03_theta_scores.csv')"
        - "pd.read_csv('data/step00_tsvr_mapping.csv')"
        - "pd.read_csv('data/step05_fixed_effects.csv')"
        - "Merge theta scores with TSVR mapping on composite_ID"
        - "Reshape wide -> long (Source and Destination as rows)"
        - "Group by LocationType x TEST, compute observed mean theta, 95% CI"
        - "Generate predicted trajectories from LMM fixed effects"
        - "Create theta-scale plot data (theta values -4 to +4)"
        - "Transform theta -> probability using IRT formula: P(theta) = 1 / (1 + exp(-1.7 * theta))"
        - "Create probability-scale plot data (probability values 0 to 1)"
        - "Save both CSVs to data/"

      input_files:
        - path: "data/step03_theta_scores.csv"
          required_columns: ["composite_ID", "theta_Source", "theta_Destination", "se_Source", "se_Destination"]
          variable_name: "theta_scores"
        - path: "data/step00_tsvr_mapping.csv"
          required_columns: ["composite_ID", "TSVR_hours", "TEST"]
          variable_name: "tsvr_mapping"
        - path: "data/step05_fixed_effects.csv"
          required_columns: ["term", "coefficient", "se", "t_value", "p_value"]
          variable_name: "fixed_effects"

      output_files:
        - path: "data/step07_trajectory_theta_data.csv"
          columns: ["time", "theta", "theta_pred", "CI_lower", "CI_upper", "LocationType"]
          description: "Plot source data for theta-scale trajectory"
          expected_rows: 8
        - path: "data/step07_trajectory_probability_data.csv"
          columns: ["time", "probability", "probability_pred", "CI_lower", "CI_upper", "LocationType"]
          description: "Plot source data for probability-scale trajectory (Decision D069 dual-scale)"
          expected_rows: 8

      parameters:
        irt_transformation: "probability = 1 / (1 + exp(-1.7 * theta))"

    validation_call:
      module: "tools.validation"
      function: "validate_data_format"
      signature: "validate_data_format(df: DataFrame, required_cols: List[str]) -> Dict[str, Any]"

      input_files:
        - path: "data/step07_trajectory_theta_data.csv"
          source: "analysis call output (plot data preparation)"

      parameters:
        df: "trajectory_theta_data"
        required_cols: ["time", "theta", "theta_pred", "CI_lower", "CI_upper", "LocationType"]

      criteria:
        - "Expected rows: 8 (2 LocationTypes x 4 timepoints)"
        - "theta in [-4, 4] (IRT ability range)"
        - "probability in [0, 1] (probability scale per Decision D069)"
        - "CI_upper > CI_lower for all rows"
        - "No NaN values (all cells valid)"

      on_failure:
        action: "raise ValueError"
        log_to: "logs/step07_prepare_trajectory_plot_data.log"

    log_file: "logs/step07_prepare_trajectory_plot_data.log"

# ============================================================================
# END OF ANALYSIS RECIPE
# ============================================================================
