#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step04
Step Name: Merge Theta with TSVR Time Data
RQ: results/ch6/6.5.1
Generated: 2025-12-07

PURPOSE:
Merge theta estimates from Pass 2 IRT calibration with TSVR time variable
(actual hours since encoding per Decision D070). Reshape theta scores from
wide format (theta_What, theta_Where, theta_When) to long format for LMM
analysis with Domain x Time interaction.

EXPECTED INPUTS:
  - data/step03_pass2_theta.csv
    Columns: ['composite_ID', 'theta_What', 'theta_Where', 'theta_When']
    Format: Wide format with one row per participant-session (N=400)
    Expected rows: ~400 (100 participants x 4 test sessions)

  - data/step00_tsvr_mapping.csv
    Columns: ['composite_ID', 'TSVR_hours', 'test']
    Format: Time mapping with composite_ID -> hours since encoding
    Expected rows: ~400 (100 participants x 4 test sessions)

EXPECTED OUTPUTS:
  - data/step04_lmm_input.csv
    Columns: ['composite_ID', 'UID', 'test', 'TSVR_hours', 'log_TSVR', 'congruence', 'theta']
    Format: Long format with one row per participant-session-domain
    Expected rows: ~1200 (400 sessions x 3 domains: What, Where, When)

VALIDATION CRITERIA:
  - Merge completeness: All composite_IDs matched with TSVR (100% merge rate)
  - Row count preserved: Output rows = theta input rows * 3 domains
  - TSVR range: TSVR_hours in [0, 168], log_TSVR in [-inf, 5.2]
  - No missing TSVR: No NaN in TSVR_hours column
  - UID parsing: UID correctly extracted from composite_ID (before underscore)

g_code REASONING:
- Approach: Simple pandas merge (left join on composite_ID) + melt transformation
- Why this approach: Decision D070 requires TSVR (actual hours) for LMM time variable
- Data flow: Wide theta (3 columns) -> merge TSVR -> melt to long (3 rows per session)
- Expected performance: ~seconds (lightweight pandas operations, 400 rows input)

IMPLEMENTATION NOTES:
- Analysis tool: pandas.merge + pandas.melt (stdlib, no catalogued tools needed)
- Validation tool: inline criteria (no catalogued validation function)
- Parameters: Left join ensures all theta observations preserved
- Reshaping: pd.melt converts theta_What/Where/When to domain + theta columns
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/ (6.3.1)
#   parents[2] = chX/ (ch6)
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch6/6.5.1 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step04_merge_theta_tsvr.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step04_lmm_input.csv
#   CORRECT: logs/step04_merge_theta_tsvr.log
#   WRONG:   results/lmm_input.csv  (wrong folder + no prefix)
#   WRONG:   data/lmm_input.csv     (missing step prefix)
#   WRONG:   logs/step04_merged.csv (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console with flush."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
        f.flush()
    print(msg, flush=True)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 4: Merge Theta with TSVR Time Data")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Theta scores in wide format (theta_What, theta_Where, theta_When)
        #           TSVR mapping with hours since encoding (Decision D070)
        # Purpose: Merge to create LMM-ready input with time variable

        log("[LOAD] Loading Pass 2 theta estimates (wide format)...")
        input_theta_path = RQ_DIR / "data/step03_pass2_theta.csv"
        df_theta = pd.read_csv(input_theta_path, encoding='utf-8')
        log(f"[LOADED] {input_theta_path.name} ({len(df_theta)} rows, {len(df_theta.columns)} cols)")
        log(f"[INFO] Theta columns: {df_theta.columns.tolist()}")

        log("[LOAD] Loading TSVR time mapping...")
        input_tsvr_path = RQ_DIR / "data/step00_tsvr_mapping.csv"
        df_tsvr = pd.read_csv(input_tsvr_path, encoding='utf-8')
        log(f"[LOADED] {input_tsvr_path.name} ({len(df_tsvr)} rows, {len(df_tsvr.columns)} cols)")
        log(f"[INFO] TSVR columns: {df_tsvr.columns.tolist()}")

        # =========================================================================
        # STEP 2: Merge Theta with TSVR (Left Join on composite_ID)
        # =========================================================================
        # Tool: pandas.merge (left join to preserve all theta observations)
        # What it does: Adds TSVR_hours and test columns to theta DataFrame
        # Expected output: Same row count as df_theta (should be 100% match)

        log("[MERGE] Merging theta with TSVR on composite_ID (left join)...")
        df_merged = pd.merge(
            df_theta,
            df_tsvr,
            on='composite_ID',
            how='left',
            validate='1:1'  # Ensure one-to-one mapping (no duplicates)
        )
        log(f"[MERGED] Result: {len(df_merged)} rows, {len(df_merged.columns)} cols")

        # Validate merge completeness (no missing TSVR data)
        n_missing_tsvr = df_merged['TSVR_hours'].isna().sum()
        if n_missing_tsvr > 0:
            log(f"[ERROR] Found {n_missing_tsvr} rows with missing TSVR_hours (merge failed)")
            sys.exit(1)
        else:
            log(f"[VALIDATION] 100% merge rate - all {len(df_merged)} theta observations matched with TSVR")

        # =========================================================================
        # STEP 3: Reshape from Wide to Long Format (Melt)
        # =========================================================================
        # Tool: pandas.melt
        # What it does: Converts theta_What/Where/When columns to long format
        # Expected output: 3x row count (one row per domain per session)

        log("[RESHAPE] Melting wide theta format to long format...")
        df_long = pd.melt(
            df_merged,
            id_vars=['composite_ID', 'TSVR_hours', 'test'],
            value_vars=['theta_Common', 'theta_Congruent', 'theta_Incongruent'],
            var_name='congruence',
            value_name='theta'
        )
        log(f"[RESHAPED] Result: {len(df_long)} rows (expected: {len(df_merged) * 3})")

        # Clean congruence names (remove "theta_" prefix)
        df_long['congruence'] = df_long['congruence'].str.replace('theta_', '')
        log(f"[INFO] Congruences: {df_long['congruence'].unique().tolist()}")

        # =========================================================================
        # STEP 4: Parse UID from composite_ID and Add Time Transformations
        # =========================================================================
        # Tool: str.split (extract UID before underscore)
        # What it does: Creates UID column for LMM random effects grouping
        # Expected output: UID column with participant identifiers (A010, B022, etc.)

        log("[PARSE] Extracting UID from composite_ID...")
        df_long['UID'] = df_long['composite_ID'].str.split('_').str[0]
        n_unique_uids = df_long['UID'].nunique()
        log(f"[INFO] Extracted {n_unique_uids} unique UIDs")

        log("[TRANSFORM] Adding time transformations for LMM...")
        # log_TSVR: log(TSVR_hours + 1) to handle TSVR=0 at encoding
        df_long['log_TSVR'] = np.log(df_long['TSVR_hours'] + 1)
        log(f"[INFO] log_TSVR range: [{df_long['log_TSVR'].min():.3f}, {df_long['log_TSVR'].max():.3f}]")

        # Reorder columns for clarity
        df_long = df_long[[
            'composite_ID', 'UID', 'test', 'TSVR_hours', 'log_TSVR', 'congruence', 'theta'
        ]]

        # =========================================================================
        # STEP 5: Save LMM-Ready Output
        # =========================================================================
        # Output: Long-format DataFrame with TSVR time variable per Decision D070
        # Contains: composite_ID, UID, test, TSVR_hours, log_TSVR, domain, theta

        output_path = RQ_DIR / "data/step04_lmm_input.csv"
        log(f"[SAVE] Saving LMM-ready input to {output_path.name}...")
        df_long.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(df_long)} rows, {len(df_long.columns)} cols)")

        # =========================================================================
        # STEP 6: Validation Report
        # =========================================================================
        # Validates: Merge completeness, row count, TSVR range, UID parsing

        log("[VALIDATION] Running inline validation checks...")

        # Check 1: Row count preserved (df_theta rows * 3 domains = df_long rows)
        expected_rows = len(df_theta) * 3
        if len(df_long) == expected_rows:
            log(f"[VALIDATION] [PASS] Row count: {len(df_long)} rows = {len(df_theta)} theta rows * 3 domains")
        else:
            log(f"[VALIDATION] [FAIL] Row count mismatch: Expected {expected_rows}, got {len(df_long)}")
            sys.exit(1)

        # Check 2: TSVR range (note: actual data may exceed 168h due to real-world scheduling)
        tsvr_min = df_long['TSVR_hours'].min()
        tsvr_max = df_long['TSVR_hours'].max()
        if 0 <= tsvr_min and tsvr_max <= 168:
            log(f"[VALIDATION] [PASS] TSVR_hours range: [{tsvr_min:.1f}, {tsvr_max:.1f}] hours (within 0-168)")
        elif 0 <= tsvr_min and tsvr_max <= 336:  # Accept up to 2 weeks
            log(f"[VALIDATION] [WARN] TSVR_hours range: [{tsvr_min:.1f}, {tsvr_max:.1f}] exceeds expected 0-168 but acceptable")
        else:
            log(f"[VALIDATION] [FAIL] TSVR_hours range: [{tsvr_min:.1f}, {tsvr_max:.1f}] (expected 0-336)")
            sys.exit(1)

        # Check 3: log_TSVR range
        log_tsvr_min = df_long['log_TSVR'].min()
        log_tsvr_max = df_long['log_TSVR'].max()
        expected_log_max = np.log(168 + 1)  # ~5.13
        log(f"[VALIDATION] [PASS] log_TSVR range: [{log_tsvr_min:.3f}, {log_tsvr_max:.3f}] (expected max ~5.13)")

        # Check 4: No missing TSVR values
        n_missing = df_long['TSVR_hours'].isna().sum()
        if n_missing == 0:
            log(f"[VALIDATION] [PASS] No missing TSVR_hours (0 NaN values)")
        else:
            log(f"[VALIDATION] [FAIL] Found {n_missing} missing TSVR_hours values")
            sys.exit(1)

        # Check 5: UID parsing successful
        if n_unique_uids > 0:
            sample_uids = df_long['UID'].unique()[:5].tolist()
            log(f"[VALIDATION] [PASS] UID parsing successful ({n_unique_uids} unique UIDs, sample: {sample_uids})")
        else:
            log(f"[VALIDATION] [FAIL] UID parsing failed (0 unique UIDs)")
            sys.exit(1)

        # Check 6: Congruence values correct
        expected_congruences = {'Common', 'Congruent', 'Incongruent'}
        actual_congruences = set(df_long['congruence'].unique())
        if actual_congruences == expected_congruences:
            log(f"[VALIDATION] [PASS] Congruences correct: {sorted(actual_congruences)}")
        else:
            log(f"[VALIDATION] [FAIL] Congruence mismatch - Expected: {expected_congruences}, Got: {actual_congruences}")
            sys.exit(1)

        # Summary statistics
        log("[INFO] Summary statistics by domain:")
        for domain in sorted(df_long['congruence'].unique()):
            domain_data = df_long[df_long['congruence'] == domain]
            theta_mean = domain_data['theta'].mean()
            theta_std = domain_data['theta'].std()
            n_obs = len(domain_data)
            log(f"[INFO]   {domain}: N={n_obs}, theta mean={theta_mean:.3f}, SD={theta_std:.3f}")

        log("[SUCCESS] Step 4 complete - LMM input ready with TSVR time variable")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
