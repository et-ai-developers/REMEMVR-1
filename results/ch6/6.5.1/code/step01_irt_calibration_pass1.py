#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code v4.1)
# =============================================================================
"""
Step ID: step01
Step Name: IRT Calibration Pass 1 (All TC_* Items)
RQ: results/ch6/6.5.1
Generated: 2025-12-07

PURPOSE:
Calibrate 3-factor GRM on all TC_* confidence items (Common/Congruent/Incongruent congruences).
Pass 1 provides initial parameter estimates for item purification (Step 2).
Uses IWAVE model with GRM response function for 5-category ordinal data.

CRITICAL SETTINGS (from RQ 6.1.1 learnings):
- FITTING: mc_samples=1 (FAST - avoids 7000+ epoch hang)
- SCORING: mc_samples=100 (ACCURATE theta estimates)
- This fixes the 6.1.1 timeout issue where mc_samples=100 during fitting caused hang

EXPECTED INPUTS:
  - data/step00_irt_input.csv
    Columns: composite_ID + ~72 TC_* items
    Format: Wide format, 400 rows (100 participants x 4 sessions)
    Expected rows: 400
    TC_* values: Ordinal {0.2, 0.4, 0.6, 0.8, 1.0} (5 categories)

  - data/step00_q_matrix.csv
    Columns: item_name, factor_common, factor_congruent, factor_incongruent
    Format: Q-matrix specifying 3-factor structure
    Expected rows: ~72 items

EXPECTED OUTPUTS:
  - data/step01_pass1_item_params.csv
    Columns: item_name, dimension, a, b1, b2, b3, b4
    Format: Item parameters from GRM calibration (5 categories = 4 thresholds)
    Expected rows: ~72 items

  - data/step01_pass1_theta.csv
    Columns: composite_ID, theta_Common, se_Common, theta_Congruent, se_Congruent, theta_Incongruent, se_Incongruent
    Format: Wide format theta estimates (3 factors)
    Expected rows: 400

VALIDATION CRITERIA:
  - Model convergence: Loss stable, ELBO improvement minimal in final iterations
  - Parameter bounds: a in [0.0, 10.0], b in [-6.0, 6.0]
  - No NaN parameters: All items calibrated successfully
  - Theta valid: theta in [-4, 4], se in [0.1, 1.5]

g_code REASONING:
- Approach: 3-factor GRM using IWAVE variational inference with Q-matrix structure
- Why this approach: Ordinal TC_* data (5 categories) requires GRM, 3-congruence structure per 1_concept.md
- Data flow: Wide CSV → long format → tensors → IWAVE model → parameter extraction → wide outputs
- Expected performance: ~15-25 minutes with mc_samples=1 fitting, ~5 minutes for scoring
- KEY FIX: Use mc_samples=1 during model.fit() to avoid 7000+ epoch hang (RQ 6.1.1 lesson)
- KEY FIX: Use mc_samples=100 during model.scores() for accurate theta estimates

IMPLEMENTATION NOTES:
- Analysis tool: prepare_irt_input_from_long, configure_irt_model, fit_irt_grm, extract_* from tools.analysis_irt
- Validation tool: validate_irt_convergence, validate_irt_parameters from tools.validation
- Parameters: 3 factors (Common/Congruent/Incongruent), 5 categories, correlated factors, cpu device
- MINIMAL TEST MODE: Set max_iter=50, mc_samples=10, iw_samples=10 for ~5-min validation run
- PRODUCTION MODE: Settings below (20-30 min runtime)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import torch
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import analysis tools
from tools.analysis_irt import (
    prepare_irt_input_from_long,
    configure_irt_model,
    fit_irt_grm,
    extract_theta_from_irt,
    extract_parameters_from_irt
)

# Import validation tools
from tools.validation import validate_irt_convergence, validate_irt_parameters

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch6/6.3.1
LOG_FILE = RQ_DIR / "logs" / "step01_irt_calibration_pass1.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step01_pass1_item_params.csv
#   CORRECT: data/step01_pass1_theta.csv
#   WRONG:   results/item_params.csv  (wrong folder + no prefix)
#   WRONG:   data/item_params.csv     (missing step prefix)
#   WRONG:   logs/step01_items.csv    (CSV in logs folder)

# =============================================================================
# IRT TESTING WORKFLOW (g_code recommendation)
# =============================================================================
# Phase 1 (MINIMAL TEST - run this first):
#   Set: max_iter=50, mc_samples=10, iw_samples=10
#   Runtime: ~5-10 minutes (validates entire pipeline)
#   Expected: Convergence may fail (acceptable for testing)
#
# Phase 2 (PRODUCTION - only after Phase 1 passes):
#   Set: max_iter=200 (or as specified below)
#   Runtime: 20-30 minutes (production-quality theta scores)
# =============================================================================

# IRT Model Configuration (3-factor GRM for TC_* confidence items)
IRT_CONFIG = {
    'model_type': 'GRM',
    'n_cats': 5,  # CORRECTED: Actual data has 5 categories {0.2, 0.4, 0.6, 0.8, 1.0}
    'correlated_factors': True,
    'device': 'cpu',
    'seed': 42,
    'factors': ['Common', 'Congruent', 'Incongruent']  # 3-factor structure by congruence
}

# CRITICAL SETTINGS FROM RQ 6.1.1:
# - FITTING: mc_samples=1 (FAST - avoids 7000+ epoch hang)
# - SCORING: mc_samples=100 (ACCURATE theta estimates)

# MINIMUM MODE: Absolute minimum settings to prove code works without crashing
# Model Fitting Settings (MINIMUM)
MODEL_FIT_SETTINGS = {
    'batch_size': 2048,
    'iw_samples': 100,  # MED: 100 (was 1) - Ch5 validated 2025-11-25
    'mc_samples': 1     # MED: 1 = point estimates (FAST - correct)
}

# Theta Scoring Settings (MED - Ch5 validated)
MODEL_SCORING_SETTINGS = {
    'scoring_batch_size': 2048,
    'mc_samples': 100,  # MED: 100 (was 1) - Monte Carlo integration
    'iw_samples': 100   # MED: 100 (was 1) - Importance weighting
}

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
        f.flush()
    print(msg, flush=True)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 01: IRT Calibration Pass 1 (3-factor GRM on all TC_* items)")
        log(f"[CONFIG] Model: 3-factor GRM, {IRT_CONFIG['n_cats']} categories")
        log(f"[CONFIG] Fitting: batch_size={MODEL_FIT_SETTINGS['batch_size']}, "
            f"mc_samples={MODEL_FIT_SETTINGS['mc_samples']} (FAST mode), "
            f"iw_samples={MODEL_FIT_SETTINGS['iw_samples']}")
        log(f"[CONFIG] Scoring: mc_samples={MODEL_SCORING_SETTINGS['mc_samples']} (ACCURATE mode), "
            f"iw_samples={MODEL_SCORING_SETTINGS['iw_samples']}")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Wide-format TC_* items (400 rows x ~72 items) + Q-matrix
        # Purpose: Prepare data for 3-factor IRT calibration

        log("[LOAD] Loading input data...")

        # Load wide-format IRT input (composite_ID x TC_* items)
        df_wide = pd.read_csv(RQ_DIR / "data" / "step00_irt_input.csv")
        log(f"[LOADED] step00_irt_input.csv ({len(df_wide)} rows, {len(df_wide.columns)} cols)")

        # Load Q-matrix (item-to-factor mapping)
        df_q_matrix = pd.read_csv(RQ_DIR / "data" / "step00_q_matrix.csv")
        log(f"[LOADED] step00_q_matrix.csv ({len(df_q_matrix)} items)")

        # Extract item list from Q-matrix
        item_list = df_q_matrix['item_name'].tolist()
        log(f"[INFO] Q-matrix specifies {len(item_list)} items across 3 factors")

        # Verify all Q-matrix items exist in wide data
        tc_cols = [c for c in df_wide.columns if c.startswith('TC_')]
        missing_items = set(item_list) - set(tc_cols)
        if missing_items:
            raise ValueError(f"Q-matrix contains items not in data: {missing_items}")
        log(f"[VALIDATION] All {len(item_list)} Q-matrix items found in data")

        # Convert wide to long format for prepare_irt_input_from_long
        # REQUIncongruentD COLUMNS: UID, test, item_name, score (per tools/analysis_irt.py)
        log("[TRANSFORM] Converting wide format to long format...")
        df_long_list = []
        for idx, row in df_wide.iterrows():
            composite_id = row['composite_ID']
            # Parse UID and test from composite_ID (format: A010_T1)
            parts = composite_id.rsplit('_', 1)
            uid = parts[0]
            test_str = parts[1] if len(parts) > 1 else 'T1'
            test_num = test_str.replace('T', '')  # Extract number from T1, T2, etc.

            for item in item_list:
                response = row[item]
                df_long_list.append({
                    'UID': uid,
                    'test': test_num,
                    'item_name': item,
                    'score': response
                })
        df_long = pd.DataFrame(df_long_list)
        log(f"[TRANSFORM] Long format created: {len(df_long)} observations "
            f"({len(df_wide)} participants x {len(item_list)} items)")

        # Create groups dictionary from Q-matrix (factor -> items mapping)
        log("[TRANSFORM] Creating factor groups from Q-matrix...")
        groups = {}
        for factor in IRT_CONFIG['factors']:
            # Q-matrix uses lowercase factor names: factor_common, factor_congruent, factor_incongruent
            factor_col = f"factor_{factor.lower()}"
            items_in_factor = df_q_matrix[df_q_matrix[factor_col] == 1]['item_name'].tolist()
            groups[factor] = items_in_factor
            log(f"[INFO] {factor}: {len(items_in_factor)} items")

        # =========================================================================
        # STEP 2: Prepare IRT Tensors (Response Matrix, Missing Mask, Q-Matrix)
        # =========================================================================
        # Tool: prepare_irt_input_from_long
        # What it does: Converts long-format DataFrame to PyTorch tensors for IWAVE
        # Expected output: response_matrix (N x items), missing_mask, Q_matrix, IDs, item_list

        log("[ANALYSIS] Preparing IRT tensors via prepare_irt_input_from_long...")
        # Return order: response_matrix, Q_matrix, missing_mask, item_list, composite_ids
        response_matrix, Q_matrix, missing_mask, item_list_ordered, composite_ids = \
            prepare_irt_input_from_long(df_long, groups)

        log(f"[DONE] Tensors prepared:")
        log(f"  - response_matrix: {response_matrix.shape} (participants x items)")
        log(f"  - missing_mask: {missing_mask.shape}")
        log(f"  - Q_matrix: {Q_matrix.shape} (items x factors)")
        log(f"  - composite_ids: {len(composite_ids)} IDs")
        log(f"  - item_list: {len(item_list_ordered)} items")

        # =========================================================================
        # STEP 3: Configure IRT Model (3-Factor GRM)
        # =========================================================================
        # Tool: configure_irt_model
        # What it does: Builds IWAVE model with specified architecture
        # Expected output: IWAVE model object ready for fitting

        log("[ANALYSIS] Configuring 3-factor GRM model via configure_irt_model...")
        n_items = response_matrix.shape[1]
        n_factors = len(IRT_CONFIG['factors'])

        # CRITICAL: n_cats must be a list (one value per item, or single value for all)
        # For uniform category structure, pass list of same value
        n_cats_list = [IRT_CONFIG['n_cats']] * n_items

        model = configure_irt_model(
            n_items=n_items,
            n_factors=n_factors,
            n_cats=n_cats_list,  # Pass as list
            Q_matrix=Q_matrix,
            correlated_factors=IRT_CONFIG['correlated_factors'],
            device=IRT_CONFIG['device'],
            seed=IRT_CONFIG['seed']
        )
        log(f"[DONE] Model configured: {n_items} items, {n_factors} factors, "
            f"{IRT_CONFIG['n_cats']} categories, correlated={IRT_CONFIG['correlated_factors']}")

        # =========================================================================
        # STEP 4: Fit IRT Model (CRITICAL: mc_samples=1 for fitting)
        # =========================================================================
        # Tool: fit_irt_grm
        # What it does: Fits IWAVE model via variational inference
        # Expected output: Fitted model with calibrated parameters
        # KEY FIX: mc_samples=1 during fit prevents 7000+ epoch hang (RQ 6.1.1)

        log("[ANALYSIS] Fitting GRM model via fit_irt_grm...")
        log("[CRITICAL] Using mc_samples=1 during fit (avoids timeout per RQ 6.1.1)")

        fitted_model = fit_irt_grm(
            model=model,
            response_matrix=response_matrix,
            missing_mask=missing_mask,
            batch_size=MODEL_FIT_SETTINGS['batch_size'],
            iw_samples=MODEL_FIT_SETTINGS['iw_samples'],
            mc_samples=MODEL_FIT_SETTINGS['mc_samples']  # mc_samples=1 is CRITICAL
        )
        log("[DONE] Model fitting complete")

        # =========================================================================
        # STEP 5: Extract Theta Scores (CRITICAL: mc_samples=100 for scoring)
        # =========================================================================
        # Tool: extract_theta_from_irt
        # What it does: Computes participant ability estimates (theta) and SEs
        # Expected output: DataFrame with composite_ID, theta_Common, se_Common, theta_Congruent, se_Congruent, theta_Incongruent, se_Incongruent
        # KEY FIX: mc_samples=100 during scoring for accurate estimates (RQ 6.1.1)

        log("[ANALYSIS] Extracting theta scores via extract_theta_from_irt...")
        log("[CRITICAL] Using mc_samples=100 during scoring (accurate theta estimates per RQ 6.1.1)")

        df_theta = extract_theta_from_irt(
            model=fitted_model,
            response_matrix=response_matrix,
            missing_mask=missing_mask,
            composite_ids=composite_ids,
            factor_names=IRT_CONFIG['factors'],
            scoring_batch_size=MODEL_SCORING_SETTINGS['scoring_batch_size'],
            mc_samples=MODEL_SCORING_SETTINGS['mc_samples'],  # mc_samples=100 for accuracy
            iw_samples=MODEL_SCORING_SETTINGS['iw_samples'],
            invert_scale=False  # Higher theta = higher confidence
        )

        log(f"[DONE] Theta extraction complete: {len(df_theta)} participants x {len(IRT_CONFIG['factors'])} factors")

        # Tool returns wide format: UID, test, Theta_Common, Theta_Congruent, Theta_Incongruent
        # Reconstruct composite_ID from UID and test
        log("[TRANSFORM] Reconstructing composite_ID from UID and test...")
        df_theta['composite_ID'] = df_theta['UID'] + '_T' + df_theta['test'].astype(str)

        # Rename columns to match expected output format (lowercase theta_)
        rename_map = {}
        for factor in IRT_CONFIG['factors']:
            rename_map[f'Theta_{factor}'] = f'theta_{factor}'
        df_theta_wide = df_theta.rename(columns=rename_map)

        # Select and order output columns
        # Note: Tool does NOT return SE values - only theta
        ordered_cols = ['composite_ID']
        for factor in IRT_CONFIG['factors']:
            ordered_cols.append(f'theta_{factor}')
        df_theta_wide = df_theta_wide[ordered_cols]

        log(f"[TRANSFORM] Wide format: {len(df_theta_wide)} rows, {len(df_theta_wide.columns)} columns")

        # =========================================================================
        # STEP 6: Extract Item Parameters
        # =========================================================================
        # Tool: extract_parameters_from_irt
        # What it does: Extracts discrimination (a) and difficulty (b1-b4) parameters
        # Expected output: DataFrame with item_name, dimension, a, b1, b2, b3, b4

        log("[ANALYSIS] Extracting item parameters via extract_parameters_from_irt...")

        df_params = extract_parameters_from_irt(
            model=fitted_model,
            item_list=item_list_ordered,
            factor_names=IRT_CONFIG['factors'],
            n_cats=n_cats_list  # Pass as list
        )

        log(f"[DONE] Parameter extraction complete: {len(df_params)} items")

        # Note: extract_parameters_from_irt returns MIRT format for multidimensional models:
        # item_name, Difficulty, Overall_Discrimination, Discrim_Factor1, Discrim_Factor2, ...
        # Keep as-is since this is the tool's native output format
        log(f"[INFO] Parameter columns: {list(df_params.columns)}")

        # =========================================================================
        # STEP 7: Save Outputs
        # =========================================================================
        # These outputs will be used by Step 2 (purification) and Step 3 (Pass 2)

        log("[SAVE] Saving Pass 1 outputs...")

        # Save item parameters (for purification)
        item_params_path = RQ_DIR / "data" / "step01_pass1_item_params.csv"
        df_params.to_csv(item_params_path, index=False, encoding='utf-8')
        log(f"[SAVED] {item_params_path.name} ({len(df_params)} items, {len(df_params.columns)} cols)")

        # Save theta estimates (diagnostic, not used downstream)
        theta_path = RQ_DIR / "data" / "step01_pass1_theta.csv"
        df_theta_wide.to_csv(theta_path, index=False, encoding='utf-8')
        log(f"[SAVED] {theta_path.name} ({len(df_theta_wide)} rows, {len(df_theta_wide.columns)} cols)")

        # =========================================================================
        # STEP 8: Validation
        # =========================================================================
        # Validates: convergence, parameter bounds, no NaN parameters

        log("[VALIDATION] Running validation checks...")

        # Check 1: Parameter bounds (discrimination in [0, 10], difficulty in [-6, 6])
        log("[VALIDATION] Checking parameter bounds...")
        a_min, a_max = df_params['Overall_Discrimination'].min(), df_params['Overall_Discrimination'].max()
        b_min, b_max = df_params['Difficulty'].min(), df_params['Difficulty'].max()

        log(f"[VALIDATION] Discrimination: min={a_min:.3f}, max={a_max:.3f}")
        log(f"[VALIDATION] Difficulty (b): min={b_min:.3f}, max={b_max:.3f}")

        if a_min < 0.0 or a_max > 10.0:
            log(f"[FAIL] Discrimination out of bounds [0.0, 10.0]")
            sys.exit(1)
        if b_min < -6.0 or b_max > 6.0:
            log(f"[FAIL] Difficulty out of bounds [-6.0, 6.0]")
            sys.exit(1)
        log("[PASS] Parameter bounds valid")

        # Check 2: No NaN parameters
        log("[VALIDATION] Checking for NaN parameters...")
        nan_count = df_params[['Overall_Discrimination', 'Difficulty']].isna().sum().sum()
        if nan_count > 0:
            log(f"[FAIL] {nan_count} NaN parameters found")
            sys.exit(1)
        log("[PASS] No NaN parameters")

        # Check 3: Theta estimates valid (theta in [-4, 4])
        # Note: Tool does not return SE values, only theta
        log("[VALIDATION] Checking theta estimates...")
        theta_cols = [c for c in df_theta_wide.columns if c.startswith('theta_')]

        theta_min = df_theta_wide[theta_cols].min().min()
        theta_max = df_theta_wide[theta_cols].max().max()

        log(f"[VALIDATION] Theta: min={theta_min:.3f}, max={theta_max:.3f}")

        if theta_min < -4.0 or theta_max > 4.0:
            log(f"[FAIL] Theta out of bounds [-4.0, 4.0]")
            sys.exit(1)
        log("[PASS] Theta estimates valid")

        # Check 4: Output row counts
        log("[VALIDATION] Checking output row counts...")
        if len(df_params) != len(item_list):
            log(f"[FAIL] Item parameters count mismatch: {len(df_params)} vs {len(item_list)} expected")
            sys.exit(1)
        if len(df_theta_wide) != len(df_wide):
            log(f"[FAIL] Theta rows mismatch: {len(df_theta_wide)} vs {len(df_wide)} expected")
            sys.exit(1)
        log(f"[PASS] Output row counts correct (items={len(df_params)}, theta={len(df_theta_wide)})")

        log("[SUCCESS] Step 01 complete - All validations passed")
        log(f"[SUMMARY] Pass 1 calibrated {len(df_params)} items for {len(df_theta_wide)} participants")
        log("[NEXT] Run Step 02 to purify items using quality thresholds (a >= 0.4, |b| <= 3.0)")

        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
