#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step00
Step Name: extract_confidence_data
RQ: results/ch6/6.4.1
Generated: 2025-12-07

PURPOSE:
Extract TC_* confidence items from dfData.csv, filter to interactive paradigms
(IFR, ICR, IRE), create 3-factor Q-matrix by PARADIGM (not domain) for IRT
calibration. Implements ADAPTIVE n_cats detection based on actual extracted
data values.

EXPECTED INPUTS:
  - data/cache/dfData.csv
    Columns: UID, TEST, TSVR, TC_* items
    Format: Wide format with paradigm embedded in column names
    Expected rows: 400 (100 participants x 4 test sessions)
    Source: Project-level RAW data

EXPECTED OUTPUTS:
  - data/step00_irt_input.csv
    Columns: composite_ID + ~102 TC_* items (IFR, ICR, IRE paradigms only)
    Format: Wide format for IRT calibration
    Expected rows: 400

  - data/step00_tsvr_mapping.csv
    Columns: composite_ID, UID, test, TSVR_hours
    Format: Time mapping for LMM (Decision D070)
    Expected rows: 400

  - data/step00_q_matrix.csv
    Columns: item_name, factor1_IFR, factor2_ICR, factor3_IRE
    Format: 3-factor structure by paradigm
    Expected rows: ~102 items

VALIDATION CRITERIA:
  - Output files exist: All 3 CSV files created
  - Expected dimensions: irt_input 400 rows x ~103 columns (composite_ID + 102 items)
  - TC_* values: Detect unique non-NaN values, print for Step 01 n_cats setting
  - TSVR range: TSVR_hours in [0, 168] hours
  - Q-matrix dimensions: All 3 paradigms represented (IFR, ICR, IRE)
  - Missing data: NaN acceptable (<10% per item expected)

g_code REASONING:
- Approach: Filter dfData.csv TC_* columns to interactive paradigms, parse
  paradigm tags from column names, create composite_ID, generate Q-matrix BY PARADIGM
- Why this approach: TC_* columns have paradigm encoded in names
  (e.g., TC_IFR-N-i1 = Interactive Free Recall confidence, item 1)
- Data flow: Wide dfData -> filter columns -> parse paradigms -> 3 outputs
- Expected performance: <5 seconds (simple pandas filtering and parsing)
- Key difference from RQ 6.3.1: Q-matrix assigns items BY PARADIGM (IFR/ICR/IRE),
  not by domain (What/Where/When). Each paradigm gets its own factor dimension.

IMPLEMENTATION NOTES:
- Analysis tool: stdlib (pandas, numpy)
- Validation tool: inline (embedded in script)
- Parameters:
  - Paradigm filter: IFR, ICR, IRE (interactive only)
  - Q-matrix encoding: IFR=factor1, ICR=factor2, IRE=factor3
  - ADAPTIVE n_cats: Detect actual unique values, print recommendation for Step 01
- n_cats CLARIFICATION: Raw data has 6 possible values {0.0, 0.2, 0.4, 0.6, 0.8, 1.0},
  but after filtering to IFR/ICR/IRE paradigms, may result in only 5 unique values
  (e.g., no 0.0 values observed). Script will detect and report actual n_cats.
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 6.4.1/
#   parents[2] = ch6/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch6/6.4.1
LOG_FILE = RQ_DIR / "logs" / "step00_extract_confidence_data.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step00_irt_input.csv
#   CORRECT: data/step00_tsvr_mapping.csv
#   WRONG:   results/irt_input.csv  (wrong folder + no prefix)
#   WRONG:   data/irt_input.csv     (missing step prefix)
#   WRONG:   logs/step00_items.csv  (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Paradigm Assignment Function
# =============================================================================

def assign_paradigm_from_tag(item_name: str) -> str:
    """
    Parse paradigm from TC_* column name based on embedded tags.

    Paradigm tags:
      - IFR: Interactive Free Recall
      - ICR: Interactive Cued Recall
      - IRE: Interactive Recognition

    Args:
        item_name: Column name like TC_IFR-N-i1

    Returns:
        Paradigm string: "IFR", "ICR", or "IRE"

    Raises:
        ValueError if paradigm tag not recognized
    """
    if 'IFR' in item_name:
        return "IFR"
    elif 'ICR' in item_name:
        return "ICR"
    elif 'IRE' in item_name:
        return "IRE"
    else:
        raise ValueError(f"Cannot determine paradigm for item: {item_name}")

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 00: Extract Confidence Data (Paradigm-Based)")

        # =========================================================================
        # STEP 1: Load Wide-Format Data
        # =========================================================================
        # Expected: 400 rows (100 participants x 4 test sessions)
        # Purpose: Extract TC_* confidence items for paradigm-based IRT analysis

        log("[LOAD] Loading data/cache/dfData.csv...")
        df_raw = pd.read_csv(PROJECT_ROOT / "data" / "cache" / "dfData.csv")
        log(f"[LOADED] dfData.csv ({len(df_raw)} rows, {len(df_raw.columns)} cols)")

        # Verify required columns exist
        required_cols = ['UID', 'TEST', 'TSVR']
        missing_cols = [col for col in required_cols if col not in df_raw.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")

        log(f"[CHECK] Required columns present: {required_cols}")

        # =========================================================================
        # STEP 2: Filter to Interactive Paradigm TC_* Columns
        # =========================================================================
        # Filter: TC_* columns containing IFR, ICR, or IRE paradigms
        # Expected: ~102 items (34 per paradigm x 3 paradigms)

        log("[FILTER] Filtering to interactive paradigm TC_* columns...")

        # Get all TC_* columns
        tc_cols = [col for col in df_raw.columns if col.startswith('TC_')]
        log(f"[FOUND] {len(tc_cols)} total TC_* columns")

        # Filter to interactive paradigms (IFR, ICR, IRE)
        interactive_paradigms = ['IFR', 'ICR', 'IRE']
        tc_interactive = [col for col in tc_cols
                         if any(paradigm in col for paradigm in interactive_paradigms)]

        log(f"[FILTERED] {len(tc_interactive)} interactive TC_* columns (IFR, ICR, IRE)")
        log(f"[PARADIGMS] IFR: {sum(1 for c in tc_interactive if 'IFR' in c)} items")
        log(f"[PARADIGMS] ICR: {sum(1 for c in tc_interactive if 'ICR' in c)} items")
        log(f"[PARADIGMS] IRE: {sum(1 for c in tc_interactive if 'IRE' in c)} items")

        # =========================================================================
        # STEP 3: Create Composite ID
        # =========================================================================
        # Format: {UID}_T{test_number}
        # Example: A010_T1, A010_T2, A010_T3, A010_T4
        # Purpose: Unique identifier for each participant-session observation

        log("[CREATE] Creating composite_ID from UID and TEST...")

        # Ensure TEST is integer for T1/T2/T3/T4 formatting
        df_raw['test_int'] = df_raw['TEST'].astype(int)
        df_raw['composite_ID'] = df_raw['UID'] + '_T' + df_raw['test_int'].astype(str)

        log(f"[CREATED] composite_ID for {len(df_raw)} observations")
        log(f"[EXAMPLE] First 3 composite_IDs: {df_raw['composite_ID'].head(3).tolist()}")

        # =========================================================================
        # STEP 4: Create IRT Input (Wide Format)
        # =========================================================================
        # Output: composite_ID + TC_* items
        # Expected: 400 rows x ~103 columns (composite_ID + 102 items)

        log("[CREATE] Creating IRT input wide-format DataFrame...")

        irt_input_cols = ['composite_ID'] + tc_interactive
        df_irt_input = df_raw[irt_input_cols].copy()

        log(f"[CREATED] IRT input: {len(df_irt_input)} rows x {len(df_irt_input.columns)} cols")

        # =========================================================================
        # STEP 5: ADAPTIVE n_cats DETECTION
        # =========================================================================
        # Detect actual unique non-NaN values in extracted TC_* items
        # Purpose: Determine correct n_cats parameter for Step 01 IRT calibration
        #
        # Background: Raw dfData.csv has 6 possible TC_* values {0.0, 0.2, 0.4, 0.6, 0.8, 1.0}
        # BUT after filtering to IFR/ICR/IRE paradigms, 0.0 values may not appear
        # (observed in RQ 6.3.1: only 5 categories present after extraction)
        #
        # Detection logic:
        #   - Collect all non-NaN unique values across all TC_* columns
        #   - Sort and report
        #   - Compute n_cats = len(unique_values)
        #   - Print recommendation for Step 01 parameters

        log("[DETECT] Detecting actual unique TC_* values for n_cats determination...")

        all_tc_values = set()
        for col in tc_interactive:
            col_values = df_irt_input[col].dropna().unique()
            all_tc_values.update(col_values)

        # Sort unique values for display
        unique_sorted = sorted(all_tc_values)
        n_cats_detected = len(unique_sorted)

        log(f"[DETECTED] Unique TC_* values found: {unique_sorted}")
        log(f"[DETECTED] Number of categories (n_cats): {n_cats_detected}")

        # Compute number of thresholds (k = n_cats - 1)
        n_thresholds = n_cats_detected - 1
        log(f"[DETECTED] Number of thresholds for GRM: {n_thresholds} (b1 to b{n_thresholds})")

        # Print recommendation for Step 01
        log("")
        log("=" * 80)
        log("RECOMMENDATION FOR STEP 01 (IRT Calibration):")
        log(f"  Set n_cats = {n_cats_detected} in 4_analysis.yaml Step 01 parameters")
        log(f"  This will create {n_thresholds} thresholds: b1, b2, ..., b{n_thresholds}")
        log(f"  Item parameter CSV will have columns: item_name, factor, a, b1, b2, ..., b{n_thresholds}")
        log("=" * 80)
        log("")

        # =========================================================================
        # STEP 6: Create TSVR Time Mapping
        # =========================================================================
        # Output: composite_ID -> TSVR_hours (actual hours since encoding per D070)
        # Purpose: Time variable for LMM analyses (Step 4+)

        log("[CREATE] Creating TSVR time mapping...")

        df_tsvr = df_raw[['composite_ID', 'UID', 'TSVR', 'test_int']].copy()
        df_tsvr.rename(columns={'TSVR': 'TSVR_hours', 'test_int': 'test'}, inplace=True)

        # Convert test to T1/T2/T3/T4 format for consistency
        df_tsvr['test'] = 'T' + df_tsvr['test'].astype(str)

        log(f"[CREATED] TSVR mapping: {len(df_tsvr)} rows")
        log(f"[TSVR RANGE] Min: {df_tsvr['TSVR_hours'].min():.2f} hours, Max: {df_tsvr['TSVR_hours'].max():.2f} hours")

        # =========================================================================
        # STEP 7: Create Q-Matrix (3-Factor Structure BY PARADIGM)
        # =========================================================================
        # Assign each TC_* item to paradigm (IFR/ICR/IRE) based on embedded tags
        # Q-matrix encoding: IFR=factor1, ICR=factor2, IRE=factor3
        # Purpose: Defines factor structure for IRT calibration
        #
        # KEY DIFFERENCE from RQ 6.3.1: Q-matrix assigns by PARADIGM (not domain)
        # RQ 6.3.1: Q-matrix by domain (What/Where/When) for domain-based confidence
        # RQ 6.4.1: Q-matrix by paradigm (IFR/ICR/IRE) for paradigm-based confidence

        log("[CREATE] Creating 3-factor Q-matrix BY PARADIGM...")

        q_matrix_data = []
        for item in tc_interactive:
            paradigm = assign_paradigm_from_tag(item)

            # Create binary loadings for 3-factor structure
            # Each item loads on exactly ONE factor (its paradigm)
            q_matrix_data.append({
                'item_name': item,
                'factor1_IFR': 1 if paradigm == 'IFR' else 0,
                'factor2_ICR': 1 if paradigm == 'ICR' else 0,
                'factor3_IRE': 1 if paradigm == 'IRE' else 0
            })

        df_q_matrix = pd.DataFrame(q_matrix_data)

        log(f"[CREATED] Q-matrix: {len(df_q_matrix)} items")
        log(f"[PARADIGMS] IFR: {df_q_matrix['factor1_IFR'].sum()} items")
        log(f"[PARADIGMS] ICR: {df_q_matrix['factor2_ICR'].sum()} items")
        log(f"[PARADIGMS] IRE: {df_q_matrix['factor3_IRE'].sum()} items")

        # =========================================================================
        # STEP 8: Save Outputs
        # =========================================================================
        # These outputs will be used by:
        #   - step00_irt_input.csv -> Step 1 (IRT Pass 1 calibration)
        #   - step00_tsvr_mapping.csv -> Step 4 (merge with theta scores)
        #   - step00_q_matrix.csv -> Step 1 (factor structure definition)

        log("[SAVE] Saving outputs...")

        # Save IRT input
        irt_input_path = RQ_DIR / "data" / "step00_irt_input.csv"
        df_irt_input.to_csv(irt_input_path, index=False, encoding='utf-8')
        log(f"[SAVED] {irt_input_path.name} ({len(df_irt_input)} rows, {len(df_irt_input.columns)} cols)")

        # Save TSVR mapping
        tsvr_path = RQ_DIR / "data" / "step00_tsvr_mapping.csv"
        df_tsvr.to_csv(tsvr_path, index=False, encoding='utf-8')
        log(f"[SAVED] {tsvr_path.name} ({len(df_tsvr)} rows, {len(df_tsvr.columns)} cols)")

        # Save Q-matrix
        q_matrix_path = RQ_DIR / "data" / "step00_q_matrix.csv"
        df_q_matrix.to_csv(q_matrix_path, index=False, encoding='utf-8')
        log(f"[SAVED] {q_matrix_path.name} ({len(df_q_matrix)} rows, {len(df_q_matrix.columns)} cols)")

        # =========================================================================
        # STEP 9: Validation
        # =========================================================================
        # Validate outputs meet expected criteria from 4_analysis.yaml

        log("[VALIDATION] Running inline validation checks...")

        validation_passed = True

        # Check 1: All output files exist
        if not irt_input_path.exists() or not tsvr_path.exists() or not q_matrix_path.exists():
            log("[FAIL] Not all output files created")
            validation_passed = False
        else:
            log("[PASS] All 3 output files exist")

        # Check 2: Expected dimensions for IRT input
        if len(df_irt_input) != 400:
            log(f"[FAIL] IRT input rows: expected 400, got {len(df_irt_input)}")
            validation_passed = False
        else:
            log(f"[PASS] IRT input dimensions: 400 rows x {len(df_irt_input.columns)} cols")

        # Check 3: TSVR_hours range (0 to 168 hours = 7 days)
        tsvr_min = df_tsvr['TSVR_hours'].min()
        tsvr_max = df_tsvr['TSVR_hours'].max()
        if tsvr_min < 0 or tsvr_max > 168:
            log(f"[FAIL] TSVR_hours range: [{tsvr_min:.2f}, {tsvr_max:.2f}] outside [0, 168]")
            validation_passed = False
        else:
            log(f"[PASS] TSVR_hours range: [{tsvr_min:.2f}, {tsvr_max:.2f}] within [0, 168]")

        # Check 4: All 3 paradigms represented in Q-matrix
        paradigms_ifr = df_q_matrix['factor1_IFR'].sum()
        paradigms_icr = df_q_matrix['factor2_ICR'].sum()
        paradigms_ire = df_q_matrix['factor3_IRE'].sum()

        if paradigms_ifr == 0 or paradigms_icr == 0 or paradigms_ire == 0:
            log(f"[FAIL] Q-matrix missing paradigm representation: IFR={paradigms_ifr}, ICR={paradigms_icr}, IRE={paradigms_ire}")
            validation_passed = False
        else:
            log(f"[PASS] Q-matrix has all 3 paradigms (IFR={paradigms_ifr}, ICR={paradigms_icr}, IRE={paradigms_ire})")

        # Check 5: Each item loads on exactly ONE factor
        row_sums = df_q_matrix[['factor1_IFR', 'factor2_ICR', 'factor3_IRE']].sum(axis=1)
        if not (row_sums == 1).all():
            log(f"[FAIL] Some items load on multiple factors (expected 1 per item)")
            validation_passed = False
        else:
            log(f"[PASS] Each item loads on exactly one factor")

        # Check 6: Missing data report (informational)
        missing_pct_per_item = df_irt_input[tc_interactive].isnull().mean() * 100
        high_missing_items = missing_pct_per_item[missing_pct_per_item > 10].sort_values(ascending=False)

        if len(high_missing_items) > 0:
            log(f"[WARN] {len(high_missing_items)} items have >10% missing data:")
            for item, pct in high_missing_items.head(10).items():
                log(f"  - {item}: {pct:.1f}% missing")
        else:
            log(f"[PASS] All items have <10% missing data")

        # Overall validation result
        if validation_passed:
            log("[VALIDATION] All critical checks PASSED")
        else:
            log("[VALIDATION] Some checks FAILED - see details above")
            sys.exit(1)

        log("[SUCCESS] Step 00 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
