#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step00
Step Name: extract_confidence_data
RQ: results/ch6/6.3.1
Generated: 2025-12-07

PURPOSE:
Extract TC_* confidence items from dfData.csv, filter to interactive paradigms
(IFR, ICR, IRE), assign domains based on embedded tags, create 3-factor Q-matrix
for IRT calibration.

EXPECTED INPUTS:
  - data/cache/dfData.csv
    Columns: UID, TEST, TSVR, TC_* items (105 total, 72 interactive)
    Format: Wide format with paradigm embedded in column names
    Expected rows: 400 (100 participants x 4 test sessions)
    Source: Project-level RAW data

EXPECTED OUTPUTS:
  - data/step00_irt_input.csv
    Columns: composite_ID + ~72 TC_* items (IFR, ICR, IRE paradigms only)
    Format: Wide format for IRT calibration
    Expected rows: 400

  - data/step00_tsvr_mapping.csv
    Columns: composite_ID, TSVR_hours, test
    Format: Time mapping for LMM (Decision D070)
    Expected rows: 400

  - data/step00_q_matrix.csv
    Columns: item_name, dimension, domain
    Format: 3-factor structure (What=1, Where=2, When=3)
    Expected rows: ~72 items

VALIDATION CRITERIA:
  - Output files exist: All 3 CSV files created
  - Expected dimensions: irt_input 400 rows x ~73 columns
  - TC_* values: All values in {0.0, 0.25, 0.5, 0.75, 1.0}
  - TSVR range: TSVR_hours in [0, 168] hours
  - Q-matrix dimensions: All 3 domains represented (What, Where, When)
  - Missing data: NaN acceptable (<10% per item expected)

g_code REASONING:
- Approach: Filter dfData.csv TC_* columns to interactive paradigms, parse
  domain tags from column names, create composite_ID, generate Q-matrix
- Why this approach: TC_* columns have paradigm/domain encoded in names
  (e.g., TC_IFR-N-i1 = Interactive Free Recall, What domain, item 1)
- Data flow: Wide dfData -> filter columns -> parse domains -> 3 outputs
- Expected performance: <5 seconds (simple pandas filtering and parsing)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib (pandas, numpy)
- Validation tool: inline (embedded in script)
- Parameters:
  - Paradigm filter: IFR, ICR, IRE (interactive only)
  - Domain tags: What=-N-, Where=-U-/-D-/-L-, When=-O-
  - Q-matrix encoding: What=1, Where=2, When=3
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 6.3.1/
#   parents[2] = ch6/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch6/6.3.1
LOG_FILE = RQ_DIR / "logs" / "step00_extract_confidence_data.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step00_irt_input.csv
#   CORRECT: data/step00_tsvr_mapping.csv
#   WRONG:   results/irt_input.csv  (wrong folder + no prefix)
#   WRONG:   data/irt_input.csv     (missing step prefix)
#   WRONG:   logs/step00_items.csv  (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Domain Assignment Function
# =============================================================================

def assign_domain_from_tag(item_name: str) -> str:
    """
    Parse domain from TC_* column name based on embedded tags.

    Domain tags:
      - What: -N- (object identity)
      - Where: -U- (up), -D- (down), -L- (lateral spatial)
      - When: -O- (temporal order)

    Args:
        item_name: Column name like TC_IFR-N-i1

    Returns:
        Domain string: "What", "Where", or "When"

    Raises:
        ValueError if domain tag not recognized
    """
    if '-N-' in item_name:
        return "What"
    elif '-U-' in item_name or '-D-' in item_name or '-L-' in item_name:
        return "Where"
    elif '-O-' in item_name:
        return "When"
    else:
        raise ValueError(f"Cannot determine domain for item: {item_name}")

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 00: Extract Confidence Data")

        # =========================================================================
        # STEP 1: Load Wide-Format Data
        # =========================================================================
        # Expected: 400 rows (100 participants x 4 test sessions)
        # Purpose: Extract TC_* confidence items for IRT analysis

        log("[LOAD] Loading data/cache/dfData.csv...")
        df_raw = pd.read_csv(PROJECT_ROOT / "data" / "cache" / "dfData.csv")
        log(f"[LOADED] dfData.csv ({len(df_raw)} rows, {len(df_raw.columns)} cols)")

        # Verify required columns exist
        required_cols = ['UID', 'TEST', 'TSVR']
        missing_cols = [col for col in required_cols if col not in df_raw.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")

        log(f"[CHECK] Required columns present: {required_cols}")

        # =========================================================================
        # STEP 2: Filter to Interactive Paradigm TC_* Columns
        # =========================================================================
        # Filter: TC_* columns containing IFR, ICR, or IRE paradigms
        # Expected: 72 items (24 per paradigm x 3 paradigms)

        log("[FILTER] Filtering to interactive paradigm TC_* columns...")

        # Get all TC_* columns
        tc_cols = [col for col in df_raw.columns if col.startswith('TC_')]
        log(f"[FOUND] {len(tc_cols)} total TC_* columns")

        # Filter to interactive paradigms (IFR, ICR, IRE)
        interactive_paradigms = ['IFR', 'ICR', 'IRE']
        tc_interactive = [col for col in tc_cols
                         if any(paradigm in col for paradigm in interactive_paradigms)]

        log(f"[FILTERED] {len(tc_interactive)} interactive TC_* columns (IFR, ICR, IRE)")
        log(f"[PARADIGMS] IFR: {sum(1 for c in tc_interactive if 'IFR' in c)} items")
        log(f"[PARADIGMS] ICR: {sum(1 for c in tc_interactive if 'ICR' in c)} items")
        log(f"[PARADIGMS] IRE: {sum(1 for c in tc_interactive if 'IRE' in c)} items")

        # =========================================================================
        # STEP 3: Create Composite ID
        # =========================================================================
        # Format: {UID}_T{test_number}
        # Example: A010_T1, A010_T2, A010_T3, A010_T4
        # Purpose: Unique identifier for each participant-session observation

        log("[CREATE] Creating composite_ID from UID and TEST...")

        # Ensure TEST is integer for T1/T2/T3/T4 formatting
        df_raw['test_int'] = df_raw['TEST'].astype(int)
        df_raw['composite_ID'] = df_raw['UID'] + '_T' + df_raw['test_int'].astype(str)

        log(f"[CREATED] composite_ID for {len(df_raw)} observations")
        log(f"[EXAMPLE] First 3 composite_IDs: {df_raw['composite_ID'].head(3).tolist()}")

        # =========================================================================
        # STEP 4: Create IRT Input (Wide Format)
        # =========================================================================
        # Output: composite_ID + TC_* items
        # Expected: 400 rows x ~73 columns (composite_ID + 72 items)

        log("[CREATE] Creating IRT input wide-format DataFrame...")

        irt_input_cols = ['composite_ID'] + tc_interactive
        df_irt_input = df_raw[irt_input_cols].copy()

        log(f"[CREATED] IRT input: {len(df_irt_input)} rows x {len(df_irt_input.columns)} cols")

        # =========================================================================
        # STEP 5: Create TSVR Time Mapping
        # =========================================================================
        # Output: composite_ID -> TSVR_hours (actual hours since encoding per D070)
        # Purpose: Time variable for LMM analyses (Step 4+)

        log("[CREATE] Creating TSVR time mapping...")

        df_tsvr = df_raw[['composite_ID', 'TSVR', 'test_int']].copy()
        df_tsvr.rename(columns={'TSVR': 'TSVR_hours', 'test_int': 'test'}, inplace=True)

        # Convert test to T1/T2/T3/T4 format for consistency
        df_tsvr['test'] = 'T' + df_tsvr['test'].astype(str)

        log(f"[CREATED] TSVR mapping: {len(df_tsvr)} rows")
        log(f"[TSVR RANGE] Min: {df_tsvr['TSVR_hours'].min():.2f} hours, Max: {df_tsvr['TSVR_hours'].max():.2f} hours")

        # =========================================================================
        # STEP 6: Create Q-Matrix (3-Factor Structure)
        # =========================================================================
        # Assign each TC_* item to domain (What/Where/When) based on embedded tags
        # Q-matrix encoding: What=1, Where=2, When=3
        # Purpose: Defines factor structure for IRT calibration

        log("[CREATE] Creating 3-factor Q-matrix...")

        q_matrix_data = []
        for item in tc_interactive:
            domain = assign_domain_from_tag(item)

            # Map domain to dimension number
            dimension_map = {'What': 1, 'Where': 2, 'When': 3}
            dimension = dimension_map[domain]

            q_matrix_data.append({
                'item_name': item,
                'dimension': dimension,
                'domain': domain
            })

        df_q_matrix = pd.DataFrame(q_matrix_data)

        log(f"[CREATED] Q-matrix: {len(df_q_matrix)} items")
        log(f"[DOMAINS] What: {(df_q_matrix['domain'] == 'What').sum()} items")
        log(f"[DOMAINS] Where: {(df_q_matrix['domain'] == 'Where').sum()} items")
        log(f"[DOMAINS] When: {(df_q_matrix['domain'] == 'When').sum()} items")

        # =========================================================================
        # STEP 7: Save Outputs
        # =========================================================================
        # These outputs will be used by:
        #   - step00_irt_input.csv -> Step 1 (IRT Pass 1 calibration)
        #   - step00_tsvr_mapping.csv -> Step 4 (merge with theta scores)
        #   - step00_q_matrix.csv -> Step 1 (factor structure definition)

        log("[SAVE] Saving outputs...")

        # Save IRT input
        irt_input_path = RQ_DIR / "data" / "step00_irt_input.csv"
        df_irt_input.to_csv(irt_input_path, index=False, encoding='utf-8')
        log(f"[SAVED] {irt_input_path.name} ({len(df_irt_input)} rows, {len(df_irt_input.columns)} cols)")

        # Save TSVR mapping
        tsvr_path = RQ_DIR / "data" / "step00_tsvr_mapping.csv"
        df_tsvr.to_csv(tsvr_path, index=False, encoding='utf-8')
        log(f"[SAVED] {tsvr_path.name} ({len(df_tsvr)} rows, {len(df_tsvr.columns)} cols)")

        # Save Q-matrix
        q_matrix_path = RQ_DIR / "data" / "step00_q_matrix.csv"
        df_q_matrix.to_csv(q_matrix_path, index=False, encoding='utf-8')
        log(f"[SAVED] {q_matrix_path.name} ({len(df_q_matrix)} rows, {len(df_q_matrix.columns)} cols)")

        # =========================================================================
        # STEP 8: Validation
        # =========================================================================
        # Validate outputs meet expected criteria from 4_analysis.yaml

        log("[VALIDATION] Running inline validation checks...")

        validation_passed = True

        # Check 1: All output files exist
        if not irt_input_path.exists() or not tsvr_path.exists() or not q_matrix_path.exists():
            log("[FAIL] Not all output files created")
            validation_passed = False
        else:
            log("[PASS] All 3 output files exist")

        # Check 2: Expected dimensions for IRT input
        if len(df_irt_input) != 400:
            log(f"[FAIL] IRT input rows: expected 400, got {len(df_irt_input)}")
            validation_passed = False
        else:
            log(f"[PASS] IRT input dimensions: 400 rows x {len(df_irt_input.columns)} cols")

        # Check 3: TC_* values are ordinal (0, 0.25, 0.5, 0.75, 1.0)
        expected_values = {0.0, 0.25, 0.5, 0.75, 1.0}
        tc_values = set()
        for col in tc_interactive:
            col_values = df_irt_input[col].dropna().unique()
            tc_values.update(col_values)

        # Allow NaN, check non-NaN values are in expected set
        unexpected_values = tc_values - expected_values
        if unexpected_values:
            log(f"[FAIL] Unexpected TC_* values: {unexpected_values}")
            validation_passed = False
        else:
            log(f"[PASS] All TC_* values in {{0.0, 0.25, 0.5, 0.75, 1.0}}")

        # Check 4: TSVR_hours range (0 to 168 hours = 7 days)
        tsvr_min = df_tsvr['TSVR_hours'].min()
        tsvr_max = df_tsvr['TSVR_hours'].max()
        if tsvr_min < 0 or tsvr_max > 168:
            log(f"[FAIL] TSVR_hours range: [{tsvr_min:.2f}, {tsvr_max:.2f}] outside [0, 168]")
            validation_passed = False
        else:
            log(f"[PASS] TSVR_hours range: [{tsvr_min:.2f}, {tsvr_max:.2f}] within [0, 168]")

        # Check 5: All 3 dimensions represented in Q-matrix
        dimensions_present = df_q_matrix['dimension'].unique()
        if set(dimensions_present) != {1, 2, 3}:
            log(f"[FAIL] Q-matrix dimensions: expected {{1, 2, 3}}, got {set(dimensions_present)}")
            validation_passed = False
        else:
            log(f"[PASS] Q-matrix has all 3 dimensions (What, Where, When)")

        # Check 6: Missing data report (informational)
        missing_pct_per_item = df_irt_input[tc_interactive].isnull().mean() * 100
        high_missing_items = missing_pct_per_item[missing_pct_per_item > 10].sort_values(ascending=False)

        if len(high_missing_items) > 0:
            log(f"[WARN] {len(high_missing_items)} items have >10% missing data:")
            for item, pct in high_missing_items.head(10).items():
                log(f"  - {item}: {pct:.1f}% missing")
        else:
            log(f"[PASS] All items have <10% missing data")

        # Overall validation result
        if validation_passed:
            log("[VALIDATION] All critical checks PASSED")
        else:
            log("[VALIDATION] Some checks FAILED - see details above")
            sys.exit(1)

        log("[SUCCESS] Step 00 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
