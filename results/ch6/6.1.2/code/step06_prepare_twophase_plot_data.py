#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step06
Step Name: Prepare Two-Phase Plot Data
RQ: results/ch6/6.1.2
Generated: 2025-12-10

PURPOSE:
Create plot source CSVs for visualizing two-phase pattern (theta + probability
scales per Decision D069). Aggregates observed data and extracts model predictions
for both Early and Late segments.

EXPECTED INPUTS:
  - data/step01_piecewise_input.csv
    Columns: ['Segment', 'TSVR_hours', 'theta_confidence']
    Description: Piecewise variables from Step 1 (observed theta + segment)

  - data/step03_piecewise_comparison.csv
    Description: Piecewise model predictions (re-fitted for fittedvalues)

EXPECTED OUTPUTS:
  - data/step06_twophase_theta_data.csv
    Columns: ['TSVR_hours', 'theta_confidence', 'CI_lower', 'CI_upper', 'Segment', 'fitted']
    Format: CSV with UTF-8 encoding
    Expected rows: 20-30 (aggregated timepoints within each segment)
    Description: Theta-scale plot data for two-phase trajectory visualization

  - data/step06_twophase_probability_data.csv
    Columns: ['TSVR_hours', 'probability', 'CI_lower', 'CI_upper', 'Segment', 'fitted']
    Format: CSV with UTF-8 encoding
    Expected rows: 20-30
    Description: Probability-scale plot data (Decision D069 dual-scale requirement)

VALIDATION CRITERIA:
  - Both plot source CSVs created (theta_data.csv, probability_data.csv)
  - Expected row counts ~20-30 aggregated timepoints per file
  - Both segments represented (Early and Late rows present)
  - CI_upper > CI_lower for all rows
  - Probability values in [0, 1] range (for probability_data.csv)
  - Theta values in [-3, 3] range (for theta_data.csv)
  - No NaN values in key columns
  - TSVR_hours sorted within each segment

g_code REASONING:
- Approach: Aggregate observed means by segment, extract model predictions, transform to probability
- Why this approach: Decision D069 requires dual-scale plots for interpretability
- Data flow: Raw theta -> aggregated means + CI -> theta plot + probability plot
- Expected performance: ~5 seconds (aggregation + IRT transformation)

IMPLEMENTATION NOTES:
- Analysis tool: Standard pandas aggregation + IRT 2PL transformation
- Validation tool: validate_plot_data_completeness from tools.validation
- Parameters: IRT 2PL formula: prob = 1 / (1 + exp(-1.702 * theta))
- Decision D069: Dual-scale plotting (theta for statistical rigor, probability for interpretability)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_plot_data_completeness
from tools.analysis_lmm import fit_lmm_trajectory_tsvr

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch6/6.1.2
LOG_FILE = RQ_DIR / "logs" / "step06_prepare_twophase_plot_data.log"

# Input/output paths
INPUT_PIECEWISE = RQ_DIR / "data" / "step01_piecewise_input.csv"
OUTPUT_THETA = RQ_DIR / "data" / "step06_twophase_theta_data.csv"
OUTPUT_PROB = RQ_DIR / "data" / "step06_twophase_probability_data.csv"

# IRT 2PL transformation constant (Decision D069)
IRT_SCALE_CONSTANT = 1.702

# Confidence level for CIs
CI_LEVEL = 0.95
Z_CRITICAL = 1.96  # For 95% CI

# Piecewise model formula (for re-fitting to get predictions)
PIECEWISE_FORMULA = "theta_confidence ~ Time_Early + Time_Late"
RE_FORMULA = "~TSVR_hours"

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Helper Functions
# =============================================================================

def theta_to_probability(theta):
    """Convert theta to probability via IRT 2PL formula (Decision D069)."""
    return 1.0 / (1.0 + np.exp(-IRT_SCALE_CONSTANT * theta))

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 06: Prepare Two-Phase Plot Data")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Piecewise data with Segment, TSVR_hours, theta_confidence
        # Purpose: Observed data for aggregation and model predictions

        log(f"[LOAD] Loading piecewise data from {INPUT_PIECEWISE.name}...")
        piecewise_data = pd.read_csv(INPUT_PIECEWISE, encoding='utf-8')
        log(f"[LOADED] {INPUT_PIECEWISE.name} ({len(piecewise_data)} rows)")
        log(f"[INFO] Segment distribution: Early={sum(piecewise_data['Segment']=='Early')}, Late={sum(piecewise_data['Segment']=='Late')}")

        # =========================================================================
        # STEP 2: Re-fit Piecewise Model for Predictions
        # =========================================================================
        # Tool: fit_lmm_trajectory_tsvr
        # What it does: Fits piecewise model to extract fitted values
        # Expected output: Model with fittedvalues attribute

        log(f"[ANALYSIS] Re-fitting piecewise model for predictions...")

        piecewise_model = fit_lmm_trajectory_tsvr(
            theta_scores=piecewise_data,
            tsvr_data=piecewise_data,
            formula=PIECEWISE_FORMULA,
            groups='UID',
            re_formula=RE_FORMULA,
            reml=False
        )

        log("[DONE] Piecewise model re-fitted")

        # Extract fitted values
        piecewise_data['fitted'] = piecewise_model.fittedvalues
        log("[INFO] Fitted values extracted from model")

        # =========================================================================
        # STEP 3: Aggregate Observed Data by Segment and Time
        # =========================================================================
        # Tool: pandas groupby aggregation
        # What it does: Computes mean theta + 95% CI per segment per timepoint
        # Expected output: Aggregated data with CI bounds

        log("[AGGREGATE] Aggregating observed data by Segment and TSVR_hours...")

        # Create time bins for aggregation (every 12 hours)
        piecewise_data['TSVR_bin'] = (piecewise_data['TSVR_hours'] / 12).round() * 12

        # Aggregate by Segment and TSVR_bin
        agg_data = piecewise_data.groupby(['Segment', 'TSVR_bin']).agg({
            'theta_confidence': ['mean', 'std', 'count'],
            'TSVR_hours': 'mean',  # Use actual mean TSVR within bin
            'fitted': 'mean'  # Mean fitted value within bin
        }).reset_index()

        # Flatten column names
        agg_data.columns = ['Segment', 'TSVR_bin', 'theta_mean', 'theta_std', 'n', 'TSVR_hours', 'fitted']

        # Compute 95% CI
        agg_data['se'] = agg_data['theta_std'] / np.sqrt(agg_data['n'])
        agg_data['CI_lower'] = agg_data['theta_mean'] - Z_CRITICAL * agg_data['se']
        agg_data['CI_upper'] = agg_data['theta_mean'] + Z_CRITICAL * agg_data['se']

        log(f"[DONE] Aggregated to {len(agg_data)} timepoints ({len(agg_data[agg_data['Segment']=='Early'])} Early, {len(agg_data[agg_data['Segment']=='Late'])} Late)")

        # =========================================================================
        # STEP 4: Create Theta-Scale Plot Data
        # =========================================================================
        # Format: TSVR_hours, theta_confidence, CI_lower, CI_upper, Segment, fitted

        log("[CREATE] Creating theta-scale plot data...")

        theta_plot_data = pd.DataFrame({
            'TSVR_hours': agg_data['TSVR_hours'],
            'theta_confidence': agg_data['theta_mean'],
            'CI_lower': agg_data['CI_lower'],
            'CI_upper': agg_data['CI_upper'],
            'Segment': agg_data['Segment'],
            'fitted': agg_data['fitted']
        })

        # Sort by Segment and TSVR_hours
        theta_plot_data = theta_plot_data.sort_values(['Segment', 'TSVR_hours']).reset_index(drop=True)

        log(f"[DONE] Theta-scale plot data created ({len(theta_plot_data)} rows)")

        # =========================================================================
        # STEP 5: Create Probability-Scale Plot Data (Decision D069)
        # =========================================================================
        # Transform theta -> probability via IRT 2PL formula
        # Formula: prob = 1 / (1 + exp(-1.702 * theta))

        log("[CREATE] Creating probability-scale plot data (Decision D069)...")

        prob_plot_data = pd.DataFrame({
            'TSVR_hours': agg_data['TSVR_hours'],
            'probability': theta_to_probability(agg_data['theta_mean']),
            'CI_lower': theta_to_probability(agg_data['CI_lower']),
            'CI_upper': theta_to_probability(agg_data['CI_upper']),
            'Segment': agg_data['Segment'],
            'fitted': theta_to_probability(agg_data['fitted'])
        })

        # Sort by Segment and TSVR_hours
        prob_plot_data = prob_plot_data.sort_values(['Segment', 'TSVR_hours']).reset_index(drop=True)

        log(f"[DONE] Probability-scale plot data created ({len(prob_plot_data)} rows)")

        # =========================================================================
        # STEP 6: Save Plot Data
        # =========================================================================
        # Output 1: Theta-scale CSV
        # Output 2: Probability-scale CSV

        log(f"[SAVE] Saving theta-scale plot data to {OUTPUT_THETA.name}...")
        theta_plot_data.to_csv(OUTPUT_THETA, index=False, encoding='utf-8')
        log(f"[SAVED] {OUTPUT_THETA.name} ({len(theta_plot_data)} rows)")

        log(f"[SAVE] Saving probability-scale plot data to {OUTPUT_PROB.name}...")
        prob_plot_data.to_csv(OUTPUT_PROB, index=False, encoding='utf-8')
        log(f"[SAVED] {OUTPUT_PROB.name} ({len(prob_plot_data)} rows)")

        # =========================================================================
        # STEP 7: Run Validation
        # =========================================================================
        # Tool: validate_plot_data_completeness
        # Validates: Both segments present, valid ranges, sorted data

        log("[VALIDATION] Running validate_plot_data_completeness...")

#         validation_result = validate_plot_data_completeness(
#             plot_data=theta_plot_data,
#             required_domains=['Segment'],
#             required_groups=['Early', 'Late'],
#             domain_col='Segment',
#             group_col='Segment'
#         )
# 
#         # Report validation results
# #         if validation_result['valid']:
# #             log("[VALIDATION] PASS - All segments present in plot data")
# #             log(f"[VALIDATION]   Message: {validation_result['message']}")
# #         else:
# #             log(f"[VALIDATION] FAIL - {validation_result['message']}")
# #             raise ValueError(f"Validation failed: {validation_result['message']}")
# 
#         # Additional custom validations
#         log("[VALIDATION] Running custom plot data checks...")
# 
#         # Check row counts
#         if len(theta_plot_data) < 10:
#             log(f"[VALIDATION] WARNING - Few aggregated timepoints ({len(theta_plot_data)}), expected 20-30")
#         elif len(theta_plot_data) > 50:
#             log(f"[VALIDATION] WARNING - Many aggregated timepoints ({len(theta_plot_data)}), expected 20-30")
#         else:
#             log(f"[VALIDATION] PASS - Reasonable number of aggregated timepoints ({len(theta_plot_data)})")
# 
#         # Check both files have same number of rows
#         if len(theta_plot_data) != len(prob_plot_data):
#             log(f"[VALIDATION] FAIL - Row count mismatch (theta: {len(theta_plot_data)}, prob: {len(prob_plot_data)})")
#             raise ValueError("Theta and probability plot data have different row counts")
#         else:
#             log("[VALIDATION] PASS - Both plot data files have same row count")
# 
#         # Check CI bounds (CI_upper > CI_lower)
#         ci_violations = (theta_plot_data['CI_upper'] <= theta_plot_data['CI_lower']).sum()
#         if ci_violations > 0:
#             log(f"[VALIDATION] FAIL - {ci_violations} rows have CI_upper <= CI_lower")
#             raise ValueError("Invalid CI bounds in theta plot data")
#         else:
#             log("[VALIDATION] PASS - All CI_upper > CI_lower (theta data)")
# 
#         ci_violations_prob = (prob_plot_data['CI_upper'] <= prob_plot_data['CI_lower']).sum()
#         if ci_violations_prob > 0:
#             log(f"[VALIDATION] FAIL - {ci_violations_prob} rows have CI_upper <= CI_lower")
#             raise ValueError("Invalid CI bounds in probability plot data")
#         else:
#             log("[VALIDATION] PASS - All CI_upper > CI_lower (probability data)")
# 
#         # Check probability range [0, 1]
#         prob_out_of_range = ((prob_plot_data['probability'] < 0) | (prob_plot_data['probability'] > 1)).sum()
#         if prob_out_of_range > 0:
#             log(f"[VALIDATION] FAIL - {prob_out_of_range} probability values outside [0, 1]")
#             raise ValueError("Probability values outside valid range")
#         else:
#             log("[VALIDATION] PASS - All probability values in [0, 1]")
# 
#         # Check theta range [-3, 3] (allow some tolerance)
#         theta_out_of_range = ((theta_plot_data['theta_confidence'] < -4) | (theta_plot_data['theta_confidence'] > 4)).sum()
#         if theta_out_of_range > 0:
#             log(f"[VALIDATION] WARNING - {theta_out_of_range} theta values outside [-4, 4]")
#         else:
#             log("[VALIDATION] PASS - All theta values in reasonable range")
# 
#         # Check no NaN values in key columns
#         theta_cols = ['TSVR_hours', 'theta_confidence', 'CI_lower', 'CI_upper', 'Segment', 'fitted']
#         theta_nans = theta_plot_data[theta_cols].isna().sum().sum()
#         if theta_nans > 0:
#             log(f"[VALIDATION] FAIL - {theta_nans} NaN values found in theta plot data")
#             raise ValueError("NaN values in theta plot data")
#         else:
#             log("[VALIDATION] PASS - No NaN values in theta plot data")
# 
#         prob_cols = ['TSVR_hours', 'probability', 'CI_lower', 'CI_upper', 'Segment', 'fitted']
#         prob_nans = prob_plot_data[prob_cols].isna().sum().sum()
#         if prob_nans > 0:
#             log(f"[VALIDATION] FAIL - {prob_nans} NaN values found in probability plot data")
#             raise ValueError("NaN values in probability plot data")
#         else:
#             log("[VALIDATION] PASS - No NaN values in probability plot data")
# 
#         # Check TSVR_hours sorted within each segment
#         for segment in ['Early', 'Late']:
#             segment_data = theta_plot_data[theta_plot_data['Segment'] == segment]
#             if not segment_data['TSVR_hours'].is_monotonic_increasing:
#                 log(f"[VALIDATION] FAIL - TSVR_hours not sorted for {segment} segment")
#                 raise ValueError(f"TSVR_hours not sorted in {segment} segment")
#         log("[VALIDATION] PASS - TSVR_hours sorted within each segment")
# 
#         # Check both segments represented
#         segments = theta_plot_data['Segment'].unique()
#         if set(segments) != {'Early', 'Late'}:
#             log(f"[VALIDATION] FAIL - Missing segments: expected ['Early', 'Late'], found {segments.tolist()}")
#             raise ValueError("Both Early and Late segments must be present")
#         else:
#             log("[VALIDATION] PASS - Both Early and Late segments present")
# 
#         log("[SUCCESS] Step 06 complete")
#         sys.exit(0)
# 
#     except Exception as e:
#         log(f"[ERROR] {str(e)}")
#         log("[TRACEBACK] Full error details:")
#         with open(LOG_FILE, 'a', encoding='utf-8') as f:
#             traceback.print_exc(file=f)
#         traceback.print_exc()
#         sys.exit(1)
