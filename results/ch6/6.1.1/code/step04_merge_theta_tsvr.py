#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step04
Step Name: Merge Theta with TSVR
RQ: results/ch6/6.1.1
Generated: 2025-12-06

PURPOSE:
Merge IRT theta scores (confidence ability estimates) with TSVR time variable
and create time transformations for LMM trajectory analysis. Decision D070
specifies TSVR as actual hours since encoding (not nominal days).

EXPECTED INPUTS:
  - data/step03_theta_confidence.csv
    Columns: ['composite_ID', 'theta_All', 'se_All']
    Format: composite_ID format is UID_TEST (e.g., A010_T1)
    Expected rows: ~400 (100 participants x 4 test sessions)

  - data/step00_tsvr_mapping.csv
    Columns: ['composite_ID', 'TSVR_hours', 'test']
    Format: composite_ID format is UID_test (e.g., A010_1)
    Expected rows: ~400 (100 participants x 4 test sessions)

EXPECTED OUTPUTS:
  - data/step04_lmm_input.csv
    Columns: ['composite_ID', 'UID', 'test', 'theta_All', 'se_All',
              'TSVR_hours', 'Days', 'Days_squared', 'log_Days_plus1']
    Format: Long format, one row per observation (UID x test combination)
    Expected rows: ~400 (no data loss from merge)

VALIDATION CRITERIA:
  - 400 rows (no data loss from merge)
  - No NaN in any column
  - TSVR_hours in [0, 250], Days in [0, 11]
  - 100 unique UIDs present
  - All 9 columns present

g_code REASONING:
- Approach: Inner join on composite_ID after standardizing format
- Why this approach: Ensures all theta scores have corresponding TSVR values
- Data flow: theta + TSVR -> merged -> time transformations -> LMM input
- Expected performance: ~seconds (simple data manipulation)
- Format handling: Standardize composite_ID format differences before merge
  (theta uses UID_TEST, tsvr uses UID_test, need to normalize)

IMPLEMENTATION NOTES:
- Analysis tool: pandas stdlib operations (pd.read_csv, pd.merge)
- Validation tool: Manual validation of data quality
- Parameters:
  - Days = TSVR_hours / 24 (continuous time in days)
  - Days_squared = Days^2 (quadratic time term)
  - log_Days_plus1 = log(Days + 1) (logarithmic time, +1 handles Day 0)
- Format normalization: Convert both composite_ID formats to consistent UID_test
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = rqY/
#   parents[2] = chX/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch6/6.1.1
LOG_FILE = RQ_DIR / "logs" / "step04_merge_theta_tsvr.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step04_lmm_input.csv
#   CORRECT: logs/step04_merge_theta_tsvr.log
#   WRONG:   data/lmm_input.csv             (missing step prefix)
#   WRONG:   results/lmm_input.csv          (wrong folder)
#   WRONG:   logs/step04_data.csv           (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 04: Merge Theta with TSVR")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Theta scores from IRT Pass 2 calibration (400 rows)
        #           TSVR mapping from data extraction (400 rows)
        # Purpose: Combine ability estimates with time variable for LMM analysis

        log("[LOAD] Loading theta scores from step03...")
        df_theta = pd.read_csv(RQ_DIR / "data" / "step03_theta_confidence.csv")
        log(f"[LOADED] step03_theta_confidence.csv ({len(df_theta)} rows, {len(df_theta.columns)} cols)")
        log(f"[INFO] Theta columns: {list(df_theta.columns)}")
        log(f"[INFO] Theta composite_ID format example: {df_theta['composite_ID'].iloc[0]}")

        log("[LOAD] Loading TSVR mapping from step00...")
        df_tsvr = pd.read_csv(RQ_DIR / "data" / "step00_tsvr_mapping.csv")
        log(f"[LOADED] step00_tsvr_mapping.csv ({len(df_tsvr)} rows, {len(df_tsvr.columns)} cols)")
        log(f"[INFO] TSVR columns: {list(df_tsvr.columns)}")
        log(f"[INFO] TSVR composite_ID format example: {df_tsvr['composite_ID'].iloc[0]}")

        # =========================================================================
        # STEP 2: Standardize composite_ID Formats
        # =========================================================================
        # Problem: Theta uses UID_TEST (e.g., A010_T1), TSVR uses UID_test (e.g., A010_1)
        # Solution: Standardize TSVR to match Theta format (UID_TEST)
        # Why: Enables direct merge without data loss

        log("[TRANSFORM] Standardizing composite_ID formats...")

        # Convert TSVR test column (1,2,3,4) to TEST format (T1,T2,T3,T4)
        df_tsvr['test_str'] = 'T' + df_tsvr['test'].astype(str)

        # Extract UID from TSVR composite_ID (everything before last underscore)
        df_tsvr['UID'] = df_tsvr['composite_ID'].str.rsplit('_', n=1).str[0]

        # Create standardized composite_ID in Theta format (UID_TEST)
        df_tsvr['composite_ID_standardized'] = df_tsvr['UID'] + '_' + df_tsvr['test_str']

        log(f"[INFO] TSVR standardized composite_ID example: {df_tsvr['composite_ID_standardized'].iloc[0]}")

        # Use standardized composite_ID for merge
        df_tsvr_merge = df_tsvr[['composite_ID_standardized', 'TSVR_hours', 'test_str']].rename(
            columns={'composite_ID_standardized': 'composite_ID', 'test_str': 'test'}
        )

        # =========================================================================
        # STEP 3: Merge Theta with TSVR
        # =========================================================================
        # Merge type: Inner join (expect all rows to match)
        # Expected: 400 rows (no data loss)

        log("[MERGE] Merging theta scores with TSVR mapping...")
        df_merged = pd.merge(
            df_theta,
            df_tsvr_merge,
            on='composite_ID',
            how='inner',
            validate='one_to_one'  # Ensures no duplicate composite_IDs
        )
        log(f"[MERGED] Result: {len(df_merged)} rows, {len(df_merged.columns)} cols")

        # Check for data loss
        if len(df_merged) != len(df_theta):
            log(f"[WARNING] Data loss detected: {len(df_theta)} theta rows -> {len(df_merged)} merged rows")
            missing_theta = set(df_theta['composite_ID']) - set(df_merged['composite_ID'])
            log(f"[WARNING] Missing composite_IDs: {missing_theta}")
        else:
            log("[PASS] No data loss from merge (all theta rows matched)")

        # =========================================================================
        # STEP 4: Extract UID and Create Time Transformations
        # =========================================================================
        # UID: Extracted from composite_ID (before first underscore)
        # Time transformations:
        #   - Days = TSVR_hours / 24 (continuous time in days)
        #   - Days_squared = Days^2 (for quadratic trajectory models)
        #   - log_Days_plus1 = log(Days + 1) (for logarithmic models, +1 handles Day 0)

        log("[TRANSFORM] Extracting UID and creating time transformations...")

        # Extract UID (everything before first underscore)
        df_merged['UID'] = df_merged['composite_ID'].str.split('_').str[0]

        # Create time transformations
        df_merged['Days'] = df_merged['TSVR_hours'] / 24.0
        df_merged['Days_squared'] = df_merged['Days'] ** 2
        df_merged['log_Days_plus1'] = np.log(df_merged['Days'] + 1.0)

        log(f"[INFO] UID count: {df_merged['UID'].nunique()} unique participants")
        log(f"[INFO] TSVR_hours range: [{df_merged['TSVR_hours'].min():.2f}, {df_merged['TSVR_hours'].max():.2f}]")
        log(f"[INFO] Days range: [{df_merged['Days'].min():.2f}, {df_merged['Days'].max():.2f}]")
        log(f"[INFO] Days_squared range: [{df_merged['Days_squared'].min():.2f}, {df_merged['Days_squared'].max():.2f}]")
        log(f"[INFO] log_Days_plus1 range: [{df_merged['log_Days_plus1'].min():.2f}, {df_merged['log_Days_plus1'].max():.2f}]")

        # =========================================================================
        # STEP 5: Reorder Columns and Save Output
        # =========================================================================
        # Output columns (as specified in 4_analysis.yaml):
        #   composite_ID, UID, test, theta_All, se_All,
        #   TSVR_hours, Days, Days_squared, log_Days_plus1

        log("[SAVE] Saving LMM input data...")

        # Select and reorder columns
        output_cols = [
            'composite_ID', 'UID', 'test', 'theta_All', 'se_All',
            'TSVR_hours', 'Days', 'Days_squared', 'log_Days_plus1'
        ]
        df_output = df_merged[output_cols]

        # Save to CSV
        output_path = RQ_DIR / "data" / "step04_lmm_input.csv"
        df_output.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path} ({len(df_output)} rows, {len(df_output.columns)} cols)")

        # =========================================================================
        # STEP 6: Validation
        # =========================================================================
        # Validation criteria from 4_analysis.yaml:
        #   - 400 rows (no data loss)
        #   - No NaN in any column
        #   - TSVR_hours in [0, 250]
        #   - Days in [0, 11]
        #   - 100 unique UIDs

        log("[VALIDATION] Running data quality checks...")

        validation_passed = True

        # Check 1: Row count
        if len(df_output) != 400:
            log(f"[FAIL] Expected 400 rows, got {len(df_output)}")
            validation_passed = False
        else:
            log("[PASS] Row count: 400 (expected)")

        # Check 2: No NaN values
        nan_counts = df_output.isnull().sum()
        if nan_counts.sum() > 0:
            log(f"[FAIL] NaN values detected:")
            for col, count in nan_counts[nan_counts > 0].items():
                log(f"  - {col}: {count} NaN values")
            validation_passed = False
        else:
            log("[PASS] No NaN values in any column")

        # Check 3: TSVR_hours range
        tsvr_min = df_output['TSVR_hours'].min()
        tsvr_max = df_output['TSVR_hours'].max()
        if tsvr_min < 0 or tsvr_max > 250:
            log(f"[FAIL] TSVR_hours out of expected range [0, 250]: [{tsvr_min:.2f}, {tsvr_max:.2f}]")
            validation_passed = False
        else:
            log(f"[PASS] TSVR_hours range: [{tsvr_min:.2f}, {tsvr_max:.2f}] (within [0, 250])")

        # Check 4: Days range
        days_min = df_output['Days'].min()
        days_max = df_output['Days'].max()
        if days_min < 0 or days_max > 11:
            log(f"[FAIL] Days out of expected range [0, 11]: [{days_min:.2f}, {days_max:.2f}]")
            validation_passed = False
        else:
            log(f"[PASS] Days range: [{days_min:.2f}, {days_max:.2f}] (within [0, 11])")

        # Check 5: UID count
        n_uids = df_output['UID'].nunique()
        if n_uids != 100:
            log(f"[FAIL] Expected 100 unique UIDs, got {n_uids}")
            validation_passed = False
        else:
            log(f"[PASS] UID count: {n_uids} unique participants (expected)")

        # Check 6: Column count
        if len(df_output.columns) != 9:
            log(f"[FAIL] Expected 9 columns, got {len(df_output.columns)}")
            validation_passed = False
        else:
            log("[PASS] Column count: 9 (expected)")

        # Final validation result
        if validation_passed:
            log("[SUCCESS] All validation checks passed")
            log("[SUCCESS] Step 04 complete")
            sys.exit(0)
        else:
            log("[ERROR] Validation failed - see above for details")
            sys.exit(1)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
