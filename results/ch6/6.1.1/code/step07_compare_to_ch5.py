#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step07
Step Name: Compare to Ch5 5.1.1 Accuracy Model Selection
RQ: results/ch6/6.1.1
Generated: 2025-12-06

PURPOSE:
Compare confidence functional form (this RQ 6.1.1) to accuracy functional form
(Ch5 RQ 5.1.1). Determines whether confidence and accuracy decline follow the
same or different temporal trajectories.

EXPECTED INPUTS:
  - data/step06_aic_comparison.csv
    Columns: ['model_name', 'AIC', 'delta_AIC', 'relative_likelihood', 'akaike_weight', 'is_best']
    Format: 5 rows (one per model), sorted by AIC ascending
    Expected rows: 5
  - results/ch5/5.1.1/data/step06_aic_comparison.csv (OPTIONAL - soft dependency)
    Columns: ['model_name', 'AIC', 'delta_AIC', 'relative_likelihood', 'akaike_weight', 'is_best']
    Format: 5 rows (one per model), sorted by AIC ascending
    Expected rows: 5
    NOTE: If this file doesn't exist, script will create placeholder output
          with NaN accuracy values and exit successfully (NOT fatal)

EXPECTED OUTPUTS:
  - data/step07_ch5_comparison.csv
    Columns: ['model_name', 'confidence_weight', 'accuracy_weight',
              'weight_difference', 'best_in_confidence', 'best_in_accuracy']
    Format: 5 rows (one per model)
    Expected rows: 5

VALIDATION CRITERIA:
  - Output file exists with expected structure
  - Exactly 5 rows (one per model: Linear, Quadratic, Logarithmic,
    Linear+Logarithmic, Quadratic+Logarithmic)
  - Exactly one best_in_confidence=True
  - If Ch5 exists: Exactly one best_in_accuracy=True
  - If Ch5 exists: confidence_weight sums to 1.0 +/- 0.01
  - If Ch5 exists: accuracy_weight sums to 1.0 +/- 0.01
  - No NaN in confidence columns
  - If Ch5 missing: NaN in accuracy columns (expected)

g_code REASONING:
- Approach: Load confidence model comparison from this RQ, attempt to load
  accuracy model comparison from Ch5 5.1.1. If Ch5 file exists, perform
  cross-RQ comparison of Akaike weights. If Ch5 file missing, create
  placeholder output with NaN accuracy values.

- Why this approach: This is a SOFT DEPENDENCY - Ch5 5.1.1 may not be complete
  when this step runs. Script must handle missing Ch5 file gracefully without
  failing, while still producing valid output structure for downstream steps.

- Data flow: step06_aic_comparison.csv (confidence) -> merge with Ch5
  step06_aic_comparison.csv (accuracy) if available -> comparison table with
  weight differences and best model flags

- Expected performance: ~seconds (pure pandas operations, no statistical computation)

IMPLEMENTATION NOTES:
- Analysis tool: stdlib pandas operations (no custom tool function)
- Validation tool: tools.validation.validate_data_columns
- Parameters: N/A (no tool function to parameterize)
- Soft dependency handling: Try to load Ch5 file, catch FileNotFoundError,
  proceed with NaN placeholders if missing
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 6.1.1/ (RQ directory)
#   parents[2] = ch6/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_data_columns

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch6/6.1.1 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step07_compare_to_ch5.log"
CH5_RQ_DIR = PROJECT_ROOT / "results" / "ch5" / "5.1.1"  # Ch5 RQ 5.1.1 directory

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step07_ch5_comparison.csv
#   WRONG:   results/ch5_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/ch5_comparison.csv     (missing step prefix)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 07: Compare to Ch5 5.1.1 Accuracy Model Selection")

        # =========================================================================
        # STEP 1: Load Confidence Model Comparison (This RQ)
        # =========================================================================
        # Expected: 5 models with Akaike weights from this RQ's LMM analysis
        # Purpose: Confidence decline trajectory model selection results

        log("[LOAD] Loading confidence model comparison from this RQ...")
        confidence_path = RQ_DIR / "data" / "step06_aic_comparison.csv"
        df_confidence = pd.read_csv(confidence_path, encoding='utf-8')
        log(f"[LOADED] {confidence_path.name} ({len(df_confidence)} rows, {len(df_confidence.columns)} cols)")
        log(f"[INFO] Confidence models: {df_confidence['model_name'].tolist()}")

        # Extract best confidence model
        best_confidence = df_confidence[df_confidence['is_best'] == True]['model_name'].values
        if len(best_confidence) == 1:
            log(f"[INFO] Best confidence model: {best_confidence[0]}")
        else:
            log(f"[WARN] Expected 1 best model, found {len(best_confidence)}")

        # =========================================================================
        # STEP 2: Attempt to Load Accuracy Model Comparison (Ch5 5.1.1)
        # =========================================================================
        # Expected: 5 models with Akaike weights from Ch5 RQ 5.1.1
        # Purpose: Accuracy decline trajectory model selection results
        # NOTE: SOFT DEPENDENCY - file may not exist yet

        log("[LOAD] Attempting to load accuracy model comparison from Ch5 5.1.1...")
        accuracy_path = CH5_RQ_DIR / "data" / "step06_aic_comparison.csv"

        ch5_exists = False
        df_accuracy = None

        try:
            if accuracy_path.exists():
                df_accuracy = pd.read_csv(accuracy_path, encoding='utf-8')
                ch5_exists = True
                log(f"[LOADED] {accuracy_path.name} ({len(df_accuracy)} rows, {len(df_accuracy.columns)} cols)")
                log(f"[INFO] Accuracy models: {df_accuracy['model_name'].tolist()}")

                # Extract best accuracy model
                best_accuracy = df_accuracy[df_accuracy['is_best'] == True]['model_name'].values
                if len(best_accuracy) == 1:
                    log(f"[INFO] Best accuracy model: {best_accuracy[0]}")
                else:
                    log(f"[WARN] Expected 1 best model, found {len(best_accuracy)}")
            else:
                log(f"[WARN] Ch5 5.1.1 file not found: {accuracy_path}")
                log(f"[WARN] This is expected if Ch5 5.1.1 hasn't run yet (soft dependency)")
                log(f"[WARN] Will create placeholder output with NaN accuracy values")
        except Exception as e:
            log(f"[WARN] Failed to load Ch5 5.1.1 data: {str(e)}")
            log(f"[WARN] Will create placeholder output with NaN accuracy values")

        # =========================================================================
        # STEP 3: Create Comparison Table
        # =========================================================================
        # What it does: Merge confidence and accuracy model weights, compute differences
        # Expected output: 5 rows with cross-RQ model comparison

        log("[ANALYSIS] Creating comparison table...")

        # Start with confidence data
        comparison = df_confidence[['model_name', 'akaike_weight', 'is_best']].copy()
        comparison.rename(columns={
            'akaike_weight': 'confidence_weight',
            'is_best': 'best_in_confidence'
        }, inplace=True)

        if ch5_exists and df_accuracy is not None:
            # Merge with accuracy data
            accuracy_subset = df_accuracy[['model_name', 'akaike_weight', 'is_best']].copy()
            accuracy_subset.rename(columns={
                'akaike_weight': 'accuracy_weight',
                'is_best': 'best_in_accuracy'
            }, inplace=True)

            comparison = comparison.merge(accuracy_subset, on='model_name', how='left')

            # Compute weight difference
            comparison['weight_difference'] = (
                comparison['confidence_weight'] - comparison['accuracy_weight']
            )

            log(f"[INFO] Merged confidence and accuracy data")
        else:
            # Create placeholder columns with NaN
            comparison['accuracy_weight'] = np.nan
            comparison['weight_difference'] = np.nan
            comparison['best_in_accuracy'] = np.nan

            log(f"[INFO] Created placeholder columns (Ch5 data not available)")

        log("[DONE] Comparison table created")

        # =========================================================================
        # STEP 4: Save Comparison Output
        # =========================================================================
        # Output: data/step07_ch5_comparison.csv
        # Contains: Cross-RQ model weight comparison with difference metrics
        # Downstream usage: Final results report, thesis figure generation

        output_path = RQ_DIR / "data" / "step07_ch5_comparison.csv"
        log(f"[SAVE] Saving {output_path.name}...")
        comparison.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(comparison)} rows, {len(comparison.columns)} cols)")

        # Log comparison summary
        log("[SUMMARY] Model Comparison Results:")
        for _, row in comparison.iterrows():
            conf_best = "[BEST CONF]" if row['best_in_confidence'] else ""
            if ch5_exists:
                acc_best = "[BEST ACC]" if row['best_in_accuracy'] else ""
                log(f"  {row['model_name']:25s} | Conf: {row['confidence_weight']:.4f} | "
                    f"Acc: {row['accuracy_weight']:.4f} | Diff: {row['weight_difference']:+.4f} | "
                    f"{conf_best} {acc_best}")
            else:
                log(f"  {row['model_name']:25s} | Conf: {row['confidence_weight']:.4f} | "
                    f"Acc: N/A | {conf_best}")

        # Determine conclusion
        if ch5_exists:
            best_conf = comparison[comparison['best_in_confidence'] == True]['model_name'].values[0]
            best_acc = comparison[comparison['best_in_accuracy'] == True]['model_name'].values[0]

            if best_conf == best_acc:
                log(f"[CONCLUSION] Confidence PARALLELS accuracy (both best: {best_conf})")
            else:
                log(f"[CONCLUSION] Confidence DIVERGES from accuracy (Conf: {best_conf}, Acc: {best_acc})")
        else:
            best_conf = comparison[comparison['best_in_confidence'] == True]['model_name'].values[0]
            log(f"[CONCLUSION] Confidence best model: {best_conf} (accuracy comparison pending)")

        # =========================================================================
        # STEP 5: Run Validation Tool
        # =========================================================================
        # Tool: validate_data_columns
        # Validates: Required columns present in output
        # Threshold: All required columns must exist

        log("[VALIDATION] Running validate_data_columns...")
        required_columns = [
            'model_name', 'confidence_weight', 'accuracy_weight',
            'weight_difference', 'best_in_confidence', 'best_in_accuracy'
        ]
        validation_result = validate_data_columns(
            df=comparison,
            required_columns=required_columns
        )

        # Report validation results
        if validation_result['valid']:
            log(f"[VALIDATION] PASS - All required columns present")
            log(f"[VALIDATION]   Expected: {validation_result['n_required']} columns")
            log(f"[VALIDATION]   Missing: {validation_result['n_missing']} columns")
        else:
            log(f"[VALIDATION] FAIL - Missing columns detected")
            log(f"[VALIDATION]   Missing: {validation_result['missing_columns']}")
            raise ValueError(f"Validation failed: {validation_result}")

        # Additional validation checks
        log("[VALIDATION] Additional checks...")

        # Check row count
        if len(comparison) != 5:
            log(f"[VALIDATION] FAIL - Expected 5 rows, got {len(comparison)}")
            raise ValueError(f"Expected 5 models, got {len(comparison)}")
        log(f"[VALIDATION] PASS - Row count (5 models)")

        # Check exactly one best_in_confidence
        n_best_conf = comparison['best_in_confidence'].sum()
        if n_best_conf != 1:
            log(f"[VALIDATION] FAIL - Expected 1 best confidence model, got {n_best_conf}")
            raise ValueError(f"Expected 1 best confidence model, got {n_best_conf}")
        log(f"[VALIDATION] PASS - Exactly one best_in_confidence=True")

        # If Ch5 exists, validate accuracy data
        if ch5_exists:
            # Check exactly one best_in_accuracy
            n_best_acc = comparison['best_in_accuracy'].sum()
            if n_best_acc != 1:
                log(f"[VALIDATION] FAIL - Expected 1 best accuracy model, got {n_best_acc}")
                raise ValueError(f"Expected 1 best accuracy model, got {n_best_acc}")
            log(f"[VALIDATION] PASS - Exactly one best_in_accuracy=True")

            # Check confidence weights sum to 1.0
            conf_sum = comparison['confidence_weight'].sum()
            if not (0.99 <= conf_sum <= 1.01):
                log(f"[VALIDATION] FAIL - Confidence weights sum to {conf_sum:.4f} (expected ~1.0)")
                raise ValueError(f"Confidence weights sum to {conf_sum:.4f}")
            log(f"[VALIDATION] PASS - Confidence weights sum to {conf_sum:.4f}")

            # Check accuracy weights sum to 1.0
            acc_sum = comparison['accuracy_weight'].sum()
            if not (0.99 <= acc_sum <= 1.01):
                log(f"[VALIDATION] FAIL - Accuracy weights sum to {acc_sum:.4f} (expected ~1.0)")
                raise ValueError(f"Accuracy weights sum to {acc_sum:.4f}")
            log(f"[VALIDATION] PASS - Accuracy weights sum to {acc_sum:.4f}")
        else:
            # Ch5 missing - validate NaN values are present
            if not comparison['accuracy_weight'].isna().all():
                log(f"[VALIDATION] FAIL - Expected all NaN in accuracy_weight (Ch5 missing)")
                raise ValueError("Expected NaN accuracy values when Ch5 file missing")
            log(f"[VALIDATION] PASS - Accuracy columns contain NaN (Ch5 not available)")

        log("[SUCCESS] Step 07 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
