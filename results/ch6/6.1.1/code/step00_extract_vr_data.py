#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step00
Step Name: Extract VR Data
RQ: results/ch6/6.1.1
Generated: 2025-12-06

PURPOSE:
Extract 5-category ordinal confidence ratings (TC_* items) from dfData.csv
for omnibus 'All' factor IRT analysis. This RQ focuses on confidence judgment
accuracy (subjective awareness of memory quality) rather than objective memory
performance.

EXPECTED INPUTS:
  - data/cache/dfData.csv
    Columns: UID (str), TEST (str), TSVR (float), TC_* (float, 5-category ordinal)
    Format: Long format with UID × TEST × item observations
    Expected rows: ~40,000+ (100 participants × 4 tests × ~102 TC items)
    Filters: Interactive paradigms only (IFR, ICR, IRE); exclude RFR, TCR, RRE

EXPECTED OUTPUTS:
  - data/step00_irt_input.csv
    Columns: composite_ID (str), ~102 TC_* items (float, 5-category ordinal)
    Format: Wide format (one row per composite_ID = UID_TEST)
    Expected rows: 400 (100 participants × 4 tests)

  - data/step00_tsvr_mapping.csv
    Columns: composite_ID (str), TSVR_hours (float), test (str)
    Format: Mapping table for merging time variable into theta scores
    Expected rows: 400

  - data/step00_q_matrix.csv
    Columns: item_name (str), All (int, always 1)
    Format: Q-matrix for omnibus single-factor IRT model
    Expected rows: ~102 items

VALIDATION CRITERIA:
  - Output files exist (irt_input.csv, tsvr_mapping.csv, q_matrix.csv)
  - Expected column counts (irt_input: ~103, tsvr_mapping: 3, q_matrix: 2)
  - Expected row counts (irt_input: 400, tsvr_mapping: 400, q_matrix: ~102)
  - TC_* values in {0, 0.25, 0.5, 0.75, 1.0, NaN} (5-category ordinal + missing)
  - TSVR_hours in [0, 168] (reasonable time range for 1-week study)
  - No fully missing items or participants
  - composite_ID format correct (UID_TEST pattern)

g_code REASONING:
- Approach: Load dfData.csv → filter paradigms → filter TC_* columns →
  pivot to wide format → create composite_ID → extract TSVR mapping →
  create Q-matrix → validate outputs

- Why this approach:
  1. Filter paradigms FIRST (reduces data volume before pivot)
  2. Pivot to wide creates IRT-ready input (composite_ID × items matrix)
  3. Separate TSVR mapping enables clean merge in step04
  4. Q-matrix with single "All" factor = omnibus model (all items load on 1 dimension)

- Data flow:
  dfData.csv (long, ~40k rows)
  → filter interactive paradigms (IFR/ICR/IRE)
  → filter TC_* columns
  → pivot wide (400 rows × ~103 cols)
  → split into 3 outputs (irt_input, tsvr_mapping, q_matrix)

- Expected performance: <10 seconds (pandas operations on ~40k rows)

IMPLEMENTATION NOTES:
- Analysis tool: pandas stdlib operations (no custom tools for data extraction)
- Validation tool: validate_data_columns from tools.validation
- Parameters:
  - Interactive paradigms: IFR, ICR, IRE (exclude room paradigms)
  - TC_* items: 5-category ordinal (0, 0.25, 0.5, 0.75, 1.0)
  - composite_ID format: UID_TEST (e.g., P001_T1)
- Critical: UID format is string "P###" with leading zeros (not integer)
- Critical: TEST format is string "T1/T2/T3/T4" (not integer 1-4)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 6.1.1/
#   parents[2] = ch6/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_data_columns

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch6/6.1.1 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step00_extract_vr_data.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step05_lmm_model_comparison.csv
#   CORRECT: data/step03_theta_scores.csv
#   WRONG:   results/lmm_model_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_scores.csv             (missing step prefix)
#   WRONG:   logs/step02_removed_items.csv     (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 00: Extract VR Data")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: dfData.csv with long-format data (UID × TEST × item observations)
        # Purpose: Extract confidence ratings for IRT analysis

        log("[LOAD] Loading dfData.csv...")
        dfdata_path = PROJECT_ROOT / "data" / "cache" / "dfData.csv"
        df = pd.read_csv(dfdata_path, encoding='utf-8')
        log(f"[LOADED] dfData.csv ({len(df)} rows, {len(df.columns)} cols)")

        # =========================================================================
        # STEP 2: Filter TC_* Columns to Interactive Paradigms
        # =========================================================================
        # dfData.csv is ALREADY in wide format (UID × items)
        # Paradigm is EMBEDDED in column names: TC_{PARADIGM}-{DOMAIN}-{ITEM}
        # Interactive paradigms: IFR, ICR, IRE (pattern: TC_IFR-*, TC_ICR-*, TC_IRE-*)
        # Exclude room paradigms: RFR, TCR, RRE

        log("[FILTER] Identifying TC_* columns (confidence ratings)...")

        # Get all TC_* columns first
        all_tc_columns = [col for col in df.columns if col.startswith('TC_')]
        log(f"[FOUND] {len(all_tc_columns)} total TC_* columns")

        if len(all_tc_columns) == 0:
            raise ValueError("No TC_* columns found in dfData.csv. Check column naming convention")

        # Filter to interactive paradigms (IFR, ICR, IRE) by column name pattern
        # Column format: TC_{PARADIGM}-{DOMAIN}-{ITEM} e.g., TC_IFR-N-i1
        interactive_paradigms = ['IFR', 'ICR', 'IRE']
        tc_columns = []
        for col in all_tc_columns:
            # Extract paradigm from column name (second part after TC_)
            # e.g., TC_IFR-N-i1 -> IFR
            parts = col.replace('TC_', '').split('-')
            if len(parts) >= 1:
                paradigm = parts[0]
                if paradigm in interactive_paradigms:
                    tc_columns.append(col)

        log(f"[FILTERED] {len(tc_columns)} TC_* columns for interactive paradigms (from {len(all_tc_columns)} total)")
        log(f"[PARADIGMS] Interactive: IFR={sum(1 for c in tc_columns if 'IFR' in c)}, "
            f"ICR={sum(1 for c in tc_columns if 'ICR' in c)}, "
            f"IRE={sum(1 for c in tc_columns if 'IRE' in c)}")

        if len(tc_columns) == 0:
            raise ValueError(f"No TC_* columns found for interactive paradigms {interactive_paradigms}")

        # Verify required columns exist
        required_cols = ['UID', 'TEST', 'TSVR']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns in dfData.csv: {missing_cols}")

        # Use df directly (already filtered by column selection)
        df_interactive = df.copy()

        # =========================================================================
        # STEP 4: Create composite_ID (UID_TEST)
        # =========================================================================
        # composite_ID = unique identifier for each UID × TEST combination
        # Format: P001_T1, P001_T2, ..., P100_T4
        # Total: 100 participants × 4 tests = 400 composite_IDs

        log("[CREATE] Creating composite_ID column...")
        df_interactive['composite_ID'] = df_interactive['UID'].astype(str) + '_' + df_interactive['TEST'].astype(str)
        log(f"[CREATED] composite_ID with {df_interactive['composite_ID'].nunique()} unique values")

        # =========================================================================
        # STEP 5: Pivot to Wide Format (composite_ID × TC_* items)
        # =========================================================================
        # IRT expects wide format: one row per person-test, one column per item
        # Result: 400 rows × ~103 columns (composite_ID + ~102 TC_* items)

        log("[PIVOT] Pivoting to wide format (composite_ID x items)...")

        # For each composite_ID, aggregate TC_* values
        # Strategy: For each TC_* column, take the value for that composite_ID
        # (assumes each composite_ID × item combination appears once; if duplicates, take first)

        # Select columns for pivot
        pivot_cols = ['composite_ID', 'UID', 'TEST', 'TSVR'] + tc_columns
        df_pivot = df_interactive[pivot_cols].copy()

        # Group by composite_ID and aggregate (take first non-NaN value for each item)
        df_wide = df_pivot.groupby('composite_ID', as_index=False).first()

        log(f"[PIVOTED] Wide format: {len(df_wide)} rows × {len(df_wide.columns)} cols")

        # Verify expected row count (400 = 100 participants × 4 tests)
        if len(df_wide) != 400:
            log(f"[WARNING] Expected 400 rows (100 participants × 4 tests), got {len(df_wide)}")

        # =========================================================================
        # STEP 6: Create Output 1 - IRT Input (composite_ID × TC_* items)
        # =========================================================================
        # Output: step00_irt_input.csv
        # Contains: composite_ID + all TC_* item columns
        # Purpose: Input for IRT calibration (step01)

        log("[CREATE] Creating IRT input file (composite_ID + TC_* items)...")

        # Select composite_ID + TC_* columns only
        irt_input_cols = ['composite_ID'] + tc_columns
        df_irt_input = df_wide[irt_input_cols].copy()

        # Save to CSV
        irt_input_path = RQ_DIR / "data" / "step00_irt_input.csv"
        df_irt_input.to_csv(irt_input_path, index=False, encoding='utf-8')
        log(f"[SAVED] {irt_input_path.name} ({len(df_irt_input)} rows, {len(df_irt_input.columns)} cols)")

        # =========================================================================
        # STEP 7: Create Output 2 - TSVR Mapping (composite_ID → TSVR_hours)
        # =========================================================================
        # Output: step00_tsvr_mapping.csv
        # Contains: composite_ID, TSVR_hours, test
        # Purpose: Merge with theta scores in step04 to add time variable

        log("[CREATE] Creating TSVR mapping file...")

        # Extract composite_ID, TSVR, TEST
        df_tsvr = df_wide[['composite_ID', 'UID', 'TEST', 'TSVR']].copy()

        # Rename columns for clarity
        df_tsvr = df_tsvr.rename(columns={
            'TSVR': 'TSVR_hours',
            'TEST': 'test'
        })

        # Drop UID (not needed in mapping; composite_ID already contains it)
        df_tsvr = df_tsvr[['composite_ID', 'TSVR_hours', 'test']]

        # Save to CSV
        tsvr_path = RQ_DIR / "data" / "step00_tsvr_mapping.csv"
        df_tsvr.to_csv(tsvr_path, index=False, encoding='utf-8')
        log(f"[SAVED] {tsvr_path.name} ({len(df_tsvr)} rows, {len(df_tsvr.columns)} cols)")

        # =========================================================================
        # STEP 8: Create Output 3 - Q-matrix (Single "All" Factor)
        # =========================================================================
        # Output: step00_q_matrix.csv
        # Contains: item_name, All (1 for all items)
        # Purpose: Specify omnibus single-factor IRT model (all items load on one dimension)

        log("[CREATE] Creating Q-matrix (omnibus 'All' factor)...")

        # Create Q-matrix: all TC_* items load on single "All" factor
        df_qmatrix = pd.DataFrame({
            'item_name': tc_columns,
            'All': 1  # All items load on the "All" factor
        })

        # Save to CSV
        qmatrix_path = RQ_DIR / "data" / "step00_q_matrix.csv"
        df_qmatrix.to_csv(qmatrix_path, index=False, encoding='utf-8')
        log(f"[SAVED] {qmatrix_path.name} ({len(df_qmatrix)} rows, {len(df_qmatrix.columns)} cols)")

        # =========================================================================
        # STEP 9: Run Validation Tool
        # =========================================================================
        # Tool: validate_data_columns
        # Validates: Required columns exist in all 3 output files
        # Threshold: All required columns must be present

        log("[VALIDATION] Validating output files...")

        # Validate irt_input.csv
        validation_result_irt = validate_data_columns(
            df=df_irt_input,
            required_columns=['composite_ID']
        )

        if not validation_result_irt['valid']:
            raise ValueError(f"IRT input validation failed: {validation_result_irt}")

        log(f"[VALIDATION] irt_input.csv: {validation_result_irt['n_required']} required columns present")

        # Validate tsvr_mapping.csv
        validation_result_tsvr = validate_data_columns(
            df=df_tsvr,
            required_columns=['composite_ID', 'TSVR_hours', 'test']
        )

        if not validation_result_tsvr['valid']:
            raise ValueError(f"TSVR mapping validation failed: {validation_result_tsvr}")

        log(f"[VALIDATION] tsvr_mapping.csv: {validation_result_tsvr['n_required']} required columns present")

        # Validate q_matrix.csv
        validation_result_qmatrix = validate_data_columns(
            df=df_qmatrix,
            required_columns=['item_name', 'All']
        )

        if not validation_result_qmatrix['valid']:
            raise ValueError(f"Q-matrix validation failed: {validation_result_qmatrix}")

        log(f"[VALIDATION] q_matrix.csv: {validation_result_qmatrix['n_required']} required columns present")

        # Additional validation checks
        log("[VALIDATION] Running additional data quality checks...")

        # Check TC_* values are in expected range (0, 0.25, 0.5, 0.75, 1.0, or NaN)
        expected_values = {0.0, 0.25, 0.5, 0.75, 1.0}
        for col in tc_columns:
            unique_vals = set(df_irt_input[col].dropna().unique())
            unexpected = unique_vals - expected_values
            if unexpected:
                log(f"[WARNING] Column {col} has unexpected values: {unexpected}")

        # Check TSVR_hours range (0-168 hours = 1 week)
        tsvr_min = df_tsvr['TSVR_hours'].min()
        tsvr_max = df_tsvr['TSVR_hours'].max()
        log(f"[VALIDATION] TSVR_hours range: [{tsvr_min:.2f}, {tsvr_max:.2f}] hours")

        if tsvr_min < 0 or tsvr_max > 168:
            log(f"[WARNING] TSVR_hours outside expected range [0, 168]")

        # Check for fully missing items or participants
        n_missing_per_item = df_irt_input[tc_columns].isna().sum()
        fully_missing_items = n_missing_per_item[n_missing_per_item == len(df_irt_input)]
        if len(fully_missing_items) > 0:
            log(f"[WARNING] {len(fully_missing_items)} items have all missing values: {list(fully_missing_items.index)}")

        n_missing_per_participant = df_irt_input[tc_columns].isna().sum(axis=1)
        fully_missing_participants = df_irt_input[n_missing_per_participant == len(tc_columns)]
        if len(fully_missing_participants) > 0:
            log(f"[WARNING] {len(fully_missing_participants)} participants have all missing TC_* values")

        # Check composite_ID format (UID_TEST pattern)
        invalid_ids = df_irt_input[~df_irt_input['composite_ID'].str.match(r'^P\d{3}_T[1-4]$')]
        if len(invalid_ids) > 0:
            log(f"[WARNING] {len(invalid_ids)} composite_IDs don't match expected format (P###_T#)")
            log(f"[WARNING] Examples: {list(invalid_ids['composite_ID'].head())}")

        log("[VALIDATION] All validation checks complete")

        # Summary statistics
        log("[SUMMARY] Extraction complete:")
        log(f"  - Input: {len(df)} rows from dfData.csv")
        log(f"  - Filtered: {len(df_interactive)} rows (interactive paradigms)")
        log(f"  - TC_* items: {len(tc_columns)}")
        log(f"  - Composite IDs: {len(df_irt_input)}")
        log(f"  - Output files: 3 (irt_input, tsvr_mapping, q_matrix)")

        log("[SUCCESS] Step 00 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
