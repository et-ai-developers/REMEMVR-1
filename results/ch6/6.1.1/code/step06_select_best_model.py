#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step06
Step Name: select_best_model
RQ: results/ch6/6.1.1
Generated: 2025-12-06

PURPOSE:
Compute Akaike weights (model probabilities) from AIC values and identify the
best LMM model for confidence decline trajectory. Uses model comparison framework
from Burnham & Anderson (2002) to quantify relative support for each candidate
functional form.

EXPECTED INPUTS:
  - data/step05_model_comparison.csv
    Columns: ['model_name', 'AIC', 'BIC', 'log_likelihood', 'num_params', 'converged']
    Format: CSV with one row per model (5 models)
    Expected rows: 5
    Source: Step 5 LMM fitting (5 candidate models)

EXPECTED OUTPUTS:
  - data/step06_aic_comparison.csv
    Columns: ['model_name', 'AIC', 'delta_AIC', 'relative_likelihood', 'akaike_weight', 'is_best']
    Format: CSV sorted by AIC ascending (best model first)
    Expected rows: 5
    Purpose: Model selection results with Akaike weights for interpretation

  - data/step06_best_model.pkl
    Format: Pickle file (copy of best model from step05)
    Purpose: Canonical reference to best model for downstream analyses

VALIDATION CRITERIA:
  - Exactly one is_best=True (single best model identified)
  - Akaike weights sum to 1.0 +/- 0.01 (probability distribution)
  - Best model has delta_AIC=0 (by definition)
  - All AIC values finite (not NaN/Inf)
  - delta_AIC in [0, Inf) (non-negative differences)
  - relative_likelihood in (0, 1] (positive probabilities)
  - akaike_weight in (0, 1) (valid probabilities)

g_code REASONING:
- Approach: This step does NOT call compare_lmm_models_by_aic (that was step05).
  Instead, it reads step05 AIC results and computes Akaike weights manually.
  This follows Decision D071 model selection protocol: fit all candidates,
  compare via AIC, select best model based on minimum AIC.

- Why this approach: Akaike weights quantify relative support for each model
  as probabilities. delta_AIC = AIC - min(AIC) measures how much worse each
  model is than the best. relative_likelihood = exp(-0.5 * delta_AIC) converts
  to likelihood scale. akaike_weight = relative_likelihood / sum(all_likelihoods)
  normalizes to probability distribution. This provides interpretable model
  comparison (e.g., "Logarithmic has 0.85 probability of being the best model").

- Data flow: step05_model_comparison.csv (5 models with AIC values) ->
  compute delta_AIC, relative_likelihood, akaike_weight ->
  identify best model (lowest AIC) -> copy best model pkl file ->
  save aic_comparison.csv (sorted by AIC ascending)

- Expected performance: ~1 second (simple arithmetic operations, file I/O)

IMPLEMENTATION NOTES:
- Analysis tool: NOT using compare_lmm_models_by_aic (that was step05)
- This step is PURE COMPUTATION from existing AIC values
- Validation tool: tools.validation.validate_numeric_range
- Parameters: None (reads AIC values, computes weights)
- Model file naming: step05_model3_logarithmic.pkl is expected best model
  based on step05 results (AIC=338.60 lowest among 5 candidates)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import shutil
import traceback

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 6.1.1/ (rqY)
#   parents[2] = ch6/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import validation tool
from tools.validation import validate_numeric_range

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch6/6.1.1 (derived from script location)
LOG_FILE = RQ_DIR / "logs" / "step06_select_best_model.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step06_aic_comparison.csv
#   CORRECT: data/step06_best_model.pkl
#   WRONG:   results/aic_comparison.csv  (wrong folder + no prefix)
#   WRONG:   data/aic_comparison.csv     (missing step prefix)
#   WRONG:   logs/step06_best_model.pkl  (PKL in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 06: Select Best Model via AIC")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Model comparison results from Step 5 (5 candidate models)
        # Purpose: Extract AIC values to compute Akaike weights

        log("[LOAD] Loading model comparison results...")
        input_path = RQ_DIR / "data" / "step05_model_comparison.csv"
        df_models = pd.read_csv(input_path, encoding='utf-8')
        log(f"[LOADED] {input_path.name} ({len(df_models)} rows, {len(df_models.columns)} cols)")

        # Verify expected structure
        expected_models = {"Linear", "Quadratic", "Logarithmic", "Linear+Logarithmic", "Quadratic+Logarithmic"}
        actual_models = set(df_models['model_name'].tolist())
        if actual_models != expected_models:
            log(f"[WARNING] Model names mismatch:")
            log(f"  Expected: {expected_models}")
            log(f"  Actual:   {actual_models}")

        log(f"[INFO] AIC range: {df_models['AIC'].min():.2f} - {df_models['AIC'].max():.2f}")

        # =========================================================================
        # STEP 2: Compute Akaike Weights
        # =========================================================================
        # Tool: Manual computation (not using compare_lmm_models_by_aic - that was step05)
        # What it does: Transform AIC differences to model probabilities
        # Expected output: Akaike weights summing to 1.0

        log("[ANALYSIS] Computing Akaike weights...")

        # Step 2a: Compute delta_AIC (difference from best model)
        min_aic = df_models['AIC'].min()
        df_models['delta_AIC'] = df_models['AIC'] - min_aic
        log(f"[COMPUTED] delta_AIC (minimum AIC = {min_aic:.4f})")

        # Step 2b: Compute relative likelihood
        # Formula: exp(-0.5 * delta_AIC)
        # Interpretation: How likely each model is relative to best model
        df_models['relative_likelihood'] = np.exp(-0.5 * df_models['delta_AIC'])
        log(f"[COMPUTED] relative_likelihood (exponential transformation)")

        # Step 2c: Compute Akaike weights (normalize to probabilities)
        # Formula: relative_likelihood / sum(all relative_likelihoods)
        # Interpretation: Probability that each model is the best model
        sum_likelihoods = df_models['relative_likelihood'].sum()
        df_models['akaike_weight'] = df_models['relative_likelihood'] / sum_likelihoods
        log(f"[COMPUTED] akaike_weight (normalized to sum=1.0)")

        # Step 2d: Identify best model (lowest AIC, highest akaike_weight)
        df_models['is_best'] = df_models['AIC'] == min_aic
        best_model_name = df_models.loc[df_models['is_best'], 'model_name'].iloc[0]
        best_aic = df_models.loc[df_models['is_best'], 'AIC'].iloc[0]
        best_weight = df_models.loc[df_models['is_best'], 'akaike_weight'].iloc[0]

        log(f"[IDENTIFIED] Best model: {best_model_name}")
        log(f"  AIC: {best_aic:.4f}")
        log(f"  Akaike weight: {best_weight:.4f}")

        # Sort by AIC ascending (best model first)
        df_models = df_models.sort_values('AIC', ascending=True).reset_index(drop=True)
        log("[SORTED] Models by AIC ascending (best first)")

        # =========================================================================
        # STEP 3: Save Analysis Outputs
        # =========================================================================
        # These outputs will be used by: Step 7 (Ch5 comparison), rq_results (final report)

        log("[SAVE] Saving AIC comparison table...")
        # Output: step06_aic_comparison.csv
        # Contains: Model names, AIC values, Akaike weights, best model flag
        # Columns: model_name, AIC, delta_AIC, relative_likelihood, akaike_weight, is_best
        output_path = RQ_DIR / "data" / "step06_aic_comparison.csv"
        df_models.to_csv(output_path, index=False, encoding='utf-8')
        log(f"[SAVED] {output_path.name} ({len(df_models)} rows, {len(df_models.columns)} cols)")

        # Print summary table for log
        log("[SUMMARY] AIC Comparison:")
        for _, row in df_models.iterrows():
            best_marker = " <- BEST" if row['is_best'] else ""
            log(f"  {row['model_name']:25s} AIC={row['AIC']:7.2f}  delta={row['delta_AIC']:6.2f}  weight={row['akaike_weight']:.4f}{best_marker}")

        # Copy best model pkl file to canonical name
        log("[SAVE] Copying best model to canonical filename...")

        # Map model names to step05 pkl filenames
        model_file_mapping = {
            "Linear": "step05_model1_linear.pkl",
            "Quadratic": "step05_model2_quadratic.pkl",
            "Logarithmic": "step05_model3_logarithmic.pkl",
            "Linear+Logarithmic": "step05_model4_linear_logarithmic.pkl",
            "Quadratic+Logarithmic": "step05_model5_quadratic_logarithmic.pkl"
        }

        source_pkl = RQ_DIR / "data" / model_file_mapping[best_model_name]
        dest_pkl = RQ_DIR / "data" / "step06_best_model.pkl"

        if not source_pkl.exists():
            log(f"[ERROR] Source model file not found: {source_pkl.name}")
            raise FileNotFoundError(f"Expected source file {source_pkl} does not exist")

        shutil.copy2(source_pkl, dest_pkl)
        log(f"[SAVED] {dest_pkl.name} (copy of {source_pkl.name})")

        # =========================================================================
        # STEP 4: Run Validation Tool
        # =========================================================================
        # Tool: validate_numeric_range
        # Validates: Akaike weights are valid probabilities, delta_AIC non-negative
        # Threshold: Weights sum to 1.0 +/- 0.01

        log("[VALIDATION] Running validate_numeric_range...")

        # Validation 1: Exactly one is_best=True
        n_best = df_models['is_best'].sum()
        if n_best == 1:
            log(f"[VALIDATION] Exactly one best model identified: PASS")
        else:
            log(f"[VALIDATION] Expected 1 best model, found {n_best}: FAIL")
            raise ValueError(f"Expected exactly 1 is_best=True, found {n_best}")

        # Validation 2: Akaike weights sum to 1.0 +/- 0.01
        weight_sum = df_models['akaike_weight'].sum()
        if abs(weight_sum - 1.0) <= 0.01:
            log(f"[VALIDATION] Akaike weights sum to 1.0: PASS (sum={weight_sum:.6f})")
        else:
            log(f"[VALIDATION] Akaike weights sum incorrect: FAIL (sum={weight_sum:.6f})")
            raise ValueError(f"Akaike weights must sum to 1.0 +/- 0.01, got {weight_sum:.6f}")

        # Validation 3: Best model has delta_AIC=0
        best_delta = df_models.loc[df_models['is_best'], 'delta_AIC'].iloc[0]
        if abs(best_delta) < 1e-10:  # Floating point tolerance
            log(f"[VALIDATION] Best model has delta_AIC=0: PASS")
        else:
            log(f"[VALIDATION] Best model delta_AIC={best_delta:.6f}: FAIL")
            raise ValueError(f"Best model must have delta_AIC=0, got {best_delta:.6f}")

        # Validation 4: All AIC values finite
        if df_models['AIC'].notna().all() and np.isfinite(df_models['AIC']).all():
            log(f"[VALIDATION] All AIC values finite: PASS")
        else:
            log(f"[VALIDATION] Non-finite AIC values detected: FAIL")
            raise ValueError("AIC values contain NaN or Inf")

        # Validation 5: delta_AIC in [0, Inf)
        val_result = validate_numeric_range(
            data=df_models['delta_AIC'].values,
            min_val=0.0,
            max_val=np.inf,
            column_name='delta_AIC'
        )
        if val_result['valid']:
            log(f"[VALIDATION] delta_AIC in [0, Inf): PASS")
        else:
            log(f"[VALIDATION] delta_AIC range check: FAIL")
            log(f"  {val_result['message']}")
            raise ValueError(val_result['message'])

        # Validation 6: relative_likelihood in (0, 1]
        if (df_models['relative_likelihood'] > 0).all() and (df_models['relative_likelihood'] <= 1.0).all():
            log(f"[VALIDATION] relative_likelihood in (0, 1]: PASS")
        else:
            log(f"[VALIDATION] relative_likelihood range check: FAIL")
            raise ValueError("relative_likelihood must be in (0, 1]")

        # Validation 7: akaike_weight in (0, 1)
        if (df_models['akaike_weight'] > 0).all() and (df_models['akaike_weight'] < 1.0).all():
            log(f"[VALIDATION] akaike_weight in (0, 1): PASS")
        else:
            log(f"[VALIDATION] akaike_weight range check: FAIL")
            raise ValueError("akaike_weight must be in (0, 1)")

        log("[SUCCESS] Step 06 complete - Best model identified: {0}".format(best_model_name))
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
