#!/usr/bin/env python3
# =============================================================================
# SCRIPT METADATA (Generated by g_code)
# =============================================================================
"""
Step ID: step07
Step Name: Prepare Trajectory Plot Data (Dual-Scale per Decision D069)
RQ: results/ch6/6.4.1
Generated: 2025-12-07

PURPOSE:
Create plot-ready trajectory data with dual-scale outputs (theta + probability scales)
per Decision D069. Aggregates observed theta scores by domain × timepoint, computes
95% CIs, and transforms to probability scale using IRT 2PL formula with mean
discrimination from Pass 2 item parameters.

EXPECTED INPUTS:
- data/step03_pass2_theta.csv
  Columns: ['composite_ID', 'theta_What', 'theta_Where', 'theta_When']
  Format: Wide-format theta scores from Pass 2 IRT calibration
  Expected rows: ~400 (100 participants × 4 timepoints)

- data/step04_lmm_input.csv
  Columns: ['composite_ID', 'UID', 'test', 'TSVR_hours', 'log_TSVR', 'paradigm', 'theta']
  Format: Long-format LMM input with TSVR time variable
  Expected rows: ~800 (if When excluded) or ~1200 (if When included)

- data/step03_pass2_item_params.csv
  Columns: ['item_name', 'Difficulty', 'Overall_Discrimination', 'Discrim_IFR', 'Discrim_ICR', 'Discrim_IRE']
  Format: Item parameters from Pass 2 calibration (for mean discrimination)
  Expected rows: 40-60 items

EXPECTED OUTPUTS:
- data/step07_trajectory_theta_data.csv
  Columns: ['time', 'theta', 'CI_lower', 'CI_upper', 'paradigm', 'n']
  Format: Aggregated theta scores per domain × timepoint (theta scale)
  Expected rows: 8 (if When excluded) or 12 (if When included)

- data/step07_trajectory_probability_data.csv
  Columns: ['time', 'probability', 'CI_lower', 'CI_upper', 'paradigm', 'n']
  Format: Transformed to probability scale (0-1 range)
  Expected rows: 8 (if When excluded) or 12 (if When included)

VALIDATION CRITERIA:
- Dual-scale data created: Both theta_data and probability_data exist
- Expected row count: 8 or 12 rows based on When domain status
- No NaN values: All cells have valid values
- CI bounds valid: CI_lower < CI_upper for all rows
- Probability range: All probability values in [0, 1]

g_code REASONING:
- Approach: Aggregate observed theta scores by domain × nominal timepoint,
  compute 95% CI using t-distribution, transform to probability using IRT 2PL
- Why this approach: Decision D069 requires dual-scale plotting to balance
  statistical rigor (theta scale) with interpretability (probability scale)
- Data flow: Long theta data → Group by domain × timepoint → Compute mean/SE/CI
  → Save theta-scale CSV → Transform using mean discrimination → Save probability-scale CSV
- Expected performance: <1 second (simple aggregation + transformation)

IMPLEMENTATION NOTES:
- Analysis tool: convert_theta_to_probability() from tools.plotting
- Validation tool: validate_probability_range() from tools.validation
- Parameters: Use mean discrimination per domain from Pass 2 item parameters
- IRT 2PL formula: P = 1 / (1 + exp(-a * (theta - b))) where b=0 (centered scale)
"""
# =============================================================================

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import traceback
from scipy import stats

# Add project root to path for imports
# CRITICAL: RQ scripts are in results/chX/rqY/code/ (4 levels deep from project root)
# Path hierarchy from script location:
#   parents[0] = code/ (immediate parent)
#   parents[1] = 6.3.1/
#   parents[2] = ch6/
#   parents[3] = results/
#   parents[4] = REMEMVR/ (project root - THIS is what we need for imports)
PROJECT_ROOT = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(PROJECT_ROOT))

# Import tools
from tools.plotting import convert_theta_to_probability
from tools.validation import validate_probability_range

# =============================================================================
# Configuration
# =============================================================================

RQ_DIR = Path(__file__).resolve().parents[1]  # results/ch6/6.4.1
LOG_FILE = RQ_DIR / "logs" / "step07_prepare_trajectory_plot_data.log"

# =============================================================================
# FOLDER CONVENTIONS (MANDATORY - NO EXCEPTIONS)
# =============================================================================
#
# code/   : ONLY .py scripts (generated by g_code)
# data/   : ALL data outputs (.csv, .pkl, .txt) - ANY file produced by code
# logs/   : ONLY .log files - execution logs
# plots/  : ONLY image files (.png, .pdf, .svg) - actual plot images
# results/: ONLY final summary reports (.md, .html)
# docs/   : RQ documentation (concept, plan, analysis specs)
#
# NAMING CONVENTION (MANDATORY):
# ALL files in data/ and logs/ MUST be prefixed with step number:
#   - stepXX_descriptive_name.csv
#   - stepXX_descriptive_name.pkl
#   - stepXX_descriptive_name.log
#
# Examples:
#   CORRECT: data/step07_trajectory_theta_data.csv
#   CORRECT: data/step07_trajectory_probability_data.csv
#   WRONG:   results/trajectory_theta_data.csv  (wrong folder + no prefix)
#   WRONG:   data/theta_data.csv                (missing step prefix)
#   WRONG:   logs/step07_trajectory.csv         (CSV in logs folder)

# =============================================================================
# Logging Function
# =============================================================================

def log(msg):
    """Write to both log file and console."""
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{msg}\n")
    print(msg)

# =============================================================================
# Main Analysis
# =============================================================================

if __name__ == "__main__":
    try:
        log("[START] Step 07: Prepare Trajectory Plot Data")

        # =========================================================================
        # STEP 1: Load Input Data
        # =========================================================================
        # Expected: Long-format theta data from Step 4 (with TSVR_hours)
        # Purpose: Aggregate by domain × timepoint for trajectory visualization

        log("[LOAD] Loading input data...")

        # Load long-format theta data (has TSVR_hours)
        lmm_input_path = RQ_DIR / "data" / "step04_lmm_input.csv"
        df_long = pd.read_csv(lmm_input_path, encoding='utf-8')
        log(f"[LOADED] step04_lmm_input.csv ({len(df_long)} rows, {len(df_long.columns)} cols)")

        # Load item parameters (for mean discrimination)
        item_params_path = RQ_DIR / "data" / "step03_pass2_item_params.csv"
        df_items = pd.read_csv(item_params_path, encoding='utf-8')
        log(f"[LOADED] step03_pass2_item_params.csv ({len(df_items)} rows, {len(df_items.columns)} cols)")

        # =========================================================================
        # STEP 2: Aggregate Theta Scores by Domain × Timepoint
        # =========================================================================
        # Tool: pandas groupby
        # What it does: Computes mean theta, SE, 95% CI per domain × timepoint
        # Expected output: Aggregated table with ~8-12 rows (2-3 domains × 4 timepoints)

        log("[ANALYSIS] Aggregating theta scores by domain and test...")

        # Group by domain and TEST (not TSVR_hours - those vary per participant)
        # Then compute mean TSVR_hours per test as the "time" coordinate
        grouped = df_long.groupby(['paradigm', 'test']).agg({
            'theta': ['mean', 'sem', 'count'],
            'TSVR_hours': 'mean'  # Average time per test
        }).reset_index()

        # Flatten column names
        grouped.columns = ['paradigm', 'test', 'theta', 'se', 'n', 'time']

        # Compute 95% CI using t-distribution
        # CI = mean ± t_critical * SE
        # For large N, t-distribution approaches normal (1.96)
        # For small N, use t.ppf for correct critical value
        grouped['CI_lower'] = grouped.apply(
            lambda row: row['theta'] - stats.t.ppf(0.975, row['n'] - 1) * row['se'] if row['n'] > 1 else np.nan,
            axis=1
        )
        grouped['CI_upper'] = grouped.apply(
            lambda row: row['theta'] + stats.t.ppf(0.975, row['n'] - 1) * row['se'] if row['n'] > 1 else np.nan,
            axis=1
        )

        # Select final columns in desired order
        theta_data = grouped[['time', 'theta', 'CI_lower', 'CI_upper', 'paradigm', 'n']].copy()

        # Sort by domain, then time
        theta_data = theta_data.sort_values(['paradigm', 'time']).reset_index(drop=True)

        log(f"[DONE] Aggregated to {len(theta_data)} rows (domains × tests)")
        log(f"[INFO] Domains present: {sorted(theta_data['paradigm'].unique())}")
        log(f"[INFO] Tests per domain: {theta_data.groupby('paradigm').size().to_dict()}")

        # =========================================================================
        # STEP 3: Save Theta-Scale Plot Data
        # =========================================================================
        # Output: data/step07_trajectory_theta_data.csv
        # Contains: Theta-scale trajectory data (statistical scale)
        # Columns: ['time', 'theta', 'CI_lower', 'CI_upper', 'paradigm', 'n']

        log("[SAVE] Saving theta-scale plot data...")
        theta_output_path = RQ_DIR / "data" / "step07_trajectory_theta_data.csv"
        theta_data.to_csv(theta_output_path, index=False, encoding='utf-8')
        log(f"[SAVED] step07_trajectory_theta_data.csv ({len(theta_data)} rows, {len(theta_data.columns)} cols)")

        # =========================================================================
        # STEP 4: Compute Mean Discrimination Per Domain
        # =========================================================================
        # Purpose: Get mean discrimination parameter for probability transformation
        # Uses Pass 2 item parameters (purified items only)

        log("[ANALYSIS] Computing mean discrimination per domain...")

        # Item parameters have domain-specific discrimination columns
        # Discrim_IFR, Discrim_ICR, Discrim_IRE
        # Non-zero values indicate which domain each item loads on

        # Compute mean discrimination per domain
        mean_discrim = {}

        for paradigm in ['IFR', 'ICR', 'IRE']:
            discrim_col = f'Discrim_{paradigm}'

            if discrim_col in df_items.columns:
                # Filter to items loading on this domain (non-zero discrimination)
                domain_items = df_items[df_items[discrim_col] > 0]

                if len(domain_items) > 0:
                    # Compute mean discrimination for this paradigm
                    mean_discrim[paradigm] = domain_items[discrim_col].mean()
                    log(f"[INFO] Mean discrimination for {paradigm}: {mean_discrim[paradigm]:.4f} ({len(domain_items)} items)")
                else:
                    log(f"[WARNING] No items found for {paradigm} paradigm (paradigm excluded after purification)")
            else:
                log(f"[WARNING] {discrim_col} column not found in item parameters")

        # Check if any domains found
        if not mean_discrim:
            raise ValueError("No domain discriminations computed - check item parameters structure")

        # =========================================================================
        # STEP 5: Transform Theta to Probability Scale
        # =========================================================================
        # Tool: convert_theta_to_probability() from tools.plotting
        # What it does: Transforms theta to P(correct) using IRT 2PL formula
        # Formula: P = 1 / (1 + exp(-a * (theta - b))) where b=0 (centered scale)
        # Expected output: Probability values in [0, 1] range

        log("[ANALYSIS] Transforming theta to probability scale...")

        # Create copy for probability transformation
        probability_data = theta_data.copy()

        # Transform theta and CIs to probability for each domain
        for domain in probability_data['paradigm'].unique():
            # Get mean discrimination for this domain
            if domain not in mean_discrim:
                log(f"[WARNING] No mean discrimination for {domain}, skipping probability transformation")
                continue

            a = mean_discrim[domain]
            b = 0.0  # Centered scale (theta mean = 0)

            # Transform theta to probability
            domain_mask = probability_data['paradigm'] == domain

            probability_data.loc[domain_mask, 'probability'] = convert_theta_to_probability(
                theta=probability_data.loc[domain_mask, 'theta'].values,
                discrimination=a,
                difficulty=b
            )

            # Transform CI bounds to probability
            probability_data.loc[domain_mask, 'CI_lower_prob'] = convert_theta_to_probability(
                theta=probability_data.loc[domain_mask, 'CI_lower'].values,
                discrimination=a,
                difficulty=b
            )

            probability_data.loc[domain_mask, 'CI_upper_prob'] = convert_theta_to_probability(
                theta=probability_data.loc[domain_mask, 'CI_upper'].values,
                discrimination=a,
                difficulty=b
            )

        # Drop theta-scale columns, rename probability columns
        probability_data = probability_data.drop(columns=['theta', 'CI_lower', 'CI_upper'])
        probability_data = probability_data.rename(columns={
            'probability': 'probability',
            'CI_lower_prob': 'CI_lower',
            'CI_upper_prob': 'CI_upper'
        })

        # Reorder columns to match theta_data structure
        probability_data = probability_data[['time', 'probability', 'CI_lower', 'CI_upper', 'paradigm', 'n']]

        log("[DONE] Probability transformation complete")

        # =========================================================================
        # STEP 6: Save Probability-Scale Plot Data
        # =========================================================================
        # Output: data/step07_trajectory_probability_data.csv
        # Contains: Probability-scale trajectory data (interpretable scale)
        # Columns: ['time', 'probability', 'CI_lower', 'CI_upper', 'paradigm', 'n']

        log("[SAVE] Saving probability-scale plot data...")
        probability_output_path = RQ_DIR / "data" / "step07_trajectory_probability_data.csv"
        probability_data.to_csv(probability_output_path, index=False, encoding='utf-8')
        log(f"[SAVED] step07_trajectory_probability_data.csv ({len(probability_data)} rows, {len(probability_data.columns)} cols)")

        # =========================================================================
        # STEP 7: Validation
        # =========================================================================
        # Tool: validate_probability_range() from tools.validation
        # Validates: Probability values in [0, 1], CI bounds valid
        # Expected: All validation checks pass

        log("[VALIDATION] Running validation checks...")

        # Check 1: Both files exist and have same row count
        if len(theta_data) != len(probability_data):
            raise ValueError(f"Row count mismatch: theta_data={len(theta_data)}, probability_data={len(probability_data)}")
        log(f"[VALIDATION] [PASS] Both datasets have {len(theta_data)} rows")

        # Check 2: Expected row count (8 or 12 based on domains present)
        n_domains = len(theta_data['paradigm'].unique())
        n_timepoints = len(theta_data['time'].unique())
        expected_rows = n_domains * n_timepoints
        if len(theta_data) != expected_rows:
            raise ValueError(f"Expected {expected_rows} rows ({n_domains} domains × {n_timepoints} timepoints), got {len(theta_data)}")
        log(f"[VALIDATION] [PASS] Row count matches expected: {expected_rows} rows ({n_domains} domains × {n_timepoints} timepoints)")

        # Check 3: No NaN values in theta data
        if theta_data.isnull().any().any():
            raise ValueError("NaN values found in theta_data")
        log("[VALIDATION] [PASS] No NaN values in theta_data")

        # Check 4: No NaN values in probability data
        if probability_data.isnull().any().any():
            raise ValueError("NaN values found in probability_data")
        log("[VALIDATION] [PASS] No NaN values in probability_data")

        # Check 5: CI bounds valid (lower < upper) for theta scale
        invalid_ci_theta = theta_data[theta_data['CI_lower'] >= theta_data['CI_upper']]
        if len(invalid_ci_theta) > 0:
            raise ValueError(f"Invalid CI bounds in theta_data: {len(invalid_ci_theta)} rows with CI_lower >= CI_upper")
        log("[VALIDATION] [PASS] All theta CI bounds valid (CI_lower < CI_upper)")

        # Check 6: CI bounds valid (lower < upper) for probability scale
        invalid_ci_prob = probability_data[probability_data['CI_lower'] >= probability_data['CI_upper']]
        if len(invalid_ci_prob) > 0:
            raise ValueError(f"Invalid CI bounds in probability_data: {len(invalid_ci_prob)} rows with CI_lower >= CI_upper")
        log("[VALIDATION] [PASS] All probability CI bounds valid (CI_lower < CI_upper)")

        # Check 7: Probability range validation using tools.validation
        validation_result = validate_probability_range(
            probability_df=probability_data,
            prob_columns=['probability', 'CI_lower', 'CI_upper']
        )

        if not validation_result['valid']:
            raise ValueError(f"Probability range validation failed: {validation_result['message']}")
        log(f"[VALIDATION] [PASS] {validation_result['message']}")

        # Final summary
        log("[VALIDATION] All validation checks passed")
        log(f"[VALIDATION] Theta range: [{theta_data['theta'].min():.4f}, {theta_data['theta'].max():.4f}]")
        log(f"[VALIDATION] Probability range: [{probability_data['probability'].min():.4f}, {probability_data['probability'].max():.4f}]")

        log("[SUCCESS] Step 07 complete")
        sys.exit(0)

    except Exception as e:
        log(f"[ERROR] {str(e)}")
        log("[TRACEBACK] Full error details:")
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            traceback.print_exc(file=f)
        traceback.print_exc()
        sys.exit(1)
