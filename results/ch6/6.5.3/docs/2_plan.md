# Analysis Plan: RQ 6.5.3 - High-Confidence Errors (Schema-Incongruent Effects)

**Research Question:** 6.5.3
**Created:** 2025-12-06
**Status:** Planning complete, ready for tool specification (rq_tools)

---

## Overview

This RQ examines high-confidence errors (HCE) at the item level to test whether schema-incongruent items produce more confidence-accuracy dissociations than schema-congruent or common items. Analysis uses ~27,200 item-responses (N=100 participants x 4 test sessions x ~68 items) to compute HCE rates (P(Accuracy=0 | Confidence >= 0.75)) across three schema congruence levels: Common (i1/i2), Congruent (i3/i4), and Incongruent (i5/i6). Statistical testing uses mixed-effects logistic regression with random effects for both participants (UID) and items (ItemID) to account for non-independence.

**Pipeline:** Item-level CTT analysis (no IRT - analyzing raw accuracy/confidence pairs)
**Steps:** 5 analysis steps (Step 0: extraction + Steps 1-4: analysis)
**Estimated Runtime:** Low to Medium (~5-15 minutes total)

**Key Decisions Applied:**
- Decision D068: Dual p-value reporting (uncorrected + Bonferroni correction for post-hoc contrasts)
- No IRT: Item-level analysis examines raw accuracy/confidence dissociations, not latent ability
- No TSVR: Time variable uses nominal test sessions (T1, T2, T3, T4) for interpretability (no continuous trajectory analysis)

---

## Analysis Plan

### Step 0: Extract Item-Level Accuracy and Confidence Data

**Dependencies:** None (first step)
**Complexity:** Low (data extraction only, ~1-2 minutes)

**Input:**

**File:** data/cache/dfData.csv (project-level data source, generated by data_prep.py)

**Required Columns:**
- `UID` (string, format: A### with leading zeros, e.g., A010)
- Tags matching patterns:
  - `TQ_*` (Test Question accuracy: 0, 0.25, 0.5, or 1.0)
  - `TC_*` (Test Confidence: 0, 0.25, 0.5, 0.75, or 1.0 - 5-point Likert rescaled)
- Paradigm codes: IFR, ICR, IRE (interactive VR paradigms only)
- Congruence item tags: i1CM, i2CM (Common), i3CG, i4CG (Congruent), i5IN, i6IN (Incongruent)
- Test session codes: T1, T2, T3, T4 (embedded in tag names)

**Filters:**
- Paradigms: IFR, ICR, IRE only (excludes RFR, TCR, RRE per concept.md - focus on interactive items)
- Item codes: i1CM, i2CM, i3CG, i4CG, i5IN, i6IN only (congruence-tagged items)
- Domain: -N- (What/object identity - schema congruence is object-level manipulation)
- Only items with BOTH TQ_* (accuracy) AND TC_* (confidence) measurements

**Expected Data Volume:**
- ~100 participants x 4 test sessions x ~17 congruence-tagged items per test = ~6,800 item-responses
- Note: Concept.md estimated ~27,200 (100 x 4 x 68), but that assumes all interactive items. Filtering to congruence-tagged items (i1-i6 only, ~6 per room x 4 rooms = 24 items total, but not all presented at all tests) yields lower N.

**Processing:**
1. Read dfData.csv
2. Filter to rows (UIDs) with complete data (all 100 participants)
3. Extract TQ_* columns for congruence-tagged items (i1CM/i2CM/i3CG/i4CG/i5IN/i6IN from IFR/ICR/IRE paradigms)
4. Extract TC_* columns for same items (accuracy-confidence pairs)
5. Parse item tags to extract:
   - ItemID (full item identifier, e.g., "RVR-T1-BAT-IFR-N-i1CM-...")
   - Congruence level (Common/Congruent/Incongruent from i1CM/i3CG/i5IN suffix)
   - Test session (T1/T2/T3/T4 from tag)
   - Paradigm (IFR/ICR/IRE from tag)
6. Create long-format dataset: one row per UID x ItemID x Test
7. Columns: UID, ItemID, Test, Paradigm, Congruence, Accuracy (TQ_), Confidence (TC_)

**Output:**

**File 1:** data/step00_item_level.csv
**Format:** CSV, long format (one row per item-response)
**Columns:**
  - `UID` (string, participant identifier, e.g., A010)
  - `ItemID` (string, full item tag identifier)
  - `Test` (string, values: T1, T2, T3, T4)
  - `Paradigm` (string, values: IFR, ICR, IRE)
  - `Congruence` (string, values: Common, Congruent, Incongruent)
  - `Accuracy` (float, values: {0, 0.25, 0.5, 1.0} - from TQ_* tags)
  - `Confidence` (float, values: {0, 0.25, 0.5, 0.75, 1.0} - from TC_* tags)
**Expected Rows:** ~6,800 item-responses (100 participants x 4 tests x ~17 items)
**Note:** Actual N may vary due to counterbalancing (not all items presented at all tests)

**Validation Requirement:**
Validation tools MUST be used after data extraction tool execution. Specific validation tools will be determined by rq_tools based on extraction type (item-level data validation, missing data checks, format validation). The rq_analysis agent will embed validation tool calls after the extraction tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step00_item_level.csv: exists (exact path)
- Expected rows: 6,000-8,000 (allowing for counterbalancing variation)
- Expected columns: 7 (UID, ItemID, Test, Paradigm, Congruence, Accuracy, Confidence)
- Data types: UID (object/string), ItemID (object/string), Test (object/string), Paradigm (object/string), Congruence (object/string), Accuracy (float64), Confidence (float64)

*Value Ranges:*
- UID in {A010, A020, ..., A109} (100 unique participants expected)
- Test in {T1, T2, T3, T4} (categorical)
- Paradigm in {IFR, ICR, IRE} (categorical)
- Congruence in {Common, Congruent, Incongruent} (categorical)
- Accuracy in {0, 0.25, 0.5, 1.0} (discrete values only - partial credit scoring)
- Confidence in {0, 0.25, 0.5, 0.75, 1.0} (5-point Likert rescaled)

*Data Quality:*
- No NaN in UID, ItemID, Test, Paradigm, Congruence columns (all must be present)
- NaN acceptable in Accuracy/Confidence columns (<5% per column - missing responses OK)
- All 100 participants present (no data loss)
- Each UID should have multiple rows (4 tests x multiple items)
- Congruence distribution: ~33% Common, ~33% Congruent, ~33% Incongruent (balanced design)

*Log Validation:*
- Required pattern: "Extracted item-level data: {N} rows" where N in [6000, 8000]
- Required pattern: "Unique participants: 100"
- Required pattern: "Congruence levels: Common, Congruent, Incongruent"
- Forbidden patterns: "ERROR", "Missing TQ_/TC_ pairs", "No congruence tags found"
- Acceptable warnings: "Some items not presented at all tests (counterbalancing)"

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Expected 100 participants, found 87")
- Log failure to logs/step00_extract_item_level.log
- Quit script immediately (do NOT proceed to Step 1)
- g_debug invoked by master to diagnose root cause

---

### Step 1: Identify High-Confidence Errors

**Dependencies:** Step 0 (requires item-level accuracy/confidence data)
**Complexity:** Low (binary flagging based on thresholds, <1 minute)

**Input:**

**File:** data/step00_item_level.csv
**Source:** Generated by Step 0 (item-level extraction)
**Format:** CSV with columns: UID, ItemID, Test, Paradigm, Congruence, Accuracy, Confidence
**Expected Rows:** ~6,800 item-responses

**Processing:**
1. Read step00_item_level.csv
2. For each row, create HCE_flag:
   - HCE_flag = 1 if (Accuracy == 0 AND Confidence >= 0.75)
   - HCE_flag = 0 otherwise
3. High-confidence threshold: 0.75 corresponds to "4" on original 5-point Likert scale (confident but not maximally so)
4. Add HCE_flag column to dataframe

**High-Confidence Error Definition:**
- **Accuracy == 0:** Completely incorrect response (no partial credit)
- **Confidence >= 0.75:** High confidence (4 or 5 on original 5-point Likert: 1=not confident, 5=very confident)
- **HCE:** Dissociation between metacognitive judgment (high confidence) and actual performance (error)

**Output:**

**File 1:** data/step01_hce_flags.csv
**Format:** CSV, long format (same structure as input + HCE_flag column)
**Columns:**
  - All columns from step00_item_level.csv (UID, ItemID, Test, Paradigm, Congruence, Accuracy, Confidence)
  - `HCE_flag` (int, values: {0, 1} - binary indicator)
**Expected Rows:** ~6,800 (same as input)

**Validation Requirement:**
Validation tools MUST be used after HCE flagging tool execution. Specific validation tools determined by rq_tools based on binary flagging validation (value range checks, logical consistency). The rq_analysis agent will embed validation tool calls after the flagging tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step01_hce_flags.csv: exists (exact path)
- Expected rows: same as input (~6,800)
- Expected columns: 8 (original 7 + HCE_flag)
- Data types: HCE_flag (int64, values {0, 1})

*Value Ranges:*
- HCE_flag in {0, 1} (binary only)
- Logical consistency: If HCE_flag == 1, then Accuracy == 0 AND Confidence >= 0.75 (verify sample)
- Logical consistency: If Accuracy > 0 OR Confidence < 0.75, then HCE_flag == 0 (verify sample)

*Data Quality:*
- No NaN in HCE_flag column (all rows must be flagged)
- HCE_flag = 1 rows should be minority (~5-20% expected - rare dissociation)
- Distribution check: HCE_flag = 1 count in [340, 1360] (5-20% of 6,800)

*Log Validation:*
- Required pattern: "HCE flagging complete: {N} high-confidence errors identified" where N in [340, 1360]
- Required pattern: "HCE rate: {X}%" where X in [5, 20]
- Forbidden patterns: "ERROR", "Invalid HCE logic", "All rows flagged as HCE"
- Acceptable warnings: None expected

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "HCE logic violation: Accuracy=1 but HCE_flag=1")
- Log failure to logs/step01_identify_hce.log
- Quit script immediately (do NOT proceed to Step 2)
- g_debug invoked to diagnose root cause

---

### Step 2: Compute HCE Rates by Congruence and Test

**Dependencies:** Step 1 (requires HCE_flag column)
**Complexity:** Low (aggregation only, <1 minute)

**Input:**

**File:** data/step01_hce_flags.csv
**Source:** Generated by Step 1 (HCE flagging)
**Format:** CSV with columns including HCE_flag, Congruence, Test
**Expected Rows:** ~6,800 item-responses

**Processing:**
1. Read step01_hce_flags.csv
2. Group by Congruence (Common/Congruent/Incongruent) x Test (T1/T2/T3/T4)
3. For each cell, compute:
   - HCE_rate = mean(HCE_flag) (proportion of responses that are high-confidence errors)
   - N_responses = count(HCE_flag) (total responses in cell)
   - N_hce = sum(HCE_flag) (total HCEs in cell)
4. Output: 12 rows (3 congruence levels x 4 tests)

**Expected Pattern (Hypothesis):**
- HCE_rate_Incongruent > HCE_rate_Congruent and HCE_rate_Common (schema intrusions produce more confidence-accuracy dissociations)
- Possible increase over time (HCE rates higher at T4 than T1 as memory degrades and schema reconstruction increases)

**Output:**

**File 1:** data/step02_hce_rates.csv
**Format:** CSV, aggregated data (one row per Congruence x Test cell)
**Columns:**
  - `Congruence` (string, values: Common, Congruent, Incongruent)
  - `Test` (string, values: T1, T2, T3, T4)
  - `HCE_rate` (float, proportion in [0, 1])
  - `N_responses` (int, total responses in cell)
  - `N_hce` (int, total HCEs in cell)
**Expected Rows:** 12 (3 congruence x 4 tests)

**Validation Requirement:**
Validation tools MUST be used after aggregation tool execution. Specific validation tools determined by rq_tools based on aggregated data validation (complete factorial design, proportion bounds). The rq_analysis agent will embed validation tool calls after the aggregation tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step02_hce_rates.csv: exists (exact path)
- Expected rows: 12 (complete 3 x 4 factorial design)
- Expected columns: 5 (Congruence, Test, HCE_rate, N_responses, N_hce)
- Data types: Congruence (object/string), Test (object/string), HCE_rate (float64), N_responses (int64), N_hce (int64)

*Value Ranges:*
- Congruence in {Common, Congruent, Incongruent} (all 3 levels present)
- Test in {T1, T2, T3, T4} (all 4 tests present)
- HCE_rate in [0, 1] (proportion)
- Expected HCE_rate range: [0.05, 0.25] (5-25% per cell - rare but not absent)
- N_responses > 0 for all cells (no empty cells)
- N_hce >= 0 for all cells (some cells may have 0 HCEs)
- Logical consistency: HCE_rate = N_hce / N_responses (verify for all rows)

*Data Quality:*
- All 12 cells present (complete factorial design - no missing Congruence x Test combinations)
- N_responses approximately balanced across cells (counterbalancing should distribute items evenly)
- Expected N_responses per cell: ~567 (6,800 / 12 cells), acceptable range: [400, 800]

*Log Validation:*
- Required pattern: "HCE rates computed: 12 cells (3 congruence x 4 tests)"
- Required pattern: "Overall HCE rate: {X}%" where X in [5, 20]
- Forbidden patterns: "ERROR", "Missing cells in factorial design", "HCE_rate > 1.0"
- Acceptable warnings: "Some cells have 0 HCEs (low base rate)"

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Missing cell: Incongruent x T3")
- Log failure to logs/step02_compute_hce_rates.log
- Quit script immediately (do NOT proceed to Step 3)
- g_debug invoked to diagnose root cause

---

### Step 3: Fit Mixed-Effects Model to Test Congruence x Time Effect on HCE Rate

**Dependencies:** Step 1 (requires item-level HCE_flags for mixed-effects modeling)
**Complexity:** Medium (GLMM fitting with crossed random effects, ~5-10 minutes)

**Input:**

**File:** data/step01_hce_flags.csv
**Source:** Generated by Step 1 (HCE flagging)
**Format:** CSV with item-level HCE_flag, Congruence, Test, UID, ItemID
**Expected Rows:** ~6,800 item-responses

**Processing:**
1. Read step01_hce_flags.csv
2. Convert Test to numeric time variable (T1=0, T2=1, T3=3, T4=6 - nominal days)
3. Fit mixed-effects logistic regression (GLMM):
   - Outcome: HCE_flag (binary: 0/1)
   - Fixed effects: Congruence (categorical: Common/Congruent/Incongruent) + Time (continuous: 0/1/3/6) + Congruence x Time interaction
   - Random effects: (Time | UID) + (1 | ItemID) - crossed random effects for participants and items
4. Test main effect of Congruence (F-test or Wald chi-square)
5. Test Congruence x Time interaction (tests if HCE rate changes over time differently for congruence levels)
6. Extract model summary (fixed effects table, random effects variance components)

**Statistical Method:**
- **Model:** Generalized Linear Mixed Model (GLMM) with logit link function (binomial family)
- **Formula:** HCE_flag ~ Congruence * Time + (Time | UID) + (1 | ItemID)
- **Interpretation:** Main effect of Congruence tests if HCE rate differs across Common/Congruent/Incongruent items. Interaction tests if HCE rate changes over time differently for congruence levels (e.g., Incongruent items show steeper increase).

**Output:**

**File 1:** data/step03_congruence_hce_model.txt
**Format:** TXT, model summary
**Contents:**
  - Fixed effects table: Coefficient, SE, z-value, p-value for Congruence main effect, Time main effect, Congruence x Time interaction
  - Random effects: Variance components for UID (intercept, slope) and ItemID (intercept)
  - Model fit: AIC, BIC, log-likelihood

**File 2:** data/step03_congruence_hce_test.csv
**Format:** CSV, hypothesis test results
**Columns:**
  - `Effect` (string, e.g., "Congruence", "Time", "Congruence x Time")
  - `F_or_ChiSq` (float, test statistic)
  - `df1` (int, numerator degrees of freedom)
  - `df2` (int, denominator degrees of freedom, may be NA for Wald tests)
  - `p_value` (float, uncorrected p-value per Decision D068)
**Expected Rows:** 3 (Congruence main effect, Time main effect, Congruence x Time interaction)

**Validation Requirement:**
Validation tools MUST be used after GLMM fitting tool execution. Specific validation tools determined by rq_tools based on LMM convergence validation (convergence status, variance components positivity, residuals checks). The rq_analysis agent will embed validation tool calls after the GLMM fitting tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step03_congruence_hce_model.txt: exists (exact path)
- data/step03_congruence_hce_test.csv: exists (exact path)
- Expected rows in test.csv: 3 (Congruence, Time, Congruence x Time)
- Expected columns in test.csv: 5 (Effect, F_or_ChiSq, df1, df2, p_value)

*Value Ranges:*
- p_value in [0, 1] (all hypothesis tests)
- F_or_ChiSq > 0 (test statistics non-negative)
- df1, df2 > 0 (degrees of freedom positive)
- Variance components in model.txt > 0 (all random effects variances must be positive)

*Data Quality:*
- Model converged successfully (no convergence warnings in model.txt)
- All fixed effects estimated (Congruence, Time, interaction - no NaN coefficients)
- All random effects estimated (UID intercept/slope, ItemID intercept - no NaN variances)

*Log Validation:*
- Required pattern: "GLMM converged successfully"
- Required pattern: "Fixed effects: Congruence, Time, Congruence x Time"
- Required pattern: "Random effects: UID (intercept, slope), ItemID (intercept)"
- Forbidden patterns: "ERROR", "Model failed to converge", "Singular fit", "NaN coefficients"
- Acceptable warnings: "Convergence tolerance met" (numerical precision messages OK)

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "GLMM failed to converge: singular fit")
- Log failure to logs/step03_fit_glmm.log
- Quit script immediately (do NOT proceed to Step 4)
- g_debug invoked to diagnose root cause (common causes: insufficient data, collinearity, model overparameterization)

---

### Step 4: Post-Hoc Comparisons if Congruence Effect Significant

**Dependencies:** Step 3 (requires GLMM results to check if Congruence main effect is significant)
**Complexity:** Low (pairwise contrasts with Bonferroni correction, <2 minutes)

**Input:**

**File 1:** data/step01_hce_flags.csv (for re-fitting model with contrasts)
**Source:** Generated by Step 1 (HCE flagging)

**File 2:** data/step03_congruence_hce_test.csv (to check if Congruence main effect significant)
**Source:** Generated by Step 3 (GLMM hypothesis tests)

**Processing:**
1. Read step03_congruence_hce_test.csv
2. Check if Congruence main effect p_value < 0.05 (uncorrected)
3. If significant, proceed with post-hoc contrasts:
   - Incongruent vs Congruent (tests primary hypothesis: schema violations increase HCE)
   - Incongruent vs Common (tests if incongruent worse than baseline)
   - Congruent vs Common (tests if congruent differs from baseline)
4. Compute dual p-values per Decision D068:
   - Uncorrected p-values (for transparency)
   - Bonferroni-corrected p-values (for family-wise error control: p_bonf = p_uncorr x 3 contrasts)
5. If Congruence main effect NOT significant (p >= 0.05), skip contrasts and report "NULL effect - no post-hoc tests conducted"

**Statistical Method:**
- **Contrasts:** Pairwise comparisons using estimated marginal means (EMMs) from GLMM
- **Correction:** Bonferroni (conservative, appropriate for 3 planned comparisons)
- **Decision D068:** Report BOTH uncorrected and corrected p-values for transparency

**Output:**

**File 1:** data/step04_post_hoc_contrasts.csv
**Format:** CSV, pairwise contrast results (or empty if Congruence NULL)
**Columns:**
  - `Contrast` (string, e.g., "Incongruent vs Congruent")
  - `Estimate` (float, log-odds difference on logit scale)
  - `SE` (float, standard error of estimate)
  - `z_value` (float, z-statistic)
  - `p_uncorrected` (float, uncorrected p-value per Decision D068)
  - `p_bonferroni` (float, Bonferroni-corrected p-value per Decision D068)
**Expected Rows:** 3 contrasts (if Congruence significant) OR 0 rows with note (if NULL effect)

**Validation Requirement:**
Validation tools MUST be used after post-hoc contrasts tool execution. Specific validation tools determined by rq_tools based on contrast validation (dual p-value presence per Decision D068, logical consistency). The rq_analysis agent will embed validation tool calls after the contrasts tool call for this step.

**Substance Validation Criteria (for rq_inspect post-execution validation):**

*Output Files:*
- data/step04_post_hoc_contrasts.csv: exists (exact path)
- Expected rows: 3 (if Congruence significant) OR 0 (if NULL effect - file contains header only or note)
- Expected columns: 6 (Contrast, Estimate, SE, z_value, p_uncorrected, p_bonferroni)
- Data types: Contrast (object/string), Estimate (float64), SE (float64), z_value (float64), p_uncorrected (float64), p_bonferroni (float64)

*Value Ranges:*
- Estimate unrestricted (log-odds can be positive or negative)
- SE > 0 (standard errors must be positive)
- z_value unrestricted (can be positive or negative)
- p_uncorrected in [0, 1]
- p_bonferroni in [0, 1]
- Logical consistency: p_bonferroni = min(p_uncorrected x 3, 1.0) for all contrasts (Bonferroni formula)

*Data Quality:*
- Decision D068 compliance: BOTH p_uncorrected AND p_bonferroni columns present (dual p-value reporting)
- All 3 contrasts present if Congruence significant (Incongruent vs Congruent, Incongruent vs Common, Congruent vs Common)
- If Congruence NULL (p >= 0.05), contrasts.csv should be empty or contain note explaining no tests conducted

*Log Validation:*
- Required pattern (if contrasts conducted): "Post-hoc contrasts: 3 pairwise comparisons"
- Required pattern (if contrasts conducted): "Bonferroni correction applied (3 comparisons)"
- Required pattern (if NULL effect): "Congruence main effect not significant (p = {X}) - no post-hoc tests"
- Forbidden patterns: "ERROR", "Missing p_bonferroni column", "Negative p-values"
- Acceptable warnings: None expected

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Missing p_bonferroni column - Decision D068 violation")
- Log failure to logs/step04_post_hoc_contrasts.log
- Quit script immediately
- g_debug invoked to diagnose root cause

---

## Expected Outputs

### Data Files (ALL analysis inputs and outputs - intermediate and final)
- data/step00_item_level.csv (from Step 0: item-level extraction, ~6,800 rows)
- data/step01_hce_flags.csv (from Step 1: HCE flagging, ~6,800 rows)
- data/step02_hce_rates.csv (from Step 2: aggregated HCE rates, 12 rows)
- data/step03_congruence_hce_model.txt (from Step 3: GLMM summary)
- data/step03_congruence_hce_test.csv (from Step 3: hypothesis test results, 3 rows)
- data/step04_post_hoc_contrasts.csv (from Step 4: pairwise contrasts, 3 rows or 0 if NULL)

### Logs (ONLY execution logs - .log files capturing stdout/stderr)
- logs/step00_extract_item_level.log
- logs/step01_identify_hce.log
- logs/step02_compute_hce_rates.log
- logs/step03_fit_glmm.log
- logs/step04_post_hoc_contrasts.log

### Plots (EMPTY until rq_plots runs)
- No plots planned for this RQ (results inspection focuses on HCE rates table and contrast p-values)

### Results (EMPTY until rq_results runs)
- results/summary.md (created by rq_results, NOT analysis steps)

---

## Expected Data Formats

### step00_item_level.csv
- Format: Long (one row per item-response: UID x ItemID x Test)
- Dimensions: ~6,800 rows x 7 columns
- Columns: UID (string), ItemID (string), Test (string), Paradigm (string), Congruence (string), Accuracy (float), Confidence (float)
- Data Types: All string except Accuracy/Confidence (float64)
- Key: UID + ItemID + Test (composite key for item-response)

### step01_hce_flags.csv
- Format: Long (same as step00 + HCE_flag column)
- Dimensions: ~6,800 rows x 8 columns
- Columns: All from step00 + HCE_flag (int, {0, 1})
- Data Types: HCE_flag (int64)
- Key: Same as step00

### step02_hce_rates.csv
- Format: Aggregated (one row per Congruence x Test cell)
- Dimensions: 12 rows x 5 columns
- Columns: Congruence (string), Test (string), HCE_rate (float), N_responses (int), N_hce (int)
- Data Types: Congruence/Test (string), HCE_rate (float64), N_responses/N_hce (int64)
- Key: Congruence + Test (factorial design)

### step03_congruence_hce_test.csv
- Format: Hypothesis test results (one row per fixed effect)
- Dimensions: 3 rows x 5 columns
- Columns: Effect (string), F_or_ChiSq (float), df1 (int), df2 (int), p_value (float)
- Data Types: Effect (string), F_or_ChiSq (float64), df1/df2 (int64), p_value (float64)
- Key: Effect name (Congruence, Time, Congruence x Time)

### step04_post_hoc_contrasts.csv
- Format: Pairwise contrast results (one row per contrast)
- Dimensions: 3 rows x 6 columns (if Congruence significant) OR 0 rows (if NULL)
- Columns: Contrast (string), Estimate (float), SE (float), z_value (float), p_uncorrected (float), p_bonferroni (float)
- Data Types: Contrast (string), all others (float64)
- Key: Contrast name (Incongruent vs Congruent, etc.)
- Decision D068: MUST include BOTH p_uncorrected AND p_bonferroni columns

---

## Cross-RQ Dependencies

**Dependency Type:** RAW Data Only (No Dependencies)

**This RQ uses:** Only dfData.csv (project-level cached data source generated by data_prep.py)
**No dependencies on other RQs:** Can be executed independently
**Execution order:** Flexible (any order within Ch6 Type 5.5)

**Data Sources:**
- dfData.csv (cached) - TQ_* (accuracy) and TC_* (confidence) columns for all VR items
- master.xlsx (original) - source for dfData.csv generation (handled by data_prep.py)

**Note:** All data extraction uses dfData.csv columns directly. No intermediate outputs from other RQs required. This RQ is self-contained within Ch6 Type 5.5 (Schema Confidence analyses).

---

## Validation Requirements

**CRITICAL MANDATE:**

Every analysis step in this plan MUST use validation tools after analysis tool execution.

This is not optional. This is the core architectural principle preventing cascading failures observed in v3.0 (where analysis errors propagated undetected through 5+ downstream steps before discovery).

**Exact Specification Requirement:**

> "Validation tools MUST be used after analysis tool execution"

**Implementation:**
- rq_tools (Step 11 workflow) will read tool_inventory.md validation tools section
- rq_tools will specify BOTH analysis tool + validation tool per step in 3_tools.yaml
- rq_analysis (Step 12 workflow) will embed validation tool call AFTER analysis tool call in 4_analysis.yaml
- g_code (Step 14 workflow) will generate stepN_name.py scripts with validation function calls
- bash execution (Step 14 workflow) will run analysis -> validation -> error on validation failure

**Downstream Agent Requirements:**
- **rq_tools:** MUST specify validation tool for EVERY analysis step (no exceptions)
- **rq_analysis:** MUST embed validation tool call for EVERY analysis step (no exceptions)
- **g_code:** MUST generate code with validation function calls (no exceptions)
- **rq_inspect:** MUST verify validation ran successfully (checks logs/stepN_name.log for validation output)

### Validation Requirements By Step

#### Step 0: Extract Item-Level Accuracy and Confidence Data

**Analysis Tool:** (determined by rq_tools - likely pandas extraction from dfData.csv)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_data_columns + validate_numeric_range)

**What Validation Checks:**
- Output file exists (data/step00_item_level.csv)
- Expected column count (7 columns: UID, ItemID, Test, Paradigm, Congruence, Accuracy, Confidence)
- Expected row count (~6,000-8,000 rows allowing counterbalancing variation)
- Required columns present (UID, ItemID, Test, Paradigm, Congruence, Accuracy, Confidence)
- Value ranges: Accuracy in {0, 0.25, 0.5, 1.0}, Confidence in {0, 0.25, 0.5, 0.75, 1.0}
- All 100 participants present (UID count = 100)
- Congruence levels present: Common, Congruent, Incongruent

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Expected 100 participants, found 87")
- Log failure to logs/step00_extract_item_level.log
- Quit script immediately (do NOT proceed to Step 1)
- g_debug invoked by master to diagnose root cause

---

#### Step 1: Identify High-Confidence Errors

**Analysis Tool:** (determined by rq_tools - likely pandas binary flagging)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_numeric_range + custom HCE logic check)

**What Validation Checks:**
- Output file exists (data/step01_hce_flags.csv)
- HCE_flag column added (8 columns total)
- HCE_flag in {0, 1} (binary only)
- HCE_flag = 1 count in reasonable range (5-20% of total, ~340-1360 rows)
- Logical consistency: HCE_flag = 1 implies Accuracy = 0 AND Confidence >= 0.75 (sample check)

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "HCE logic violation: Accuracy=1 but HCE_flag=1")
- Log failure to logs/step01_identify_hce.log
- Quit script immediately (do NOT proceed to Step 2)
- g_debug invoked to diagnose root cause

---

#### Step 2: Compute HCE Rates by Congruence and Test

**Analysis Tool:** (determined by rq_tools - likely pandas groupby aggregation)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_plot_data_completeness + validate_probability_range)

**What Validation Checks:**
- Output file exists (data/step02_hce_rates.csv)
- Expected row count (12 rows - complete 3 x 4 factorial design)
- All Congruence levels present (Common, Congruent, Incongruent)
- All Test levels present (T1, T2, T3, T4)
- HCE_rate in [0, 1] (proportion)
- Logical consistency: HCE_rate = N_hce / N_responses (verify for all rows)
- N_responses > 0 for all cells (no empty cells)

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Missing cell: Incongruent x T3")
- Log failure to logs/step02_compute_hce_rates.log
- Quit script immediately (do NOT proceed to Step 3)
- g_debug invoked to diagnose root cause

---

#### Step 3: Fit Mixed-Effects Model to Test Congruence x Time Effect on HCE Rate

**Analysis Tool:** (determined by rq_tools - likely statsmodels GLMM or pymer4)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_model_convergence + validate_variance_positivity)

**What Validation Checks:**
- Output files exist (data/step03_congruence_hce_model.txt, data/step03_congruence_hce_test.csv)
- Model converged successfully (no convergence warnings)
- All fixed effects estimated (Congruence, Time, interaction - no NaN coefficients)
- All random effects estimated (UID intercept/slope, ItemID intercept - no NaN variances)
- Variance components > 0 (all random effects variances positive)
- p_value in [0, 1] for all hypothesis tests

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "GLMM failed to converge: singular fit")
- Log failure to logs/step03_fit_glmm.log
- Quit script immediately (do NOT proceed to Step 4)
- g_debug invoked to diagnose root cause

---

#### Step 4: Post-Hoc Comparisons if Congruence Effect Significant

**Analysis Tool:** (determined by rq_tools - likely emmeans or custom contrasts)
**Validation Tool:** (determined by rq_tools - likely tools.validation.validate_contrasts_dual_pvalues per Decision D068)

**What Validation Checks:**
- Output file exists (data/step04_post_hoc_contrasts.csv)
- Decision D068 compliance: BOTH p_uncorrected AND p_bonferroni columns present
- If Congruence significant: 3 contrasts present (Incongruent vs Congruent, Incongruent vs Common, Congruent vs Common)
- If Congruence NULL: contrasts.csv empty or contains explanatory note
- Logical consistency: p_bonferroni = min(p_uncorrected x 3, 1.0) for all contrasts
- All p-values in [0, 1]

**Expected Behavior on Validation Failure:**
- Raise error with specific failure message (e.g., "Missing p_bonferroni column - Decision D068 violation")
- Log failure to logs/step04_post_hoc_contrasts.log
- Quit script immediately
- g_debug invoked to diagnose root cause

---

## Summary

**Total Steps:** 5 (Step 0: extraction + Steps 1-4: analysis)
**Estimated Runtime:** Low to Medium (~5-15 minutes total)
**Cross-RQ Dependencies:** None (RAW data only from dfData.csv)
**Primary Outputs:** HCE rates table (step02), GLMM hypothesis tests (step03), post-hoc contrasts (step04)
**Validation Coverage:** 100% (all 5 steps have validation requirements with 4-layer substance criteria)

**Key Methodological Notes:**
- Item-level analysis (no IRT aggregation) enables testing HCE dissociations directly
- GLMM with crossed random effects (UID + ItemID) accounts for non-independence
- Decision D068: Dual p-value reporting (uncorrected + Bonferroni) for transparency
- No continuous time variable (TSVR not used - nominal test sessions T1/T2/T3/T4 sufficient for interpretability)
- High-confidence threshold (0.75) corresponds to "4" on original 5-point Likert (confident but not maximally so)

---

**Next Steps (Workflow):**
1. User reviews and approves this plan (Step 7 user gate)
2. Workflow continues to Step 11: rq_tools reads this plan -> creates 3_tools.yaml
3. Workflow continues to Step 12: rq_analysis reads this plan + 3_tools.yaml -> creates 4_analysis.yaml
4. Workflow continues to Step 14: g_code reads 4_analysis.yaml -> generates stepN_name.py scripts

---

**Version History:**
- v1.0 (2025-12-06): Initial plan created by rq_planner agent for RQ 6.5.3
